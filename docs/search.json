[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Data Analysis: An Introduction (Wprowadzenie do analizy danych społecznych)",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nImportant\n\n\n\nThis is a preliminary draft of a Quarto class notes on social data analysis. Please do not cite or reproduce its contents, as it may contain errors!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "",
    "text": "1.1 What is Data Science?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#what-is-data-science",
    "href": "chapter1.html#what-is-data-science",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "",
    "text": "Important\n\n\n\nStatistics and Data Science are The Art and Science of Learning from Data.\n\n\n\nData science and statistics are powerful tools that help us understand complex phenomena across various social sciences, including political science, economics, and sociology. These complementary fields provide researchers and practitioners with the means to analyze trends, behaviors, and outcomes in society, offering insights that can shape policy and advance our understanding of human interaction.\nStatistics provides the mathematical foundation for analyzing societal trends and outcomes, offering methods for designing studies, summarizing data, and making inferences. Data science expands on this foundation by incorporating computational methods and domain expertise to handle larger datasets and perform more complex analyses.\nTogether, these disciplines allow us to collect and process large datasets, visualize complex information, uncover patterns in social interactions, evaluate policy impacts, and support evidence-based decision-making. Their applications are vast and varied, from studying voting patterns and analyzing economic indicators to researching social inequalities and examining human behavior.\nAs our world becomes increasingly data-driven, the importance of data science and statistics in social sciences continues to grow.\n\n\n\n\n\n\n\nNote\n\n\n\nIn social sciences, data science combines statistical methods, computational tools, and domain expertise to analyze complex social phenomena and human behavior.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-relationship-between-statistics-and-data-science",
    "href": "chapter1.html#the-relationship-between-statistics-and-data-science",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.2 The Relationship Between Statistics and Data Science",
    "text": "1.2 The Relationship Between Statistics and Data Science\nStatistics and data science are closely interrelated fields with significant overlap, especially in social sciences. Rather than strict divisions, it’s more accurate to view them as complementary approaches on a continuum:\n\nTraditional StatisticsModern Data ScienceEvolving Landscape\n\n\n\nRooted in mathematical theories and methods for data analysis\nEmphasizes statistical inference, hypothesis testing, and probability theory\nHistorically central to social sciences for analyzing surveys, experiments, and observational studies\n\n\n\n\nIntegrates statistical methods with computer science and domain expertise\nExpands focus to include machine learning, big data processing, and predictive modeling\nIn social sciences, often tackles large-scale digital data and complex behavioral datasets\n\n\n\n\nBoundaries between statistics and data science are increasingly blurred\nMany techniques and tools are shared across both fields\nSocial scientists often combine traditional statistical approaches with newer data science methods\nThe choice of approach depends on research questions, data characteristics, and specific analytical needs\n\n\n\n\nData science can be seen as an evolution and expansion of traditional statistics, incorporating new technologies and methodologies to handle larger and more complex social science datasets.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#essential-concepts-in-data-science-and-statistics",
    "href": "chapter1.html#essential-concepts-in-data-science-and-statistics",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.3 Essential Concepts in Data Science and Statistics",
    "text": "1.3 Essential Concepts in Data Science and Statistics\n\n1.3.1 Population, Sample and related concepts\n\n\n\n\n\n\nImportant\n\n\n\n\nData: Observations or measurements collected from a sample or population.\nPopulation: The entire set of individuals or items under study at a specific time.\n\nExample: All eligible voters in a country during a specific election year.\n\nSample: A subset of the population that is actually measured. A representative sample is a subset of a larger population that accurately reflects the characteristics of that population. The sample should mirror the population in terms of important traits like age, gender, socioeconomic status, etc. Often uses random sampling methods to avoid bias. Large enough to be statistically significant, but smaller than the full population.\n\nExample: 1500 randomly selected eligible voters surveyed in a pre-election poll.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nData Generating Process (DGP) and Superpopulation: Expanding on Traditional Concepts\nIn traditional statistics, we often work with two key concepts:\n\nPopulation: The entire group we want to study.\nSample: A subset of the population that we actually observe and analyze.\n\nWhile these concepts are fundamental, modern research often requires us to think beyond this dichotomy. This is where Data Generating Process (DGP) and superpopulation come in, extending our understanding of data and populations.\nData Generating Process (DGP):\nThe DGP is the underlying mechanism that produces the data we observe in the real world, whether in our sample or the entire population.\nIntuitive explanation: Think of the DGP as a complex system that takes various inputs and produces observable outcomes. It’s the “black box” that transforms causes into effects, not just for our sample, but for the entire population and beyond.\nExample: Consider a study on voter behavior. The traditional approach might define the population as “all registered voters” and take a sample from this group. The DGP, however, would include factors like demographic characteristics, economic conditions, political events, and media influence that shape voting behavior for all voters, sampled or not.\nSuperpopulation:\nThe superpopulation is a theoretical concept that extends beyond both the sample and the observable population to include all potential outcomes that could occur under similar conditions or processes.\nExamples:\n\nTraditional approach vs. Superpopulation approach:\n\nTraditional: population (all registered voters in a state), sample (1000 surveyed voters)\nSuperpopulation: All possible voters and voting scenarios, including future elections and hypothetical political contexts\n\nWhen sample equals population:\nIn studies of all 50 U.S. states:\n\nTraditional view: No distinction between sample and population\nSuperpopulation view: Considers these 50 states as a “sample” from a theoretical set of all possible state-policy interactions\n\n\nReal-world application: Let’s say researchers are studying the impact of a new urban planning policy across several cities:\n\nTraditional approach:\n\nPopulation: All cities in the country\nSample: The cities included in the study\n\nSuperpopulation approach:\n\nObserved data: The cities in the study\nSuperpopulation: All cities (existing or potential) where similar urban planning principles could be applied\n\n\nThe DGP in this case would be the complex set of factors that determine how urban planning policies affect city outcomes, applicable not just to the sampled cities or even all existing cities, but to the broader concept of “city” itself.\nImportant considerations:\n\nScope and limitations: Researchers should clearly define what units or processes they’re trying to understand, beyond just describing their sample and population.\nGeneralizability: When making claims about a superpopulation, researchers should explicitly state the “scope conditions” - the boundaries within which their findings are expected to hold true.\nContext-specificity: While the superpopulation concept allows for broader inferences than traditional sampling, it’s important to recognize that DGPs can vary across different contexts.\n\nBy incorporating these concepts alongside traditional population-sample thinking, researchers can make more nuanced inferences and be more transparent about the extent to which their findings might apply beyond their specific observed data, while still respecting the foundational principles of statistical inference.\nSummary example: Pizza Quality in New York City\nPopulation: All pizzerias currently operating in New York City. This is a finite, countable group of establishments that exist at the present moment.\nSample: A selection of 50 pizzerias chosen randomly from different boroughs of New York City. These are the specific pizzerias where researchers will taste and rate pizzas.\nSuperpopulation: All possible pizzerias that could exist in New York City, including:\n\nCurrent pizzerias\nFuture pizzerias that haven’t opened yet\nPizzerias that have closed down\nHypothetical pizzerias that might exist under different economic or cultural conditions\n\nThe superpopulation concept allows us to think about pizza quality beyond just the current snapshot of New York pizzerias.\nData Generating Process (DGP): The DGP is the complex set of factors that contribute to the quality of pizza in any given pizzeria. This might include:\n\nIngredients: Quality and source of flour, tomatoes, cheese, etc.\nChef’s skill: Training, experience, and personal touch of the pizza maker\nEquipment: Type and condition of the oven, tools used\nRecipe: Proportions of ingredients, preparation methods\nEnvironmental factors: Humidity, water quality in New York\nCultural influences: Local pizza-making traditions, customer preferences\nEconomic factors: Cost of ingredients, rent prices affecting business decisions\n\nThe DGP is like the “pizza quality recipe” that applies not just to our sample or even the current population, but to all potential pizzerias in the superpopulation.\nIntuitive Breakdown:\n\nIf you visit all current NYC pizzerias and rate them, you’ve assessed the population.\nIf you randomly select 50 pizzerias to visit and rate, you’ve taken a sample.\nIf you consider how pizza quality might vary in all possible NYC pizzerias (past, present, future, and hypothetical), you’re thinking about the superpopulation.\nIf you’re trying to understand all the factors that go into making a quality pizza in NYC, regardless of whether a pizzeria currently exists or not, you’re exploring the Data Generating Process.\n\n\n\n\n\n\n\n\ngraph TD\n    A[Data Generating Process DGP]\n    B(Population)\n    C[Sample]\n    A --&gt;|Generates| B\n    B --&gt;|Sampled from| C\n    C -.-&gt;|Inference| B\n    C -.-&gt;|Inference| A\n    B -.-&gt;|Inference| A\n    \n    classDef dgp fill:#f9f,stroke:#333,stroke-width:4px;\n    classDef pop fill:#bbf,stroke:#333,stroke-width:2px;\n    classDef sam fill:#bfb,stroke:#333,stroke-width:2px;\n    \n    class A dgp;\n    class B pop;\n    class C sam;\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation of the DGP, Population, and Sample Diagram\n\n\n\nThis diagram illustrates the relationships between the Data Generating Process (DGP), population, and sample, including paths of inference:\n\nDirect relationships (solid arrows):\n\nThe DGP generates the population\nSamples are drawn from the population\n\nInference paths (dashed arrows):\n\nFrom Sample to Population: Traditional statistical inference\nFrom Sample to DGP: Inferring about the underlying process from sample data\nFrom Population to DGP: Inferring about the DGP using complete population data\n\n\nFor example, in our study on the effect of electoral rules on voter turnout in Polish municipalities (1998-2010 municipal elections):\n\nWe have data for the entire population of municipalities, so we don’t need to infer from a sample to the population.\nOur focus is on using the complete population data (rightmost dashed arrow) to make inferences about the underlying DGP—the complex processes by which electoral rules influence voter turnout across municipalities.\nThis approach allows us to potentially understand the mechanisms behind how different electoral systems (e.g., proportional representation vs. plurality vote) affect turnout rates, and make informed predictions about how changes in electoral rules might impact future turnout or how these effects might generalize to similar contexts.\n\n\n\n\n\n\nPopulation vs. sample. Retrieved from: https://allmodelsarewrong.github.io/mse.html\n\n\nData forms the foundation of statistical analysis. It can be:\n\nPrimary data: Collected firsthand for a specific purpose\nSecondary data: Obtained from existing sources\n\nExample: In a study of university students’ heights, the population is all university students in the country, while a sample might be 1000 randomly selected students.\n\n\n1.3.2 Variables and Constants\nVariables are characteristics that can take different values across a dataset. They can be:\n\nQuantitative (numeric):\n\nContinuous: Height, weight, temperature\nDiscrete: Number of children, count of errors in a program\n\nQualitative (categorical):\n\nNominal: Blood type, eye color\nOrdinal: Education level, customer satisfaction rating\n\n\nConstants are fixed values that remain unchanged throughout an analysis.\n\n1.3.2.1 Types of Data in Social Sciences\nSocial science research deals with various types of data:\n\nQuantitative Data: Numerical data (e.g., survey responses, economic indicators)\nQualitative Data: Non-numerical data (e.g., interview transcripts, open-ended survey responses)\nBig Data: Large-scale digital traces (e.g., social media posts, online behavior logs)\n\n\n\n\n1.3.3 Population Parameters and Estimands\nPopulation parameters are numerical characteristics of a population. Key points:\n\nThey describe the entire population, not just a sample.\nThey are usually denoted by Greek letters.\nIn most cases, they cannot be directly calculated because we can’t measure the entire population.\nThey are determined by the underlying Data Generating Process (DGP).\n\nCommon population parameters include:\n\nPopulation mean (\\(\\mu\\)): The average value of a variable in the population.\nPopulation variance (\\(\\sigma^2\\)): A measure of variability in the population.\nPopulation proportion (\\(p\\)): The proportion of individuals in the population with a certain characteristic.\n\nAn estimand is the target of estimation - the specific population parameter or function of parameters that we aim to estimate. It defines what we want to know about the population.\n\n\n\n\n\n\nExample: Height of University Students\n\n\n\nConsider the height of all university students in a country:\n\n\\(\\mu\\) (estimand): The true average height of all university students (population mean)\n\\(\\sigma^2\\) (estimand): The true variance of heights in the population\n\nThese parameters are unknown estimands that we aim to estimate using sample data.\n\n\n\n\n1.3.4 Statistic(s) and Estimators\nA statistic (singular) or sample statistic is any quantity computed from values in a sample, which is considered for a statistical purpose.\nWhen a statistic is used for estimating an estimand (population parameter), it is called an estimator. Estimators are functions of sample data that provide approximate values for unknown population parameters.\nExamples of statistics/estimators:\n\nSample mean: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\) (estimates \\(\\mu\\))\nSample variance: \\(s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\\) (estimates \\(\\sigma^2\\))\nSample proportion: \\(\\hat{p} = \\frac{x}{n}\\) (estimates \\(p\\))\n\n\n\n1.3.5 Estimates\nAn estimate is the specific value obtained by applying an estimator to a particular sample. It is a point value that approximates the true estimand (population parameter).\nExample: If we calculate a sample mean height of 68 inches from our data, then 68 inches is our estimate of the estimand \\(\\mu\\) (population mean height).\n\n\n1.3.6 Statistical Models\n\n\n\n\n\n\nNote\n\n\n\nA model in science is a simplified representation of a complex system or phenomenon. It’s designed to help us understand, explain, and make predictions about the real world. Models can take various forms, including mathematical equations, computer simulations, or conceptual frameworks. They allow scientists to focus on key aspects of a system while ignoring less relevant details, making complex problems more manageable and easier to study.\n\n\nStatistical models represent relationships between variables and help in making predictions or inferences about estimands (population parameters).\nExample: A linear regression model \\(y = \\beta_0 + \\beta_1x + \\epsilon\\) describes the relationship between an independent variable \\(x\\) and a dependent variable \\(y\\), where:\n\n\\(y\\) is the dependent variable (e.g. quantity demanded)\n\\(x\\) is the independent variable (e.g. price, income level of the consumer)\n\\(\\beta_0\\) and \\(\\beta_1\\) are parameters, estimands to be estimated\n\\(\\epsilon\\) is the error term, representing unexplained variation\n\n\n\n1.3.7 Inference\nStatistical inference is the process of drawing conclusions about estimands (population parameters) based on sample data. It involves two main types:\n\nEstimation: Using sample statistics (estimators) to estimate estimands (population parameters)\nHypothesis testing: Making decisions about estimands based on sample evidence\n\n\n\n\n\n\n\nEstimation and Hypothesis Testing\n\n\n\n\nEstimation\n\nEstimation is about determining the likely value of a population parameter based on sample data. In the context of a binomial distribution, we might be interested in estimating the probability of success (p) for a certain event.\nExample: Coin Flipping\nLet’s say we’re flipping a coin 100 times and want to estimate the probability of getting heads.\n\nWe flip the coin 100 times and observe 55 heads.\nOur point estimate for p (probability of heads) would be 55/100 = 0.55\nWe might also calculate a confidence interval, e.g., a 95% confidence interval might be (0.45, 0.65).\n\nThe confidence interval tells us a range where we think the true probability might lie. In plain English, this means: “We’re 95% confident that the true probability of getting heads is between 45% and 65%.”\nThe goal here is to provide our best guess of the true probability of heads, along with a range of plausible values.\nImportant Concepts in Estimation:\n\nBias\n\nBias refers to the tendency of an estimator to systematically overestimate or underestimate the true value of a population parameter (estimand).\n\nAn unbiased estimator is one whose average value (when estimation is repeated multiple times) equals the true value of the parameter.\nBias can be understood as the difference between the average value of the estimator and the true value of the parameter.\n\n\nEfficiency\n\nEfficiency refers to the precision of an estimator. A more efficient estimator produces results closer to the true parameter value, i.e., it has less dispersion in its results.\n\nIt is most often measured by the variance of the estimator (lower variance means higher efficiency)\nFor unbiased estimators, efficiency is often compared using Mean Squared Error (MSE)\n\n\nHypothesis Testing\n\nHypothesis testing, on the other hand, is about making a decision between two competing claims about a population parameter. We typically have a null hypothesis (H0) and an alternative hypothesis (H1).\nExample: Is the Coin Fair?\nUsing the same coin-flipping scenario, let’s say we want to test if the coin is fair (p = 0.5) or biased towards heads (p &gt; 0.5).\n\nNull hypothesis (H0): p = 0.5 (the coin is fair)\nAlternative hypothesis (H1): p &gt; 0.5 (the coin is biased towards heads)\nWe observe 55 heads out of 100 flips\n\nIntroducing p-values and “Probabilistic Proof by Contradiction”\nNow, let’s dive into the concept of p-values and how hypothesis testing works as a kind of “probabilistic proof by contradiction”:\n\nWe start by assuming the null hypothesis (H0) is true. In this case, we assume the coin is fair.\nWe then ask: “If the coin were truly fair, how likely would it be to observe 55 or more heads out of 100 flips?”\nThis probability is called the p-value. It’s the probability of observing our data (or more extreme data) assuming the null hypothesis is true.\nIf this probability (the p-value) is very small, we have a contradiction: we’ve observed something that should be very rare if our assumption (H0) were true.\nWe typically set a threshold called the significance level (often 0.05 or 5%) for what we consider “very small.”\nIf the p-value is less than our chosen significance level, we reject H0. We conclude that our observation is too unlikely under H0, so we favor the alternative hypothesis instead.\nIf the p-value is greater than our significance level, we fail to reject H0. We don’t have enough evidence to conclude the coin is biased.\n\nThis process is like a “probabilistic proof by contradiction” because:\n\nWe start by assuming H0 (like assuming the opposite of what we want to prove in a proof by contradiction).\nWe see if this assumption leads to a very unlikely situation (our observed data).\nIf it does, we reject the assumption (H0) and favor the alternative.\n\nThe p-value quantifies exactly how unlikely our observation is under H0. A very small p-value (like 0.01) means: “If H0 were true, we’d only expect to see data this extreme about 1% of the time.”\nHypothesis testing and estimation are related but distinct statistical procedures; hypothesis testing can be used to make inferences about estimates and can complement estimation in several ways, e.g.:\n\nTesting Point Estimates: Hypothesis testing can be used to evaluate whether a point estimate is significantly different from a hypothesized value. For example, if we estimate that a coin has a 0.55 probability of landing heads, we could use a hypothesis test to determine if this is significantly different from 0.5 (a fair coin).\nParameter Significance: In multivariate models, hypothesis tests (like t-tests in regression) can help determine which estimated parameters are significantly different from zero, providing insight into which variables are important in the model.\n\n\n\n\n\n1.3.8 Relationships Between Concepts\n\nThe Data Generating Process (DGP) determines the actual values of population parameters (estimands).\nEstimands are estimated using statistics calculated from the sample (estimators).\nThe quality of estimators is assessed based on properties such as bias and efficiency in estimating the estimand.\nStatistical models use estimated parameters to describe relationships between variables in the population.\nStatistical inference involves drawing conclusions about estimands based on sample data, utilizing the properties of estimators.\n\n\n\n\n\n\n\nExample: Studying Voting Behavior\n\n\n\n\nPopulation: All eligible voters in a country\nEstimand: \\(p\\) = true proportion of voters supporting a given candidate\nSample: 1000 randomly selected eligible voters\nEstimator: \\(\\hat{p}\\) = proportion of voters in the sample supporting the candidate\nEstimate: Specific value of \\(\\hat{p}\\) calculated from the sample (e.g., 0.52)\nDGP: Complex interaction of factors influencing voting decisions, such as political beliefs, economic conditions, media exposure, and social networks.\n\nUnderstanding the DGP helps researchers interpret why the estimand \\(p\\) has a certain value and how it might change over time. For example, a sudden change in the economy might affect voters’ preferences, thereby changing the value of \\(p\\).\nBias and efficiency in the context of the example:\n\nIf \\(\\hat{p}\\) is an unbiased estimator, it means that when the survey is repeated multiple times with different samples, the average value of \\(\\hat{p}\\) will be close to the true value of \\(p\\).\nThe efficiency of \\(\\hat{p}\\) determines how dispersed the results of individual surveys are around this average. The less dispersion, the more efficient the estimator.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#core-components-of-data-science-in-scientific-research",
    "href": "chapter1.html#core-components-of-data-science-in-scientific-research",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.4 Core Components of Data Science in Scientific Research",
    "text": "1.4 Core Components of Data Science in Scientific Research\n\nData CollectionData ProcessingExploratory Data Analysis (EDA)Statistical InferenceMachine LearningData Visualization and CommunicationReproducibility and Open Science\n\n\n\nExperimental methods: Controlled studies where researchers manipulate variables to observe effects\nObservational studies: Gathering data by watching and recording without interfering\nSurveys and interviews: Collecting information directly from people through questions\nDigital data collection: Gathering data from online sources, sensors, or computer systems\nEthical considerations: Ensuring research respects participants’ rights and well-being\n\n\n\n\nData cleaning: Removing errors and inconsistencies from raw data\nHandling missing values: Addressing gaps in the dataset that could affect analysis\nData transformation: Converting data into formats suitable for analysis, like changing text to numbers\n\n\n\n\nDescriptive statistics: Summarizing data with measures like mean, median, and standard deviation\nData visualization: Creating graphs and charts to visually represent data patterns\nPattern identification: Discovering trends or relationships in the data\n\n\n\n\nHypothesis testing: Using data to evaluate claims about populations\nRegression analysis: Examining relationships between variables and making predictions\nCausal inference: Determining if one variable directly influences another\n\n\n\n\nSupervised learning: Training models to predict outcomes using data with known answers\nUnsupervised learning: Finding hidden patterns in data without predefined categories\nNatural Language Processing (NLP): Teaching computers to understand and analyze human language\n\n\n\n\nEffective visualizations: Creating clear, informative graphics to represent complex data\nScience communication: Explaining findings to different audiences, from experts to the public\nScientific writing: Preparing research papers and reports to share results\n\n\n\n\nVersion control: Tracking changes in data and code throughout the research process\nOpen data practices: Sharing research data and methods for verification and further study\nReproducible workflows: Documenting research steps so others can repeat the study",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#tools-for-data-science-in-social-sciences",
    "href": "chapter1.html#tools-for-data-science-in-social-sciences",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.5 Tools for Data Science in Social Sciences",
    "text": "1.5 Tools for Data Science in Social Sciences\nIn this course, we’ll use R for our data analysis, as it’s widely used in social science research.\n\n1.5.1 R for Social Science Data Analysis\nR offers powerful capabilities for social science research, from data manipulation to advanced statistical modeling.\n\n\nKliknij, aby pokazać/ukryć kod R\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nKliknij, aby pokazać/ukryć kod R\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate example data with a Simpson's Paradox\nn &lt;- 1000\ndata &lt;- tibble(\n  age_group = sample(c(\"Young\", \"Middle\", \"Old\"), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),\n  education_years = case_when(\n    age_group == \"Young\" ~ rnorm(n, mean = 10, sd = 1),\n    age_group == \"Middle\" ~ rnorm(n, mean = 13, sd = 1),\n    age_group == \"Old\" ~ rnorm(n, mean = 16, sd = 1)\n  ),\n  income = case_when(\n    age_group == \"Young\" ~ 70000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Middle\" ~ 50000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Old\" ~ 30000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000)\n  )\n)\n\n# Basic data summary\nsummary(data)\n\n\n  age_group         education_years      income     \n Length:1000        Min.   : 6.628   Min.   :34068  \n Class :character   1st Qu.:10.913   1st Qu.:51508  \n Mode  :character   Median :13.004   Median :63376  \n                    Mean   :12.986   Mean   :63307  \n                    3rd Qu.:14.934   3rd Qu.:75023  \n                    Max.   :18.861   Max.   :96620  \n\n\nKliknij, aby pokazać/ukryć kod R\n# Correlation analysis\ncor(data %&gt;% select(education_years, income))\n\n\n                education_years     income\neducation_years       1.0000000 -0.8152477\nincome               -0.8152477  1.0000000\n\n\nKliknij, aby pokazać/ukryć kod R\n# Overall trend (Simpson's Paradox)\noverall_plot &lt;- ggplot(data, aes(x = education_years, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Overall Relationship between Education and Income\",\n       subtitle = \"Simpson's Paradox: Appears negative\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Trend by age group (Resolving Simpson's Paradox)\ngrouped_plot &lt;- ggplot(data, aes(x = education_years, y = income, color = age_group)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Relationship between Education and Income by Age Group\",\n       subtitle = \"Resolving Simpson's Paradox: Positive relationship within groups\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Statistical analysis\nmodel_overall &lt;- lm(income ~ education_years, data = data)\nmodel_by_age &lt;- lm(income ~ education_years + age_group, data = data)\n\n# Print results\nprint(overall_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(grouped_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_overall))\n\n\n\nCall:\nlm(formula = income ~ education_years, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24451  -5439    235   5262  34328 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     121814.7     1339.5   90.94   &lt;2e-16 ***\neducation_years  -4505.4      101.3  -44.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7976 on 998 degrees of freedom\nMultiple R-squared:  0.6646,    Adjusted R-squared:  0.6643 \nF-statistic:  1978 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_by_age))\n\n\n\nCall:\nlm(formula = income ~ education_years + age_group, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-14827  -3369    118   3356  16388 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      48270.8     2028.4  23.797  &lt; 2e-16 ***\neducation_years   1135.5      154.6   7.345 4.26e-13 ***\nage_groupOld    -19942.8      593.2 -33.619  &lt; 2e-16 ***\nage_groupYoung   20461.1      600.7  34.064  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4950 on 996 degrees of freedom\nMultiple R-squared:  0.8711,    Adjusted R-squared:  0.8707 \nF-statistic:  2244 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\n# Calculate and print correlations\noverall_cor &lt;- cor(data$education_years, data$income)\ngroup_cors &lt;- data %&gt;%\n  group_by(age_group) %&gt;%\n  summarize(correlation = cor(education_years, income))\n\nprint(\"Overall correlation:\")\n\n\n[1] \"Overall correlation:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(overall_cor)\n\n\n[1] -0.8152477\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(\"Correlations by age group:\")\n\n\n[1] \"Correlations by age group:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(group_cors)\n\n\n# A tibble: 3 × 2\n  age_group correlation\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Middle          0.185\n2 Old             0.291\n3 Young           0.223\n\n\nThis example demonstrates basic data manipulation, summary statistics, and visualization using R, which are common tasks in social science research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#causal-inference-vs.-observational-studies",
    "href": "chapter1.html#causal-inference-vs.-observational-studies",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.6 Causal Inference vs. Observational Studies",
    "text": "1.6 Causal Inference vs. Observational Studies\nIn social sciences and beyond, understanding the relationship between variables is crucial. Two key approaches to this are causal inference and observational studies, each with its own strengths and limitations.\n\nCausal InferenceObservational StudiesKey Distinction: Correlation vs. Causation\n\n\n\nAims to establish cause-and-effect relationships\nOften involves experimental designs or advanced statistical techniques\nSeeks to answer “What if?” questions and determine the impact of interventions\nExamples: Randomized controlled trials, quasi-experimental designs, instrumental variables\n\n\n\n\nExamine relationships between variables without direct intervention\nRely on data collected from natural settings or existing datasets\nCan identify correlations and patterns but struggle to establish causation\nExamples: Cohort studies, case-control studies, cross-sectional surveys\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember: Correlation Does Not Imply Causation\n\n\n\nA fundamental principle in research is that correlation between two variables does not necessarily imply a causal relationship. This concept is crucial when interpreting results from observational studies.\n\nCorrelation: Measures the strength and direction of a relationship between variables\nCausation: Indicates that changes in one variable directly cause changes in another\n\nWhile strong correlations can suggest potential causal links, additional evidence and rigorous methods are required to establish causality.\n\n\n\nChallenges in Establishing CausalityMethods to Strengthen Causal ClaimsImportance in Social Sciences\n\n\n\nConfounding variables: Unmeasured factors that affect both the presumed cause and effect\nReverse causality: The presumed effect might actually be causing the presumed cause\nSelection bias: Non-random selection of subjects into study groups\n\n\n\n\nRandomized controlled trials (when ethical and feasible)\nNatural experiments or quasi-experimental designs\nPropensity score matching\nDifference-in-differences analysis\nInstrumental variable approaches\nDirected acyclic graphs (DAGs) for visualizing causal relationships\n\n\n\nUnderstanding the distinction between causal inference and observational studies is crucial in social sciences, where ethical considerations often limit experimental manipulation. Researchers must carefully design studies and interpret results to avoid misleading conclusions about causality.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#models-in-science-from-deterministic-to-stochastic",
    "href": "chapter1.html#models-in-science-from-deterministic-to-stochastic",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.7 Models in Science: From Deterministic to Stochastic (*)",
    "text": "1.7 Models in Science: From Deterministic to Stochastic (*)\nModels are essential tools in scientific research, helping scientists to represent, understand, and predict complex phenomena. This section explores the main types of models used in science, along with examples of their applications. It’s important to note that these categories often overlap, and many scientific models incorporate multiple aspects.\n\n1.7.1 Mathematical Models\nMathematical models use equations and mathematical concepts to describe and analyze systems or phenomena. They can be further divided into several subcategories, though it’s important to note that some complex models may incorporate elements from multiple categories:\n\n1.7.1.1 a. Deterministic Models\nDeterministic models provide precise predictions based on a set of variables, without incorporating randomness at the macroscopic level.\nExample: Newton’s laws of motion, which can precisely predict the motion of objects under known forces in classical mechanics.\n\n\n1.7.1.2 b. Stochastic Models\nStochastic models incorporate randomness and probability. However, it’s crucial to distinguish between two fundamentally different types of stochastic models:\n\n1.7.1.2.1 i. Classical Stochastic Models\nThese models deal with randomness arising from incomplete information or complex interactions in classical systems. The underlying system is deterministic, but practical limitations in measurement or computation lead to the use of probabilistic descriptions.\nExample: Regression models in statistics, where the randomness represents unexplained variation or measurement error:\n\\[y = β_0 + β_1x + ε\\]\nWhere:\n\n\\(y\\) is the dependent variable (e.g. quantity demanded)\n\\(x\\) is the independent variable (e.g. price, income level of the consumer)\n\\(β_0\\) and \\(β_1\\) are parameters\n\\(ε\\) is the error term, representing unexplained variation\n\n\n\n1.7.1.2.2 ii. Quantum Stochastic Models\nThese models deal with the fundamental, irreducible randomness inherent in quantum mechanical systems. This randomness is not due to lack of information, but is a core feature of quantum reality.\nExample: The Standard Model in particle physics, which describes particle interactions using quantum field theory. For instance, the decay of a particle is inherently probabilistic:\n\\[P(t) = e^{-t/τ}\\]\nWhere:\n\n\\(P(t)\\) is the probability that the particle has not decayed after time t\n\\(τ\\) is the mean lifetime of the particle\n\n\n\n\n1.7.1.3 c. Computer Simulation Models\nComputer simulations use algorithms and computational methods based on mathematical models to simulate complex systems and predict their behavior over time. These can be deterministic or stochastic.\nExample: Climate models that simulate the Earth’s climate system, incorporating factors such as atmospheric composition, ocean currents, and solar radiation to project future climate scenarios.\n\n\n\n1.7.2 Conceptual Models\nConceptual models are abstract representations of systems or processes, often using diagrams or flowcharts to illustrate relationships between components.\nExample: The water cycle model in Earth sciences, which illustrates the continuous movement of water within the Earth and atmosphere through processes such as evaporation, precipitation, and runoff.\n\n\n1.7.3 Physical Models\nPhysical models are tangible representations of objects or systems, often scaled down or simplified versions of the real thing.\nExample: Wind tunnel models in aerodynamics research, used to study the effects of air moving past solid objects and optimize designs for aircraft, vehicles, or buildings.\n\n\n1.7.4 Theoretical Models\nTheoretical models are abstract frameworks based on fundamental principles and hypotheses, often used to explain observed phenomena or predict new ones. These models frequently employ mathematical formulations and can be deterministic or stochastic in nature.\nExample: The theory of evolution by natural selection, which provides a framework for understanding the diversity and adaptation of life forms over time.\n\n\n1.7.5 Conclusion\nThese various forms of models play crucial roles in scientific research, each offering unique advantages for understanding and predicting natural phenomena. Scientists often use multiple types of models in conjunction to gain comprehensive insights into complex systems and processes.\nIt’s important to recognize that these categories are not mutually exclusive and often overlap:\n\nMathematical models form the foundation for many other types of models, including computer simulations and some theoretical models.\nComputer simulation models are essentially mathematical models implemented through computational methods, and can be either deterministic or stochastic.\nTheoretical models often employ mathematical formulations and may be implemented as computer simulations.\nPhysical models may be designed based on mathematical models and can be used to validate computer simulations.\n\nThe choice of model type often depends on the specific research question, the nature of the system being studied, the available data, and the computational resources at hand. As science progresses, the boundaries between these model types continue to blur, leading to increasingly sophisticated and interdisciplinary approaches to modeling complex phenomena.\nIt’s crucial to distinguish between different types of stochastic models. Classical stochastic models, such as those used in regression analysis, deal with randomness arising from incomplete information or complex interactions in otherwise deterministic systems. In contrast, quantum stochastic models, like those in particle physics, deal with fundamental, irreducible randomness inherent in quantum mechanical systems. This distinction reflects the profound differences between classical and quantum paradigms in physics and highlights the diverse ways in which probability is used in scientific modeling.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#understanding-spurious-correlations-confounders-and-colliders",
    "href": "chapter1.html#understanding-spurious-correlations-confounders-and-colliders",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.8 Understanding Spurious Correlations, Confounders, and Colliders (*)",
    "text": "1.8 Understanding Spurious Correlations, Confounders, and Colliders (*)\nIn this tutorial, we’ll explore three important concepts in statistical analysis: spurious correlations, confounders, and colliders. Understanding these concepts is crucial for avoiding misinterpretation of data and drawing incorrect conclusions from statistical analyses.\nLet’s start by loading the necessary libraries:\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nset.seed(123) # for reproducibility\n\n\n1.8.1 Spurious Correlations\nSpurious correlations are relationships between variables that appear to be causal but are actually coincidental or caused by an unseen third factor.\n\n\n1.8.2 Example: Ice Cream Sales and Drowning Incidents\nLet’s create a dataset that shows a spurious correlation between ice cream sales and drowning incidents:\n\nn &lt;- 100\nspurious_data &lt;- tibble(\n  temperature = rnorm(n, mean = 25, sd = 5),\n  ice_cream_sales = 100 + 5 * temperature + rnorm(n, sd = 10),\n  drowning_incidents = 1 + 0.5 * temperature + rnorm(n, sd = 2)\n)\n\nggplot(spurious_data, aes(x = ice_cream_sales, y = drowning_incidents)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Spurious Correlation: Ice Cream Sales vs. Drowning Incidents\",\n       x = \"Ice Cream Sales\", y = \"Drowning Incidents\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis plot shows a positive correlation between ice cream sales and drowning incidents. However, this relationship is spurious. The real cause for both is the temperature:\n\nggplot(spurious_data, aes(x = temperature)) +\n  geom_point(aes(y = ice_cream_sales), color = \"blue\") +\n  geom_point(aes(y = drowning_incidents * 10), color = \"red\") +\n  geom_smooth(aes(y = ice_cream_sales), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(aes(y = drowning_incidents * 10), method = \"lm\", se = FALSE, color = \"red\") +\n  scale_y_continuous(\n    name = \"Ice Cream Sales\",\n    sec.axis = sec_axis(~./10, name = \"Drowning Incidents\")\n  ) +\n  labs(title = \"Temperature as the Common Cause\",\n       x = \"Temperature\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n1.8.3 Confounders\nA confounder is a variable that influences both the dependent variable and independent variable, causing a spurious association.\n\n\n1.8.4 Example: Education, Income, and Age\n\nlibrary(tidyverse)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nn &lt;- 1000\nconfounder_data &lt;- tibble(\n  age = runif(n, 25, 65),\n  education = round(10 + 0.1 * age + rnorm(n, sd = 2)),\n  income = 20000 + 1000 * education + 500 * age + rnorm(n, sd = 5000)\n)\n\n# Without controlling for age\nmodel_naive &lt;- lm(income ~ education, data = confounder_data)\n# Controlling for age\nmodel_adjusted &lt;- lm(income ~ education + age, data = confounder_data)\n\n# Create age groups for visualization\nconfounder_data &lt;- confounder_data %&gt;%\n  mutate(age_group = cut(age, breaks = 3, labels = c(\"Young\", \"Middle\", \"Old\")))\n\n# Visualize\nggplot(confounder_data, aes(x = education, y = income)) +\n  geom_point(aes(color = age), alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1.2) +\n  geom_smooth(aes(group = age_group, color = as.numeric(age_group)), \n              method = \"lm\", se = FALSE, linewidth = 1) +\n  scale_color_viridis_c(name = \"Age\", \n                        breaks = c(30, 45, 60), \n                        labels = c(\"Young\", \"Middle\", \"Old\")) +\n  labs(title = \"Education vs Income, Confounded by Age\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCompare the coefficients:\n\nsummary(model_naive)$coefficients[\"education\", \"Estimate\"]\n\n[1] 2328.718\n\nsummary(model_adjusted)$coefficients[\"education\", \"Estimate\"]\n\n[1] 1101.783\n\n\nThe effect of education on income is overestimated when we don’t control for age.\n\n\n1.8.5 Colliders\nA collider is a variable that is influenced by both the independent variable and the dependent variable. Controlling for a collider can introduce a spurious correlation.\n\n\n1.8.6 Example: Job Satisfaction, Salary, and Work-Life Balance\nLet’s create a dataset where work-life balance is a collider between job satisfaction and salary:\n\nn &lt;- 1000\ncollider_data &lt;- tibble(\n  job_satisfaction = rnorm(n),\n  salary = rnorm(n),\n  work_life_balance = -0.5 * job_satisfaction - 0.5 * salary + rnorm(n, sd = 0.5)\n)\n\n# Without controlling for work-life balance\nmodel_correct &lt;- lm(salary ~ job_satisfaction, data = collider_data)\n\n# Incorrectly controlling for work-life balance\nmodel_collider &lt;- lm(salary ~ job_satisfaction + work_life_balance, data = collider_data)\n\n# Visualize\nggplot(collider_data, aes(x = job_satisfaction, y = salary, color = work_life_balance)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_color_viridis_c() +\n  labs(title = \"Job Satisfaction vs Salary, Work-Life Balance as Collider\",\n       x = \"Job Satisfaction\", y = \"Salary\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCompare the coefficients:\n\nsummary(model_correct)$coefficients[\"job_satisfaction\", \"Estimate\"]\n\n[1] 0.02063487\n\nsummary(model_collider)$coefficients[\"job_satisfaction\", \"Estimate\"]\n\n[1] -0.4794016\n\n\nControlling for the collider (work-life balance) introduces a spurious correlation between job satisfaction and salary.\n\n\n1.8.7 Conclusion\nUnderstanding spurious correlations, confounders, and colliders is crucial for proper statistical analysis and causal inference. Always consider the underlying causal structure of your data and be cautious about which variables you control for in your analyses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#further-reading",
    "href": "chapter1.html#further-reading",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.9 Further Reading",
    "text": "1.9 Further Reading\n\nPearl, J. (2009). Causality: Models, Reasoning, and Inference. Cambridge University Press.\nHernán, M. A., & Robins, J. M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#ethical-considerations-in-social-science-data-analysis",
    "href": "chapter1.html#ethical-considerations-in-social-science-data-analysis",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.10 Ethical Considerations in Social Science Data Analysis",
    "text": "1.10 Ethical Considerations in Social Science Data Analysis\nEthics play a crucial role in social science research:\n\nPrivacy and Consent: Ensuring participant privacy and informed consent\nData Protection: Securely storing and managing sensitive personal data\nBias and Representation: Addressing sampling bias and ensuring diverse representation\nTransparency: Clearly communicating research methods and limitations\nSocial Impact: Considering the potential societal implications of research findings\n\n\n\n\n\n\n\nImportant\n\n\n\nSocial scientists must carefully consider the ethical implications of their data collection, analysis, and dissemination practices.\n\n\n\n1.10.1 Key Takeaways\n\nData science in social sciences builds upon traditional statistical methods, incorporating new technologies to analyze complex social phenomena.\nUnderstanding concepts like population, sample, and data generating processes is crucial for valid social science research.\nThe data science process in social research involves multiple steps from ethical data collection to the communication of insights.\nR is a powerful tool for social science data analysis, offering a wide range of capabilities.\nEthical considerations should be at the forefront of any social science data project.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-a-classical-vs-quantum-randomness-understanding-the-fundamental-differences",
    "href": "chapter1.html#appendix-a-classical-vs-quantum-randomness-understanding-the-fundamental-differences",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.11 Appendix A: Classical vs Quantum Randomness: Understanding the Fundamental Differences",
    "text": "1.11 Appendix A: Classical vs Quantum Randomness: Understanding the Fundamental Differences\nTo understand how the randomness in quantum mechanics differs from the randomness represented by the error term in regression models, we need to examine their origins, nature, and implications.\n\n1.11.1 Origin of Randomness\n\n1.11.1.1 Classical Randomness (Regression Models)\n\nSource: Incomplete information or complex interactions in an otherwise deterministic system.\nNature: Epistemic uncertainty (due to lack of knowledge).\nExample: In a regression model, \\(y = β_0 + β_1x + ε\\), the error term ε represents unexplained variation.\n\n\n\n1.11.1.2 Quantum Randomness\n\nSource: Fundamental property of quantum systems.\nNature: Ontic uncertainty (inherent to the system, not due to lack of knowledge).\nExample: The exact time of decay of a radioactive atom cannot be predicted, only its probability.\n\n\n\n\n1.11.2 Philosophical Implications\n\n1.11.2.1 Classical Randomness\n\nDeterminism: Underlying reality is deterministic; randomness reflects our ignorance.\nHidden Variables: In principle, if we had complete information, we could predict outcomes precisely.\n\n\n\n1.11.2.2 Quantum Randomness\n\nIndeterminism: Randomness is a fundamental feature of reality, not just our description of it.\nNo Hidden Variables: Even with complete information about a quantum system, some outcomes remain unpredictable (as suggested by Bell’s theorem).\n\n\n\n\n1.11.3 Mathematical Treatment\n\n1.11.3.1 Classical Randomness\n\nProbability Theory: Based on classical probability theory.\nDistribution: Often assumed to follow known distributions (e.g., normal distribution in many regression models).\nCentral Limit Theorem: Applies to large samples of random variables.\n\n\n\n1.11.3.2 Quantum Randomness\n\nQuantum Probability: Based on the mathematical framework of quantum mechanics.\nWave Function: Describes the quantum state and its evolution.\nBorn Rule: Gives probabilities of measurement outcomes from the wave function.\n\n\n\n\n1.11.4 Predictability and Control\n\n1.11.4.1 Classical Randomness\n\nReducible: In principle, can be reduced by gathering more data or improving measurement precision.\nControllable: Systematic errors can be identified and corrected.\n\n\n\n1.11.4.2 Quantum Randomness\n\nIrreducible: Cannot be eliminated even with perfect measurements.\nFundamentally Uncontrollable: The act of measurement itself affects the system (measurement problem).\n\n\n\n\n1.11.5 Practical Implications\n\n1.11.5.1 Classical Randomness\n\nError Reduction: Focus on improving measurement techniques and data collection.\nModel Refinement: Aim to explain more variance and reduce the error term.\n\n\n\n1.11.5.2 Quantum Randomness\n\nInherent Limitation: Accept fundamental limits on predictability.\nProbabilistic Predictions: Focus on accurate probability distributions rather than exact outcomes.\n\n\n\n\n1.11.6 Examples to Understand the Difference\n\n1.11.6.1 Classical Randomness Example\nImagine flipping a coin. Classical physics says the outcome is determined by initial conditions (force applied, air resistance, etc.). The “randomness” comes from our inability to precisely measure and account for all these factors.\n\n\n1.11.6.2 Quantum Randomness Example\nIn the double-slit experiment, individual particles show interference patterns as if they went through both slits simultaneously. The exact path of any individual particle is fundamentally undetermined until measured, and this indeterminacy cannot be resolved by more precise measurements.\n\n\n\n1.11.7 Conclusion\nWhile both types of randomness lead to probabilistic predictions, their fundamental natures are quite different:\n\nClassical randomness in regression models is a reflection of our incomplete knowledge or measurement limitations in an otherwise deterministic system.\nQuantum randomness is a fundamental property of quantum systems, representing an inherent indeterminacy in nature that persists even with perfect knowledge and measurement.\n\nUnderstanding these differences is crucial for correctly interpreting and applying statistical models in different scientific contexts, from social sciences using regression analysis to quantum physics experiments.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-b-large-language-models---understanding-their-stochastic-nature",
    "href": "chapter1.html#appendix-b-large-language-models---understanding-their-stochastic-nature",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.12 Appendix B: Large Language Models - Understanding Their Stochastic Nature",
    "text": "1.12 Appendix B: Large Language Models - Understanding Their Stochastic Nature\nLarge Language Models (LLMs) like GPT-3, BERT, and Claude have revolutionized natural language processing but can make puzzling mistakes, especially in mathematical tasks. This appendix explains LLMs’ functioning, stochastic nature, and compares them to classical statistical models.\n\n1.12.1 LLM Basics and Stochastic Nature\nLLMs are trained on vast text data to predict the probability distribution of the next token in a sequence. They use transformer architectures for processing and generating text. Key aspects of their stochastic nature include:\n\nProbabilistic token selection: LLMs choose each word based on calculated probabilities, not fixed rules.\nTemperature-controlled randomness: A “temperature” parameter adjusts the randomness of selections, balancing creativity and coherence.\nNon-deterministic outputs: The same input can produce different outputs in separate runs.\nContextual ambiguity: LLMs interpret context probabilistically, sometimes leading to misunderstandings.\n\n\n\n1.12.2 Comparison to Classical Statistical Models\nTo understand LLMs better, let’s compare them to Ordinary Least Squares (OLS) regression:\n\n\n\n\n\n\n\n\nAspect\nOLS Regression\nLarge Language Models\n\n\n\n\nBasic Function\nPredicts continuous outcomes based on input variables\nPredicts probability distribution of next token based on previous tokens\n\n\nInput-Output\nContinuous variables, linear relationships\nDiscrete tokens, non-linear relationships\n\n\nPrediction Type\nPoint predictions with confidence intervals\nProbability distributions over possible tokens\n\n\nModel Complexity\nFew parameters\nBillions of parameters\n\n\nInterpretability\nClear coefficient interpretations\nLargely opaque internal workings\n\n\nNoise Handling\nAssumes random noise in outcome variable\nDeals with natural language variability\n\n\nExtrapolation\nLess reliable outside training range\nLess reliable on unfamiliar topics\n\n\n\nBoth models aim to learn input-output mappings based on training data patterns.\n\n\n1.12.3 Implications for Mathematical Tasks\nLLMs’ stochastic nature affects mathematical operations:\n\nVariable outputs for repeated calculations: Each attempt might yield a different result due to probabilistic token selection.\nConfidence doesn’t guarantee correctness: High model confidence can occur even for incorrect answers.\nApproximation rather than exact computation: LLMs pattern-match rather than perform precise calculations.\n\nLimitations in mathematical tasks stem from:\n\nTraining objective mismatch: LLMs are trained for language prediction, not mathematical accuracy.\nLack of explicit mathematical reasoning: They don’t have built-in mathematical rules or operations.\nAbsence of working memory: LLMs can’t reliably store and manipulate intermediate results.\nLimited context window: They may lose track of relevant information in long problems.\nTraining data limitations: Underrepresentation of certain math concepts can lead to poor performance.\nLack of consistency checks: LLMs don’t verify the logical consistency of their outputs.\n\n\n\n1.12.4 Best Practices and Conclusion\nWhen using LLMs for mathematical tasks:\n\nFocus on conceptual explanations, not precise calculations: LLMs excel at explaining concepts but may falter on exact computations.\nVerify results with dedicated software: Always double-check LLM calculations with proper math tools.\nBreak down complex problems: Splitting tasks into smaller steps can improve LLM performance.\nBe aware of rephrasing effects: Different phrasings of the same problem may yield different results.\nUse as assistive tools, not replacements for expertise: LLMs should complement, not substitute, mathematical expertise.\n\nUnderstanding LLMs’ probabilistic nature helps leverage their strengths in language tasks while recognizing their limitations in domains requiring deterministic precision, like mathematics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-c-deterministic-and-stochastic-models",
    "href": "chapter1.html#appendix-c-deterministic-and-stochastic-models",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.13 Appendix C: Deterministic and Stochastic Models (*)",
    "text": "1.13 Appendix C: Deterministic and Stochastic Models (*)\n\n1.13.1 Deterministic Models\nDeterministic models are those where the output is fully determined by the parameter values and the initial conditions. These models are often used in physics and engineering.\n\n\n1.13.2 Example: Uniformly Accelerated Motion\nA classic example of a deterministic model is uniformly accelerated motion, described by the equation:\n\\[x(t) = x_0 + v_0t + \\frac{1}{2}at^2\\]\nWhere:\n\n\\(x(t)\\) is the position at time \\(t\\)\n\\(x_0\\) is the initial position\n\\(v_0\\) is the initial velocity\n\\(a\\) is the acceleration\n\\(t\\) is time\n\nLet’s simulate this in R:\n\n# Uniformly accelerated motion\nsimulate_accelerated_motion &lt;- function(x0, v0, a, t) {\n  x0 + v0 * t + 0.5 * a * t^2\n}\n\n# Generating data\nt &lt;- seq(0, 10, by = 0.1)\nx &lt;- simulate_accelerated_motion(x0 = 0, v0 = 2, a = 1, t = t)\n\n# Plot\nplot(t, x, type = \"l\", xlab = \"Time\", ylab = \"Position\", \n     main = \"Uniformly Accelerated Motion\")\n\n\n\n\n\n\n\n\nThis code will generate a plot of uniformly accelerated motion, which is an intuitive example from Newtonian dynamics. In this case, an object starts moving with an initial velocity and accelerates uniformly, resulting in a parabolic trajectory on the position-time graph.\n\n\n1.13.3 Stochastic Models in Social Sciences\nStochastic models incorporate randomness and are often used in social sciences where there’s inherent uncertainty in the systems being studied.\n\n\n1.13.4 Example: Ordinary Least Squares (OLS) Regression\nOLS is a fundamental stochastic model in social sciences. It’s represented as:\n\\[Y = \\beta_0 + \\beta_1X + \\epsilon\\]\nWhere:\n\n\\(Y\\) is the dependent variable\n\\(X\\) is the independent variable\n\\(\\beta_0\\) and \\(\\beta_1\\) are parameters\n\\(\\epsilon\\) is the error term (stochastic component)\n\nLet’s demonstrate OLS in R:\n\n# Generate some sample data\nset.seed(123)\nX &lt;- rnorm(100)\nY &lt;- 2 + 3*X + rnorm(100, sd = 0.5)\n\n# Fit OLS model\nmodel &lt;- lm(Y ~ X)\n\n# Summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95367 -0.34175 -0.04375  0.29032  1.64520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.94860    0.04878   39.95   &lt;2e-16 ***\nX            2.97376    0.05344   55.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4854 on 98 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.969 \nF-statistic:  3097 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n# Plot\nplot(X, Y, main = \"OLS Regression\")\nabline(model, col = \"red\")\n\n\n\n\n\n\n\n\nThis will fit an OLS model to some simulated data and plot the results.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff\n\n\n\n\n1.13.5 Advanced Stochastic Models: Large Language Models\nLarge Language Models (LLMs) like GPT-3 are complex stochastic models used in natural language processing. While we can’t implement a full LLM in this tutorial, we can discuss its principles.\nLLMs are based on the transformer architecture and use self-attention mechanisms. They’re trained on vast amounts of text data and learn to predict the next token in a sequence.\nThe core of an LLM can be thought of as a conditional probability distribution:\n\\[P(x_t | x_{&lt;t}, \\theta)\\]\nWhere: - \\(x_t\\) is the current token - \\(x_{&lt;t}\\) represents all previous tokens - \\(\\theta\\) are the model parameters\n\n\n\n\n\n\nNote\n\n\n\nTokens in Large Language Models (LLMs) are the basic units of text that the model processes. They can be thought of as pieces of words or punctuation marks. Here are key points about tokens:\nDefinition: Tokens are the smallest units of text that an LLM processes. They can be whole words, parts of words, or even individual characters or punctuation marks. Tokenization: The process of breaking text into tokens is called tokenization. LLMs use specific algorithms to perform this task. Examples:\nThe word “cat” might be a single token. A longer word like “understanding” might be broken into multiple tokens, e.g., “under” and “standing”. Punctuation marks like “.” or “?” are often individual tokens. Common prefixes or suffixes might be their own tokens.\nVocabulary: LLMs have a fixed vocabulary of tokens they recognize. This vocabulary typically ranges from tens of thousands to hundreds of thousands of tokens. Significance: The way text is tokenized can affect how the model understands and generates language. It’s particularly important for handling different languages, rare words, or specialized vocabulary. Context: In the equation for LLMs: \\[P(x_t | x_{&lt;t}, \\theta)\\] Where:\n\\(x_t\\) represents the current token \\(x_{&lt;t}\\) represents all previous tokens in the sequence \\(\\theta\\) represents the model parameters\n\n\nUnlike deterministic models, LLMs produce different outputs even for the same input due to their stochastic nature.\n\n\n1.13.6 Conclusion\nWe’ve explored a range of models from deterministic to highly complex stochastic ones. Each type of model has its place in science, depending on the system being studied and the level of uncertainty involved.\nRemember, the choice between deterministic and stochastic models often depends on the nature of the system you’re studying and the questions you’re trying to answer. Deterministic models are great for systems with well-understood mechanics, while stochastic models shine when dealing with inherent randomness or complex, not fully understood systems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-d-introduction-to-r-rstudio-and-tidyverse",
    "href": "chapter1.html#appendix-d-introduction-to-r-rstudio-and-tidyverse",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.14 Appendix D: Introduction to R, RStudio, and tidyverse",
    "text": "1.14 Appendix D: Introduction to R, RStudio, and tidyverse\nR is a powerful programming language and environment for statistical computing and graphics. It’s widely used in academia, especially in fields like social sciences, for data analysis and visualization.\n\n1.14.0.1 Key features of R:\n\nOpen-source and free\nExtensive package ecosystem\nStrong community support\nExcellent for statistical analysis and data visualization\n\n\n\n1.14.1 Getting Started with RStudio\nRStudio is an Integrated Development Environment (IDE) for R that makes it easier to work with R.\n\n1.14.1.1 Installing R and RStudio\n\nDownload and install R from CRAN\nDownload and install RStudio from RStudio’s website\n\n\n\n1.14.1.2 RStudio Interface\nRStudio has four main panes:\n\nSource Editor: Where you write and edit your R scripts\nConsole: Where you can type R commands and see output\nEnvironment/History: Shows all objects in your workspace and command history\nFiles/Plots/Packages/Help: Multipurpose pane for file management, viewing plots, managing packages, and accessing help\n\n\n\n1.14.1.3 Basic RStudio Features\n\nCreating a new R script: File &gt; New File &gt; R Script\nRunning code: Select code and press Ctrl+Enter (Cmd+Enter on Mac)\nInstalling packages: Tools &gt; Install Packages\nGetting help: Type ?function_name in the console\n\n\n\n\n1.14.2 R Basics\n\n1.14.2.1 Data Types in R\n\n# Numeric\nx &lt;- 10.5\nclass(x)\n\n[1] \"numeric\"\n\n# Integer\ny &lt;- 1L\nclass(y)\n\n[1] \"integer\"\n\n# Character\nname &lt;- \"Alice\"\nclass(name)\n\n[1] \"character\"\n\n# Logical\nis_student &lt;- TRUE\nclass(is_student)\n\n[1] \"logical\"\n\n\n\n\n1.14.2.2 Data Structures\n\n1.14.2.2.1 Vectors\n\n# Create a vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\n\n# Vector operations\nnumbers + 2\n\n[1] 3 4 5 6 7\n\nnumbers * 2\n\n[1]  2  4  6  8 10\n\nmean(numbers)\n\n[1] 3\n\nlength(fruits)\n\n[1] 3\n\n\n\n\n1.14.2.2.2 Matrices\n\n# Create a matrix\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\nprint(m)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n# Matrix operations\nt(m)  # transpose\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nm * 2  # scalar multiplication\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    4    8   12\n\n\n\n\n1.14.2.2.3 Data Frames\n\n# Create a data frame\ndf &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  student = c(TRUE, FALSE, TRUE)\n)\nprint(df)\n\n     name age student\n1   Alice  25    TRUE\n2     Bob  30   FALSE\n3 Charlie  35    TRUE\n\n# Accessing data frame elements\ndf$name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\ndf[1, 2]\n\n[1] 25\n\ndf[df$age &gt; 25, ]\n\n     name age student\n2     Bob  30   FALSE\n3 Charlie  35    TRUE\n\n\n\n\n\n1.14.2.3 Functions\n\n# Define a function\ngreet &lt;- function(name) {\n  paste(\"Hello,\", name, \"!\")\n}\n\n# Use the function\ngreet(\"Alice\")\n\n[1] \"Hello, Alice !\"\n\n# Function with multiple arguments\ncalculate_bmi &lt;- function(weight, height) {\n  bmi &lt;- weight / (height^2)\n  return(bmi)\n}\n\ncalculate_bmi(70, 1.75)\n\n[1] 22.85714\n\n\n\n\n1.14.2.4 Control Structures\n\n# If-else statement\nx &lt;- 10\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n} else {\n  print(\"x is not greater than 5\")\n}\n\n[1] \"x is greater than 5\"\n\n# For loop\nfor (i in 1:5) {\n  print(paste(\"Iteration\", i))\n}\n\n[1] \"Iteration 1\"\n[1] \"Iteration 2\"\n[1] \"Iteration 3\"\n[1] \"Iteration 4\"\n[1] \"Iteration 5\"\n\n# While loop\ncounter &lt;- 1\nwhile (counter &lt;= 5) {\n  print(paste(\"Counter:\", counter))\n  counter &lt;- counter + 1\n}\n\n[1] \"Counter: 1\"\n[1] \"Counter: 2\"\n[1] \"Counter: 3\"\n[1] \"Counter: 4\"\n[1] \"Counter: 5\"\n\n\n\n\n\n1.14.3 Introduction to tidyverse\nThe tidyverse is a collection of R packages designed for data science. These packages share a common philosophy and are designed to work together seamlessly.\n\n1.14.3.1 Key tidyverse Packages\n\nggplot2: for data visualization\ndplyr: for data manipulation\ntidyr: for tidying data\nreadr: for reading rectangular data\npurrr: for functional programming\ntibble: modern reimagining of data frames\n\n\n\n1.14.3.2 Getting Started with tidyverse\n\n# Install tidyverse (run once)\n# install.packages(\"tidyverse\")\n\n# Load tidyverse\nlibrary(tidyverse)\n\n\n\n1.14.3.3 Data Import with readr\n\n# Reading CSV files\ndata &lt;- read_csv(\"social_data.csv\")\n\n# Reading other file formats\nread_tsv(\"data.tsv\")  # Tab-separated values\nread_delim(\"data.txt\", delim = \"|\")  # Custom delimiter\n\n\n\n1.14.3.4 Data Manipulation with dplyr\n\n# Let's use the built-in mtcars dataset\ndata(\"mtcars\")\n\n# Selecting columns\nmtcars %&gt;% \n  select(mpg, cyl, hp)\n\n                     mpg cyl  hp\nMazda RX4           21.0   6 110\nMazda RX4 Wag       21.0   6 110\nDatsun 710          22.8   4  93\nHornet 4 Drive      21.4   6 110\nHornet Sportabout   18.7   8 175\nValiant             18.1   6 105\nDuster 360          14.3   8 245\nMerc 240D           24.4   4  62\nMerc 230            22.8   4  95\nMerc 280            19.2   6 123\nMerc 280C           17.8   6 123\nMerc 450SE          16.4   8 180\nMerc 450SL          17.3   8 180\nMerc 450SLC         15.2   8 180\nCadillac Fleetwood  10.4   8 205\nLincoln Continental 10.4   8 215\nChrysler Imperial   14.7   8 230\nFiat 128            32.4   4  66\nHonda Civic         30.4   4  52\nToyota Corolla      33.9   4  65\nToyota Corona       21.5   4  97\nDodge Challenger    15.5   8 150\nAMC Javelin         15.2   8 150\nCamaro Z28          13.3   8 245\nPontiac Firebird    19.2   8 175\nFiat X1-9           27.3   4  66\nPorsche 914-2       26.0   4  91\nLotus Europa        30.4   4 113\nFord Pantera L      15.8   8 264\nFerrari Dino        19.7   6 175\nMaserati Bora       15.0   8 335\nVolvo 142E          21.4   4 109\n\n# Filtering rows\nmtcars %&gt;% \n  filter(cyl == 4)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n# Arranging data\nmtcars %&gt;% \n  arrange(desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n# Creating new variables\nmtcars %&gt;% \n  mutate(kpl = mpg * 0.425)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb     kpl\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  8.9250\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  8.9250\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1  9.6900\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  9.0950\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  7.9475\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  7.6925\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  6.0775\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 10.3700\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  9.6900\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  8.1600\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  7.5650\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  6.9700\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  7.3525\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  6.4600\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  4.4200\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  4.4200\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  6.2475\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 13.7700\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 12.9200\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 14.4075\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  9.1375\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  6.5875\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  6.4600\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  5.6525\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  8.1600\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 11.6025\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 11.0500\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 12.9200\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  6.7150\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  8.3725\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  6.3750\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2  9.0950\n\n# Summarizing data\nmtcars %&gt;% \n  group_by(cyl) %&gt;% \n  summarize(mean_mpg = mean(mpg),\n            count = n())\n\n# A tibble: 3 × 3\n    cyl mean_mpg count\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1     4     26.7    11\n2     6     19.7     7\n3     8     15.1    14\n\n\n\n\n1.14.3.5 Data Visualization with ggplot2\n\n# Scatter plot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Car Weight vs. Fuel Efficiency\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles per Gallon\")\n\n\n\n\nCar Weight vs. Fuel Efficiency\n\n\n\n\n\n# Bar chart\nmtcars %&gt;% \n  count(cyl) %&gt;% \n  ggplot(aes(x = factor(cyl), y = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Number of Cars by Cylinder Count\",\n       x = \"Number of Cylinders\",\n       y = \"Count\")\n\n\n\n\nNumber of Cars by Cylinder Count\n\n\n\n\n\n# Box plot\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Fuel Efficiency by Number of Cylinders\",\n       x = \"Number of Cylinders\",\n       y = \"Miles per Gallon\")\n\n\n\n\nFuel Efficiency by Number of Cylinders\n\n\n\n\n\n\n\n1.14.4 Additional Resources\n\nR for Data Science\ntidyverse documentation\nRStudio Cheat Sheets\nQuarto Guide\nR Cookbook\n\nRemember to experiment with the code, modify examples, and don’t hesitate to use the built-in R help system (accessed by typing ?function_name in the console) when you encounter unfamiliar functions or concepts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html",
    "href": "rozdzial1.html",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "",
    "text": "2.1 Czym są Statystyka i Nauka o Danych?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#czym-są-statystyka-i-nauka-o-danych",
    "href": "rozdzial1.html#czym-są-statystyka-i-nauka-o-danych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "",
    "text": "Important\n\n\n\nStatystyka i data science to sztuka i nauka (o metodach, technikach lub narzędziach) uczenia się z danych.\n\n\n\nNauka o danych i statystyka to potężne narzędzia, które pomagają nam zrozumieć złożone zjawiska w różnych naukach społecznych, w tym w politologii, ekonomii i socjologii. Te uzupełniające się dziedziny dostarczają badaczom i praktykom środków do analizy trendów, zachowań i wyników w społeczeństwie, oferując wgląd, który może kształtować politykę i pogłębiać nasze zrozumienie ludzkich zachowań.\nStatystyka dostarcza matematycznych podstaw do analizy trendów i wyników społecznych, oferując metody projektowania badań, podsumowywania danych i wyciągania wniosków. Nauka o danych rozszerza tę podstawę, włączając metody obliczeniowe i wiedzę dziedzinową, aby radzić sobie z większymi zbiorami danych i przeprowadzać bardziej złożone analizy.\nRazem te dyscypliny pozwalają nam zbierać i przetwarzać duże zbiory danych, wizualizować złożone informacje, odkrywać wzorce w interakcjach społecznych, oceniać wpływ polityk i wspierać podejmowanie decyzji opartych na dowodach. Ich zastosowania są rozległe i zróżnicowane, od badania wzorców głosowania i analizy wskaźników ekonomicznych po badanie nierówności społecznych i analizę zachowań ludzkich.\nW miarę jak nasz świat staje się coraz bardziej oparty na danych, znaczenie nauki o danych i statystyki w naukach społecznych nadal rośnie.\n\n\n\n\n\n\n\nNote\n\n\n\nW naukach społecznych nauka o danych łączy metody statystyczne, narzędzia obliczeniowe i wiedzę dziedzinową do analizy złożonych zjawisk społecznych i zachowań ludzkich.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#związek-między-statystyką-a-nauką-o-danych",
    "href": "rozdzial1.html#związek-między-statystyką-a-nauką-o-danych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.2 Związek Między Statystyką a Nauką o Danych",
    "text": "2.2 Związek Między Statystyką a Nauką o Danych\nStatystyka i data science to ściśle powiązane dziedziny o znaczącym nakładaniu się, szczególnie w naukach społecznych. Zamiast ścisłego podziału, trafniej jest postrzegać je jako komplementarne podejścia na pewnym kontinuum:\n\nTradycyjna StatystykaNowoczesna Data ScienceEwoluujący Krajobraz\n\n\n\nZakorzeniona w teoriach matematycznych i metodach analizy danych\nKładzie nacisk na wnioskowanie statystyczne, testowanie hipotez i teorię prawdopodobieństwa\nHistorycznie kluczowa w naukach społecznych do analizy badań ankietowych, eksperymentów i badań obserwacyjnych\n\n\n\n\nIntegruje metody statystyczne z nauką o komputerach i wiedzą dziedzinową\nPoszerza fokus o uczenie maszynowe, przetwarzanie big data i modelowanie predykcyjne\nW naukach społecznych często zajmuje się wielkoskalowymi danymi cyfrowymi i złożonymi zbiorami danych behawioralnych\n\n\n\n\nGranice między statystyką a data science są coraz bardziej rozmyte\nWiele technik i narzędzi jest wspólnych dla obu dziedzin\nNaukowcy społeczni często łączą tradycyjne podejścia statystyczne z nowszymi metodami data science\nWybór podejścia zależy od pytań badawczych, charakterystyki danych i konkretnych potrzeb analitycznych\n\n\n\n\nNauka o danych może być postrzegana jako wynik ewolucji i rozszerzenie tradycyjnej statystyki, włączając nowe technologie i metody do obsługi większych i bardziej złożonych zbiorów danych w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#podstawowe-koncepcje-w-nauce-o-danych-i-statystyce",
    "href": "rozdzial1.html#podstawowe-koncepcje-w-nauce-o-danych-i-statystyce",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.3 Podstawowe Koncepcje w Nauce o Danych i Statystyce",
    "text": "2.3 Podstawowe Koncepcje w Nauce o Danych i Statystyce\n\n2.3.1 Dane i Populacje (Data and Populations) oraz pojęcia pokrewne\n\n\n\n\n\n\nImportant\n\n\n\n\nDane: Obserwacje lub pomiary zebrane z próby lub populacji.\nPopulacja: Cały zbiór osób lub elementów badanych w określonym czasie.\n\nPrzykład: Wszyscy uprawnieni wyborcy w kraju podczas konkretnego roku wyborczego.\n\nPróba: Podzbiór populacji, który jest faktycznie mierzony. Reprezentatywna próba to podzbiór większej populacji, który dokładnie odzwierciedla cechy tej populacji. Próba powinna odzwierciedlać populację pod względem ważnych cech, takich jak wiek, płeć, status społeczno-ekonomiczny itp. Często wykorzystuje metody losowego doboru próby, aby uniknąć stronniczości. Jest wystarczająco duża, aby być statystycznie istotna, ale mniejsza niż cała populacja.\n\nPrzykład: 1500 losowo wybranych uprawnionych wyborców ankietowanych w przedwyborczym sondażu.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nProces Generowania Danych (PGD) i Superpopulacja: Rozszerzenie Tradycyjnych Koncepcji\nW tradycyjnej statystyce często pracujemy z dwoma kluczowymi pojęciami:\n\nPopulacja: Cała grupa, którą chcemy badać.\nPróba: Podzbiór populacji, który faktycznie obserwujemy i analizujemy.\n\nChociaż te pojęcia są fundamentalne, współczesne badania często wymagają myślenia wykraczającego poza ten dychotomiczny podział. Tu wkraczają koncepcje Procesu Generowania Danych (PGD) i superpopulacji, rozszerzając nasze rozumienie danych i populacji.\nProces Generowania Danych (PGD; Data Generating Process, DGP):\nPGD to podstawowy mechanizm, który produkuje dane obserwowane w rzeczywistym świecie, zarówno w naszej próbie, jak i w całej populacji.\nIntuicyjne wyjaśnienie: Wyobraź sobie PGD jako złożony system, który przyjmuje różne dane wejściowe i produkuje obserwowalne wyniki. To “czarna skrzynka”, która przekształca przyczyny w skutki, nie tylko dla naszej próby, ale dla całej populacji i poza nią.\nPrzykład: Rozważmy badanie zachowań wyborczych. Tradycyjne podejście mogłoby zdefiniować populację jako “wszyscy zarejestrowani wyborcy” i pobrać z tej grupy próbę. PGD natomiast obejmowałby czynniki takie jak cechy demograficzne, warunki ekonomiczne, wydarzenia polityczne i wpływ mediów, które kształtują zachowania wyborcze wszystkich wyborców, niezależnie od tego, czy zostali uwzględnieni w próbie, czy nie.\nSuperpopulacja:\nSuperpopulacja to teoretyczna koncepcja, która wykracza zarówno poza próbę, jak i obserwowalną populację, obejmując wszystkie potencjalne wyniki, które mogłyby wystąpić w podobnych warunkach lub procesach.\nPrzykłady:\n\nPodejście tradycyjne vs. podejście superpopulacyjne:\n\nTradycyjne: populacja (wszyscy zarejestrowani wyborcy w województwie), próba (1000 ankietowanych wyborców)\nSuperpopulacja: Wszyscy możliwi wyborcy i scenariusze głosowania, w tym przyszłe wybory i hipotetyczne konteksty polityczne\n\nGdy próba równa się populacji:\nW badaniach wszystkich 16 województw Polski:\n\nTradycyjne spojrzenie: Brak rozróżnienia między próbą a populacją\nSpojrzenie superpopulacyjne: Traktuje te 16 województw jako “próbę” z teoretycznego zbioru wszystkich możliwych interakcji między województwami a polityką\n\n\nZastosowanie w rzeczywistości: Załóżmy, że badacze studiują wpływ nowej polityki planowania urbanistycznego w kilku miastach:\n\nPodejście tradycyjne:\n\nPopulacja: Wszystkie miasta w kraju\nPróba: Miasta uwzględnione w badaniu\n\nPodejście superpopulacyjne:\n\nObserwowane dane: Miasta w badaniu\nSuperpopulacja: Wszystkie miasta (istniejące lub potencjalne), w których można by zastosować podobne zasady planowania urbanistycznego\n\n\nPGD (DGP) w tym przypadku byłby złożonym zestawem czynników, które determinują, jak polityki planowania urbanistycznego wpływają na wyniki miast, mające zastosowanie nie tylko do badanych miast czy nawet wszystkich istniejących miast, ale do szerszej koncepcji “miasta” jako takiego.\nWażne kwestie do rozważenia:\n\nZakres i ograniczenia: Badacze powinni jasno określić, jakie jednostki lub procesy starają się zrozumieć, wykraczając poza samo opisanie próby i populacji.\nMożliwość uogólnienia: Przy formułowaniu wniosków dotyczących superpopulacji, badacze powinni wyraźnie określić granice, w których ich ustalenia mają zastosowanie.\nSpecyfika kontekstu: Chociaż koncepcja superpopulacji pozwala na szersze wnioskowanie niż tradycyjne pobieranie próbek, ważne jest, aby zdawać sobie sprawę, że PGD może się różnić w zależności od kontekstu.\n\nPrzykład podsumowujący: Jakość Pizzy w Nowym Jorku\nPopulacja: Wszystkie obecnie działające pizzerie w Nowym Jorku. To skończona, policzalna grupa lokali istniejących w danym momencie.\nPróba: Wybór 50 pizzerii losowo wybranych z różnych dzielnic Nowego Jorku. To konkretne pizzerie, w których badacze będą degustować i oceniać pizze.\nSuperpopulacja: Wszystkie możliwe pizzerie, które mogłyby istnieć w Nowym Jorku, w tym:\n\nObecnie działające pizzerie\nPrzyszłe pizzerie, które jeszcze nie zostały otwarte\nPizzerie, które zostały zamknięte\nHipotetyczne pizzerie, które mogłyby istnieć w innych warunkach ekonomicznych lub kulturowych\n\nKoncepcja superpopulacji pozwala nam myśleć o jakości pizzy wykraczając poza obecny “zrzut ekranu” nowojorskich pizzerii.\nProces Generowania Danych (PGD): PGD to złożony zestaw czynników, które przyczyniają się do jakości pizzy w każdej pizzerii. Może to obejmować:\n\nSkładniki: Jakość i źródło mąki, pomidorów, sera itp.\nUmiejętności szefa kuchni: Szkolenie, doświadczenie i osobiste podejście pizzaiolo\nSprzęt: Rodzaj i stan pieca, używane narzędzia\nPrzepis: Proporcje składników, metody przygotowania\nCzynniki środowiskowe: Wilgotność, jakość wody w Nowym Jorku\nWpływy kulturowe: Lokalne tradycje robienia pizzy, preferencje klientów\nCzynniki ekonomiczne: Koszty składników, ceny wynajmu wpływające na decyzje biznesowe\n\nPGD jest jak “przepis na jakość pizzy”, który ma zastosowanie nie tylko do naszej próby czy nawet obecnej populacji, ale do wszystkich potencjalnych pizzerii w superpopulacji.\nIntuicyjne Wyjaśnienie:\n\nJeśli odwiedzisz wszystkie obecnie działające pizzerie w Nowym Jorku i je ocenisz, zbadałeś populację.\nJeśli losowo wybierzesz 50 pizzerii do odwiedzenia i oceny, pobrałeś próbę.\nJeśli zastanawiasz się, jak jakość pizzy mogłaby się różnić we wszystkich możliwych nowojorskich pizzeriach (przeszłych, obecnych, przyszłych i hipotetycznych), myślisz o superpopulacji.\nJeśli próbujesz zrozumieć wszystkie czynniki, które składają się na jakość pizzy w Nowym Jorku, niezależnie od tego, czy dana pizzeria obecnie istnieje czy nie, badasz Proces Generowania Danych.\n\n\n\n\n\n\n\n\ngraph TD\n    A[Data Generating Process DGP]\n    B(Population)\n    C[Sample]\n    A --&gt;|Generates| B\n    B --&gt;|Sampled from| C\n    C -.-&gt;|Inference| B\n    C -.-&gt;|Inference| A\n    B -.-&gt;|Inference| A\n    \n    classDef dgp fill:#f9f,stroke:#333,stroke-width:4px;\n    classDef pop fill:#bbf,stroke:#333,stroke-width:2px;\n    classDef sam fill:#bfb,stroke:#333,stroke-width:2px;\n    \n    class A dgp;\n    class B pop;\n    class C sam;\n\n\n\n\n\n\n\n\n\n\n\n\nObjaśnienie diagramu PGD, Populacji i Próby\n\n\n\nDiagram przedstawia relacje między Procesem Generującym Dane (PGD), populacją i próbą, wraz ze ścieżkami wnioskowania:\n\nRelacje bezpośrednie (ciągłe strzałki):\n\nPGD generuje populację\nZ populacji pobierane są próby\n\nŚcieżki wnioskowania (przerywane strzałki):\n\nOd Próby do Populacji: Tradycyjne wnioskowanie statystyczne\nOd Próby do PGD: Wnioskowanie o podstawowym procesie na podstawie danych z próby\nOd Populacji do PGD: Wnioskowanie o PGD przy użyciu pełnych danych populacji\n\n\nNa przykład, w naszym badaniu wpływu ordynacji wyborczej na frekwencję w polskich gminach (wybory samorządowe 1998-2010):\n\nDysponujemy danymi dla całej populacji gmin, więc nie musimy wnioskować z próby o populacji.\nSkupiamy się na wykorzystaniu pełnych danych populacyjnych (prawa przerywana strzałka) do wnioskowania o leżącym u podstaw PGD—złożonych procesach, poprzez które ordynacja wyborcza wpływa na frekwencję wyborczą w gminach.\nTakie podejście pozwala nam potencjalnie zrozumieć mechanizmy, dzięki którym różne systemy wyborcze (np. reprezentacja proporcjonalna vs. większościowa) wpływają na poziom frekwencji, oraz formułować uzasadnione przewidywania o tym, jak zmiany w ordynacji wyborczej mogłyby wpłynąć na przyszłą frekwencję lub jak te efekty mogłyby się uogólniać na podobne konteksty.\n\n\n\n\n\n\nPopulacja vs. próba. Retrieved from: https://allmodelsarewrong.github.io/mse.html\n\n\nDane stanowią podstawę analizy statystycznej. Mogą być:\n\nDane pierwotne (Primary data): Zebrane bezpośrednio w określonym celu\nDane wtórne (Secondary data): Uzyskane z istniejących źródeł\n\nPrzykład: W badaniu wzrostu studentów uniwersyteckich, populacją są wszyscy studenci uniwersyteccy w kraju, podczas gdy próba może składać się z 1000 losowo wybranych studentów.\n\n\n2.3.2 Zmienne i Stałe (Variables and Constants)\nZmienne to cechy, które mogą przyjmować różne wartości w zbiorze danych. Mogą być:\n\nIlościowe (Quantitative):\n\nCiągłe (Continuous): Wzrost, waga, temperatura\nDyskretne (Discrete): Liczba dzieci, liczba błędów w programie\n\nJakościowe (Qualitative):\n\nNominalne (Nominal): Grupa krwi, kolor oczu\nPorządkowe (Ordinal): Poziom wykształcenia, ocena satysfakcji klienta\n\n\nStałe to wartości, które pozostają niezmienne w trakcie analizy.\n\n2.3.2.1 Rodzaje Danych w Naukach Społecznych\nBadania w naukach społecznych zajmują się różnymi rodzajami danych:\n\nDane Ilościowe: Dane liczbowe (np. odpowiedzi z ankiet, wskaźniki ekonomiczne)\nDane Jakościowe: Dane nieliczbowe (np. transkrypcje wywiadów, odpowiedzi na pytania otwarte w ankietach)\nBig Data: Dane cyfrowe na dużą skalę (np. posty w mediach społecznościowych, logi zachowań online)\n\n\n\n\n2.3.3 Parametry Populacji i Estymanda (Population Parameters and Estimands)\nParametry populacji to liczbowe charakterystyki populacji. Kluczowe punkty:\n\nOpisują całą populację, nie tylko próbę.\nZwykle oznaczane są greckimi literami.\nW większości przypadków nie mogą być bezpośrednio obliczone, ponieważ nie możemy zmierzyć całej populacji.\nSą determinowane przez podstawowy Proces Generujący Dane (DGP).\n\nTypowe parametry populacji to:\n\nŚrednia populacji (Population mean) (\\(\\mu\\)): Średnia/oczekiwana wartość zmiennej w populacji.\nWariancja populacji (Population variance) (\\(\\sigma^2\\)): Miara zmienności w populacji.\nProporcja populacji (Population proportion) (\\(p\\)): Proporcja osób w populacji posiadających daną cechę.\n\nEstymand (Estimand) to cel estymacji - konkretny parametr populacji lub funkcja parametrów, którą chcemy oszacować. Definiuje to, co chcemy wiedzieć o populacji.\n\n\n\n\n\n\nPrzykład: Wzrost Studentów Uniwersyteckich\n\n\n\nRozważmy wzrost wszystkich studentów uniwersyteckich w kraju:\n\n\\(\\mu\\) (estymand): Prawdziwa średnia wysokość wszystkich studentów uniwersyteckich (średnia populacji)\n\\(\\sigma^2\\) (estymand): Prawdziwa wariancja wysokości w populacji\n\nTe parametry są nieznanymi estymandami, które chcemy oszacować na podstawie danych z próby.\n\n\n\n\n2.3.4 Statystyki i Estymatory (Statistic(s) and Estimators)\nStatystyka (pojedyncza) lub statystyka z próby to dowolna wielkość obliczona na podstawie wartości z próby, która jest rozważana w celu statystycznym.\nGdy statystyka jest używana do oszacowania estymandy (parametru populacji), nazywana jest estymatorem. Estymatory są funkcjami danych z próby, które dostarczają przybliżonych wartości dla nieznanych parametrów populacji.\nPrzykłady statystyk/estymatorów:\n\nŚrednia z próby (Sample mean): \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\) (szacuje \\(\\mu\\))\nWariancja z próby (Sample variance): \\(s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\\) (szacuje \\(\\sigma^2\\))\nProporcja z próby (Sample proportion): \\(\\hat{p} = \\frac{x}{n}\\) (szacuje \\(p\\))\n\n\n\n2.3.5 Oszacowania (Estimates)\nOszacowanie to konkretna wartość uzyskana przez zastosowanie estymatora do konkretnej próby. Jest to wartość punktowa, która przybliża prawdziwą estymandę (parametr populacji).\nPrzykład: Jeśli obliczamy średnią wysokość z próby wynoszącą 173 cm, to 173 cm jest naszym oszacowaniem estymandy \\(\\mu\\) (średniej wysokości populacji).\n\n\n2.3.6 Modele Statystyczne (Statistical Models)\n\n\n\n\n\n\nNote\n\n\n\nModel w nauce to uproszczona reprezentacja złożonego systemu lub zjawiska. Jest on ta zaprojektowany, aby pomóc nam zrozumieć, wyjaśnić i przewidywać zjawiska zachodzące w rzeczywistym świecie. Modele mogą przybierać różne formy, w tym równania matematyczne, symulacje komputerowe lub ramy koncepcyjne. Pozwalają naukowcom skupić się na kluczowych aspektach systemu, ignorując mniej istotne szczegóły, co sprawia, że złożone problemy stają się łatwiejsze do zrozumienia i badania.\n\n\nModele statystyczne reprezentują relacje między zmiennymi i pomagają w przewidywaniu lub wnioskowaniu o estymandach (parametrach populacji).\nPrzykład: Model regresji liniowej \\(y = \\beta_0 + \\beta_1x + \\epsilon\\) opisuje relację między zmienną niezależną \\(x\\) a zmienną zależną \\(y\\), gdzie:\n\n\\(y\\) to zmienna zależna (np. wielkość popytu na dobro)\n\\(x\\) to zmienna niezależna (np. cena lub dochód konsumenta)\n\\(\\beta_0\\) i \\(\\beta_1\\) to parametry, estymandy do oszacowania\n\\(\\epsilon\\) to składnik błędu, reprezentujący niewyjaśnioną zmienność\n\n\n\n2.3.7 Wnioskowanie (Inference)\nWnioskowanie statystyczne to proces wyciągania wniosków o estymandach (parametrach populacji) na podstawie danych z próby. Obejmuje dwa główne typy:\n\nEstymacja (Estimation): Używanie statystyk z próby (estymatorów) do oszacowania estymand (parametrów populacji)\nTestowanie hipotez (Hypothesis testing): Podejmowanie decyzji o estymandach na podstawie dowodów z próby\n\n\n\n\n\n\n\nEstymacja i testowanie hipotez: Dogłębna analiza\n\n\n\n\nEstymacja\n\nEstymacja polega na określeniu prawdopodobnej wartości parametru populacji na podstawie danych z próby. W kontekście rozkładu dwumianowego możemy być zainteresowani oszacowaniem prawdopodobieństwa sukcesu (p) dla określonego zdarzenia.\nPrzykład: Rzucanie monetą\nPowiedzmy, że rzucamy monetą 100 razy i chcemy oszacować prawdopodobieństwo wypadnięcia orła.\n\nRzucamy monetą 100 razy i obserwujemy 55 orłów.\nNasze punktowe oszacowanie p (prawdopodobieństwo wypadnięcia orła) wynosi 55/100 = 0,55\nMożemy również obliczyć przedział ufności, np. 95% przedział ufności może wynosić (0,45; 0,65).\n\nPrzedział ufności mówi nam o zakresie, w którym może leżeć prawdziwe prawdopodobieństwo. Mówiąc prościej: “Jesteśmy w 95% pewni, że prawdziwe prawdopodobieństwo wypadnięcia orła mieści się między 45% a 65%.”\nCelem jest tutaj dostarczenie naszego najlepszego oszacowania prawdziwego prawdopodobieństwa wypadnięcia orła, wraz z zakresem prawdopodobnych wartości.\nWażne Pojęcia Teorii Estymacji:\n\nObciążenie (Bias)\n\nObciążenie odnosi się do tendencji estymatora do systematycznego przeszacowania lub niedoszacowania prawdziwej wartości parametru populacji (estymandy).\n\nEstymator nieobciążony to taki, którego średnia wartość (przy wielokrotnym powtórzeniu estymacji) jest równa prawdziwej wartości parametru.\nObciążenie można rozumieć jako różnicę między średnią wartością estymatora a prawdziwą wartością parametru.\n\n\nEfektywność (Efficiency)\n\nEfektywność odnosi się do precyzji estymatora. Bardziej efektywny estymator daje wyniki bliższe prawdziwej wartości parametru, czyli ma mniejsze rozproszenie wyników.\n\nMierzona jest najczęściej wariancją estymatora (im mniejsza wariancja, tym większa efektywność)\nDla nieobciążonych estymatorów efektywność często porównuje się za pomocą Błędu Średniokwadratowego (Mean Squared Error, MSE)\n\n\nTestowanie hipotez\n\nTestowanie hipotez z kolei polega na podejmowaniu decyzji między dwoma konkurencyjnymi twierdzeniami dotyczącymi parametru populacji. Zazwyczaj mamy hipotezę zerową (H0) i hipotezę alternatywną (H1).\nPrzykład: Czy moneta jest uczciwa?\nKorzystając z tego samego scenariusza rzucania monetą, powiedzmy, że chcemy sprawdzić, czy moneta jest uczciwa (p = 0,5), czy też stronnicza na korzyść orła (p &gt; 0,5).\n\nHipoteza zerowa (H0): p = 0,5 (moneta jest uczciwa)\nHipoteza alternatywna (H1): p &gt; 0,5 (moneta jest stronnicza na korzyść orła)\nObserwujemy 55 orłów na 100 rzutów\n\n**P-wartość i “probabilistyczny dowód nie wprost”\nTeraz zagłębmy się w koncepcję wartości p i jak testowanie hipotez działa jako rodzaj “probabilistycznego dowodu nie wprost”:\n\nZaczynamy od założenia, że hipoteza zerowa (H0) jest prawdziwa. W tym przypadku zakładamy, że moneta jest uczciwa.\nNastępnie pytamy: “Jeśli moneta byłaby naprawdę uczciwa, jakie byłoby prawdopodobieństwo zaobserwowania 55 lub więcej orłów na 100 rzutów?”\nTo prawdopodobieństwo nazywa się wartością p. Jest to prawdopodobieństwo zaobserwowania naszych danych (lub bardziej ekstremalnych) przy założeniu, że hipoteza zerowa jest prawdziwa.\nJeśli to prawdopodobieństwo (wartość p) jest bardzo małe, mamy sprzeczność: zaobserwowaliśmy coś, co powinno być bardzo rzadkie, gdyby nasze założenie (H0) było prawdziwe.\nZwykle ustalamy próg zwany poziomem istotności (często 0,05 lub 5%) dla tego, co uważamy za “bardzo małe”.\nJeśli wartość p jest mniejsza niż wybrany poziom istotności, odrzucamy H0. Wnioskujemy, że nasza obserwacja jest zbyt mało prawdopodobna przy H0, więc faworyzujemy hipotezę alternatywną.\nJeśli wartość p jest większa niż nasz poziom istotności, nie odrzucamy H0. Nie mamy wystarczających dowodów, aby stwierdzić, że moneta jest stronnicza.\n\nTen proces jest jak “probabilistyczny dowód nie wprost”, ponieważ:\n\nZaczynamy od założenia H0 (podobnie jak zakładamy przeciwieństwo tego, co chcemy udowodnić w dowodzie nie wprost).\nSprawdzamy, czy to założenie prowadzi do bardzo mało prawdopodobnej sytuacji (naszych zaobserwowanych danych).\nJeśli tak, odrzucamy założenie (H0) i faworyzujemy alternatywę.\n\nWartość p dokładnie określa, jak mało prawdopodobna jest nasza obserwacja przy H0. Bardzo mała wartość p (np. 0,01) oznacza: “Gdyby H0 była prawdziwa, spodziewalibyśmy się zobaczyć tak ekstremalne dane tylko około 1% czasu.”\nTestowanie hipotez i estymacja to powiązane, ale odrębne procedury statystyczne; testowanie hipotez może być wykorzystane do wyciągania wniosków o oszacowaniach i może uzupełniać estymację na kilka sposobów, np.:\n\nTestowanie oszacowań punktowych: Testowanie hipotez może być wykorzystane do oceny, czy oszacowanie punktowe różni się istotnie od hipotetycznej wartości. Na przykład, jeśli oszacujemy, że moneta ma prawdopodobieństwo 0,55 wypadnięcia orłem, możemy użyć testu hipotezy, aby określić, czy ta wartość różni się istotnie od 0,5 (uczciwa moneta).\nIstotność parametrów: W modelach wielowymiarowych, testy hipotez (takie jak testy t w regresji) mogą pomóc określić, które oszacowane parametry różnią się istotnie od zera, dając wgląd w to, które zmienne są ważne w modelu.\n\n\n\n\n\n2.3.8 Relacje Między Pojęciami\n\nProces Generujący Dane (DGP - Data Generating Process) określa rzeczywiste wartości parametrów populacji (estymand).\nEstymandy są szacowane za pomocą statystyk obliczonych na podstawie próby (estymatorów).\nJakość estymatorów ocenia się na podstawie właściwości takich jak obciążenie i efektywność w szacowaniu estymandy.\nModele statystyczne wykorzystują oszacowane parametry do opisania relacji między zmiennymi w populacji.\nWnioskowanie statystyczne polega na wyciąganiu wniosków o estymandach na podstawie danych z próby, wykorzystując właściwości estymatorów.\n\n\n\n\n\n\n\nPrzykład: Badanie Zachowań Wyborczych\n\n\n\n\nPopulacja: Wszyscy uprawnieni wyborcy w kraju\nEstymanda: \\(p\\) = rzeczywista proporcja wyborców popierających danego kandydata\nPróba: 1000 losowo wybranych uprawnionych wyborców\nEstymator: \\(\\hat{p}\\) = proporcja wyborców z próby popierających kandydata\nOszacowanie: Konkretna wartość \\(\\hat{p}\\) obliczona z próby (np. 0,52)\nDGP: Złożona interakcja czynników wpływających na decyzje wyborcze, takich jak przekonania polityczne, warunki ekonomiczne, ekspozycja na media i sieci społeczne.\n\nZrozumienie DGP pomaga badaczom interpretować, dlaczego estymanda \\(p\\) ma określoną wartość i jak może się zmieniać w czasie. Na przykład, nagła zmiana w gospodarce może wpłynąć na preferencje wyborców, zmieniając tym samym wartość \\(p\\).\nObciążenie i efektywność w kontekście przykładu:\n\nJeśli \\(\\hat{p}\\) jest nieobciążonym estymatorem, oznacza to, że przy wielokrotnym powtórzeniu badania na różnych próbach, średnia wartość \\(\\hat{p}\\) będzie bliska rzeczywistej wartości \\(p\\).\nEfektywność \\(\\hat{p}\\) określa, jak bardzo rozproszone są wyniki poszczególnych badań wokół tej średniej. Im mniejsze rozproszenie, tym estymator jest bardziej efektywny.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#główne-komponenty-nauki-o-danych-w-badaniach-naukowych",
    "href": "rozdzial1.html#główne-komponenty-nauki-o-danych-w-badaniach-naukowych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.4 Główne Komponenty Nauki o Danych w Badaniach Naukowych",
    "text": "2.4 Główne Komponenty Nauki o Danych w Badaniach Naukowych\n\nZbieranie DanychPrzetwarzanie DanychEksploracyjna Analiza Danych (EDA)Wnioskowanie StatystyczneUczenie MaszynoweWizualizacja Danych i KomunikacjaPowtarzalność i Otwarta Nauka\n\n\n\nMetody eksperymentalne: Kontrolowane badania, w których naukowcy manipulują zmiennymi, aby obserwować efekty\nBadania obserwacyjne: Gromadzenie danych poprzez obserwację i rejestrację bez ingerencji\nAnkiety i wywiady: Zbieranie informacji bezpośrednio od ludzi poprzez zadawanie pytań\nCyfrowe zbieranie danych: Gromadzenie danych ze źródeł internetowych, czujników lub systemów komputerowych\nAspekty etyczne: Zapewnienie, że badania respektują prawa i dobro uczestników\n\n\n\n\nCzyszczenie danych: Usuwanie błędów i niespójności z surowych danych\nObsługa brakujących wartości: Radzenie sobie z lukami w zbiorze danych, które mogłyby wpłynąć na analizę\nTransformacja danych: Konwertowanie danych na formaty odpowiednie do analizy, np. zmiana tekstu na liczby\n\n\n\n\nStatystyki opisowe: Podsumowanie danych za pomocą miar takich jak średnia, mediana i odchylenie standardowe\nWizualizacja danych: Tworzenie wykresów i diagramów do wizualnego przedstawienia wzorców w danych\nIdentyfikacja wzorców: Odkrywanie trendów lub zależności w danych\n\n\n\n\nTestowanie hipotez: Wykorzystanie danych do oceny twierdzeń o populacjach\nAnaliza regresji: Badanie zależności między zmiennymi i dokonywanie przewidywań\nWnioskowanie przyczynowe: Określanie, czy jedna zmienna bezpośrednio wpływa na inną\n\n\n\n\nUczenie nadzorowane: Trenowanie modeli do przewidywania wyników przy użyciu danych ze znanymi odpowiedziami\nUczenie nienadzorowane: Znajdowanie ukrytych wzorców w danych bez predefiniowanych kategorii\nPrzetwarzanie języka naturalnego (NLP): Nauczanie komputerów rozumienia i analizy ludzkiego języka\n\n\n\n\nEfektywne wizualizacje: Tworzenie czytelnych, informatywnych grafik do przedstawiania złożonych danych\nKomunikacja naukowa: Wyjaśnianie wyników różnym odbiorcom, od ekspertów po ogół społeczeństwa\nPisanie naukowe: Przygotowywanie artykułów i raportów naukowych w celu dzielenia się wynikami\n\n\n\n\nKontrola wersji: Śledzenie zmian w danych i kodzie w trakcie procesu badawczego\nPraktyki otwartych danych: Udostępnianie danych i metod badawczych do weryfikacji i dalszych badań\nPowtarzalne procesy badawcze: Dokumentowanie kroków badawczych, aby inni mogli powtórzyć badanie",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#narzędzia-do-nauki-o-danych-w-naukach-społecznych",
    "href": "rozdzial1.html#narzędzia-do-nauki-o-danych-w-naukach-społecznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.5 Narzędzia do Nauki o Danych w Naukach Społecznych",
    "text": "2.5 Narzędzia do Nauki o Danych w Naukach Społecznych\nW tym kursie będziemy głównie używać R do naszej analizy danych, ponieważ jest on szeroko stosowany w badaniach nauk społecznych.\n\n2.5.1 R w Analizie Danych Nauk Społecznych\nR oferuje potężne możliwości dla badań w naukach społecznych, od manipulacji danymi po zaawansowane modelowanie statystyczne.\n\n\nKliknij, aby pokazać/ukryć kod R\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nKliknij, aby pokazać/ukryć kod R\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate example data with a Simpson's Paradox\nn &lt;- 1000\ndata &lt;- tibble(\n  age_group = sample(c(\"Young\", \"Middle\", \"Old\"), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),\n  education_years = case_when(\n    age_group == \"Young\" ~ rnorm(n, mean = 10, sd = 1),\n    age_group == \"Middle\" ~ rnorm(n, mean = 13, sd = 1),\n    age_group == \"Old\" ~ rnorm(n, mean = 16, sd = 1)\n  ),\n  income = case_when(\n    age_group == \"Young\" ~ 70000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Middle\" ~ 50000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Old\" ~ 30000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000)\n  )\n)\n\n# Basic data summary\nsummary(data)\n\n\n  age_group         education_years      income     \n Length:1000        Min.   : 6.628   Min.   :34068  \n Class :character   1st Qu.:10.913   1st Qu.:51508  \n Mode  :character   Median :13.004   Median :63376  \n                    Mean   :12.986   Mean   :63307  \n                    3rd Qu.:14.934   3rd Qu.:75023  \n                    Max.   :18.861   Max.   :96620  \n\n\nKliknij, aby pokazać/ukryć kod R\n# Correlation analysis\ncor(data %&gt;% select(education_years, income))\n\n\n                education_years     income\neducation_years       1.0000000 -0.8152477\nincome               -0.8152477  1.0000000\n\n\nKliknij, aby pokazać/ukryć kod R\n# Overall trend (Simpson's Paradox)\noverall_plot &lt;- ggplot(data, aes(x = education_years, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Overall Relationship between Education and Income\",\n       subtitle = \"Simpson's Paradox: Appears negative\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Trend by age group (Resolving Simpson's Paradox)\ngrouped_plot &lt;- ggplot(data, aes(x = education_years, y = income, color = age_group)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Relationship between Education and Income by Age Group\",\n       subtitle = \"Resolving Simpson's Paradox: Positive relationship within groups\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Statistical analysis\nmodel_overall &lt;- lm(income ~ education_years, data = data)\nmodel_by_age &lt;- lm(income ~ education_years + age_group, data = data)\n\n# Print results\nprint(overall_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(grouped_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_overall))\n\n\n\nCall:\nlm(formula = income ~ education_years, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24451  -5439    235   5262  34328 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     121814.7     1339.5   90.94   &lt;2e-16 ***\neducation_years  -4505.4      101.3  -44.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7976 on 998 degrees of freedom\nMultiple R-squared:  0.6646,    Adjusted R-squared:  0.6643 \nF-statistic:  1978 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_by_age))\n\n\n\nCall:\nlm(formula = income ~ education_years + age_group, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-14827  -3369    118   3356  16388 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      48270.8     2028.4  23.797  &lt; 2e-16 ***\neducation_years   1135.5      154.6   7.345 4.26e-13 ***\nage_groupOld    -19942.8      593.2 -33.619  &lt; 2e-16 ***\nage_groupYoung   20461.1      600.7  34.064  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4950 on 996 degrees of freedom\nMultiple R-squared:  0.8711,    Adjusted R-squared:  0.8707 \nF-statistic:  2244 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\n# Calculate and print correlations\noverall_cor &lt;- cor(data$education_years, data$income)\ngroup_cors &lt;- data %&gt;%\n  group_by(age_group) %&gt;%\n  summarize(correlation = cor(education_years, income))\n\nprint(\"Overall correlation:\")\n\n\n[1] \"Overall correlation:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(overall_cor)\n\n\n[1] -0.8152477\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(\"Correlations by age group:\")\n\n\n[1] \"Correlations by age group:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(group_cors)\n\n\n# A tibble: 3 × 2\n  age_group correlation\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Middle          0.185\n2 Old             0.291\n3 Young           0.223\n\n\nTen przykład demonstruje podstawowe operacje na danych, statystyki opisowe i wizualizację danych przy użyciu R.\nCertainly. Here’s the Polish version of the section on causal inference versus observational studies:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wnioskowanie-przyczynowe-a-badania-obserwacyjne",
    "href": "rozdzial1.html#wnioskowanie-przyczynowe-a-badania-obserwacyjne",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.6 Wnioskowanie przyczynowe a badania obserwacyjne",
    "text": "2.6 Wnioskowanie przyczynowe a badania obserwacyjne\nW naukach społecznych i nie tylko, zrozumienie relacji między zmiennymi jest kluczowe. Dwa główne podejścia to wnioskowanie przyczynowe i badania obserwacyjne, każde z własnymi mocnymi stronami i ograniczeniami.\n\nWnioskowanie przyczynoweBadania obserwacyjneKluczowe rozróżnienie: Korelacja vs. Przyczynowość\n\n\n\nDąży do ustalenia związków przyczynowo-skutkowych\nCzęsto obejmuje plany eksperymentalne lub zaawansowane techniki statystyczne\nStara się odpowiedzieć na pytania “Co by było, gdyby?” i określić wpływ interwencji\nPrzykłady: Randomizowane badania kontrolowane, projekty quasi-eksperymentalne, zmienne instrumentalne\n\n\n\n\nBadają relacje między zmiennymi bez bezpośredniej interwencji\nOpierają się na danych zebranych w naturalnych warunkach lub z istniejących zbiorów danych\nMogą identyfikować korelacje i wzorce, ale mają trudności z ustaleniem przyczynowości\nPrzykłady: Badania kohortowe, badania kliniczno-kontrolne, przekrojowe badania ankietowe\n\n\n\n\n\n\n\n\n\n\n\n\n\nPamiętaj: Korelacja nie implikuje przyczynowości\n\n\n\nFundamentalna zasada w badaniach głosi, że korelacja między dwiema zmiennymi niekoniecznie implikuje związek przyczynowy. Ta koncepcja jest kluczowa przy interpretacji wyników badań obserwacyjnych.\n\nKorelacja: Mierzy siłę i kierunek związku między zmiennymi\nPrzyczynowość: Wskazuje, że zmiany w jednej zmiennej bezpośrednio powodują zmiany w drugiej\n\nChociaż silne korelacje mogą sugerować potencjalne związki przyczynowe, do ustalenia przyczynowości wymagane są dodatkowe dowody i rygorystyczne metody.\n\n\n\nWyzwania w ustalaniu przyczynowościMetody wzmacniania twierdzeń przyczynowychZnaczenie w naukach społecznych\n\n\n\nZmienne zakłócające: Niezmierzone czynniki wpływające zarówno na domniemaną przyczynę, jak i skutek\nOdwrotna przyczynowość: Domniemany skutek może w rzeczywistości powodować domniemaną przyczynę\nBłąd selekcji: Nielosowy dobór uczestników do grup badawczych\n\n\n\n\nRandomizowane badania kontrolowane (gdy są etyczne i wykonalne)\nNaturalne eksperymenty lub projekty quasi-eksperymentalne\nDopasowanie według propensity score\nAnaliza różnicy w różnicach\nPodejścia oparte na zmiennych instrumentalnych\nSkierowane grafy acykliczne (DAG) do wizualizacji relacji przyczynowych\n\n\n\nZrozumienie różnicy między wnioskowaniem przyczynowym a badaniami obserwacyjnymi jest kluczowe w naukach społecznych, gdzie względy etyczne często ograniczają manipulacje eksperymentalne. Badacze muszą starannie projektować badania i interpretować wyniki, aby uniknąć wprowadzających w błąd wniosków dotyczących przyczynowości.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#modele-w-nauce-od-deterministycznych-do-stochastycznych",
    "href": "rozdzial1.html#modele-w-nauce-od-deterministycznych-do-stochastycznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.7 Modele w Nauce: Od Deterministycznych do Stochastycznych",
    "text": "2.7 Modele w Nauce: Od Deterministycznych do Stochastycznych\nModele są niezbędnymi narzędziami w badaniach naukowych, pomagając naukowcom reprezentować, rozumieć i przewidywać złożone zjawiska. Ta sekcja omawia główne typy modeli stosowanych w nauce, wraz z przykładami ich zastosowań. Należy pamiętać, że te kategorie często się nakładają, a wiele modeli naukowych łączy w sobie różne aspekty.\n\n2.7.1 Modele Matematyczne\nModele matematyczne wykorzystują równania i koncepcje matematyczne do opisywania i analizowania systemów lub zjawisk. Można je podzielić na kilka podkategorii, choć należy pamiętać, że niektóre złożone modele mogą zawierać elementy z wielu kategorii:\n\n2.7.1.1 a. Modele Deterministyczne\nModele deterministyczne dostarczają precyzyjnych przewidywań na podstawie zestawu zmiennych, bez uwzględniania losowości na poziomie makroskopowym.\nPrzykład: Prawa ruchu Newtona, które mogą precyzyjnie przewidzieć ruch obiektów pod wpływem znanych sił w mechanice klasycznej.\n\n\n2.7.1.2 b. Modele Stochastyczne\nModele stochastyczne uwzględniają losowość i prawdopodobieństwo. Jednak kluczowe jest rozróżnienie dwóch fundamentalnie różnych typów modeli stochastycznych:\n\n2.7.1.2.1 i. Klasyczne Modele Stochastyczne\nTe modele zajmują się losowością wynikającą z niepełnej informacji lub złożonych interakcji w systemach klasycznych. Podstawowy system jest deterministyczny, ale praktyczne ograniczenia w pomiarach lub obliczeniach prowadzą do użycia opisów probabilistycznych.\nPrzykład: Modele regresji w statystyce, gdzie losowość reprezentuje niewyjaśnioną zmienność lub błąd pomiaru:\n\\[y = β_0 + β_1x + ε\\]\nGdzie:\n\n\\(y\\) to zmienna zależna (np. wielkość popytu na dobro)\n\\(x\\) to zmienna niezależna (np. cena lub dochód konsumenta)\n\\(β_0\\) i \\(β_1\\) to parametry\n\\(ε\\) to składnik błędu, reprezentujący niewyjaśnioną zmienność\n\n\n\n2.7.1.2.2 ii. Kwantowe Modele Stochastyczne\nTe modele zajmują się fundamentalną, nieredukowalną losowością nieodłącznie związaną z systemami mechaniki kwantowej. Ta losowość nie wynika z braku informacji, ale jest podstawową cechą rzeczywistości kwantowej.\nPrzykład: Model Standardowy w fizyce cząstek elementarnych, który opisuje interakcje cząstek za pomocą kwantowej teorii pola. Na przykład, rozpad cząstki jest z natury probabilistyczny:\n\\[P(t) = e^{-t/τ}\\]\nGdzie:\n\n\\(P(t)\\) to prawdopodobieństwo, że cząstka nie rozpadła się po czasie t\n\\(τ\\) to średni czas życia cząstki\n\n\n\n\n2.7.1.3 c. Modele Symulacji Komputerowych\nSymulacje komputerowe wykorzystują algorytmy i metody obliczeniowe oparte na modelach matematycznych do symulowania złożonych systemów i przewidywania ich zachowania w czasie. Mogą być deterministyczne lub stochastyczne.\nPrzykład: Modele klimatyczne symulujące system klimatyczny Ziemi, uwzględniające czynniki takie jak skład atmosfery, prądy oceaniczne i promieniowanie słoneczne do prognozowania przyszłych scenariuszy klimatycznych.\n\n\n\n2.7.2 Modele Koncepcyjne\nModele koncepcyjne to abstrakcyjne reprezentacje systemów lub procesów, często wykorzystujące diagramy lub schematy blokowe do ilustrowania relacji między komponentami.\nPrzykład: Model obiegu wody w naukach o Ziemi, który ilustruje ciągły ruch wody w obrębie Ziemi i atmosfery poprzez procesy takie jak parowanie, opady i spływ powierzchniowy.\n\n\n2.7.3 Modele Fizyczne\nModele fizyczne to namacalne reprezentacje obiektów lub systemów, często w formie pomniejszonej lub uproszczonej wersji rzeczywistego obiektu.\nPrzykład: Modele tunelu aerodynamicznego w badaniach aerodynamiki, używane do badania efektów przepływu powietrza wokół obiektów stałych i optymalizacji projektów samolotów, pojazdów lub budynków.\n\n\n2.7.4 Modele Teoretyczne\nModele teoretyczne to abstrakcyjne ramy oparte na fundamentalnych zasadach i hipotezach, często używane do wyjaśniania obserwowanych zjawisk lub przewidywania nowych. Te modele często wykorzystują równania matematyczne i mogą być deterministyczne lub stochastyczne.\nPrzykład: Teoria ewolucji poprzez dobór naturalny, która dostarcza ram do zrozumienia różnorodności i adaptacji form życia w czasie.\n\n\n2.7.5 Podsumowanie\nTe różne formy modeli odgrywają kluczową rolę w badaniach naukowych, każda oferując unikalne zalety dla zrozumienia i przewidywania zjawisk naturalnych. Naukowcy często używają wielu typów modeli jednocześnie, aby uzyskać kompleksowy wgląd w złożone systemy i procesy.\nWażne jest, aby zdawać sobie sprawę, że te kategorie nie są wzajemnie wykluczające i często się nakładają:\n\nModele matematyczne stanowią podstawę dla wielu innych typów modeli, w tym symulacji komputerowych i niektórych modeli teoretycznych.\nModele symulacji komputerowych są zasadniczo modelami matematycznymi implementowanymi za pomocą metod obliczeniowych i mogą być deterministyczne lub stochastyczne.\nModele teoretyczne często wykorzystują sformułowania matematyczne i mogą być implementowane jako symulacje komputerowe.\nModele fizyczne mogą być projektowane na podstawie modeli matematycznych i mogą być używane do walidacji symulacji komputerowych.\n\nWybór typu modelu często zależy od konkretnego pytania badawczego, natury badanego systemu, dostępnych danych oraz zasobów obliczeniowych. W miarę postępu nauki granice między tymi typami modeli coraz bardziej się zacierają, prowadząc do coraz bardziej wyrafinowanych i interdyscyplinarnych podejść do modelowania złożonych zjawisk.\nKluczowe jest rozróżnienie różnych typów modeli stochastycznych. Klasyczne modele stochastyczne, takie jak te używane w analizie regresji, zajmują się losowością wynikającą z niepełnej informacji lub złożonych interakcji w systemach, które są zasadniczo deterministyczne. Z drugiej strony, kwantowe modele stochastyczne, jak te w fizyce cząstek, zajmują się fundamentalną, nieredukowalną losowością nieodłącznie związaną z systemami mechaniki kwantowej. To rozróżnienie odzwierciedla głębokie różnice między klasycznymi a kwantowymi paradygmatami w fizyce i podkreśla różnorodne sposoby, w jakie prawdopodobieństwo jest wykorzystywane w modelowaniu naukowym.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zrozumienie-pozornych-korelacji-zmiennych-zakłócających-i-kolizyjnych",
    "href": "rozdzial1.html#zrozumienie-pozornych-korelacji-zmiennych-zakłócających-i-kolizyjnych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.8 Zrozumienie Pozornych Korelacji, Zmiennych Zakłócających i Kolizyjnych (*)",
    "text": "2.8 Zrozumienie Pozornych Korelacji, Zmiennych Zakłócających i Kolizyjnych (*)\nW tej sekcji zbadamy trzy ważne pojęcia w analizie statystycznej: pozorne korelacje, zmienne zakłócające i zmienne kolizyjne. Zrozumienie tych pojęć jest kluczowe dla uniknięcia błędnej interpretacji danych i wyciągania nieprawidłowych wniosków z analiz statystycznych.\nZacznijmy od załadowania niezbędnych bibliotek:\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nset.seed(123) # dla powtarzalności\n\n\n2.8.1 Pozorne Korelacje\nPozorne korelacje to związki między zmiennymi, które wydają się przyczynowe, ale w rzeczywistości są przypadkowe lub spowodowane przez niewidoczny trzeci czynnik.\n\n2.8.1.1 Przykład: Sprzedaż lodów a przypadki utonięć\nStwórzmy zbiór danych, który pokazuje pozorną korelację między sprzedażą lodów a przypadkami utonięć:\n\nn &lt;- 100\ndane_pozorne &lt;- tibble(\n  temperatura = rnorm(n, mean = 25, sd = 5),\n  sprzedaz_lodow = 100 + 5 * temperatura + rnorm(n, sd = 10),\n  przypadki_utoniec = 1 + 0.5 * temperatura + rnorm(n, sd = 2)\n)\n\nggplot(dane_pozorne, aes(x = sprzedaz_lodow, y = przypadki_utoniec)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Pozorna Korelacja: Sprzedaż Lodów vs Przypadki Utonięć\",\n       x = \"Sprzedaż Lodów\", y = \"Przypadki Utonięć\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTen wykres pokazuje pozytywną korelację między sprzedażą lodów a przypadkami utonięć. Jednak ta relacja jest pozorna. Prawdziwą przyczyną obu zjawisk jest temperatura:\n\nggplot(dane_pozorne, aes(x = temperatura)) +\n  geom_point(aes(y = sprzedaz_lodow), color = \"blue\") +\n  geom_point(aes(y = przypadki_utoniec * 10), color = \"red\") +\n  geom_smooth(aes(y = sprzedaz_lodow), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(aes(y = przypadki_utoniec * 10), method = \"lm\", se = FALSE, color = \"red\") +\n  scale_y_continuous(\n    name = \"Sprzedaż Lodów\",\n    sec.axis = sec_axis(~./10, name = \"Przypadki Utonięć\")\n  ) +\n  labs(title = \"Temperatura jako Wspólna Przyczyna\",\n       x = \"Temperatura\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n2.8.2 Zmienne Zakłócające\nZmienna zakłócająca to zmienna, która wpływa zarówno na zmienną zależną, jak i niezależną, powodując pozorny związek.\n\n2.8.2.1 Przykład: Edukacja, Dochód i Wiek\nStwórzmy zbiór danych, w którym wiek zakłóca relację między edukacją a dochodem:\n\nlibrary(tidyverse)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nn &lt;- 1000\nconfounder_data &lt;- tibble(\n  age = runif(n, 25, 65),\n  education = round(10 + 0.1 * age + rnorm(n, sd = 2)),\n  income = 20000 + 1000 * education + 500 * age + rnorm(n, sd = 5000)\n)\n\n# Without controlling for age\nmodel_naive &lt;- lm(income ~ education, data = confounder_data)\n# Controlling for age\nmodel_adjusted &lt;- lm(income ~ education + age, data = confounder_data)\n\n# Create age groups for visualization\nconfounder_data &lt;- confounder_data %&gt;%\n  mutate(age_group = cut(age, breaks = 3, labels = c(\"Young\", \"Middle\", \"Old\")))\n\n# Visualize\nggplot(confounder_data, aes(x = education, y = income)) +\n  geom_point(aes(color = age), alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1.2) +\n  geom_smooth(aes(group = age_group, color = as.numeric(age_group)), \n              method = \"lm\", se = FALSE, linewidth = 1) +\n  scale_color_viridis_c(name = \"Age\", \n                        breaks = c(30, 45, 60), \n                        labels = c(\"Young\", \"Middle\", \"Old\")) +\n  labs(title = \"Education vs Income, Confounded by Age\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPorównajmy współczynniki:\n\nsummary(model_naive)$coefficients[\"education\", \"Estimate\"]\n\n[1] 2328.718\n\nsummary(model_adjusted)$coefficients[\"education\", \"Estimate\"]\n\n[1] 1101.783\n\n\nEfekt edukacji na dochód jest przeszacowany, gdy nie kontrolujemy wieku.\n\n\n\n2.8.3 Zmienne Kolizyjne\nZmienna kolizyjna to zmienna, na którą wpływają zarówno zmienna niezależna, jak i zmienna zależna. Kontrolowanie zmiennej kolizyjnej może wprowadzić pozorną korelację.\n\n2.8.3.1 Przykład: Satysfakcja z pracy, Wynagrodzenie i Równowaga między pracą a życiem prywatnym\nStwórzmy zbiór danych, w którym równowaga między pracą a życiem prywatnym jest zmienną kolizyjną między satysfakcją z pracy a wynagrodzeniem:\n\nn &lt;- 1000\ndane_kolizyjne &lt;- tibble(\n  satysfakcja_z_pracy = rnorm(n),\n  wynagrodzenie = rnorm(n),\n  rownowaga_praca_zycie = -0.5 * satysfakcja_z_pracy - 0.5 * wynagrodzenie + rnorm(n, sd = 0.5)\n)\n\n# Bez kontrolowania równowagi praca-życie\nmodel_poprawny &lt;- lm(wynagrodzenie ~ satysfakcja_z_pracy, data = dane_kolizyjne)\n\n# Błędne kontrolowanie równowagi praca-życie\nmodel_kolizyjny &lt;- lm(wynagrodzenie ~ satysfakcja_z_pracy + rownowaga_praca_zycie, data = dane_kolizyjne)\n\n# Wizualizacja\nggplot(dane_kolizyjne, aes(x = satysfakcja_z_pracy, y = wynagrodzenie, color = rownowaga_praca_zycie)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_color_viridis_c() +\n  labs(title = \"Satysfakcja z Pracy vs Wynagrodzenie, Równowaga Praca-Życie jako Zmienna Kolizyjna\",\n       x = \"Satysfakcja z Pracy\", y = \"Wynagrodzenie\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPorównajmy współczynniki:\n\nsummary(model_poprawny)$coefficients[\"satysfakcja_z_pracy\", \"Estimate\"]\n\n[1] 0.02063487\n\nsummary(model_kolizyjny)$coefficients[\"satysfakcja_z_pracy\", \"Estimate\"]\n\n[1] -0.4794016\n\n\nKontrolowanie zmiennej kolizyjnej (równowaga praca-życie) wprowadza pozorną korelację między satysfakcją z pracy a wynagrodzeniem.\n\n\n\n2.8.4 Podsumowanie\nZrozumienie pozornych korelacji, zmiennych zakłócających i kolizyjnych jest kluczowe dla prawidłowej analizy statystycznej i wnioskowania przyczynowego. Zawsze rozważ podstawową strukturę przyczynową swoich danych i bądź ostrożny w kwestii tego, które zmienne kontrolujesz w swoich analizach.\n\n\n2.8.5 Dalsza Lektura\n\nPearl, J. (2009). Causality: Models, Reasoning, and Inference. Cambridge University Press.\nHernán, M. A., & Robins, J. M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#etyczne-aspekty-w-analizie-danych-nauk-społecznych",
    "href": "rozdzial1.html#etyczne-aspekty-w-analizie-danych-nauk-społecznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.9 Etyczne Aspekty w Analizie Danych Nauk Społecznych",
    "text": "2.9 Etyczne Aspekty w Analizie Danych Nauk Społecznych\nEtyka odgrywa kluczową rolę w badaniach nauk społecznych:\n\nPrywatność i Zgoda: Zapewnienie prywatności uczestników i świadomej zgody\nOchrona Danych: Bezpieczne przechowywanie i zarządzanie wrażliwymi danymi osobowymi\nBłędy i Reprezentacja: Adresowanie błędów próbkowania i zapewnienie różnorodnej reprezentacji\nPrzejrzystość: Jasne komunikowanie metod badawczych i ograniczeń\nWpływ Społeczny: Rozważanie potencjalnych społecznych implikacji wyników badań\n\n\n\n\n\n\n\nWarning\n\n\n\nNaukowcy społeczni muszą starannie rozważyć etyczne implikacje swoich praktyk zbierania, analizy i rozpowszechniania danych.\n\n\n\n2.9.1 Kluczowe Wnioski\n\nNauka o danych w naukach społecznych bazuje na tradycyjnych metodach statystycznych, włączając nowe technologie do analizy złożonych zjawisk społecznych.\nZrozumienie koncepcji takich jak populacja, próba i procesy generowania danych jest kluczowe dla prawidłowych badań w naukach społecznych.\nProces nauki o danych w badaniach społecznych obejmuje wiele etapów, od etycznego zbierania danych po komunikację wniosków.\nR jest potężnym narzędziem do analizy danych w naukach społecznych, oferującym szeroki zakres możliwości.\nAspekty etyczne powinny być na pierwszym planie każdego projektu związanego z danymi w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-a-losowość-klasyczna-a-kwantowa-zrozumienie-fundamentalnych-różnic",
    "href": "rozdzial1.html#appendix-a-losowość-klasyczna-a-kwantowa-zrozumienie-fundamentalnych-różnic",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.10 Appendix A: Losowość Klasyczna a Kwantowa: Zrozumienie Fundamentalnych Różnic",
    "text": "2.10 Appendix A: Losowość Klasyczna a Kwantowa: Zrozumienie Fundamentalnych Różnic\nAby zrozumieć, jak losowość w mechanice kwantowej różni się od losowości reprezentowanej przez składnik błędu w modelach regresji, musimy przeanalizować ich pochodzenie, naturę i implikacje.\n\n2.10.1 Pochodzenie Losowości\n\n2.10.1.1 Losowość Klasyczna (Modele Regresji)\n\nŹródło: Niekompletna informacja lub złożone interakcje w systemie, który w zasadzie jest deterministyczny.\nNatura: Niepewność epistemiczna (wynikająca z braku wiedzy).\nPrzykład: W modelu regresji, \\(y = β_0 + β_1x + ε\\), składnik błędu ε reprezentuje niewyjaśnioną zmienność.\n\n\n\n2.10.1.2 Losowość Kwantowa\n\nŹródło: Fundamentalna właściwość systemów kwantowych.\nNatura: Niepewność ontyczna (nieodłączna cecha systemu, nie wynika z braku wiedzy).\nPrzykład: Dokładny moment rozpadu atomu radioaktywnego nie może być przewidziany, można określić jedynie jego prawdopodobieństwo.\n\n\n\n\n2.10.2 Implikacje Filozoficzne\n\n2.10.2.1 Losowość Klasyczna\n\nDeterminizm: Podstawowa rzeczywistość jest deterministyczna; losowość odzwierciedla naszą niewiedzę.\nUkryte Zmienne: W zasadzie, gdybyśmy mieli pełną informację, moglibyśmy dokładnie przewidzieć wyniki.\n\n\n\n2.10.2.2 Losowość Kwantowa\n\nIndeterminizm: Losowość jest fundamentalną cechą rzeczywistości, nie tylko naszego jej opisu.\nBrak Ukrytych Zmiennych: Nawet przy pełnej informacji o systemie kwantowym, niektóre wyniki pozostają nieprzewidywalne (co sugeruje twierdzenie Bella).\n\n\n\n\n2.10.3 Ujęcie Matematyczne\n\n2.10.3.1 Losowość Klasyczna\n\nTeoria Prawdopodobieństwa: Oparta na klasycznej teorii prawdopodobieństwa.\nRozkład: Często zakłada się znane rozkłady (np. rozkład normalny w wielu modelach regresji).\nCentralne Twierdzenie Graniczne: Stosuje się do dużych prób zmiennych losowych.\n\n\n\n2.10.3.2 Losowość Kwantowa\n\nPrawdopodobieństwo Kwantowe: Oparte na matematycznych podstawach mechaniki kwantowej.\nFunkcja Falowa: Opisuje stan kwantowy i jego ewolucję.\nReguła Borna: Określa prawdopodobieństwa wyników pomiarów na podstawie funkcji falowej.\n\n\n\n\n2.10.4 Przewidywalność i Kontrola\n\n2.10.4.1 Losowość Klasyczna\n\nRedukowalna: W zasadzie można ją zmniejszyć, zbierając więcej danych lub poprawiając dokładność pomiarów.\nKontrolowalna: Błędy systematyczne można zidentyfikować i skorygować.\n\n\n\n2.10.4.2 Losowość Kwantowa\n\nNieredukowalna: Nie można jej wyeliminować nawet przy idealnych pomiarach.\nFundamentalnie Niekontrolowalna: Sam akt pomiaru wpływa na system (problem pomiaru).\n\n\n\n\n2.10.5 Praktyczne Implikacje\n\n2.10.5.1 Losowość Klasyczna\n\nRedukcja Błędów: Koncentracja na udoskonalaniu technik pomiarowych i zbierania danych.\nUdoskonalanie Modelu: Dążenie do wyjaśnienia większej wariancji i zmniejszenia składnika błędu.\n\n\n\n2.10.5.2 Losowość Kwantowa\n\nNieodłączne Ograniczenie: Akceptacja fundamentalnych granic przewidywalności.\nPrzewidywania Probabilistyczne: Skupienie na dokładnych rozkładach prawdopodobieństwa zamiast na dokładnych wynikach.\n\n\n\n\n2.10.6 Przykłady Pomagające Zrozumieć Różnicę\n\n2.10.6.1 Przykład Losowości Klasycznej\nWyobraź sobie rzut monetą. Fizyka klasyczna mówi, że wynik jest zdeterminowany przez warunki początkowe (przyłożona siła, opór powietrza itp.). “Losowość” wynika z naszej niezdolności do precyzyjnego zmierzenia i uwzględnienia wszystkich tych czynników.\n\n\n2.10.6.2 Przykład Losowości Kwantowej\nW eksperymencie z podwójną szczeliną pojedyncze cząstki wykazują wzory interferencyjne, jakby przechodziły przez obie szczeliny jednocześnie. Dokładna ścieżka każdej pojedynczej cząstki jest fundamentalnie nieokreślona do momentu pomiaru, a tej nieokreśloności nie można rozwiązać przez bardziej precyzyjne pomiary.\n\n\n\n2.10.7 Podsumowanie\nChociaż oba rodzaje losowości prowadzą do probabilistycznych przewidywań, ich fundamentalne natury są zupełnie różne:\n\nLosowość klasyczna w modelach regresji jest odzwierciedleniem naszej niepełnej wiedzy lub ograniczeń pomiarowych w systemie, który w zasadzie jest deterministyczny.\nLosowość kwantowa jest fundamentalną właściwością systemów kwantowych, reprezentującą nieodłączną nieokreśloność w naturze, która utrzymuje się nawet przy doskonałej wiedzy i pomiarze.\n\nZrozumienie tych różnic jest kluczowe dla prawidłowej interpretacji i stosowania modeli statystycznych w różnych kontekstach naukowych, od nauk społecznych wykorzystujących analizę regresji po eksperymenty z fizyki kwantowej.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-b-duże-modele-językowe---zrozumienie-ich-stochastycznej-natury",
    "href": "rozdzial1.html#appendix-b-duże-modele-językowe---zrozumienie-ich-stochastycznej-natury",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.11 Appendix B: Duże Modele Językowe - Zrozumienie Ich Stochastycznej Natury",
    "text": "2.11 Appendix B: Duże Modele Językowe - Zrozumienie Ich Stochastycznej Natury\nDuże Modele Językowe (LLM), takie jak GPT-3, BERT i Claude, zrewolucjonizowały przetwarzanie języka naturalnego, ale mogą popełniać zagadkowe błędy, szczególnie w zadaniach matematycznych. Ten dodatek wyjaśnia funkcjonowanie LLM, ich stochastyczną naturę i porównuje je z klasycznymi modelami statystycznymi.\n\n2.11.1 Podstawy LLM i Ich Stochastyczna Natura\nLLM są trenowane na ogromnych zbiorach danych tekstowych, aby przewidywać rozkład prawdopodobieństwa następnego tokenu w sekwencji. Wykorzystują architektury transformerowe do przetwarzania i generowania tekstu. Kluczowe aspekty ich stochastycznej natury obejmują:\n\nProbabilistyczny wybór tokenów: LLM wybierają każde słowo na podstawie obliczonych prawdopodobieństw, a nie stałych reguł.\nLosowość kontrolowana temperaturą: Parametr “temperatury” dostosowuje losowość wyborów, równoważąc kreatywność i spójność.\nNiedeterministyczne wyniki: Te same dane wejściowe mogą prowadzić do różnych wyników w oddzielnych uruchomieniach.\nKontekstowa niejednoznaczność: LLM interpretują kontekst probabilistycznie, co czasami prowadzi do nieporozumień.\n\n\n\n2.11.2 Porównanie z Klasycznymi Modelami Statystycznymi\nAby lepiej zrozumieć LLM, porównajmy je z regresją Najmniejszych Kwadratów (OLS):\n\n\n\n\n\n\n\n\nAspekt\nRegresja OLS\nDuże Modele Językowe\n\n\n\n\nPodstawowa funkcja\nPrzewiduje ciągłe wyniki na podstawie zmiennych wejściowych\nPrzewiduje rozkład prawdopodobieństwa następnego tokenu na podstawie poprzednich tokenów\n\n\nWejście-Wyjście\nZmienne ciągłe, relacje liniowe\nDyskretne tokeny, relacje nieliniowe\n\n\nTyp predykcji\nPredykcje punktowe z przedziałami ufności\nRozkłady prawdopodobieństwa dla możliwych tokenów\n\n\nZłożoność modelu\nNiewiele parametrów\nMiliardy parametrów\n\n\nInterpretowalność\nJasne interpretacje współczynników\nLargely nieprzejrzyste działanie wewnętrzne\n\n\nObsługa szumu\nZakłada losowy szum w zmiennej wynikowej\nRadzi sobie ze zmiennością języka naturalnego\n\n\nEkstrapolacja\nMniej wiarygodna poza zakresem treningu\nMniej wiarygodna dla nieznanych tematów\n\n\n\nOba modele dążą do nauczenia się mapowania wejścia-wyjścia na podstawie wzorców w danych treningowych.\n\n\n2.11.3 Implikacje dla Zadań Matematycznych\nStochastyczna natura LLM wpływa na operacje matematyczne:\n\nZmienne wyniki dla powtarzanych obliczeń: Każda próba może dać inny wynik ze względu na probabilistyczny wybór tokenów.\nPewność nie gwarantuje poprawności: Wysoka pewność modelu może wystąpić nawet dla niepoprawnych odpowiedzi.\nAproksymacja zamiast dokładnych obliczeń: LLM dopasowują wzorce zamiast wykonywać precyzyjne obliczenia.\n\nOgraniczenia w zadaniach matematycznych wynikają z:\n\nNiedopasowania celu treningu: LLM są trenowane do przewidywania języka, nie dokładności matematycznej.\nBraku jawnego rozumowania matematycznego: Nie mają wbudowanych reguł czy operacji matematycznych.\nBraku pamięci roboczej: LLM nie mogą niezawodnie przechowywać i manipulować wynikami pośrednimi.\nOgraniczonego okna kontekstowego: Mogą tracić istotne informacje w długich problemach.\nOgraniczeń danych treningowych: Niedoreprezentowanie pewnych koncepcji matematycznych może prowadzić do słabych wyników.\nBraku kontroli spójności: LLM nie weryfikują logicznej spójności swoich wyników.\n\n\n\n2.11.4 Najlepsze Praktyki i Wnioski\nPrzy korzystaniu z LLM do zadań matematycznych:\n\nSkup się na wyjaśnieniach koncepcyjnych, nie na dokładnych obliczeniach: LLM doskonale wyjaśniają koncepcje, ale mogą zawodzić w dokładnych obliczeniach.\nWeryfikuj wyniki dedykowanym oprogramowaniem: Zawsze sprawdzaj obliczenia LLM odpowiednimi narzędziami matematycznymi.\nRozbijaj złożone problemy: Podział zadań na mniejsze kroki może poprawić wydajność LLM.\nBądź świadomy efektów przeformułowania: Różne sformułowania tego samego problemu mogą dawać różne wyniki.\nUżywaj jako narzędzi wspomagających, nie zamienników dla ekspertyzy: LLM powinny uzupełniać, a nie zastępować wiedzę matematyczną.\n\nZrozumienie probabilistycznej natury LLM pomaga wykorzystać ich mocne strony w zadaniach językowych, jednocześnie uznając ich ograniczenia w dziedzinach wymagających deterministycznej precyzji, takich jak matematyka.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-c-modele-deterministyczne-a-modele-stochastyczne",
    "href": "rozdzial1.html#appendix-c-modele-deterministyczne-a-modele-stochastyczne",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.12 Appendix C: Modele Deterministyczne a Modele Stochastyczne (*)",
    "text": "2.12 Appendix C: Modele Deterministyczne a Modele Stochastyczne (*)\n\n2.12.1 Modele Deterministyczne\nModele deterministyczne to te, w których wynik jest w pełni określony przez wartości parametrów i warunki początkowe. Modele te są często używane w fizyce i inżynierii.\n\n\n2.12.2 Przykład: Ruch Jednostajnie Przyspieszony\nKlasycznym przykładem modelu deterministycznego jest ruch jednostajnie przyspieszony, opisany równaniem:\n\\[x(t) = x_0 + v_0t + \\frac{1}{2}at^2\\]\nGdzie:\n\n\\(x(t)\\) to położenie w czasie \\(t\\)\n\\(x_0\\) to położenie początkowe\n\\(v_0\\) to prędkość początkowa\n\\(a\\) to przyspieszenie\n\\(t\\) to czas\n\nZasymulujmy to w R:\n\n# Ruch jednostajnie przyspieszony\nsymuluj_ruch_przyspieszony &lt;- function(x0, v0, a, t) {\n  x0 + v0 * t + 0.5 * a * t^2\n}\n\n# Generowanie danych\nt &lt;- seq(0, 10, by = 0.1)\nx &lt;- symuluj_ruch_przyspieszony(x0 = 0, v0 = 2, a = 1, t = t)\n\n# Wykres\nplot(t, x, type = \"l\", xlab = \"Czas\", ylab = \"Położenie\", \n     main = \"Ruch Jednostajnie Przyspieszony\")\n\n\n\n\n\n\n\n\nTen kod wygeneruje wykres ruchu jednostajnie przyspieszonego, który jest intuicyjnym przykładem z dynamiki Newtona. W tym przypadku obiekt zaczyna ruch z początkową prędkością i przyspiesza jednostajnie, co prowadzi do parabolicznej trajektorii na wykresie położenia w funkcji czasu.\n\n\n2.12.3 Modele Stochastyczne w Naukach Społecznych\nModele stochastyczne uwzględniają losowość i są często używane w naukach społecznych, gdzie istnieje nieodłączna niepewność w badanych systemach.\n\n\n2.12.4 Przykład: Regresja Metodą Najmniejszych Kwadratów (OLS)\nOLS to podstawowy model stochastyczny w naukach społecznych. Jest reprezentowany jako:\n\\[Y = \\beta_0 + \\beta_1X + \\epsilon\\]\nGdzie:\n\n\\(Y\\) to zmienna zależna\n\\(X\\) to zmienna niezależna\n\\(\\beta_0\\) i \\(\\beta_1\\) to parametry\n\\(\\epsilon\\) to składnik błędu (komponent stochastyczny)\n\nZademonstrujmy OLS w R:\n\n# Generowanie przykładowych danych\nset.seed(123)\nX &lt;- rnorm(100)\nY &lt;- 2 + 3*X + rnorm(100, sd = 0.5)\n\n# Dopasowanie modelu OLS\nmodel &lt;- lm(Y ~ X)\n\n# Podsumowanie modelu\nsummary(model)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95367 -0.34175 -0.04375  0.29032  1.64520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.94860    0.04878   39.95   &lt;2e-16 ***\nX            2.97376    0.05344   55.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4854 on 98 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.969 \nF-statistic:  3097 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n# Wykres\nplot(X, Y, main = \"Regresja OLS\")\nabline(model, col = \"red\")\n\n\n\n\n\n\n\n\nTo dopasuje model OLS do symulowanych danych i wykreśli wyniki.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff\n\n\n\n\n2.12.5 Zaawansowane Modele Stochastyczne: Duże Modele Językowe\nDuże Modele Językowe (LLM), takie jak GPT-3, to złożone modele stochastyczne używane w przetwarzaniu języka naturalnego. Chociaż nie możemy zaimplementować pełnego LLM w tym tutorialu, możemy omówić jego zasady.\nLLM opierają się na architekturze transformatora i wykorzystują mechanizmy samouwagi. Są trenowane na ogromnych ilościach danych tekstowych i uczą się przewidywać następny token w sekwencji.\nRdzeń LLM można postrzegać jako warunkowy rozkład prawdopodobieństwa:\n\\[P(x_t | x_{&lt;t}, \\theta)\\]\nGdzie:\n\n\\(x_t\\) to aktualny token\n\\(x_{&lt;t}\\) reprezentuje wszystkie poprzednie tokeny\n\\(\\theta\\) to parametry modelu\n\n\n\n\n\n\n\nNote\n\n\n\nTokeny w Dużych Modelach Językowych (LLM) to podstawowe jednostki tekstu, które model przetwarza. Można je postrzegać jako części słów lub znaki interpunkcyjne. Oto kluczowe informacje o tokenach:\nDefinicja: Tokeny to najmniejsze jednostki tekstu, które LLM przetwarza. Mogą to być całe słowa, części słów, a nawet pojedyncze znaki lub znaki interpunkcyjne. Tokenizacja: Proces dzielenia tekstu na tokeny nazywa się tokenizacją. LLM używają specyficznych algorytmów do wykonania tego zadania. Przykłady:\nSłowo “kot” może być pojedynczym tokenem. Dłuższe słowo jak “zrozumienie” może być podzielone na wiele tokenów, np. “zrozum” i “ienie”. Znaki interpunkcyjne jak “.” czy “?” są często oddzielnymi tokenami. Powszechne przedrostki lub przyrostki mogą być własnymi tokenami.\nSłownictwo: LLM mają ustalone słownictwo tokenów, które rozpoznają. To słownictwo zazwyczaj obejmuje od dziesiątek tysięcy do setek tysięcy tokenów. Znaczenie: Sposób tokenizacji tekstu może wpływać na to, jak model rozumie i generuje język. Jest to szczególnie ważne przy obsłudze różnych języków, rzadkich słów lub specjalistycznego słownictwa. Kontekst: W równaniu dla LLM: \\[P(x_t | x_{&lt;t}, \\theta)\\] Gdzie:\n\\(x_t\\) reprezentuje bieżący token \\(x_{&lt;t}\\) reprezentuje wszystkie poprzednie tokeny w sekwencji \\(\\theta\\) reprezentuje parametry modelu\n\n\nW przeciwieństwie do modeli deterministycznych, LLM produkują różne wyniki nawet dla tego samego wejścia ze względu na ich stochastyczną naturę.\n\n\n2.12.6 Podsumowanie\nKażdy rodzaj modelu ma swoje miejsce w nauce, w zależności od badanego systemu i poziomu niepewności.\nPamiętaj, że wybór między modelami deterministycznymi a stochastycznymi często zależy od natury badanego systemu i pytań, na które próbujesz odpowiedzieć. Modele deterministyczne są świetne dla systemów o dobrze zrozumiałej mechanice, podczas gdy modele stochastyczne sprawdzają się przy radzeniu sobie z nieodłączną losowością lub złożonymi, nie w pełni zrozumiałymi systemami.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-d-wprowadzenie-do-r-rstudio-i-tidyverse",
    "href": "rozdzial1.html#appendix-d-wprowadzenie-do-r-rstudio-i-tidyverse",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.13 Appendix D: Wprowadzenie do R, RStudio i tidyverse",
    "text": "2.13 Appendix D: Wprowadzenie do R, RStudio i tidyverse\nR to potężny język programowania i środowisko do obliczeń statystycznych i grafiki. Jest szeroko stosowany w środowisku akademickim, szczególnie w naukach społecznych, do analizy danych i wizualizacji.\n\n2.13.0.1 Kluczowe cechy R:\n\nOtwarty kod źródłowy i darmowy\nRozbudowany ekosystem pakietów\nSilne wsparcie społeczności\nDoskonały do analizy statystycznej i wizualizacji danych\n\n\n\n2.13.1 Pierwsze kroki z RStudio\nRStudio to zintegrowane środowisko programistyczne (IDE) dla R, które ułatwia pracę z R.\n\n2.13.1.1 Instalacja R i RStudio\n\nPobierz i zainstaluj R ze strony CRAN\nPobierz i zainstaluj RStudio ze strony RStudio\n\n\n\n2.13.1.2 Interfejs RStudio\nRStudio ma cztery główne panele:\n\nEdytor źródłowy: Gdzie piszesz i edytujesz skrypty R\nKonsola: Gdzie możesz wpisywać polecenia R i widzieć wyniki\nŚrodowisko/Historia: Pokazuje wszystkie obiekty w twoim obszarze roboczym i historię poleceń\nPliki/Wykresy/Pakiety/Pomoc: Wielofunkcyjny panel do zarządzania plikami, przeglądania wykresów, zarządzania pakietami i dostępu do pomocy\n\n\n\n2.13.1.3 Podstawowe funkcje RStudio\n\nTworzenie nowego skryptu R: Plik &gt; Nowy plik &gt; Skrypt R\nUruchamianie kodu: Zaznacz kod i naciśnij Ctrl+Enter (Cmd+Enter na Macu)\nInstalowanie pakietów: Narzędzia &gt; Instaluj pakiety\nUzyskiwanie pomocy: Wpisz ?nazwa_funkcji w konsoli\n\n\n\n\n2.13.2 Podstawy R\n\n2.13.2.1 Typy danych w R\n\n# Numeryczny\nx &lt;- 10.5\nclass(x)\n\n[1] \"numeric\"\n\n# Całkowity\ny &lt;- 1L\nclass(y)\n\n[1] \"integer\"\n\n# Znakowy\nimie &lt;- \"Alicja\"\nclass(imie)\n\n[1] \"character\"\n\n# Logiczny\njest_studentem &lt;- TRUE\nclass(jest_studentem)\n\n[1] \"logical\"\n\n\n\n\n2.13.2.2 Struktury danych\n\n2.13.2.2.1 Wektory\n\n# Tworzenie wektora\nliczby &lt;- c(1, 2, 3, 4, 5)\nowoce &lt;- c(\"jabłko\", \"banan\", \"wiśnia\")\n\n# Operacje na wektorach\nliczby + 2\n\n[1] 3 4 5 6 7\n\nliczby * 2\n\n[1]  2  4  6  8 10\n\nmean(liczby)\n\n[1] 3\n\nlength(owoce)\n\n[1] 3\n\n\n\n\n2.13.2.2.2 Macierze\n\n# Tworzenie macierzy\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\nprint(m)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n# Operacje na macierzach\nt(m)  # transpozycja\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nm * 2  # mnożenie skalarne\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    4    8   12\n\n\n\n\n2.13.2.2.3 Ramki danych\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(\n  imie = c(\"Alicja\", \"Bartek\", \"Celina\"),\n  wiek = c(25, 30, 35),\n  student = c(TRUE, FALSE, TRUE)\n)\nprint(df)\n\n    imie wiek student\n1 Alicja   25    TRUE\n2 Bartek   30   FALSE\n3 Celina   35    TRUE\n\n# Dostęp do elementów ramki danych\ndf$imie\n\n[1] \"Alicja\" \"Bartek\" \"Celina\"\n\ndf[1, 2]\n\n[1] 25\n\ndf[df$wiek &gt; 25, ]\n\n    imie wiek student\n2 Bartek   30   FALSE\n3 Celina   35    TRUE\n\n\n\n\n\n2.13.2.3 Funkcje\n\n# Definiowanie funkcji\npowitaj &lt;- function(imie) {\n  paste(\"Cześć,\", imie, \"!\")\n}\n\n# Użycie funkcji\npowitaj(\"Alicja\")\n\n[1] \"Cześć, Alicja !\"\n\n# Funkcja z wieloma argumentami\noblicz_bmi &lt;- function(waga, wzrost) {\n  bmi &lt;- waga / (wzrost^2)\n  return(bmi)\n}\n\noblicz_bmi(70, 1.75)\n\n[1] 22.85714\n\n\n\n\n2.13.2.4 Struktury kontrolne\n\n# Instrukcja if-else\nx &lt;- 10\nif (x &gt; 5) {\n  print(\"x jest większe niż 5\")\n} else {\n  print(\"x nie jest większe niż 5\")\n}\n\n[1] \"x jest większe niż 5\"\n\n# Pętla for\nfor (i in 1:5) {\n  print(paste(\"Iteracja\", i))\n}\n\n[1] \"Iteracja 1\"\n[1] \"Iteracja 2\"\n[1] \"Iteracja 3\"\n[1] \"Iteracja 4\"\n[1] \"Iteracja 5\"\n\n# Pętla while\nlicznik &lt;- 1\nwhile (licznik &lt;= 5) {\n  print(paste(\"Licznik:\", licznik))\n  licznik &lt;- licznik + 1\n}\n\n[1] \"Licznik: 1\"\n[1] \"Licznik: 2\"\n[1] \"Licznik: 3\"\n[1] \"Licznik: 4\"\n[1] \"Licznik: 5\"\n\n\n\n\n\n2.13.3 Wprowadzenie do tidyverse\nTidyverse to kolekcja pakietów R zaprojektowanych do nauki o danych. Te pakiety mają wspólną filozofię i są zaprojektowane do bezproblemowej współpracy.\n\n2.13.3.1 Kluczowe pakiety tidyverse\n\nggplot2: do wizualizacji danych\ndplyr: do manipulacji danymi\ntidyr: do porządkowania danych\nreadr: do odczytu danych prostokątnych\npurrr: do programowania funkcyjnego\ntibble: nowoczesne ujęcie ramek danych\n\n\n\n2.13.3.2 Rozpoczęcie pracy z tidyverse\n\n# Instalacja tidyverse (uruchom raz)\n# install.packages(\"tidyverse\")\n\n# Wczytanie tidyverse\nlibrary(tidyverse)\n\n\n\n2.13.3.3 Import danych z readr\n\n# Odczyt plików CSV\ndane &lt;- read_csv(\"dane_spoleczne.csv\")\n\n# Odczyt innych formatów plików\nread_tsv(\"dane.tsv\")  # Wartości oddzielone tabulatorem\nread_delim(\"dane.txt\", delim = \"|\")  # Niestandardowy separator\n\n\n\n2.13.3.4 Manipulacja danymi z dplyr\n\n# Użyjmy wbudowanego zbioru danych mtcars\ndata(\"mtcars\")\n\n# Wybieranie kolumn\nmtcars %&gt;% \n  select(mpg, cyl, hp)\n\n                     mpg cyl  hp\nMazda RX4           21.0   6 110\nMazda RX4 Wag       21.0   6 110\nDatsun 710          22.8   4  93\nHornet 4 Drive      21.4   6 110\nHornet Sportabout   18.7   8 175\nValiant             18.1   6 105\nDuster 360          14.3   8 245\nMerc 240D           24.4   4  62\nMerc 230            22.8   4  95\nMerc 280            19.2   6 123\nMerc 280C           17.8   6 123\nMerc 450SE          16.4   8 180\nMerc 450SL          17.3   8 180\nMerc 450SLC         15.2   8 180\nCadillac Fleetwood  10.4   8 205\nLincoln Continental 10.4   8 215\nChrysler Imperial   14.7   8 230\nFiat 128            32.4   4  66\nHonda Civic         30.4   4  52\nToyota Corolla      33.9   4  65\nToyota Corona       21.5   4  97\nDodge Challenger    15.5   8 150\nAMC Javelin         15.2   8 150\nCamaro Z28          13.3   8 245\nPontiac Firebird    19.2   8 175\nFiat X1-9           27.3   4  66\nPorsche 914-2       26.0   4  91\nLotus Europa        30.4   4 113\nFord Pantera L      15.8   8 264\nFerrari Dino        19.7   6 175\nMaserati Bora       15.0   8 335\nVolvo 142E          21.4   4 109\n\n# Filtrowanie wierszy\nmtcars %&gt;% \n  filter(cyl == 4)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n# Sortowanie danych\nmtcars %&gt;% \n  arrange(desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n# Tworzenie nowych zmiennych\nmtcars %&gt;% \n  mutate(kpl = mpg * 0.425)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb     kpl\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  8.9250\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  8.9250\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1  9.6900\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  9.0950\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  7.9475\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  7.6925\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  6.0775\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 10.3700\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  9.6900\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  8.1600\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  7.5650\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  6.9700\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  7.3525\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  6.4600\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  4.4200\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  4.4200\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  6.2475\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 13.7700\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 12.9200\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 14.4075\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  9.1375\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  6.5875\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  6.4600\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  5.6525\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  8.1600\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 11.6025\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 11.0500\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 12.9200\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  6.7150\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  8.3725\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  6.3750\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2  9.0950\n\n# Podsumowywanie danych\nmtcars %&gt;% \n  group_by(cyl) %&gt;% \n  summarize(srednie_mpg = mean(mpg),\n            liczba = n())\n\n# A tibble: 3 × 3\n    cyl srednie_mpg liczba\n  &lt;dbl&gt;       &lt;dbl&gt;  &lt;int&gt;\n1     4        26.7     11\n2     6        19.7      7\n3     8        15.1     14\n\n\n\n\n2.13.3.5 Wizualizacja danych z ggplot2\n\n# Wykres rozrzutu\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Waga samochodu vs. Zużycie paliwa\",\n       x = \"Waga (1000 funtów)\",\n       y = \"Mile na galon\")\n\n\n\n\nWaga samochodu vs. Zużycie paliwa\n\n\n\n\n\n# Wykres słupkowy\nmtcars %&gt;% \n  count(cyl) %&gt;% \n  ggplot(aes(x = factor(cyl), y = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Liczba samochodów według liczby cylindrów\",\n       x = \"Liczba cylindrów\",\n       y = \"Liczba\")\n\n\n\n\nLiczba samochodów według liczby cylindrów\n\n\n\n\n\n# Wykres pudełkowy\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Zużycie paliwa według liczby cylindrów\",\n       x = \"Liczba cylindrów\",\n       y = \"Mile na galon\")\n\n\n\n\nZużycie paliwa według liczby cylindrów\n\n\n\n\n\n\n\n2.13.4 Dodatkowe zasoby\n\nR for Data Science\nDokumentacja tidyverse\nŚciągawki RStudio\nPrzewodnik Quarto\nR Cookbook\n\nPamiętaj, aby eksperymentować z kodem, modyfikować przykłady i nie wahaj się korzystać z wbudowanego systemu pomocy R (dostępnego przez wpisanie ?nazwa_funkcji w konsoli), gdy napotkasz nieznane funkcje lub koncepcje.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1 Foundations in Number Sets\nBefore diving into data types, it’s essential to understand the basic number sets that form the foundation of our understanding of data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#foundations-in-number-sets",
    "href": "chapter2.html#foundations-in-number-sets",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1.1 Basic Number Sets\n\nNatural Numbers (ℕ): The counting numbers {1, 2, 3, …}\nIntegers (ℤ): Includes natural numbers, their negatives, and zero {…, -2, -1, 0, 1, 2, …}\nRational Numbers (ℚ): Numbers that can be expressed as a fraction of two integers\nReal Numbers (ℝ): All numbers on the number line, including rationals and irrationals\n\n\n\n3.1.2 Properties of Sets\n\nCountable Sets: Sets whose elements can be put in a one-to-one correspondence with the natural numbers. For example, the set of integers is countable.\nUncountable Sets: Sets that are not countable. The set of real numbers is uncountable.\nDiscrete Sets: Sets where each element is separated from other elements by a finite gap. The integers form a discrete set.\nDense Sets: Sets where between any two elements, there is always another element of the set. The rational numbers and real numbers are dense sets.\n\n\n\n\n\n\n\nNote\n\n\n\nUnderstanding these set properties is crucial for grasping the nature of different data types in social sciences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#discrete-vs.-continuous-data",
    "href": "chapter2.html#discrete-vs.-continuous-data",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.2 Discrete vs. Continuous Data",
    "text": "3.2 Discrete vs. Continuous Data\nIn data science and statistics, we often categorize variables as either discrete or continuous. However, the distinction is not always clear-cut, and some variables exhibit characteristics of both types. This document explores the concepts of discrete and continuous data, their differences, and the interesting case of variables that can be treated as both.\n\n\n3.2.1 Discrete Data\nDiscrete data can only take on specific, countable values. These values are often (but not always) integers.\n\n3.2.1.1 Characteristics of Discrete Data:\n\nCountable\nOften represented by integers\nCan be finite or infinite\nNo values between two adjacent data points\n\n\n\n3.2.1.2 Examples:\n\nNumber of students in a class\nNumber of cars sold by a dealership\nShoe sizes\n\n\n\n\n3.2.2 Continuous Data\nContinuous data can take on any value within a given range, including fractional and decimal values.\n\n3.2.2.1 Characteristics of Continuous Data:\n\nUncountable\nCan be measured to any level of precision\nRepresented by real numbers\nThere are always values between any two data points\n\n\n\n3.2.2.2 Examples:\n\nHeight\nWeight\nTemperature\n\n\n\n\n3.2.3 The Discrete-Continuous Spectrum\nIn practice, some variables that are mathematically discrete are often treated as if they are continuous. This dual nature provides flexibility in how these variables can be analyzed and interpreted.\n\n3.2.3.1 Reasons for Treating Discrete Data as Continuous:\n\nDense Granularity\n\nWhen a discrete variable has a large number of possible values within a range, it can approximate continuity.\nExample: Income measured in individual dollars. While technically discrete (you can’t earn a fraction of a cent), the large number of possible values makes it behave similarly to a continuous variable.\n\nAnalytical Convenience\n\nContinuous methods often yield reasonable and useful results even for dense discrete variables.\nIt’s often easier to use existing statistical tools if continuity is assumed, as this allows the use of calculus-based methods.\n\nApproximation of Underlying Phenomena\n\nIn some cases, a discrete measurement might be an approximation of an underlying continuous process.\nExample: While we measure time in discrete units (seconds, minutes, hours), time itself is continuous.\n\n\n\n\n3.2.3.2 Examples of Variables with Dual Discrete-Continuous Nature:\n\nAge\n\nDiscrete: Typically measured in whole years\nContinuous: Can be considered as a continuous variable in many analyses, especially when dealing with large populations\n\nPrice\n\nDiscrete: Prices are often rounded to the nearest cent\nContinuous: In economic models, prices are often treated as continuous variables\n\nTest Scores\n\nDiscrete: Often given as whole numbers\nContinuous: In statistical analyses, test scores might be treated as continuous, especially when the range of possible scores is large\n\n\n\n\n\n3.2.4 Implications for Data Analysis\nThe ability to treat some variables as either discrete or continuous has important implications for data analysis:\n\nFlexibility in Modeling: It allows for the use of a wider range of statistical techniques.\nSimplified Calculations: Treating dense discrete data as continuous can simplify calculations and make certain analyses more tractable.\nImproved Interpretability: In some cases, treating discrete data as continuous can lead to more intuitive or useful interpretations of results.\nPotential for Error: It’s important to be aware of when this approximation is appropriate and when it might lead to misleading results.\n\n\n\n3.2.5 Conclusion\nUnderstanding the nature of variables as discrete or continuous is crucial in data science and statistics. However, it’s equally important to recognize that this distinction is not always rigid. The ability to treat some variables as either discrete or continuous, depending on the context and analytical needs, provides a powerful tool in the data scientist’s toolkit. As with all analytical decisions, the choice to treat a variable as discrete or continuous should be made thoughtfully, with consideration for the nature of the data, the goals of the analysis, and the potential implications of the choice.\n\n\n3.2.6 R Code Example\nHere’s a simple R code example to illustrate how we might analyze a variable (age) as both discrete and continuous:\n\n# Generate some sample age data\nset.seed(123)\nages &lt;- round(runif(1000, min = 18, max = 80))\n\n# Treat age as discrete\nage_table &lt;- table(ages)\nbarplot(age_table, main = \"Age Distribution (Discrete)\", xlab = \"Age\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Treat age as continuous\nhist(ages, main = \"Age Distribution (Continuous)\", xlab = \"Age\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Compare mean and median\ncat(\"Mean age:\", mean(ages), \"\\n\")\n\nMean age: 48.848 \n\ncat(\"Median age:\", median(ages), \"\\n\")\n\nMedian age: 48 \n\n# Linear regression (treating age as continuous)\nincome &lt;- 20000 + 500 * ages + rnorm(1000, 0, 5000)\nmodel &lt;- lm(income ~ ages)\nsummary(model)\n\n\nCall:\nlm(formula = income ~ ages)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14214.7  -3504.0     66.3   3280.8  17032.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 20286.728    462.218   43.89   &lt;2e-16 ***\nages          495.352      8.889   55.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5009 on 998 degrees of freedom\nMultiple R-squared:  0.7568,    Adjusted R-squared:  0.7565 \nF-statistic:  3105 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n# Plot regression line\nplot(ages, income, main = \"Income vs. Age\", xlab = \"Age\", ylab = \"Income\")\nabline(model, col = \"red\")\n\n\n\n\n\n\n\n\nThis example demonstrates how we can analyze age data both discretely (using a bar plot) and continuously (using a histogram and linear regression). The choice between these approaches would depend on the specific research question and the level of precision required in the analysis.\n\n\n\n\n\n\nRational Numbers and Percentage Measures: Discrete Nature in a Continuous Guise\n\n\n\nVariables measured in percentages, such as unemployment rates or voter turnout, present an interesting case in the discrete-continuous spectrum. They challenge our intuitive understanding of discreteness and continuity, and highlight the importance of considering the underlying nature of our data.\n\n3.2.6.1 Key Points:\n\nRational Nature of Percentages:\n\nPercentages are essentially fractions of the form m/n where n = 100.\nThis makes them rational numbers by definition.\n\nDiscreteness of Percentage Measures:\n\nDespite appearing continuous, percentage measures are inherently discrete.\nThey can only take on a finite number of values between 0 and 100.\n\nLimited Density:\n\nUnlike the set of all rational numbers, percentage measures are not dense.\nThere is a smallest possible increment (1/100 or 0.01%), below which no value can be represented.\n\nPractical Implications:\n\nVariables like unemployment rates (e.g., 5.2%) or turnout percentages (e.g., 67.8%) have a finite precision.\nIn most cases, they are reported to one or two decimal places, further limiting their possible values.\n\nAnalytical Approach:\n\nDespite their discrete nature, in data analysis, we often treat percentage measures as continuous variables.\nThis treatment allows for the application of statistical techniques designed for continuous data.\nThe choice between discrete and continuous treatment depends on the specific analytical context and the level of precision required.\n\nThe Paradox:\n\nPercentage measures bridge the gap between discrete and continuous, appearing continuous but being fundamentally discrete.\nThey form a finite set of rational numbers that can approximate continuity in many practical situations.\n\n\n\n\n3.2.6.2 Example in R:\n\n# Simulating unemployment rates\nset.seed(123)\nunemployment_rates &lt;- round(runif(1000, min = 3, max = 10), 1)\n\n# Treating as discrete\nbarplot(table(unemployment_rates), \n        main = \"Unemployment Rates (Discrete View)\",\n        xlab = \"Rate (%)\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Treating as continuous\nhist(unemployment_rates, breaks = 30,\n     main = \"Unemployment Rates (Continuous View)\",\n     xlab = \"Rate (%)\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Both approaches can yield insights, depending on the research question\n\n# Demonstrating the discrete nature\nunique_values &lt;- length(unique(unemployment_rates))\ncat(\"Number of unique values:\", unique_values)\n\nNumber of unique values: 71\n\n\nUnderstanding the discrete nature of percentage measures, despite their appearance of continuity, enriches our approach to data analysis. It reminds us to consider the fundamental nature of our data when choosing analytical methods and interpreting results.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#introduction-to-stevens-data-typology",
    "href": "chapter2.html#introduction-to-stevens-data-typology",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.3 Introduction to Stevens’ Data Typology",
    "text": "3.3 Introduction to Stevens’ Data Typology\nS. S. Stevens, an American psychologist, introduced a classification system for scales of measurement in his 1946 paper “On the Theory of Scales of Measurement.” This system, known as Stevens’ data typology or levels of measurement, has become fundamental in understanding how different types of data should be analyzed and interpreted.\nStevens proposed four levels of measurement:\n\nNominal\nOrdinal\nInterval\nRatio\n\nEach level has specific properties and allows for different types of statistical operations and analyses.\n\n\n3.3.1 Nominal Scale\n\n3.3.1.1 Definition\nThe nominal scale is the most basic level of measurement. It uses labels or categories to classify data without any quantitative value or order.\n\n\n3.3.1.2 Properties\n\nCategories are mutually exclusive\nNo inherent order among categories\nNo meaningful arithmetic operations can be performed\n\n\n\n3.3.1.3 Examples\n\nGender (Male, Female, Non-binary)\nBlood types (A, B, AB, O)\nEye color (Blue, Brown, Green, Hazel)\n\n\n\n3.3.1.4 Permissible Statistics\n\nMode\nFrequency distributions\nChi-square test\n\n\n\n3.3.1.5 R Example\n\n# Creating a nominal variable\neye_colors &lt;- factor(c(\"Blue\", \"Brown\", \"Green\", \"Brown\", \"Blue\", \"Hazel\"))\n\n# Frequency distribution\ntable(eye_colors)\n\neye_colors\n Blue Brown Green Hazel \n    2     2     1     1 \n\n# Mode\nnames(which.max(table(eye_colors)))\n\n[1] \"Blue\"\n\n\n\n\n\n3.3.2 Ordinal Scale\n\n3.3.2.1 Definition\nThe ordinal scale categorizes data into ordered categories, but the intervals between categories are not necessarily equal or meaningful.\n\n\n3.3.2.2 Properties\n\nCategories have a defined order\nDifferences between categories are not quantifiable\nArithmetic operations on the numbers are not meaningful\n\n\n\n3.3.2.3 Examples\n\nEducation levels (High School, Bachelor’s, Master’s, PhD)\nLikert scales (Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree)\nSocioeconomic status (Low, Medium, High)\n\n\n\n3.3.2.4 Permissible Statistics\n\nMedian\nPercentiles\nSpearman’s rank correlation\nMann-Whitney U test\n\n\n\n3.3.2.5 R Example\n\n# Creating an ordinal variable\neducation_levels &lt;- ordered(c(\"High School\", \"Bachelor's\", \"Master's\", \"PhD\", \"Bachelor's\", \"Master's\"),\n                            levels = c(\"High School\", \"Bachelor's\", \"Master's\", \"PhD\"))\n\n# Median\nmedian(as.integer(education_levels))\n\n[1] 2.5\n\n# Percentiles\nquantile(as.integer(education_levels), probs = c(0.25, 0.5, 0.75))\n\n25% 50% 75% \n2.0 2.5 3.0 \n\n\n\n\n\n3.3.3 Interval Scale\n\n3.3.3.1 Definition\nThe interval scale has ordered categories with equal intervals between adjacent categories. However, it lacks a true zero point.\n\n\n3.3.3.2 Properties\n\nEqual intervals between adjacent categories\nNo true zero point (zero is arbitrary)\nRatios between values are not meaningful\n\n\n\n3.3.3.3 Examples\n\nTemperature in Celsius or Fahrenheit\nCalendar years\nIQ scores (traditionally treated as interval, though this is debated)\n\n\n\n3.3.3.4 Permissible Statistics\n\nMean\nStandard deviation\nPearson’s correlation\nT-tests and ANOVA\n\n\n\n3.3.3.5 R Example\n\n# Creating an interval variable (temperature in Celsius)\ntemperatures &lt;- c(22, 25, 19, 28, 23, 20)\n\n# Mean and standard deviation\nmean(temperatures)\n\n[1] 22.83333\n\nsd(temperatures)\n\n[1] 3.311596\n\n# Pearson's correlation with another variable\nhumidity &lt;- c(60, 65, 55, 70, 62, 58)\ncor(temperatures, humidity)\n\n[1] 0.9958407\n\n\n\n\n\n3.3.4 Ratio Scale\n\n3.3.4.1 Definition\nThe ratio scale is the highest level of measurement. It has all the properties of the interval scale plus a true zero point, making ratios between values meaningful.\n\n\n3.3.4.2 Properties\n\nAll properties of interval scales\nTrue zero point\nRatios between values are meaningful\n\n\n\n3.3.4.3 Examples\n\nHeight\nWeight\nAge\nIncome\n\n\n\n3.3.4.4 Permissible Statistics\n\nAll statistical operations\nGeometric mean\nCoefficient of variation\n\n\n\n3.3.4.5 R Example\n\n# Creating a ratio variable (age)\nages &lt;- c(25, 30, 22, 35, 28, 40)\n\n# Geometric mean\nexp(mean(log(ages)))\n\n[1] 29.40774\n\n# Coefficient of variation\nsd(ages) / mean(ages)\n\n[1] 0.220101\n\n\n\n\n\n3.3.5 Importance in Research and Analysis\nUnderstanding Stevens’ data typology is crucial for several reasons:\n\nChoosing appropriate statistical tests: The level of measurement determines which statistical analyses are appropriate for a given dataset.\nInterpreting results: The meaning of statistical results depends on the level of measurement of the variables involved.\nDesigning measurement instruments: When creating surveys or other measurement tools, researchers must consider the level of measurement they want to achieve.\nData transformation: Sometimes, data can be transformed from one level to another, but this must be done carefully to avoid misinterpretation.\n\n\n\n3.3.6 Controversies and Limitations\nWhile Stevens’ typology is widely used, it has faced some criticisms:\n\nRigidity: Some argue that the typology is too rigid and that many real-world measurements fall between these categories.\nTreatment of ordinal data: There’s ongoing debate about when it’s appropriate to treat ordinal data as interval for certain analyses.\nPsychological scaling: Some psychological constructs (like intelligence) are difficult to categorize definitively within this system.\n\n\n\n3.3.7 Conclusion\nStevens’ data typology provides a fundamental framework for understanding different types of data and their properties. By recognizing the level of measurement of their variables, researchers can make informed decisions about data collection, analysis, and interpretation. However, it’s important to remember that while this typology is a useful guide, real-world data often requires nuanced consideration and may not always fit neatly into these categories.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not “twice as acidic” as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#likert-scales-and-ordinal-variables-in-psychology",
    "href": "chapter2.html#likert-scales-and-ordinal-variables-in-psychology",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.4 Likert Scales and Ordinal Variables in Psychology",
    "text": "3.4 Likert Scales and Ordinal Variables in Psychology\nLikert scales are widely used in psychology and social sciences to measure attitudes, opinions, and perceptions. Named after psychologist Rensis Likert, these scales typically consist of a series of statements or questions that respondents rate on a scale, often from “Strongly Disagree” to “Strongly Agree.”\n\n\n3.4.1 Why Likert Scales are Ordinal Variables\nLikert scales are considered ordinal variables for several reasons:\n\nOrder without equal intervals: While the responses have a clear order (e.g., “Strongly Disagree” &lt; “Disagree” &lt; “Neutral” &lt; “Agree” &lt; “Strongly Agree”), the intervals between these categories are not necessarily equal.\nSubjective interpretation: The difference between “Strongly Disagree” and “Disagree” may not be the same as the difference between “Agree” and “Strongly Agree” for all respondents.\nLack of true zero point: Likert scales typically don’t have a true zero point, which is a characteristic of interval or ratio scales.\n\nLet’s illustrate this with an example in R:\n\n# Create a factor variable to represent Likert scale responses\nlikert_responses &lt;- factor(c(\"Strongly Disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly Agree\"),\n                           ordered = TRUE,\n                           levels = c(\"Strongly Disagree\", \"Disagree\", \"Neutral\", \"Agree\", \"Strongly Agree\"))\n\n# Print the levels to show the ordering\nprint(levels(likert_responses))\n\n[1] \"Strongly Disagree\" \"Disagree\"          \"Neutral\"          \n[4] \"Agree\"             \"Strongly Agree\"   \n\n# Attempt to calculate mean (which is inappropriate for ordinal data)\nnumeric_responses &lt;- as.numeric(likert_responses)\nprint(mean(numeric_responses))\n\n[1] 3\n\n\nAs we can see, while we can order the responses, treating them as numeric and calculating the mean doesn’t provide meaningful information due to the ordinal nature of the data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#iq-and-other-psychological-variables-as-ordinal-measures",
    "href": "chapter2.html#iq-and-other-psychological-variables-as-ordinal-measures",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.5 IQ and Other Psychological Variables as Ordinal Measures",
    "text": "3.5 IQ and Other Psychological Variables as Ordinal Measures\nMany psychological measures, including IQ, are often treated as interval scales but are, in fact, ordinal. Here’s why:\n\nIQ Scores:\n\nWhile IQ scores are presented as numbers, the difference between an IQ of 100 and 110 may not represent the same cognitive difference as between 130 and 140.\nThe scale is normalized and adjusted over time, making it difficult to claim true interval properties.\n\nOther Psychological Measures:\n\nDepression scales (e.g., Beck Depression Inventory)\nAnxiety measures (e.g., State-Trait Anxiety Inventory)\nPersonality assessments (e.g., Big Five Inventory)\n\n\nThese measures often use summed Likert-type items or other scoring methods that don’t guarantee equal intervals between scores.\n\nLet’s illustrate this with a simulated depression scale:\n\nset.seed(123)\ndepression_scores &lt;- sample(0:27, 100, replace = TRUE)  # Simulating BDI-II scores\n\n# Create categories\ndepression_categories &lt;- cut(depression_scores, \n                             breaks = c(-Inf, 13, 19, 28, Inf),\n                             labels = c(\"Minimal\", \"Mild\", \"Moderate\", \"Severe\"))\n\n# Show distribution\ntable(depression_categories)\n\ndepression_categories\n Minimal     Mild Moderate   Severe \n      46       20       34        0 \n\n# Plot distribution\nbarplot(table(depression_categories), \n        main = \"Distribution of Depression Severity\",\n        xlab = \"Severity Category\",\n        ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nWhile we can order these categories, we can’t assume that the difference between “Minimal” and “Mild” is the same as between “Moderate” and “Severe” in terms of the underlying construct of depression.\n\n3.5.1 Implications for Analysis\nRecognizing these measures as ordinal has important implications for data analysis:\n\nAppropriate statistical tests: Use non-parametric tests (e.g., Mann-Whitney U, Kruskal-Wallis) instead of parametric ones.\nCorrelation analysis: Use Spearman’s rank correlation instead of Pearson’s correlation.\nCentral tendency: Report median and mode rather than mean.\nData visualization: Use methods appropriate for ordinal data, such as bar plots or stacked bar charts.\n\n\n\n3.5.2 Conclusion\nWhile Likert scales and many psychological measures are often treated as interval data for practical reasons, it’s crucial to remember their ordinal nature. This understanding should inform our choice of statistical analyses and interpretations of results in psychological research.\n\n\n\n\n\n\nExercise: Identifying Measurement Scales\n\n\n\nFor each of the following variables, determine the most appropriate scale of measurement (Nominal, Ordinal, Interval, or Ratio). Provide your answers and rationale in your assignment submission.\n\nFamily size: 1 child, 2 children, 3 children, …\nCustomer satisfaction: Poor, Fair, Good, Excellent\nHeight (questionnaire): “I am: very short, short, average, tall, very tall”\nHeight (inches)\nReaction time (milliseconds)\nPostal codes: e.g., 61548, 61761, 62461, 47424, 65233\nAge (years)\nNationality\nStreet addresses\nMilitary ranks\nLeft-Right political scale placement\nGender: nominal level of measurement\nIQ score: …\nShirt size (S, M, L, …): …\nMovie ratings (1 star, 2 stars, 3 stars): …\nTemperature (Celsius): …\nTemperature (Kelvin): …\nBlood types: A, B, AB, O\nIncome categories: low, medium, high\nVoter turnout: …\nPolitical party affiliation: …\nElectoral district magnitude: …\n\nRemember to justify your choices for each variable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html",
    "href": "rozdzial2.html",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "",
    "text": "4.1 Zbiory liczbowe\nZanim zagłębimy się w typy danych, istotne jest zrozumienie podstawowych zbiorów liczbowych, które tworzą fundament naszego rozumienia danych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#zbiory-liczbowe",
    "href": "rozdzial2.html#zbiory-liczbowe",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "",
    "text": "4.1.1 Podstawowe Zbiory Liczbowe\n\nLiczby Naturalne (ℕ): Liczby do liczenia {1, 2, 3, …}\nLiczby Całkowite (ℤ): Obejmują liczby naturalne, ich przeciwne i zero {…, -2, -1, 0, 1, 2, …}\nLiczby Wymierne (ℚ): Liczby, które można wyrazić jako iloraz dwóch liczb całkowitych\nLiczby Rzeczywiste (ℝ): Wszystkie liczby na osi liczbowej, włączając wymierne i niewymierne\n\n\n\n4.1.2 Właściwości Zbiorów\n\nZbiory Przeliczalne: Zbiory, których elementy można ustawić w odpowiedniości jeden-do-jednego z liczbami naturalnymi. Na przykład, zbiór liczb całkowitych jest przeliczalny.\nZbiory Nieprzeliczalne: Zbiory, które nie są przeliczalne. Zbiór liczb rzeczywistych jest nieprzeliczalny.\nZbiory Dyskretne: Zbiory, w których każdy element jest oddzielony od innych elementów skończoną przerwą. Liczby całkowite tworzą zbiór dyskretny.\nZbiory Gęste: Zbiory, w których między dowolnymi dwoma elementami zawsze znajduje się inny element tego zbioru. Liczby wymierne i rzeczywiste są zbiorami gęstymi.\n\n\n\n\n\n\n\nNote\n\n\n\nZrozumienie tych właściwości zbiorów jest kluczowe dla uchwycenia natury różnych typów danych w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#dane-dyskretne-vs.-dane-ciągłe",
    "href": "rozdzial2.html#dane-dyskretne-vs.-dane-ciągłe",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.2 Dane Dyskretne vs. Dane Ciągłe",
    "text": "4.2 Dane Dyskretne vs. Dane Ciągłe\nW nauce o danych i statystyce często kategoryzujemy zmienne jako dyskretne lub ciągłe. Jednak to rozróżnienie nie zawsze jest jednoznaczne, a niektóre zmienne wykazują cechy obu typów. Ten dokument omawia koncepcje danych dyskretnych i ciągłych, ich różnice oraz interesujący przypadek zmiennych, które mogą być traktowane jako oba typy.\n\n\n4.2.1 Dane dyskretne\nDane dyskretne mogą przyjmować tylko konkretne, policzalne wartości. Te wartości często (ale nie zawsze) są liczbami całkowitymi.\n\n4.2.1.1 Cechy danych dyskretnych:\n\nPoliczalne\nCzęsto reprezentowane przez liczby całkowite\nMogą być skończone lub nieskończone\nBrak wartości pomiędzy dwoma sąsiednimi punktami danych\n\n\n\n4.2.1.2 Przykłady:\n\nLiczba studentów w klasie\nLiczba samochodów sprzedanych przez salon\nRozmiary butów\n\n\n\n\n4.2.2 Dane ciągłe\nDane ciągłe mogą przyjmować dowolną wartość w danym zakresie, włącznie z wartościami ułamkowymi i dziesiętnymi.\n\n4.2.2.1 Cechy danych ciągłych:\n\nNiepoliczalne\nMogą być mierzone z dowolną precyzją\nReprezentowane przez liczby rzeczywiste\nZawsze istnieją wartości pomiędzy dowolnymi dwoma punktami danych\n\n\n\n4.2.2.2 Przykłady:\n\nWzrost\nWaga\nTemperatura\n\n\n\n\n4.2.3 Spektrum dyskretno-ciągłe\nW praktyce niektóre zmienne, które matematycznie są dyskretne, często traktuje się tak, jakby były ciągłe. Ta podwójna natura zapewnia elastyczność w analizie i interpretacji tych zmiennych.\n\n4.2.3.1 Powody traktowania danych dyskretnych jako ciągłych:\n\nGęsta granularność\n\nGdy zmienna dyskretna ma dużą liczbę możliwych wartości w danym zakresie, może przybliżać ciągłość.\nPrzykład: Dochód mierzony w pojedynczych złotych. Chociaż technicznie jest dyskretny (nie można zarobić ułamka grosza), duża liczba możliwych wartości sprawia, że zachowuje się podobnie do zmiennej ciągłej.\n\nWygoda analityczna\n\nMetody ciągłe często dają rozsądne i użyteczne wyniki nawet dla gęstych zmiennych dyskretnych.\nCzęsto łatwiej jest używać istniejących narzędzi statystycznych, jeśli założymy ciągłość, ponieważ pozwala to na stosowanie metod opartych na rachunku różniczkowym.\n\nPrzybliżenie zjawisk bazowych\n\nW niektórych przypadkach dyskretny pomiar może być przybliżeniem ciągłego procesu bazowego.\nPrzykład: Chociaż mierzymy czas w dyskretnych jednostkach (sekundy, minuty, godziny), sam czas jest ciągły.\n\n\n\n\n4.2.3.2 Przykłady zmiennych o podwójnej naturze dyskretno-ciągłej:\n\nWiek\n\nDyskretny: Zazwyczaj mierzony w pełnych latach\nCiągły: Może być traktowany jako zmienna ciągła w wielu analizach, szczególnie przy dużych populacjach\n\nCena\n\nDyskretna: Ceny są często zaokrąglane do najbliższego grosza\nCiągła: W modelach ekonomicznych ceny często traktuje się jako zmienne ciągłe\n\nWyniki testów\n\nDyskretne: Często podawane jako liczby całkowite\nCiągłe: W analizach statystycznych wyniki testów mogą być traktowane jako ciągłe, szczególnie gdy zakres możliwych wyników jest duży\n\n\n\n\n\n4.2.4 Implikacje dla analizy danych\nMożliwość traktowania niektórych zmiennych jako dyskretnych lub ciągłych ma ważne implikacje dla analizy danych:\n\nElastyczność w modelowaniu: Pozwala na zastosowanie szerszego zakresu technik statystycznych.\nUproszczone obliczenia: Traktowanie gęstych danych dyskretnych jako ciągłych może uprościć obliczenia i uczynić niektóre analizy bardziej możliwymi do przeprowadzenia.\nLepsza interpretacja: W niektórych przypadkach traktowanie danych dyskretnych jako ciągłych może prowadzić do bardziej intuicyjnej lub użytecznej interpretacji wyników.\nPotencjalne błędy: Ważne jest, aby być świadomym, kiedy to przybliżenie jest odpowiednie, a kiedy może prowadzić do mylących wyników.\n\n\n\n4.2.5 Podsumowanie\nZrozumienie natury zmiennych jako dyskretnych lub ciągłych jest kluczowe w nauce o danych i statystyce. Równie ważne jest jednak rozpoznanie, że to rozróżnienie nie zawsze jest sztywne. Możliwość traktowania niektórych zmiennych jako dyskretnych lub ciągłych, w zależności od kontekstu i potrzeb analitycznych, stanowi potężne narzędzie w arsenale naukowca danych. Jak w przypadku wszystkich decyzji analitycznych, wybór traktowania zmiennej jako dyskretnej lub ciągłej powinien być dokonywany z rozwagą, biorąc pod uwagę naturę danych, cele analizy i potencjalne implikacje tego wyboru.\n\n\n4.2.6 Przykład kodu R\nOto prosty przykład kodu R ilustrujący, jak możemy analizować zmienną (wiek) zarówno jako dyskretną, jak i ciągłą:\n\n# Generowanie przykładowych danych wieku\nset.seed(123)\nwiek &lt;- round(runif(1000, min = 18, max = 80))\n\n# Traktowanie wieku jako dyskretnego\ntabela_wieku &lt;- table(wiek)\nbarplot(tabela_wieku, main = \"Rozkład wieku (dyskretny)\", xlab = \"Wiek\", ylab = \"Częstość\")\n\n\n\n\n\n\n\n# Traktowanie wieku jako ciągłego\nhist(wiek, main = \"Rozkład wieku (ciągły)\", xlab = \"Wiek\", ylab = \"Częstość\")\n\n\n\n\n\n\n\n# Porównanie średniej i mediany\ncat(\"Średni wiek:\", mean(wiek), \"\\n\")\n\nŚredni wiek: 48.848 \n\ncat(\"Mediana wieku:\", median(wiek), \"\\n\")\n\nMediana wieku: 48 \n\n# Regresja liniowa (traktowanie wieku jako ciągłego)\ndochod &lt;- 20000 + 500 * wiek + rnorm(1000, 0, 5000)\nmodel &lt;- lm(dochod ~ wiek)\nsummary(model)\n\n\nCall:\nlm(formula = dochod ~ wiek)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14214.7  -3504.0     66.3   3280.8  17032.3 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 20286.728    462.218   43.89   &lt;2e-16 ***\nwiek          495.352      8.889   55.72   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5009 on 998 degrees of freedom\nMultiple R-squared:  0.7568,    Adjusted R-squared:  0.7565 \nF-statistic:  3105 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n# Wykres linii regresji\nplot(wiek, dochod, main = \"Dochód vs. Wiek\", xlab = \"Wiek\", ylab = \"Dochód\")\nabline(model, col = \"red\")\n\n\n\n\n\n\n\n\nTen przykład pokazuje, jak możemy analizować dane dotyczące wieku zarówno dyskretnie (używając wykresu słupkowego), jak i w sposób ciągły (używając histogramu i regresji liniowej). Wybór między tymi podejściami zależałby od konkretnego pytania badawczego i wymaganego poziomu precyzji w analizie.\n\n\n\n\n\n\nLiczby wymierne i miary procentowe: Dyskretna natura w pozornie ciągłej formie\n\n\n\nZmienne mierzone w procentach, takie jak stopa bezrobocia czy frekwencja wyborcza, stanowią interesujący przypadek w spektrum dyskretno-ciągłym. Rzucają one wyzwanie naszemu intuicyjnemu rozumieniu dyskretności i ciągłości oraz podkreślają znaczenie rozważania podstawowej natury naszych danych.\n\n4.2.6.1 Kluczowe punkty:\n\nWymierna natura procentów:\n\nProcenty są w istocie ułamkami postaci m/n, gdzie n = 100.\nTo sprawia, że z definicji są liczbami wymiernymi.\n\nDyskretność miar procentowych:\n\nMimo pozorów ciągłości, miary procentowe są z natury dyskretne.\nMogą przyjmować tylko skończoną liczbę wartości między 0 a 100.\n\nOgraniczona gęstość:\n\nW przeciwieństwie do zbioru wszystkich liczb wymiernych, miary procentowe nie są gęste.\nIstnieje najmniejszy możliwy przyrost (1/100 lub 0,01%), poniżej którego nie można reprezentować żadnej wartości.\n\nPraktyczne implikacje:\n\nZmienne takie jak stopa bezrobocia (np. 5,2%) czy frekwencja wyborcza (np. 67,8%) mają skończoną precyzję.\nW większości przypadków są raportowane z dokładnością do jednego lub dwóch miejsc po przecinku, co dodatkowo ogranicza ich możliwe wartości.\n\nPodejście analityczne:\n\nMimo ich dyskretnej natury, w analizie danych często traktujemy miary procentowe jako zmienne ciągłe.\nTakie podejście pozwala na zastosowanie technik statystycznych przeznaczonych dla danych ciągłych.\nWybór między traktowaniem jako dyskretne lub ciągłe zależy od konkretnego kontekstu analitycznego i wymaganego poziomu precyzji.\n\nParadoks:\n\nMiary procentowe łączą w sobie cechy dyskretne i ciągłe, wydając się ciągłe, ale będąc fundamentalnie dyskretne.\nTworzą one skończony zbiór liczb wymiernych, które mogą aproksymować ciągłość w wielu praktycznych sytuacjach.\n\n\n\n\n4.2.6.2 Przykład w R:\n\n# Symulacja stóp bezrobocia\nset.seed(123)\nstopy_bezrobocia &lt;- round(runif(1000, min = 3, max = 10), 1)\n\n# Traktowanie jako dyskretne\nbarplot(table(stopy_bezrobocia), \n        main = \"Stopy bezrobocia (ujęcie dyskretne)\",\n        xlab = \"Stopa (%)\", ylab = \"Częstość\")\n\n\n\n\n\n\n\n# Traktowanie jako ciągłe\nhist(stopy_bezrobocia, breaks = 30,\n     main = \"Stopy bezrobocia (ujęcie ciągłe)\",\n     xlab = \"Stopa (%)\", ylab = \"Częstość\")\n\n\n\n\n\n\n\n# Oba podejścia mogą dostarczyć cennych informacji, w zależności od pytania badawczego\n\n# Demonstracja dyskretnej natury\nunikalne_wartosci &lt;- length(unique(stopy_bezrobocia))\ncat(\"Liczba unikalnych wartości:\", unikalne_wartosci)\n\nLiczba unikalnych wartości: 71\n\n\nZrozumienie dyskretnej natury miar procentowych, mimo ich pozorów ciągłości, wzbogaca nasze podejście do analizy danych. Przypomina nam o konieczności rozważania fundamentalnej natury naszych danych przy wyborze metod analitycznych i interpretacji wyników.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "href": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.3 Wprowadzenie do Typologii Danych Stevensa",
    "text": "4.3 Wprowadzenie do Typologii Danych Stevensa\nS. S. Stevens, amerykański psycholog, wprowadził system klasyfikacji skal pomiarowych w swoim artykule z 1946 roku “On the Theory of Scales of Measurement”. Ten system, znany jako typologia danych Stevensa lub poziomy pomiaru, stał się fundamentalny dla zrozumienia, jak różne typy danych powinny być analizowane i interpretowane.\nStevens zaproponował cztery poziomy pomiaru:\n\nNominalny\nPorządkowy\nInterwałowy\nIlorazowy\n\nKażdy poziom ma specyficzne właściwości i pozwala na różne rodzaje operacji statystycznych i analiz.\n\n\n4.3.1 Skala Nominalna\n\n4.3.1.1 Definicja\nSkala nominalna jest najbardziej podstawowym poziomem pomiaru. Używa etykiet lub kategorii do klasyfikacji danych bez żadnej wartości ilościowej ani porządku.\n\n\n4.3.1.2 Właściwości\n\nKategorie są wzajemnie wykluczające się\nBrak inherentnego porządku między kategoriami\nNie można wykonywać znaczących operacji arytmetycznych\n\n\n\n4.3.1.3 Przykłady\n\nPłeć (Mężczyzna, Kobieta, Niebinarna)\nGrupy krwi (A, B, AB, O)\nKolor oczu (Niebieskie, Brązowe, Zielone, Piwne)\n\n\n\n4.3.1.4 Dozwolone Statystyki\n\nModa (dominanta)\nRozkłady częstości\nTest chi-kwadrat\n\n\n\n4.3.1.5 Przykład w R\n\n# Tworzenie zmiennej nominalnej\nkolory_oczu &lt;- factor(c(\"Niebieskie\", \"Brązowe\", \"Zielone\", \"Brązowe\", \"Niebieskie\", \"Piwne\"))\n\n# Rozkład częstości\ntable(kolory_oczu)\n\nkolory_oczu\n   Brązowe Niebieskie      Piwne    Zielone \n         2          2          1          1 \n\n# Moda (dominanta)\nnames(which.max(table(kolory_oczu)))\n\n[1] \"Brązowe\"\n\n\n\n\n\n4.3.2 Skala Porządkowa\n\n4.3.2.1 Definicja\nSkala porządkowa kategoryzuje dane w uporządkowane kategorie, ale odstępy między kategoriami niekoniecznie są równe lub znaczące.\n\n\n4.3.2.2 Właściwości\n\nKategorie mają zdefiniowany porządek\nRóżnice między kategoriami nie są kwantyfikowalne\nOperacje arytmetyczne na liczbach nie są znaczące\n\n\n\n4.3.2.3 Przykłady\n\nPoziomy wykształcenia (Szkoła Średnia, Licencjat, Magister, Doktorat)\nSkale Likerta (Zdecydowanie się nie zgadzam, Nie zgadzam się, Neutralnie, Zgadzam się, Zdecydowanie się zgadzam)\nStatus społeczno-ekonomiczny (Niski, Średni, Wysoki)\n\n\n\n4.3.2.4 Dozwolone Statystyki\n\nMediana\nPercentyle\nKorelacja rangowa Spearmana\nTest U Manna-Whitneya\n\n\n\n4.3.2.5 Przykład w R\n\n# Tworzenie zmiennej porządkowej\npoziomy_edukacji &lt;- ordered(c(\"Szkoła Średnia\", \"Licencjat\", \"Magister\", \"Doktorat\", \"Licencjat\", \"Magister\"),\n                            levels = c(\"Szkoła Średnia\", \"Licencjat\", \"Magister\", \"Doktorat\"))\n\n# Mediana\nmedian(as.integer(poziomy_edukacji))\n\n[1] 2.5\n\n# Percentyle\nquantile(as.integer(poziomy_edukacji), probs = c(0.25, 0.5, 0.75))\n\n25% 50% 75% \n2.0 2.5 3.0 \n\n\n\n\n\n4.3.3 Skala Interwałowa\n\n4.3.3.1 Definicja\nSkala interwałowa ma uporządkowane kategorie z równymi odstępami między sąsiednimi kategoriami. Jednak brakuje jej prawdziwego punktu zerowego.\n\n\n4.3.3.2 Właściwości\n\nRówne odstępy między sąsiednimi kategoriami\nBrak prawdziwego punktu zerowego (zero jest umowne)\nStosunki między wartościami nie są znaczące\n\n\n\n4.3.3.3 Przykłady\n\nTemperatura w stopniach Celsjusza lub Fahrenheita\nLata kalendarzowe\nWyniki IQ (tradycyjnie traktowane jako interwałowe, chociaż jest to dyskusyjne)\n\n\n\n4.3.3.4 Dozwolone Statystyki\n\nŚrednia\nOdchylenie standardowe\nKorelacja Pearsona\nTesty t i ANOVA\n\n\n\n4.3.3.5 Przykład w R\n\n# Tworzenie zmiennej interwałowej (temperatura w stopniach Celsjusza)\ntemperatury &lt;- c(22, 25, 19, 28, 23, 20)\n\n# Średnia i odchylenie standardowe\nmean(temperatury)\n\n[1] 22.83333\n\nsd(temperatury)\n\n[1] 3.311596\n\n# Korelacja Pearsona z inną zmienną\nwilgotnosc &lt;- c(60, 65, 55, 70, 62, 58)\ncor(temperatury, wilgotnosc)\n\n[1] 0.9958407\n\n\n\n\n\n4.3.4 Skala Ilorazowa\n\n4.3.4.1 Definicja\nSkala ilorazowa jest najwyższym poziomem pomiaru. Ma wszystkie właściwości skali interwałowej plus prawdziwy punkt zerowy, co sprawia, że stosunki między wartościami są znaczące.\n\n\n4.3.4.2 Właściwości\n\nWszystkie właściwości skal interwałowych\nPrawdziwy punkt zerowy\nStosunki między wartościami są znaczące\n\n\n\n4.3.4.3 Przykłady\n\nWzrost\nWaga\nWiek\nDochód\n\n\n\n4.3.4.4 Dozwolone Statystyki\n\nWszystkie operacje statystyczne\nŚrednia geometryczna\nWspółczynnik zmienności\n\n\n\n4.3.4.5 Przykład w R\n\n# Tworzenie zmiennej ilorazowej (wiek)\nwieki &lt;- c(25, 30, 22, 35, 28, 40)\n\n# Średnia geometryczna\nexp(mean(log(wieki)))\n\n[1] 29.40774\n\n# Współczynnik zmienności\nsd(wieki) / mean(wieki)\n\n[1] 0.220101\n\n\n\n\n\n4.3.5 Znaczenie w Badaniach i Analizie\nZrozumienie typologii danych Stevensa jest kluczowe z kilku powodów:\n\nWybór odpowiednich testów statystycznych: Poziom pomiaru determinuje, które analizy statystyczne są odpowiednie dla danego zbioru danych.\nInterpretacja wyników: Znaczenie wyników statystycznych zależy od poziomu pomiaru zaangażowanych zmiennych.\nProjektowanie narzędzi pomiarowych: Przy tworzeniu ankiet lub innych narzędzi pomiarowych badacze muszą wziąć pod uwagę poziom pomiaru, który chcą osiągnąć.\nTransformacja danych: Czasami dane mogą być przekształcane z jednego poziomu na drugi, ale musi to być robione ostrożnie, aby uniknąć błędnej interpretacji.\n\n\n\n4.3.6 Kontrowersje i Ograniczenia\nChociaż typologia Stevensa jest szeroko stosowana, spotkała się z pewnymi krytykami:\n\nSztywność: Niektórzy twierdzą, że typologia jest zbyt sztywna i że wiele rzeczywistych pomiarów mieści się pomiędzy tymi kategoriami.\nTraktowanie danych porządkowych: Trwa debata na temat tego, kiedy właściwe jest traktowanie danych porządkowych jako interwałowych dla pewnych analiz.\nSkalowanie psychologiczne: Niektóre konstrukty psychologiczne (jak inteligencja) są trudne do jednoznacznego skategoryzowania w ramach tego systemu.\n\n\n\n4.3.7 Podsumowanie\nTypologia danych Stevensa dostarcza fundamentalnych ram dla zrozumienia różnych rodzajów danych i ich właściwości. Rozpoznając poziom pomiaru swoich zmiennych, badacze mogą podejmować świadome decyzje dotyczące gromadzenia danych, analizy i interpretacji. Jednak ważne jest, aby pamiętać, że chociaż ta typologia jest użytecznym przewodnikiem, rzeczywiste dane często wymagają niuansowego podejścia i nie zawsze pasują idealnie do tych kategorii.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not “twice as acidic” as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#skale-likerta-i-ich-charakter-porządkowy",
    "href": "rozdzial2.html#skale-likerta-i-ich-charakter-porządkowy",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.4 Skale Likerta i Ich Charakter Porządkowy",
    "text": "4.4 Skale Likerta i Ich Charakter Porządkowy\n\n4.4.1 Wprowadzenie do Skal Likerta\nSkale Likerta są szeroko stosowane w psychologii i naukach społecznych do pomiaru postaw, opinii i percepcji. Nazwane na cześć psychologa Rensisa Likerta, skale te zazwyczaj składają się z serii stwierdzeń lub pytań, które respondenci oceniają na skali, często od “Zdecydowanie się nie zgadzam” do “Zdecydowanie się zgadzam”.\n\n\n\n4.4.2 Dlaczego Skale Likerta są Zmiennymi Porządkowymi\nSkale Likerta są uważane za zmienne porządkowe z kilku powodów:\n\nPorządek bez równych odstępów: Chociaż odpowiedzi mają wyraźną kolejność (np. “Zdecydowanie się nie zgadzam” &lt; “Nie zgadzam się” &lt; “Neutralnie” &lt; “Zgadzam się” &lt; “Zdecydowanie się zgadzam”), odstępy między tymi kategoriami niekoniecznie są równe.\nSubiektywna interpretacja: Różnica między “Zdecydowanie się nie zgadzam” a “Nie zgadzam się” może nie być taka sama jak różnica między “Zgadzam się” a “Zdecydowanie się zgadzam” dla wszystkich respondentów.\nBrak prawdziwego punktu zerowego: Skale Likerta zazwyczaj nie mają prawdziwego punktu zerowego, co jest cechą charakterystyczną skal interwałowych lub ilorazowych.\n\nZilustrujmy to przykładem w R:\n\n# Tworzymy zmienną typu factor reprezentującą odpowiedzi w skali Likerta\nodpowiedzi_likert &lt;- factor(c(\"Zdecydowanie się nie zgadzam\", \"Nie zgadzam się\", \"Neutralnie\", \"Zgadzam się\", \"Zdecydowanie się zgadzam\"),\n                           ordered = TRUE,\n                           levels = c(\"Zdecydowanie się nie zgadzam\", \"Nie zgadzam się\", \"Neutralnie\", \"Zgadzam się\", \"Zdecydowanie się zgadzam\"))\n\n# Wyświetlamy poziomy, aby pokazać uporządkowanie\nprint(levels(odpowiedzi_likert))\n\n[1] \"Zdecydowanie się nie zgadzam\" \"Nie zgadzam się\"             \n[3] \"Neutralnie\"                   \"Zgadzam się\"                 \n[5] \"Zdecydowanie się zgadzam\"    \n\n# Próba obliczenia średniej (co jest niewłaściwe dla danych porządkowych)\nodpowiedzi_numeryczne &lt;- as.numeric(odpowiedzi_likert)\nprint(mean(odpowiedzi_numeryczne))\n\n[1] 3\n\n\nJak widzimy, chociaż możemy uporządkować odpowiedzi, traktowanie ich jako dane numeryczne i obliczanie średniej nie dostarcza znaczących informacji ze względu na porządkowy charakter danych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#iq-i-inne-zmienne-psychologiczne-jako-miary-porządkowe",
    "href": "rozdzial2.html#iq-i-inne-zmienne-psychologiczne-jako-miary-porządkowe",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.5 IQ i Inne Zmienne Psychologiczne jako Miary Porządkowe",
    "text": "4.5 IQ i Inne Zmienne Psychologiczne jako Miary Porządkowe\n\n4.5.1 Przykłady Zmiennych Psychologicznych\nWiele miar psychologicznych, w tym IQ, jest często traktowanych jako skale interwałowe, ale w rzeczywistości są to skale porządkowe. Oto dlaczego:\n\nWyniki IQ:\n\nChociaż wyniki IQ są przedstawiane jako liczby, różnica między IQ 100 a 110 może nie reprezentować takiej samej różnicy poznawczej jak między 130 a 140.\nSkala jest normalizowana i dostosowywana w czasie, co utrudnia stwierdzenie, że ma właściwości prawdziwie interwałowe.\n\nInne Miary Psychologiczne:\n\nSkale depresji (np. Inwentarz Depresji Becka)\nMiary lęku (np. Inwentarz Stanu i Cechy Lęku)\nOceny osobowości (np. Inwentarz Wielkiej Piątki)\n\n\nTe miary często wykorzystują sumowane pozycje typu Likerta lub inne metody punktacji, które nie gwarantują równych odstępów między wynikami.\n\n\n\n4.5.2 Przykład: Symulowana Skala Depresji\nZilustrujmy to na symulowanej skali depresji:\n\nset.seed(123)\nwyniki_depresji &lt;- sample(0:27, 100, replace = TRUE)  # Symulacja wyników BDI-II\n\n# Tworzenie kategorii\nkategorie_depresji &lt;- cut(wyniki_depresji, \n                          breaks = c(-Inf, 13, 19, 28, Inf),\n                          labels = c(\"Minimalna\", \"Łagodna\", \"Umiarkowana\", \"Ciężka\"))\n\n# Pokazanie rozkładu\ntable(kategorie_depresji)\n\nkategorie_depresji\n  Minimalna     Łagodna Umiarkowana      Ciężka \n         46          20          34           0 \n\n# Wykres rozkładu\nbarplot(table(kategorie_depresji), \n        main = \"Rozkład Nasilenia Depresji\",\n        xlab = \"Kategoria Nasilenia\",\n        ylab = \"Częstość\")\n\n\n\n\n\n\n\n\nChociaż możemy uporządkować te kategorie, nie możemy założyć, że różnica między “Minimalna” a “Łagodna” jest taka sama jak między “Umiarkowana” a “Ciężka” w odniesieniu do bazowego konstruktu depresji.\n\n\n4.5.3 Implikacje dla Analizy\nUznanie tych miar za porządkowe ma ważne implikacje dla analizy danych:\n\nOdpowiednie testy statystyczne: Używaj testów nieparametrycznych (np. test U Manna-Whitneya, test Kruskala-Wallisa) zamiast parametrycznych.\nAnaliza korelacji: Używaj korelacji rangowej Spearmana zamiast korelacji Pearsona.\nTendencja centralna: Raportuj medianę i dominantę zamiast średniej.\nWizualizacja danych: Stosuj metody odpowiednie dla danych porządkowych, takie jak wykresy słupkowe lub skumulowane wykresy słupkowe.\n\n\n\n4.5.4 Podsumowanie\nChociaż skale Likerta i wiele miar psychologicznych jest często traktowanych jako dane interwałowe ze względów praktycznych, ważne jest, aby pamiętać o ich porządkowym charakterze. To zrozumienie powinno wpływać na nasz wybór analiz statystycznych i interpretację wyników w badaniach psychologicznych.\n\n\n\n\n\n\nĆwiczenie: Identyfikacja Skal Pomiarowych\n\n\n\nDla każdej z poniższych zmiennych określ najbardziej odpowiednią skalę pomiaru (Nominalna, Porządkowa, Przedziałowa lub Stosunkowa). W pliku z zadaniem podaj swoje odpowiedzi wraz z uzasadnieniem.\n\nLiczba dzieci w rodzinie: 1 dziecko, 2 dzieci, 3 dzieci, …\nSatysfakcja klienta: Niska, Średnia, Dobra, Doskonała\nWzrost (ankieta): “Jestem: bardzo niski, niski, przeciętnego wzrostu, wysoki, bardzo wysoki”\nWzrost mierzony w centymetrach\nCzas reakcji (w milisekundach)\nKody pocztowe: np. 00-001, 00-950, 80-452, 31-072\nWiek (w latach)\nMarki samochodów\nNarodowość\nPłeć: skala nominalna\nWynik IQ: …\nTemperatura (skala Celsjusza): …\nTemperatura (skala Kelvina): …\nFrekwencja wyborcza: …\nPrzynależność partyjna: …\nWielkość okręgu wyborczego: …\nWspółrzędne w układzie kartezjańskim: …\nData (względem określonej epoki, np. n.e.): …\nWysokość nad poziomem morza: …\nGrupy krwi: A, B, AB, 0\nKategorie dochodów: niskie, średnie, wysokie\nStopnie wojskowe: …\n\nPamiętaj, aby uzasadnić swój wybór skali dla każdej zmiennej.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "",
    "text": "5.1 Introduction to Randomness\nRandomness is a cornerstone concept in statistics and scientific research. It refers to the unpredictability of individual outcomes, even when the overall pattern may be predictable. In the social sciences, understanding randomness is crucial for designing studies, collecting data, and interpreting results.\nConsider flipping a fair coin. While we know that the probability of getting heads is 50%, we can’t predict with certainty the outcome of any single flip. This unpredictability is the essence of randomness.\nExamples of random phenomena in social sciences include:\nUnderstanding randomness helps researchers distinguish between genuine effects and chance occurrences. For instance, if we observe a slight difference in test scores between two groups, randomness helps us determine whether this difference is likely due to a real effect or just chance variation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#introduction-to-randomness",
    "href": "chapter3.html#introduction-to-randomness",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "",
    "text": "Participant Selection: In a psychology experiment studying reaction times, the order in which participants arrive at the lab may be random.\nEconomic Behavior: The daily fluctuations in stock prices often exhibit random patterns, influenced by countless unpredictable factors.\nSocial Interactions: The occurrence of chance encounters between individuals in a community can be considered random events.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-bridging-sample-and-population",
    "href": "chapter3.html#sampling-bridging-sample-and-population",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.2 Sampling: Bridging Sample and Population",
    "text": "5.2 Sampling: Bridging Sample and Population\nSampling is the process of selecting a subset (sample) from a larger group (population) to make inferences about the population. It’s a critical skill in social science research, as studying entire populations is often impractical, too expensive, or sometimes impossible.\nKey Terms:\n\nPopulation: The entire group about which we want to draw conclusions.\nSample: A subset of the population that we actually study.\nSampling Frame: The list or procedure used to identify all members of the population.\n\nExample: Suppose we want to study the job satisfaction of all teachers in the United States (the population). Instead of surveying millions of teachers, we might select a sample of 5,000 teachers from various states, school districts, and grade levels.\nRandomness in sampling helps ensure that the sample is representative of the population, reducing bias and allowing for more accurate inferences. This is why probability sampling methods, which we’ll discuss next, are often preferred in scientific research.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-methods",
    "href": "chapter3.html#sampling-methods",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.3 Sampling Methods",
    "text": "5.3 Sampling Methods\n\n5.3.1 Probability Sampling\nProbability sampling methods involve random selection, giving each member of the population a known, non-zero chance of being selected.\n\nSimple Random Sampling: Each member of the population has an equal chance of being selected.\nExample: To select 100 students from a university with 10,000 students, you could assign each student a number from 1 to 10,000, then use a random number generator to select 100 numbers.\nStratified Random Sampling: The population is divided into subgroups (strata) based on shared characteristics, then samples are randomly selected from each stratum.\nExample: In a national political survey, you might divide the population into strata based on geographic regions (Northeast, Midwest, South, West) and then randomly sample from each region. This ensures representation from all areas of the country.\nCluster Sampling: The population is divided into clusters (usually geographic), some clusters are randomly selected, and all members within those clusters are studied.\nExample: To study high school students’ study habits, you might randomly select 20 high schools from across the country and then survey all students in those schools.\nSystematic Sampling: Selecting every kth item from a list after a random start.\nExample: At a busy shopping mall, you might survey every 20th person who enters the mall, starting with a randomly chosen number between 1 and 20.\n\n\n\n5.3.2 Non-probability Sampling\nNon-probability sampling doesn’t involve random selection. While it can introduce bias, it may be necessary in certain situations, especially when dealing with hard-to-reach populations or when resources are limited.\n\nConvenience Sampling: Selecting easily accessible subjects.\nExample: A researcher studying college students’ sleep patterns might survey students in their own classes or around campus.\nPurposive Sampling: Selecting subjects based on specific characteristics.\nExample: For a study on the experiences of CEOs in the tech industry, a researcher might intentionally seek out and interview CEOs from various tech companies.\nSnowball Sampling: Participants recruit other participants.\nExample: In a study of undocumented immigrants’ access to healthcare, researchers might ask initial participants to refer other potential participants from their community.\nQuota Sampling: Selecting participants to meet specific quotas for certain characteristics.\nExample: In a market research study, researchers might ensure they interview a specific number of people from different age groups, genders, and income levels to match the demographics of the target market.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#making-inferences-from-samples",
    "href": "chapter3.html#making-inferences-from-samples",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.4 Making Inferences from Samples",
    "text": "5.4 Making Inferences from Samples\nStatistical inference is the process of drawing conclusions about a population based on a sample. This allows researchers to estimate characteristics of the entire population (parameters) using characteristics of the sample (statistics).\n\n\n\n\n\n\nNote\n\n\n\nThe Soup Analogy: A Taste of Statistics\n\n\nWhen you taste a spoonful of soup and decide it isn’t salty enough, that’s exploratory/descriptive analysis.\nIf you generalize and conclude that your entire pot of soup needs salt, that’s an inference.\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population).\nIf the soup is not well stirred (heterogeneous population), it doesn’t matter how large a spoon you have (sample size), it will still not accurately represent the whole. If the soup is well stirred (homogeneous population), even a small spoon will suffice to test the soup.\n\n\n\n\n\n\n\n\ngraph TD\n    DGP[Data Generating Process] --&gt;|Generates| A[Population]\n    A --&gt;|Random Selection| B[Sample]\n    B --&gt;|Statistical Inference| C[Estimates & Conclusions]\n    C --&gt;|Generalize back to| A\n    C -.-&gt;|Attempt to understand| DGP\n\n    style DGP fill:#1E90FF,stroke:#000,stroke-width:4px,color:#FFF\n    style A fill:#DC143C,stroke:#000,stroke-width:4px,color:#FFF\n    style B fill:#228B22,stroke:#000,stroke-width:2px,color:#FFF\n    style C fill:#8B4513,stroke:#000,stroke-width:2px,color:#FFF\n    \n    classDef note fill:#F0F0F0,stroke:#000,stroke-width:1px;\n    D[[\"DGP:\n    Underlying process\n    that generates data\"]]\n    E[[\"Population:\n    Entire group of interest\"]]\n    F[[\"Sample:\n    Subset of population\"]]\n    G[[\"Inference:\n    Drawing conclusions\n    about population\n    and DGP\"]]\n    \n    class D,E,F,G note\n    \n    D --&gt; DGP\n    E --&gt; A\n    F --&gt; B\n    G --&gt; C\n\n\n\n\n\n\nKey Concepts:\n\nPoint Estimates: A single value used to estimate a population parameter.\nExample: The mean income of a sample of 1000 workers might be used to estimate the mean income of all workers in a country.\nConfidence Intervals: A range of values likely to contain the true population parameter.\nExample: We might say, “We are 95% confident that the true population mean income falls between $45,000 and $55,000.”\nMargin of Error: The range of values above and below the sample statistic in a confidence interval.\nExample: In political polling, you might see a statement like “Candidate A is preferred by 52% of voters, with a margin of error of ±3%.”\nHypothesis Testing: A method for making decisions about population parameters based on sample data.\nExample: A researcher might test whether there’s a significant difference in test scores between students who study with music and those who study in silence.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-and-non-sampling-errors",
    "href": "chapter3.html#sampling-and-non-sampling-errors",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.5 Sampling and Non-sampling Errors",
    "text": "5.5 Sampling and Non-sampling Errors\nUnderstanding potential errors in research is crucial for interpreting results accurately.\nSampling Error: The difference between a sample statistic and the true population parameter, occurring due to chance variations in the selection of sample members.\nExample: If we estimate the average height of all adult males in a country based on a sample, our estimate will likely differ somewhat from the true average due to sampling error.\nNon-sampling Errors: Errors not due to chance, which can occur in both sample surveys and censuses.\n\nCoverage Error: When the sampling frame doesn’t accurately represent the population.\nExample: A telephone survey that only calls landlines would miss people who only have cell phones, potentially biasing the results.\nNon-response Error: When selected participants fail to respond, potentially introducing bias.\nExample: In a survey about job satisfaction, highly satisfied or highly dissatisfied employees might be more likely to respond, skewing the results.\nMeasurement Error: Inaccuracies in the data collected.\nExample: A poorly worded survey question might be interpreted differently by different respondents, leading to inconsistent data.\nProcessing Error: Mistakes made during data entry, coding, or analysis.\nExample: Accidentally entering “99” instead of “9” for a participant’s response could significantly skew the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sample-size-and-power",
    "href": "chapter3.html#sample-size-and-power",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.6 Sample Size and Power",
    "text": "5.6 Sample Size and Power\nDetermining the appropriate sample size involves balancing the need for precision with available resources.\nSample Size Considerations: - Larger samples generally provide more precise estimates but are more costly and time-consuming to obtain. - The required sample size depends on factors such as the desired level of precision, the variability in the population, and the type of analysis planned.\nExample: To estimate the proportion of voters who support a particular policy with a margin of error of ±3% at a 95% confidence level, you would need a sample size of about 1067 voters (assuming maximum variability).\nStatistical Power: The probability that a study will detect an effect when there is an effect to be detected.\nFactors affecting power: 1. Sample size 2. Effect size (the magnitude of the difference or relationship you’re trying to detect) 3. Chosen significance level (usually 0.05)\nExample: In a study comparing two teaching methods, having a larger sample size would increase the likelihood of detecting a significant difference between the methods, if such a difference exists.\n\n\n\n\n\n\nNote\n\n\n\nWhat is Study Power?\nStudy power is about how likely we are to find something if it really exists. It’s like having a good flashlight when you’re looking for something in the dark - the better your flashlight, the more likely you are to find what you’re looking for.\n\nEffect Size: How big the thing (effect, difference) we’re looking for is.\nSample Size: How many people or things we look at in our study.\nStudy Power: How likely we are to find the effect if it’s really there.\n\nThe Relationship Between Effect Size and Sample Size:\nImagine you’re trying to find coins hidden in sand:\n\nBig Effects (Big Coins):\n\nIf you’re looking for big coins (like quarters), you don’t need to search through as much sand to find them.\nIn research, if the effect is big, you can use a smaller sample.\n\nExample: Testing if a new study method improves test scores by 20 points out of 100.\n\nYou might only need to test 30 students to see this big difference.\n\nSmall Effects (Small Coins):\n\nIf you’re looking for tiny coins (like pennies), you’ll need to search through more sand.\nIn research, if the effect is small, you need a larger sample.\n\nExample: Seeing if using social media affects happiness by a tiny amount.\n\nYou might need to study 500 or more people to detect this small effect.\n\n\nWhy Study Power Matters:\n\nNot Missing Real Effects:\n\nWith low power, you might miss real effects, like using a weak flashlight and missing something that’s actually there.\n\nConfidence in Results:\n\nHigher power gives you more confidence that what you found is real and not just luck.\n\n\nExample:\nLet’s say we want to study if a new teaching method helps students learn better:\n\nSmall Study (Low Power):\n\nWe try the method with just 10 students.\nEven if the method works, with such a small group, it’s hard to tell if improvements are due to the new method or just chance.\n\nLarger Study (Higher Power):\n\nWe use the method with 100 students.\nNow we’re more likely to see if the method really helps because we have more data to look at.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-in-the-digital-age",
    "href": "chapter3.html#sampling-in-the-digital-age",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.7 Sampling in the Digital Age",
    "text": "5.7 Sampling in the Digital Age\nThe advent of big data and digital technologies has transformed sampling practices in many fields.\nBig Data Opportunities and Challenges: - Unprecedented volumes of information available - Potential lack of representativeness - Data quality concerns - Privacy and ethical issues\nExample: Social media data can provide real-time insights into public opinion, but users of a particular platform may not be representative of the general population.\nWeb-based Surveys: - Offer new opportunities for data collection - Face challenges such as coverage bias (not everyone has internet access) and self-selection bias\nExample: An online survey about internet usage habits would inherently exclude people without internet access, potentially biasing the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#ethical-considerations-in-sampling",
    "href": "chapter3.html#ethical-considerations-in-sampling",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.8 Ethical Considerations in Sampling",
    "text": "5.8 Ethical Considerations in Sampling\nEthical sampling practices are crucial in social science research:\n\nInformed Consent: Participants should understand the study’s purpose and agree to participate.\nExample: Before conducting interviews about sensitive topics like mental health, researchers must clearly explain the study’s aims and potential risks to participants.\nPrivacy and Confidentiality: Researchers must protect participants’ personal information.\nExample: In a study on workplace harassment, researchers might use code numbers instead of names to protect participants’ identities.\nRepresentativeness and Inclusivity: Samples should fairly represent diverse populations, including marginalized groups.\nExample: A study on urban housing should make efforts to include participants from various socioeconomic backgrounds, ethnicities, and housing situations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#conclusion",
    "href": "chapter3.html#conclusion",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.9 Conclusion",
    "text": "5.9 Conclusion\nSampling remains a cornerstone of social science research, even in the era of big data. Understanding sampling principles helps researchers design studies, interpret results, and make valid inferences about populations. As we’ve seen, the journey from sample to population involves careful consideration of sampling methods, potential errors, ethical issues, and the ever-evolving landscape of data collection in the digital age.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#statistical-errors---summary",
    "href": "chapter3.html#statistical-errors---summary",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.10 Statistical Errors - Summary",
    "text": "5.10 Statistical Errors - Summary\n\n5.10.1 Systematic Error vs. Random Error\nSystematic errors and random errors are two fundamental types of errors in statistical measurements and experiments.\n\nSystematic Error:\n\nDefinition: Consistent, predictable deviations from the true value\nCharacteristics:\n\nBiases the results in a specific direction\nRepeatable and often constant across measurements\nCan be corrected if identified\n\nExamples:\n\nMiscalibrated measuring instrument\nConsistent rounding error in data entry\nBiased sampling method\n\n\nRandom Error:\n\nDefinition: Unpredictable fluctuations in measurements due to chance\nCharacteristics:\n\nVaries in magnitude and direction\nFollows a probability distribution (often normal)\nCan be reduced by increasing sample size or repeated measurements\n\nExamples:\n\nNatural variations in the phenomenon being measured\nSmall fluctuations in measuring instruments\nHuman errors in reading or recording data\n\n\n\n\n\n5.10.2 Sampling Errors vs. Non-Sampling Errors\nSampling and non-sampling errors are categories of errors that can occur in statistical studies, particularly in survey research.\n\nSampling Errors:\n\nDefinition: Errors that occur due to the sample not perfectly representing the population\nCharacteristics:\n\nInherent in any sample-based study\nCan be estimated and quantified using statistical methods\nDecreases as sample size increases\n\nExamples:\n\nRandom fluctuations in sample statistics\nOver- or under-representation of certain groups in the sample\n\n\nNon-Sampling Errors:\n\nDefinition: All errors in a study that are not related to sampling\nCharacteristics:\n\nCan occur in both sample and census studies\nOften more difficult to quantify and control than sampling errors\nCan introduce bias into results\nCan be either systematic or random\n\nExamples:\n\nResponse errors (e.g., misunderstanding questions, deliberate misreporting)\nNonresponse bias (when certain groups are less likely to respond)\nData processing errors (e.g., coding mistakes, data entry errors)\nCoverage errors (when the sampling frame doesn’t accurately represent the population)\n\n\n\nImportant clarification: Non-sampling errors can indeed be either systematic or random. This is a crucial distinction that should have been included in the original description. Non-sampling errors encompass a wide range of potential errors that are not directly related to the sampling process. Some of these can be systematic (e.g., a miscalibrated measuring instrument), while others can be random (e.g., occasional mistakes in data entry).\nThe distinction between sampling and non-sampling errors is independent of the division between systematic and random errors. In practice, non-sampling errors can fall into both of these categories, which makes their identification and control a particularly important aspect of statistical research.\nUnderstanding these types of errors is crucial for designing robust statistical studies, interpreting results accurately, and making valid inferences about populations based on sample data.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#review-questions-and-exercises",
    "href": "chapter3.html#review-questions-and-exercises",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.11 Review Questions and Exercises",
    "text": "5.11 Review Questions and Exercises\n(…)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html",
    "href": "rozdzial3.html",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "",
    "text": "6.1 Wprowadzenie do Losowości\nLosowość jest fundamentalnym pojęciem w statystyce i badaniach naukowych. Odnosi się do nieprzewidywalności indywidualnych wyników, nawet gdy ogólny wzorzec może być przewidywalny. W naukach społecznych zrozumienie losowości jest kluczowe dla projektowania badań, zbierania danych i interpretacji wyników.\nRozważmy rzut uczciwą monetą. Chociaż wiemy, że prawdopodobieństwo wypadnięcia orła wynosi 50%, nie możemy z pewnością przewidzieć wyniku pojedynczego rzutu. Ta nieprzewidywalność jest istotą losowości.\nPrzykłady losowych zjawisk w naukach społecznych obejmują:\nZrozumienie losowości pomaga badaczom odróżnić rzeczywiste efekty od przypadkowych zdarzeń. Na przykład, jeśli zaobserwujemy niewielką różnicę w wynikach testów między dwiema grupami, losowość pomaga nam określić, czy ta różnica jest prawdopodobnie spowodowana rzeczywistym efektem, czy tylko przypadkową zmiennością.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wprowadzenie-do-losowości",
    "href": "rozdzial3.html#wprowadzenie-do-losowości",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "",
    "text": "Wybór uczestników: W eksperymencie psychologicznym badającym czasy reakcji, kolejność, w jakiej uczestnicy przybywają do laboratorium, może być losowa.\nZachowania ekonomiczne: Codzienne wahania cen akcji często wykazują losowe wzorce, na które wpływa niezliczona ilość nieprzewidywalnych czynników.\nInterakcje społeczne: Występowanie przypadkowych spotkań między osobami w społeczności można uznać za zdarzenia losowe.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#próbkowanie-łączenie-próby-i-populacji",
    "href": "rozdzial3.html#próbkowanie-łączenie-próby-i-populacji",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.2 Próbkowanie: Łączenie Próby i Populacji",
    "text": "6.2 Próbkowanie: Łączenie Próby i Populacji\nPróbkowanie to proces wybierania podzbioru (próby) z większej grupy (populacji) w celu wyciągnięcia wniosków o populacji. Jest to kluczowa umiejętność w badaniach nauk społecznych, ponieważ badanie całych populacji jest często niepraktyczne, zbyt kosztowne lub czasami niemożliwe.\nKluczowe pojęcia:\n\nPopulacja: Cała grupa, o której chcemy wyciągnąć wnioski.\nPróba: Podzbiór populacji, który faktycznie badamy.\nOperat losowania: Lista lub procedura używana do identyfikacji wszystkich członków populacji.\n\nPrzykład: Załóżmy, że chcemy zbadać satysfakcję z pracy wszystkich nauczycieli w Polsce (populacja). Zamiast ankietować setki tysięcy nauczycieli, możemy wybrać próbę 5000 nauczycieli z różnych województw, powiatów i poziomów nauczania.\nLosowość w próbkowaniu pomaga zapewnić, że próba jest reprezentatywna dla populacji, zmniejszając błędy systematyczne i umożliwiając dokładniejsze wnioskowanie. Dlatego metody próbkowania probabilistycznego, które omówimy dalej, są często preferowane w badaniach naukowych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#metody-próbkowania",
    "href": "rozdzial3.html#metody-próbkowania",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.3 Metody Próbkowania",
    "text": "6.3 Metody Próbkowania\n\n6.3.1 Próbkowanie Probabilistyczne\nMetody próbkowania probabilistycznego obejmują losowy wybór, dając każdemu członkowi populacji znaną, niezerową szansę na wybór.\n\nProsty Dobór Losowy: Każdy członek populacji ma równą szansę na wybór.\nPrzykład: Aby wybrać 100 studentów z uniwersytetu liczącego 10 000 studentów, można przypisać każdemu studentowi numer od 1 do 10 000, a następnie użyć generatora liczb losowych do wybrania 100 numerów.\nDobór Losowy Warstwowy: Populacja jest podzielona na podgrupy (warstwy) na podstawie wspólnych cech, a następnie próbki są losowo wybierane z każdej warstwy.\nPrzykład: W ogólnopolskim badaniu politycznym można podzielić populację na warstwy na podstawie regionów geograficznych (np. Polska Zachodnia, Centralna, Wschodnia) i losowo pobierać próbki z każdego regionu. Zapewnia to reprezentację ze wszystkich obszarów kraju.\nDobór Losowy Grupowy: Populacja jest podzielona na skupiska (zwykle geograficzne), niektóre skupiska są losowo wybierane, a wszyscy członkowie w tych skupiskach są badani.\nPrzykład: Aby zbadać nawyki uczenia się uczniów szkół średnich, można losowo wybrać 20 szkół z całego kraju, a następnie przeprowadzić ankietę wśród wszystkich uczniów w tych szkołach.\nDobór Systematyczny: Wybieranie co k-tego elementu z listy po losowym starcie.\nPrzykład: W ruchliwym centrum handlowym można ankietować co 20. osobę wchodzącą do centrum, zaczynając od losowo wybranej liczby między 1 a 20.\n\n\n\n6.3.2 Próbkowanie Nieprobabilistyczne\nPróbkowanie nieprobabilistyczne nie obejmuje losowego wyboru. Chociaż może wprowadzać błędy systematyczne, może być konieczne w niektórych sytuacjach, zwłaszcza w przypadku trudno dostępnych populacji lub gdy zasoby są ograniczone.\n\nDobór Wygodny: Wybieranie łatwo dostępnych podmiotów.\nPrzykład: Badacz studiujący wzorce snu studentów może przeprowadzić ankietę wśród studentów na własnych zajęciach lub na terenie kampusu.\nDobór Celowy: Wybieranie podmiotów na podstawie określonych cech.\nPrzykład: W badaniu doświadczeń prezesów w branży technologicznej badacz może celowo szukać i przeprowadzać wywiady z prezesami różnych firm technologicznych.\nDobór Metodą Kuli Śnieżnej: Uczestnicy rekrutują innych uczestników.\nPrzykład: W badaniu dostępu imigrantów bez dokumentów do opieki zdrowotnej, badacze mogą poprosić początkowych uczestników o polecenie innych potencjalnych uczestników z ich społeczności.\nDobór Kwotowy: Wybieranie uczestników w celu spełnienia określonych kwot dla pewnych cech.\nPrzykład: W badaniu rynku badacze mogą zapewnić, że przeprowadzają wywiady z określoną liczbą osób z różnych grup wiekowych, płci i poziomów dochodów, aby dopasować się do demografii rynku docelowego.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wnioskowanie-z-prób",
    "href": "rozdzial3.html#wnioskowanie-z-prób",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.4 Wnioskowanie z Prób",
    "text": "6.4 Wnioskowanie z Prób\nWnioskowanie statystyczne to proces wyciągania wniosków o populacji na podstawie próby. Pozwala to badaczom oszacować charakterystyki całej populacji (parametry) przy użyciu charakterystyk próby (statystyk).\n\n\n\n\n\n\nNote\n\n\n\nThe Soup Analogy: A Taste of Statistics\n\n\nWhen you taste a spoonful of soup and decide it isn’t salty enough, that’s exploratory/descriptive analysis.\nIf you generalize and conclude that your entire pot of soup needs salt, that’s an inference.\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population).\nIf the soup is not well stirred (heterogeneous population), it doesn’t matter how large a spoon you have (sample size), it will still not accurately represent the whole. If the soup is well stirred (homogeneous population), even a small spoon will suffice to test the soup.\n\n\n\nKluczowe pojęcia:\n\nEstymatory punktowe: Pojedyncza wartość używana do oszacowania parametru populacji.\nPrzykład: Średni dochód z próby 1000 pracowników może być użyty do oszacowania średniego dochodu wszystkich pracowników w kraju.\nPrzedziały ufności: Zakres wartości, który prawdopodobnie zawiera prawdziwy parametr populacji.\nPrzykład: Możemy powiedzieć: “Jesteśmy w 95% pewni, że prawdziwy średni dochód populacji mieści się między 4500 a 5500 złotych”.\nMargines błędu: Zakres wartości powyżej i poniżej statystyki z próby w przedziale ufności.\nPrzykład: W sondażach politycznych można zobaczyć stwierdzenie: “Kandydat A jest preferowany przez 52% wyborców, z marginesem błędu ±3%”.\nTestowanie hipotez: Metoda podejmowania decyzji o parametrach populacji na podstawie danych z próby.\nPrzykład: Badacz może testować, czy istnieje istotna różnica w wynikach testów między uczniami, którzy uczą się przy muzyce, a tymi, którzy uczą się w ciszy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#błędy-próbkowania-i-błędy-niepróbkowe",
    "href": "rozdzial3.html#błędy-próbkowania-i-błędy-niepróbkowe",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.5 Błędy Próbkowania i Błędy Niepróbkowe",
    "text": "6.5 Błędy Próbkowania i Błędy Niepróbkowe\nZrozumienie potencjalnych błędów w badaniach jest kluczowe dla dokładnej interpretacji wyników.\nBłąd próbkowania: Różnica między statystyką z próby a prawdziwym parametrem populacji, występująca z powodu przypadkowych wahań w wyborze członków próby.\nPrzykład: Jeśli oszacujemy średni wzrost wszystkich dorosłych mężczyzn w kraju na podstawie próby, nasze oszacowanie prawdopodobnie będzie się nieco różnić od prawdziwej średniej z powodu błędu próbkowania.\nBłędy niepróbkowe: Błędy nie wynikające z przypadku, które mogą wystąpić zarówno w badaniach próbkowych, jak i spisach.\n\nBłąd pokrycia: Gdy operat losowania nie reprezentuje dokładnie populacji.\nPrzykład: Badanie telefoniczne, które dzwoni tylko na telefony stacjonarne, pominęłoby osoby posiadające tylko telefony komórkowe, potencjalnie wypaczając wyniki.\nBłąd braku odpowiedzi: Gdy wybrani uczestnicy nie odpowiadają, potencjalnie wprowadzając błąd systematyczny.\nPrzykład: W badaniu satysfakcji z pracy, bardzo zadowoleni lub bardzo niezadowoleni pracownicy mogą być bardziej skłonni do odpowiedzi, wypaczając wyniki.\nBłąd pomiaru: Niedokładności w zebranych danych.\nPrzykład: Źle sformułowane pytanie ankietowe może być różnie interpretowane przez różnych respondentów, prowadząc do niespójnych danych.\nBłąd przetwarzania: Błędy popełnione podczas wprowadzania danych, kodowania lub analizy.\nPrzykład: Przypadkowe wprowadzenie “99” zamiast “9” dla odpowiedzi uczestnika mogłoby znacząco wypaczyć wyniki.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wielkość-próby-i-moc-statystyczna",
    "href": "rozdzial3.html#wielkość-próby-i-moc-statystyczna",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.6 Wielkość Próby i Moc Statystyczna",
    "text": "6.6 Wielkość Próby i Moc Statystyczna\nOkreślenie odpowiedniej wielkości próby wymaga zrównoważenia potrzeby precyzji z dostępnymi zasobami.\nRozważania dotyczące wielkości próby: - Większe próby generalnie zapewniają bardziej precyzyjne oszacowania, ale są bardziej kosztowne i czasochłonne do uzyskania. - Wymagana wielkość próby zależy od czynników takich jak pożądany poziom precyzji, zmienność w populacji i rodzaj planowanej analizy.\nPrzykład: Aby oszacować proporcję wyborców popierających konkretną politykę z marginesem błędu ±3% na poziomie ufności 95%, potrzebna byłaby próba około 1067 wyborców (zakładając maksymalną zmienność).\nMoc statystyczna: Prawdopodobieństwo, że badanie wykryje efekt, gdy taki efekt istnieje.\nCzynniki wpływające na moc: 1. Wielkość próby 2. Wielkość efektu (wielkość różnicy lub związku, który próbujemy wykryć) 3. Wybrany poziom istotności (zwykle 0,05)\nPrzykład: W badaniu porównującym dwie metody nauczania, większa wielkość próby zwiększyłaby prawdopodobieństwo wykrycia istotnej różnicy między metodami, jeśli taka różnica istnieje.\n\n\n\n\n\n\nNote\n\n\n\nCzym jest Moc Badania?\nMoc badania dotyczy tego, jak prawdopodobne jest, że znajdziemy coś, jeśli to naprawdę istnieje. To jak posiadanie dobrej latarki, gdy szukasz czegoś w ciemności - im lepsza latarka, tym bardziej prawdopodobne, że znajdziesz to, czego szukasz.\n\nWielkość Efektu: Jak duża jest rzecz (efekt, różnica, itp.), której szukamy.\nWielkość Próby: Ile osób lub rzeczy badamy w naszym studium.\nMoc Badania: Jak prawdopodobne jest, że znajdziemy efekt, jeśli naprawdę istnieje.\n\nZwiązek Między Wielkością Efektu a Wielkością Próby:\nWyobraź sobie, że próbujesz znaleźć monety ukryte w piasku:\n\nDuże Efekty (Duże Monety):\n\nJeśli szukasz dużych monet (jak 5 złotych), nie musisz przeszukiwać tak dużo piasku, aby je znaleźć.\nW badaniach, jeśli efekt jest duży, możesz użyć mniejszej próby.\n\nPrzykład: Testowanie, czy nowa metoda nauki poprawia wyniki testów o 20 punktów na 100.\n\nMoże wystarczyć przetestować 30 uczniów, aby zobaczyć tę dużą różnicę.\n\nMałe Efekty (Małe Monety):\n\nJeśli szukasz maleńkich monet (jak 1 grosz), będziesz musiał przeszukać więcej piasku.\nW badaniach, jeśli efekt jest mały, potrzebujesz większej próby.\n\nPrzykład: Sprawdzanie, czy korzystanie z mediów społecznościowych wpływa na szczęście o niewielką ilość.\n\nMoże być potrzeba zbadania 500 lub więcej osób, aby wykryć ten mały efekt.\n\n\nDlaczego Moc Badania Jest Ważna:\n\nNie Przegapienie Rzeczywistych Efektów:\n\nPrzy niskiej mocy możesz przeoczyć rzeczywiste efekty, jak używanie słabej latarki i przeoczenie czegoś, co faktycznie tam jest.\n\nPewność Wyników:\n\nWyższa moc daje większą pewność, że to, co znalazłeś, jest prawdziwe, a nie tylko przypadkiem.\n\n\nPrzykład:\nZałóżmy, że chcemy zbadać, czy nowa metoda nauczania pomaga uczniom lepiej się uczyć:\n\nMałe Badanie (Niska Moc):\n\nPróbujemy metody z zaledwie 10 uczniami.\nNawet jeśli metoda działa, przy tak małej grupie trudno stwierdzić, czy poprawa wynika z nowej metody, czy to tylko przypadek.\n\nWiększe Badanie (Wyższa Moc):\n\nStosujemy metodę ze 100 uczniami.\nTeraz jest bardziej prawdopodobne, że zobaczymy, czy metoda naprawdę pomaga, ponieważ mamy więcej danych do analizy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#próbkowanie-w-erze-cyfrowej",
    "href": "rozdzial3.html#próbkowanie-w-erze-cyfrowej",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.7 Próbkowanie w Erze Cyfrowej",
    "text": "6.7 Próbkowanie w Erze Cyfrowej\nPojawienie się big data i technologii cyfrowych zmieniło praktyki próbkowania w wielu dziedzinach.\nMożliwości i wyzwania Big Data: - Bezprecedensowe ilości dostępnych informacji - Potencjalny brak reprezentatywności - Problemy z jakością danych - Kwestie prywatności i etyki\nPrzykład: Dane z mediów społecznościowych mogą dostarczyć wglądu w opinię publiczną w czasie rzeczywistym, ale użytkownicy konkretnej platformy mogą nie być reprezentatywni dla ogólnej populacji.\nBadania internetowe: - Oferują nowe możliwości zbierania danych - Stają przed wyzwaniami takimi jak błąd pokrycia (nie każdy ma dostęp do internetu) i błąd samoselekcji\nPrzykład: Ankieta online na temat nawyków korzystania z internetu z natury wykluczałaby osoby bez dostępu do internetu, potencjalnie wypaczając wyniki.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#etyczne-aspekty-próbkowania",
    "href": "rozdzial3.html#etyczne-aspekty-próbkowania",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.8 Etyczne Aspekty Próbkowania",
    "text": "6.8 Etyczne Aspekty Próbkowania\nEtyczne praktyki próbkowania są kluczowe w badaniach nauk społecznych:\n\nŚwiadoma zgoda: Uczestnicy powinni rozumieć cel badania i zgodzić się na udział.\nPrzykład: Przed przeprowadzeniem wywiadów na temat wrażliwych tematów, takich jak zdrowie psychiczne, badacze muszą jasno wyjaśnić cele badania i potencjalne ryzyko uczestnikom.\nPrywatność i poufność: Badacze muszą chronić dane osobowe uczestników.\nPrzykład: W badaniu dotyczącym mobbingu w miejscu pracy, badacze mogą używać kodów numerycznych zamiast nazwisk, aby chronić tożsamość uczestników.\nReprezentatywność i inkluzywność: Próby powinny sprawiedliwie reprezentować zróżnicowane populacje, w tym grupy marginalizowane.\n\nPrzykład: Badanie dotyczące mieszkalnictwa miejskiego powinno dołożyć starań, aby uwzględnić uczestników z różnych środowisk społeczno-ekonomicznych, grup etnicznych i sytuacji mieszkaniowych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#podsumowanie",
    "href": "rozdzial3.html#podsumowanie",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.9 Podsumowanie",
    "text": "6.9 Podsumowanie\nPróbkowanie pozostaje fundamentem badań w naukach społecznych, nawet w erze big data. Zrozumienie zasad próbkowania pomaga badaczom projektować badania, interpretować wyniki i wyciągać trafne wnioski o populacjach. Jak widzieliśmy, droga od próby do populacji wymaga starannego rozważenia metod próbkowania, potencjalnych błędów, kwestii etycznych i stale ewoluującego krajobrazu gromadzenia danych w erze cyfrowej.\nDziękuję za to bardzo trafne pytanie. Ma Pan/Pani rację, i doceniam tę uwagę. Rzeczywiście, tłumaczenie “błąd samplowy vs. niesamplowy” jest bardziej precyzyjne i lepiej oddaje istotę tych pojęć w polskiej terminologii statystycznej. Pozwolę sobie wprowadzić odpowiednie korekty i wyjaśnienia:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#błędy-statystyczne---podsumowanie",
    "href": "rozdzial3.html#błędy-statystyczne---podsumowanie",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.10 Błędy Statystyczne - podsumowanie",
    "text": "6.10 Błędy Statystyczne - podsumowanie\n\n6.10.1 Błąd Systematyczny vs. Błąd Losowy\nBłędy systematyczne i losowe to dwa podstawowe rodzaje błędów w pomiarach i eksperymentach statystycznych.\n\nBłąd Systematyczny:\n\nDefinicja: Konsekwentne, przewidywalne odchylenia od prawdziwej wartości\nCharakterystyka:\n\nZniekształca wyniki w określonym kierunku\nPowtarzalny i często stały w różnych pomiarach\nMoże być skorygowany, jeśli zostanie zidentyfikowany\n\nPrzykłady:\n\nŹle skalibrowany przyrząd pomiarowy\nKonsekwentny błąd zaokrąglania przy wprowadzaniu danych\nStronnicza metoda pobierania próbek\n\n\nBłąd Losowy:\n\nDefinicja: Nieprzewidywalne wahania w pomiarach wynikające z przypadku\nCharakterystyka:\n\nZmienia się co do wielkości i kierunku\nPodąża za rozkładem prawdopodobieństwa (często normalnym)\nMożna go zmniejszyć zwiększając wielkość próby lub powtarzając pomiary\n\nPrzykłady:\n\nNaturalne wahania w badanym zjawisku\nMałe fluktuacje w przyrządach pomiarowych\nBłędy ludzkie przy odczytywaniu lub zapisywaniu danych\n\n\n\n\n\n6.10.2 Błędy Samplowe vs. Błędy Niesamplowe\n\nBłędy Samplowe (lub Błędy Próbkowania):\n\nDefinicja: Błędy wynikające z tego, że próba (sample) nie reprezentuje idealnie populacji\nCharakterystyka:\n\nNieodłączne w każdym badaniu opartym na próbie\nMożna je oszacować i skwantyfikować za pomocą metod statystycznych\nZmniejszają się wraz ze wzrostem wielkości próby\n\nPrzykłady:\n\nLosowe wahania w statystykach (z) próby\nNadreprezentacja lub niedoreprezentowanie niektórych grup w próbie\n\n\nBłędy Niesamplowe:\n\nDefinicja: Wszystkie błędy w badaniu, które nie są związane z próbkowaniem (samplingiem)\nCharakterystyka:\n\nMogą wystąpić zarówno w badaniach próbkowych, jak i pełnych (spisach)\nCzęsto trudniejsze do skwantyfikowania i kontrolowania niż błędy samplowe\nMogą wprowadzać stronniczość do wyników\nMogą być systematyczne lub losowe\n\nPrzykłady:\n\nBłędy odpowiedzi (np. niezrozumienie pytań, celowe błędne raportowanie)\nBłąd braku odpowiedzi (gdy niektóre grupy są mniej skłonne do odpowiedzi)\nBłędy przetwarzania danych (np. błędy kodowania, błędy wprowadzania danych)\nBłędy pokrycia (gdy operat losowania nie reprezentuje dokładnie populacji)\n\n\n\nWażne wyjaśnienie: Błędy niesamplowe mogą być zarówno systematyczne, jak i losowe. To kluczowe rozróżnienie, które powinienem był uwzględnić w pierwotnym opisie. Błędy niesamplowe obejmują szeroki zakres możliwych błędów, które nie są bezpośrednio związane z procesem próbkowania. Niektóre z nich mogą być systematyczne (np. błędnie skalibrowany instrument pomiarowy), podczas gdy inne mogą być losowe (np. przypadkowe błędy przy wprowadzaniu danych).\nRozróżnienie na błędy samplowe i niesamplowe jest niezależne od podziału na błędy systematyczne i losowe. W praktyce, błędy niesamplowe mogą należeć do obu tych kategorii, co czyni ich identyfikację i kontrolę szczególnie ważnym aspektem badań statystycznych.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff\n\n\nKluczowe punkty do zapamiętania:\n\nLosowość jest podstawą wielu metod próbkowania i pomaga zapewnić reprezentatywność próby.\nIstnieją różne metody próbkowania, zarówno probabilistyczne, jak i nieprobabilistyczne, każda z własnymi zaletami i ograniczeniami.\nWnioskowanie statystyczne pozwala nam wyciągać wnioski o populacji na podstawie danych z próby.\nBłędy próbkowania i niepróbkowe mogą wpływać na jakość naszych wniosków, dlatego ważne jest ich zrozumienie i minimalizowanie.\nWielkość próby i moc statystyczna są kluczowe dla zapewnienia wiarygodności wyników badań.\nEra cyfrowa przynosi nowe możliwości i wyzwania w zakresie próbkowania i gromadzenia danych.\nEtyczne aspekty próbkowania, w tym świadoma zgoda, prywatność i reprezentatywność, są nieodłączną częścią procesu badawczego.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "chapter3b.html",
    "href": "chapter3b.html",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "",
    "text": "7.1 Defining Reliability and Validity\nReliability refers to the consistency of a measure. A reliable measurement or study produces similar results under consistent conditions.\nValidity refers to the accuracy of a measure. A valid measurement or study accurately represents what it claims to measure.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#the-four-combinations-of-reliability-and-validity",
    "href": "chapter3b.html#the-four-combinations-of-reliability-and-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.2 The Four Combinations of Reliability and Validity",
    "text": "7.2 The Four Combinations of Reliability and Validity\nThere are four possible combinations of reliability and validity:\n\nHigh Reliability, High Validity\nHigh Reliability, Low Validity\nLow Reliability, High Validity\nLow Reliability, Low Validity\n\nLet’s explore each of these combinations with examples and visualizations.\n\n7.2.1 1. High Reliability, High Validity\nThis is the ideal scenario in research. Measurements are both consistent and accurate.\nExample: A well-calibrated digital scale used to measure weight. It consistently gives the same reading for the same object and accurately represents the true weight.\n\n\n7.2.2 2. High Reliability, Low Validity\nIn this case, measurements are consistent but not accurate.\nExample: A miscalibrated scale that always measures 5 kg too heavy. It gives consistent results (high reliability) but doesn’t represent the true weight (low validity).\n\n\n7.2.3 3. Low Reliability, High Validity\nHere, measurements are accurate on average but inconsistent.\nExample: A scale that fluctuates around the true weight. Sometimes it’s a bit over, sometimes a bit under, but on average, it’s correct.\n\n\n7.2.4 4. Low Reliability, Low Validity\nThis is the worst-case scenario, where measurements are neither consistent nor accurate.\nExample: A broken scale that gives random readings unrelated to the true weight.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#visualizing-reliability-and-validity",
    "href": "chapter3b.html#visualizing-reliability-and-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.3 Visualizing Reliability and Validity",
    "text": "7.3 Visualizing Reliability and Validity\nTo better understand these concepts, let’s create visualizations using ggplot2 in R. We’ll simulate measurement data for each scenario and plot them.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generate data for each scenario\nn &lt;- 100\ntrue_value &lt;- 50\n\ndata &lt;- tibble(\n  high_rel_high_val = rnorm(n, mean = true_value, sd = 1),\n  high_rel_low_val = rnorm(n, mean = true_value + 5, sd = 1),\n  low_rel_high_val = rnorm(n, mean = true_value, sd = 5),\n  low_rel_low_val = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenario\", values_to = \"measurement\")\n\n# Create the scatterplot\nscatter_plot &lt;- ggplot(data, aes(x = id, y = measurement, color = scenario)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = true_value, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Scatterplots of Measurements\",\n       subtitle = \"Dashed line represents the true value\",\n       x = \"Measurement ID\",\n       y = \"Measured Value\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Create the histogram\nhist_plot &lt;- ggplot(data, aes(x = measurement, fill = scenario)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = true_value, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Histograms of Measurements\",\n       subtitle = \"Red dashed line represents the true value\",\n       x = \"Measured Value\",\n       y = \"Count\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Combine the plots\ncombined_plot &lt;- scatter_plot / hist_plot +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Reliability and Validity in Measurements\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Display the combined plot\ncombined_plot\n\n\n\n\n\n\n\n\n\n7.3.1 Interpreting the Visualizations\n\nHigh Reliability, High Validity: Points cluster tightly around the true value (dashed line).\nHigh Reliability, Low Validity: Points cluster tightly, but consistently above the true value.\nLow Reliability, High Validity: Points scatter widely but center around the true value.\nLow Reliability, Low Validity: Points scatter randomly with no clear pattern or relation to the true value.\n\nUnderstanding reliability and validity is crucial in data science and research. High reliability ensures consistent measurements, while high validity ensures accurate representations of what we intend to measure. By considering both aspects, researchers can design more robust studies and draw more meaningful conclusions from their data.\nWhen conducting your own research or analyzing others’ work, always consider: - How reliable are the measurements? - How valid is the approach for measuring the intended concept? - Do the methods used support both reliability and validity?\nBy keeping these questions in mind, you’ll be better equipped to produce and interpret high-quality research in data science.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#types-of-reliability",
    "href": "chapter3b.html#types-of-reliability",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.4 Types of Reliability",
    "text": "7.4 Types of Reliability\nReliability can be assessed in several ways, each focusing on a different aspect of consistency:\n\nTest-Retest Reliability: This measures the consistency of a test over time. It involves administering the same test to the same group of participants at different times and comparing the results.\nInter-Rater Reliability: This assesses the degree of agreement among different raters or observers. It’s crucial when subjective judgments are involved in data collection.\nInternal Consistency: This evaluates how well different items on a test or scale measure the same construct. Cronbach’s alpha is a common measure of internal consistency.\nParallel Forms Reliability: This involves creating two equivalent forms of a test and administering them to the same group. The correlation between the two sets of scores indicates reliability.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#types-of-validity",
    "href": "chapter3b.html#types-of-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.5 Types of Validity",
    "text": "7.5 Types of Validity\nValidity is a multifaceted concept, with several types that researchers need to consider:\n\nContent Validity: This ensures that a measure covers all aspects of the construct it aims to measure. It’s often assessed by expert judgment.\nConstruct Validity: This evaluates whether a test measures the intended theoretical construct. It includes:\n\nConvergent Validity: The degree to which the measure correlates with other measures of the same construct.\nDiscriminant Validity: The extent to which the measure does not correlate with measures of different constructs.\n\nCriterion Validity: This assesses how well a measure predicts an outcome. It includes:\n\nConcurrent Validity: How well the measure correlates with other measures of the same construct at the same time.\nPredictive Validity: How well the measure predicts future outcomes.\n\nFace Validity: Face validity describes how test subjects perceive the test and whether - from their point of view - it is adequate for the purpose it is supposed to serve. A lack of face validity, even though the test may be valid from the perspective of a specific purpose, can contribute to a decrease in motivation among test subjects, which directly affects the results achieved or may lead to rejection of the test. While not a scientific measure, it can be important for participant buy-in.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#internal-vs.-external-validity",
    "href": "chapter3b.html#internal-vs.-external-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.6 Internal vs. External Validity",
    "text": "7.6 Internal vs. External Validity\nThese concepts are crucial in experimental design and the generalizability of research findings:\n\n7.6.1 Internal Validity\nInternal validity refers to the extent to which a study establishes a causal relationship between the independent and dependent variables. It answers the question: “Did the experimental treatment actually cause the observed effects?”\nFactors that can threaten internal validity include: - History: External events occurring between pre-test and post-test - Maturation: Natural changes in participants over time - Testing effects: Changes due to taking a pre-test - Instrumentation: Changes in the measurement tool or observers - Selection bias: Non-random assignment to groups - Attrition: Loss of participants during the study\n\n\n7.6.2 External Validity\nExternal validity refers to the extent to which the results of a study can be generalized to other situations, populations, or settings. It addresses the question: “To what extent can the findings be applied beyond the specific context of the study?”\nFactors that can affect external validity include: - Population validity: How well the sample represents the larger population - Ecological validity: How well the study setting represents real-world conditions - Temporal validity: Whether the results hold true across time",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#consistency-in-research",
    "href": "chapter3b.html#consistency-in-research",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.7 Consistency in Research",
    "text": "7.7 Consistency in Research\nConsistency is closely related to reliability but extends beyond just measurement. In research, consistency refers to the overall coherence and stability of results across different contexts, methods, or studies.\nKey aspects of consistency in research include:\n\nReplicability: The ability to reproduce study results using the same methods and data.\nRobustness: The stability of findings across different analytical approaches or slight variations in methodology.\nConvergence: The alignment of results from different studies or methods investigating the same phenomenon.\nLongitudinal Consistency: The stability of findings over time, especially important in longitudinal studies.\n\nEnsuring consistency in research involves: - Using standardized procedures and measures - Thoroughly documenting methods and analytical decisions - Conducting replication studies - Meta-analyses to synthesize findings across multiple studies",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#balancing-reliability-validity-and-consistency",
    "href": "chapter3b.html#balancing-reliability-validity-and-consistency",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.8 Balancing Reliability, Validity, and Consistency",
    "text": "7.8 Balancing Reliability, Validity, and Consistency\nWhile reliability, validity, and consistency are all crucial for high-quality research, they sometimes involve trade-offs:\n\nA highly reliable measure might lack validity if it consistently measures the wrong thing.\nStriving for perfect internal validity (e.g., in tightly controlled lab experiments) might reduce external validity.\nEnsuring high consistency across diverse contexts might require sacrificing some degree of precision or depth in specific situations.\n\nResearchers must carefully balance these aspects based on their research questions and the nature of their study. A comprehensive understanding of reliability, validity, and consistency helps in designing robust studies, interpreting results accurately, and contributing meaningfully to the body of scientific knowledge.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#bias-variance-tradeoff",
    "href": "chapter3b.html#bias-variance-tradeoff",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.9 Bias-Variance Tradeoff",
    "text": "7.9 Bias-Variance Tradeoff\nThe concepts of reliability and validity are closely related to the statistical notion of the bias-variance tradeoff. This tradeoff is fundamental in machine learning and statistical modeling.\n\nBias refers to the error introduced by approximating a real-world problem with a simplified model. High bias can lead to underfitting.\nVariance refers to the error introduced by the model’s sensitivity to small fluctuations in the training set. High variance can lead to overfitting.\n\nLet’s visualize this concept with a simplified plot:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_true &lt;- sin(x)\ny_low_bias_high_var &lt;- y_true + rnorm(100, 0, 0.3)\ny_high_bias_low_var &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_true, y_low_bias_high_var, y_high_bias_low_var),\n                 type = rep(c(\"True Function\", \"Low Bias, High Variance\", \"High Bias, Low Variance\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = type)) +\n  geom_line() +\n  geom_point(data = subset(df, type != \"True Function\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Bias-Variance Tradeoff\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Model Type\") +\n  theme_minimal()\n\n\n\n\nVisualization of Bias-Variance Tradeoff\n\n\n\n\nIn this plot: - The black line represents the true underlying function. - The blue points represent a model with low bias but high variance. It follows the true function closely on average but has a lot of noise. - The red line represents a model with high bias but low variance. It consistently underestimates the true function but has less noise.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#accuracy-and-precision",
    "href": "chapter3b.html#accuracy-and-precision",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.10 Accuracy and Precision",
    "text": "7.10 Accuracy and Precision\nThe concepts of accuracy and precision are closely related to validity and reliability:\n\nAccuracy refers to how close a measurement is to the true value (similar to validity).\nPrecision refers to how consistent or reproducible the measurements are (similar to reliability).\n\nWe can visualize these concepts using a simplified target analogy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"High Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Low Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"High Accuracy\\nLow Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Low Accuracy\\nLow Precision\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Accuracy vs Precision\")\n\n\n\n\nVisualization of Accuracy vs Precision\n\n\n\n\nIn this visualization: - High accuracy means the points are close to the center (bullseye). - High precision means the points are tightly clustered. - Each panel represents a different combination of accuracy and precision.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#conclusion",
    "href": "chapter3b.html#conclusion",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.11 Conclusion",
    "text": "7.11 Conclusion\nUnderstanding reliability and validity is crucial for conducting robust research. These concepts help us ensure that our measurements are both consistent and accurate. By relating them to ideas like the bias-variance tradeoff and accuracy-precision, we gain a deeper appreciation of the challenges involved in measurement and modeling in scientific research. As researchers, we must strive to develop measures and models that are both reliable and valid, balancing the tradeoffs between bias and variance, and between accuracy and precision. This requires careful design of research methodologies, rigorous testing of our measurement instruments, and thoughtful interpretation of our results.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#understanding-bias-vs.-variance-in-statistical-measurement",
    "href": "chapter3b.html#understanding-bias-vs.-variance-in-statistical-measurement",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.12 Understanding Bias vs. Variance in Statistical Measurement",
    "text": "7.12 Understanding Bias vs. Variance in Statistical Measurement\n\n7.12.1 Introduction\nIn statistics and machine learning, two important concepts that affect the performance of our models are bias and variance. Understanding these concepts is crucial for building effective predictive models and avoiding common pitfalls like overfitting and underfitting.\n\nBias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can lead to underfitting.\n\nThink of bias as how far off our predictions are from the true values on average.\nIn terms of validity, high bias means our model isn’t capturing the true relationship in the data.\n\nVariance refers to the amount by which our model would change if we estimated it using a different training dataset. High variance can lead to overfitting.\n\nThink of variance as how much our predictions would fluctuate if we used different datasets.\nIn terms of reliability, high variance means our model is too sensitive to the specific data it was trained on.\n\n\nWe’ll explore four scenarios to illustrate different combinations of bias and variance using synthetic data and regression models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#data-generation-and-model-fitting-function",
    "href": "chapter3b.html#data-generation-and-model-fitting-function",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.13 Data Generation and Model Fitting Function",
    "text": "7.13 Data Generation and Model Fitting Function\nFirst, let’s create a function that will help us generate data and fit models for each scenario:\n\ngenerate_and_fit &lt;- function(n, intercept, slope, noise_sd, model_degree) {\n  # Generate synthetic data\n  x &lt;- runif(n, 0, 10)\n  y &lt;- intercept + slope * x + rnorm(n, 0, noise_sd)\n  \n  # Fit model\n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", model_degree, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  # Generate predictions\n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  # Plot\n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    geom_abline(intercept = intercept, slope = slope, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Model Degree:\", model_degree),\n         subtitle = paste(\"Noise SD:\", noise_sd),\n         x = \"X (Input Variable)\",\n         y = \"Y (Target Variable)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nThis function does the following: 1. Generates synthetic data based on our parameters 2. Fits a polynomial regression model 3. Creates a plot showing the true relationship (blue dashed line), our model’s predictions (red solid line), and the data points\nNow, let’s explore our four scenarios!\n\n7.13.1 Scenario 1: Low Bias, Low Variance\nIn this ideal scenario, we use a linear model to fit linear data with low noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 1, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The model (red line) closely follows the true relationship (blue dashed line). - Data points are clustered tightly around the line, indicating low noise. - This scenario represents a good fit: the model captures the underlying trend without being overly complex.\n\n\n7.13.2 Scenario 2: Low Bias, High Variance\nHere, we use a linear model to fit linear data, but with high noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 5, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The model still captures the general trend, but data points are more scattered. - This high variance means our model’s predictions would be less reliable. - In real-world terms, this might represent a situation where our measurements are correct on average but have a lot of random error.\n\n\n7.13.3 Scenario 3: High Bias, Low Variance\nIn this case, we use a linear model to fit quadratic (curved) data with low noise.\n\nquadratic_data &lt;- function(n, intercept, slope, noise_sd, model_degree) {\n  x &lt;- runif(n, 0, 10)\n  y &lt;- intercept + slope * x^2 + rnorm(n, 0, noise_sd)\n  \n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", model_degree, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    stat_function(fun = function(x) intercept + slope * x^2, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Model Degree:\", model_degree),\n         subtitle = paste(\"Noise SD:\", noise_sd),\n         x = \"X (Input Variable)\",\n         y = \"Y (Target Variable)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nquadratic_data(n = 100, intercept = 1, slope = 0.2, noise_sd = 1, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The linear model (red line) fails to capture the curvature of the true relationship (blue dashed line). - This high bias means our model is consistently off in its predictions. - In real-world terms, this might represent using an overly simplistic model for a complex phenomenon.\n\n\n7.13.4 Scenario 4: High Bias, High Variance\nFinally, we use a high-degree polynomial to fit linear data with high noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 5, model_degree = 5)\n\n\n\n\n\n\n\n\nExplanation: - The model (red line) is overly complex, trying to fit the noise rather than the underlying trend. - This combination of high bias and high variance leads to poor generalization. - In real-world terms, this might represent overcomplicating our analysis and drawing false conclusions from random fluctuations in our data.\n\n\n7.13.5 Conclusion\nUnderstanding the bias-variance trade-off is crucial in statistical modeling:\n\nLow Bias, Low Variance: The ideal scenario, where our model accurately captures the underlying relationship without being overly sensitive to noise.\nLow Bias, High Variance: Our model is correct on average but unreliable due to high sensitivity to individual data points.\nHigh Bias, Low Variance: Our model is consistently wrong due to oversimplification but gives stable predictions.\nHigh Bias, High Variance: The worst-case scenario, where our model is both inaccurate and unreliable.\n\nIn practice, we often need to balance bias and variance. Techniques like cross-validation, regularization, and ensemble methods can help find this balance.\nRemember: - A model with high bias is too simple and misses important patterns in the data. - A model with high variance is too complex and fits noise in the training data. - The goal is to find a sweet spot that captures true patterns without overfitting to noise.\nBy understanding these concepts, you’ll be better equipped to choose appropriate models, avoid overfitting and underfitting, and build more effective predictive models in your future statistical analyses!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html",
    "href": "rozdzial3b.html",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "",
    "text": "8.1 Definiowanie Rzetelności i Trafności\nRzetelność odnosi się do spójności pomiaru. Rzetelny pomiar lub badanie daje podobne wyniki w spójnych warunkach.\nTrafność odnosi się do dokładności pomiaru. Trafny pomiar lub badanie dokładnie reprezentuje to, co twierdzi, że mierzy.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#cztery-kombinacje-rzetelności-i-trafności",
    "href": "rozdzial3b.html#cztery-kombinacje-rzetelności-i-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.2 Cztery Kombinacje Rzetelności i Trafności",
    "text": "8.2 Cztery Kombinacje Rzetelności i Trafności\nIstnieją cztery możliwe kombinacje rzetelności i trafności:\n\nWysoka Rzetelność, Wysoka Trafność\nWysoka Rzetelność, Niska Trafność\nNiska Rzetelność, Wysoka Trafność\nNiska Rzetelność, Niska Trafność\n\nPrzyjrzyjmy się każdej z tych kombinacji z przykładami i wizualizacjami.\n\n8.2.1 1. Wysoka Rzetelność, Wysoka Trafność\nTo idealny scenariusz w badaniach. Pomiary są zarówno spójne, jak i dokładne.\nPrzykład: Dobrze skalibrowana waga cyfrowa używana do pomiaru wagi. Konsekwentnie daje ten sam odczyt dla tego samego obiektu i dokładnie reprezentuje prawdziwą wagę.\n\n\n8.2.2 2. Wysoka Rzetelność, Niska Trafność\nW tym przypadku pomiary są spójne, ale niedokładne.\nPrzykład: Źle skalibrowana waga, która zawsze mierzy 5 kg za ciężko. Daje spójne wyniki (wysoka rzetelność), ale nie reprezentuje prawdziwej wagi (niska trafność).\n\n\n8.2.3 3. Niska Rzetelność, Wysoka Trafność\nTutaj pomiary są dokładne średnio, ale niespójne.\nPrzykład: Waga, która waha się wokół prawdziwej wagi. Czasami pokazuje trochę więcej, czasami trochę mniej, ale średnio jest poprawna.\n\n\n8.2.4 4. Niska Rzetelność, Niska Trafność\nTo najgorszy scenariusz, gdzie pomiary nie są ani spójne, ani dokładne.\nPrzykład: Zepsuta waga, która daje losowe odczyty niezwiązane z prawdziwą wagą.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#wizualizacja-rzetelności-i-trafności",
    "href": "rozdzial3b.html#wizualizacja-rzetelności-i-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.3 Wizualizacja Rzetelności i Trafności",
    "text": "8.3 Wizualizacja Rzetelności i Trafności\nAby lepiej zrozumieć te pojęcia, stwórzmy wizualizacje przy użyciu ggplot2 w R. Zasymulujemy dane pomiarowe dla każdego scenariusza i narysujemy je.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generowanie danych dla każdego scenariusza\nn &lt;- 100\nprawdziwa_wartosc &lt;- 50\n\ndane &lt;- tibble(\n  wysoka_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 1),\n  wysoka_rz_niska_tr = rnorm(n, mean = prawdziwa_wartosc + 5, sd = 1),\n  niska_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 5),\n  niska_rz_niska_tr = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenariusz\", values_to = \"pomiar\")\n\n# Tworzenie wykresu punktowego\nwykres_punktowy &lt;- ggplot(dane, aes(x = id, y = pomiar, color = scenariusz)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = prawdziwa_wartosc, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelność, Wysoka Trafność\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelność, Niska Trafność\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelność, Wysoka Trafność\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelność, Niska Trafność\"\n             ))) +\n  labs(title = \"Wykresy punktowe pomiarów\",\n       subtitle = \"Przerywana linia reprezentuje prawdziwą wartość\",\n       x = \"ID pomiaru\",\n       y = \"Zmierzona wartość\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Tworzenie histogramu\nwykres_hist &lt;- ggplot(dane, aes(x = pomiar, fill = scenariusz)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = prawdziwa_wartosc, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelność, Wysoka Trafność\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelność, Niska Trafność\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelność, Wysoka Trafność\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelność, Niska Trafność\"\n             ))) +\n  labs(title = \"Histogramy pomiarów\",\n       subtitle = \"Czerwona przerywana linia reprezentuje prawdziwą wartość\",\n       x = \"Zmierzona wartość\",\n       y = \"Liczba\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Łączenie wykresów\nwykres_polaczony &lt;- wykres_punktowy / wykres_hist +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Rzetelność i Trafność w Pomiarach\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Wyświetlanie połączonego wykresu\nwykres_polaczony\n\n\n\n\n\n\n\n\n\n8.3.1 Interpretacja Wizualizacji\n\nWysoka Rzetelność, Wysoka Trafność: Punkty grupują się ciasno wokół prawdziwej wartości (przerywana linia).\nWysoka Rzetelność, Niska Trafność: Punkty grupują się ciasno, ale konsekwentnie powyżej prawdziwej wartości.\nNiska Rzetelność, Wysoka Trafność: Punkty rozpraszają się szeroko, ale centrują się wokół prawdziwej wartości.\nNiska Rzetelność, Niska Trafność: Punkty rozpraszają się losowo bez wyraźnego wzoru lub relacji do prawdziwej wartości.\n\nZrozumienie rzetelności i trafności jest kluczowe w naukach o danych i badaniach. Wysoka rzetelność zapewnia spójne pomiary, podczas gdy wysoka trafność zapewnia dokładne reprezentacje tego, co zamierzamy zmierzyć. Biorąc pod uwagę oba aspekty, badacze mogą projektować bardziej solidne badania i wyciągać bardziej znaczące wnioski ze swoich danych.\nProwadząc własne badania lub analizując pracę innych, zawsze należy rozważyć: - Jak rzetelne są pomiary? - Jak trafne jest podejście do pomiaru zamierzonego pojęcia? - Czy stosowane metody wspierają zarówno rzetelność, jak i trafność?\nMając na uwadze te pytania, będziesz lepiej przygotowany do prowadzenia i interpretowania wysokiej jakości badań w naukach o danych.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#rodzaje-rzetelności",
    "href": "rozdzial3b.html#rodzaje-rzetelności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.4 Rodzaje Rzetelności",
    "text": "8.4 Rodzaje Rzetelności\nRzetelność można oceniać na kilka sposobów, każdy skupiający się na innym aspekcie spójności:\n\nRzetelność test-retest: Mierzy spójność testu w czasie. Polega na przeprowadzeniu tego samego testu na tej samej grupie uczestników w różnych momentach i porównaniu wyników.\nRzetelność między oceniającymi: Ocenia stopień zgodności między różnymi oceniającymi lub obserwatorami. Jest kluczowa, gdy w zbieraniu danych biorą udział subiektywne osądy.\nSpójność wewnętrzna: Ocenia, jak dobrze różne elementy testu lub skali mierzą ten sam konstrukt. Alfa Cronbacha jest powszechną miarą spójności wewnętrznej.\nRzetelność form równoległych: Polega na stworzeniu dwóch równoważnych form testu i przeprowadzeniu ich na tej samej grupie. Korelacja między dwoma zestawami wyników wskazuje na rzetelność.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#rodzaje-trafności",
    "href": "rozdzial3b.html#rodzaje-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.5 Rodzaje Trafności",
    "text": "8.5 Rodzaje Trafności\nTrafność jest pojęciem wieloaspektowym, z kilkoma rodzajami, które badacze muszą wziąć pod uwagę:\n\nTrafność treściowa: Zapewnia, że pomiar obejmuje wszystkie aspekty konstruktu, który ma mierzyć. Często jest oceniana przez osąd ekspertów.\nTrafność konstrukcyjna: Ocenia, czy test mierzy zamierzony konstrukt teoretyczny. Obejmuje:\n\nTrafność zbieżną: Stopień, w jakim pomiar koreluje z innymi pomiarami tego samego konstruktu.\nTrafność różnicową: Zakres, w jakim pomiar nie koreluje z pomiarami różnych konstruktów.\n\nTrafność kryterialną: Ocenia, jak dobrze pomiar przewiduje wynik. Obejmuje:\n\nTrafność współbieżną: Jak dobrze pomiar koreluje z innymi pomiarami tego samego konstruktu w tym samym czasie.\nTrafność predykcyjną: Jak dobrze pomiar przewiduje przyszłe wyniki.\n\nTrafność fasadowa: Trafność fasadowa odnosi się do tego, jak osoby badane postrzegają test i czy uważają go za odpowiedni do celu, któremu ma służyć. Brak trafności fasadowej może mieć negatywne konsekwencje, nawet jeśli test jest faktycznie trafny (czyli mierzy to, co powinien mierzyć) z punktu widzenia jego zamierzonego celu. Choć nie jest to naukowa miara, może być ważna dla zaangażowania uczestników.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#trafność-wewnętrzna-vs-zewnętrzna",
    "href": "rozdzial3b.html#trafność-wewnętrzna-vs-zewnętrzna",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.6 Trafność Wewnętrzna vs Zewnętrzna",
    "text": "8.6 Trafność Wewnętrzna vs Zewnętrzna\nTe pojęcia są kluczowe w projektowaniu eksperymentów i możliwości uogólniania wyników badań:\n\n8.6.1 Trafność Wewnętrzna\nTrafność wewnętrzna odnosi się do zakresu, w jakim badanie ustanawia związek przyczynowy między zmiennymi niezależnymi a zależnymi. Odpowiada na pytanie: “Czy eksperymentalne traktowanie rzeczywiście spowodowało zaobserwowane efekty?”\nCzynniki, które mogą zagrażać trafności wewnętrznej, obejmują: - Historia: Zewnętrzne wydarzenia występujące między pre-testem a post-testem - Dojrzewanie: Naturalne zmiany u uczestników w czasie - Efekty testowania: Zmiany wynikające z przeprowadzenia pre-testu - Instrumentacja: Zmiany w narzędziu pomiarowym lub obserwatorach - Błąd selekcji: Nielosowy przydział do grup - Utrata: Utrata uczestników podczas badania\n\n\n8.6.2 Trafność Zewnętrzna\nTrafność zewnętrzna odnosi się do zakresu, w jakim wyniki badania mogą być uogólnione na inne sytuacje, populacje lub ustawienia. Odpowiada na pytanie: “W jakim stopniu wyniki mogą być zastosowane poza konkretnym kontekstem badania?”\nCzynniki, które mogą wpływać na trafność zewnętrzną, obejmują: - Trafność populacyjna: Jak dobrze próba reprezentuje szerszą populację - Trafność ekologiczna: Jak dobrze ustawienie badania reprezentuje warunki świata rzeczywistego - Trafność czasowa: Czy wyniki pozostają prawdziwe w czasie",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#spójność-w-badaniach",
    "href": "rozdzial3b.html#spójność-w-badaniach",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.7 Spójność w Badaniach",
    "text": "8.7 Spójność w Badaniach\nSpójność jest ściśle związana z rzetelnością, ale wykracza poza sam pomiar. W badaniach spójność odnosi się do ogólnej koherencji i stabilności wyników w różnych kontekstach, metodach lub badaniach.\nKluczowe aspekty spójności w badaniach obejmują:\n\nReplikowalność: Zdolność do odtworzenia wyników badania przy użyciu tych samych metod i danych.\nOdporność: Stabilność wyników w różnych podejściach analitycznych lub niewielkich zmianach w metodologii.\nKonwergencja: Zbieżność wyników z różnych badań lub metod badających to samo zjawisko.\nSpójność długoterminowa: Stabilność wyników w czasie, szczególnie ważna w badaniach długoterminowych.\n\nZapewnienie spójności w badaniach obejmuje: - Stosowanie standaryzowanych procedur i miar - Dokładne dokumentowanie metod i decyzji analitycznych - Przeprowadzanie badań replikacyjnych - Meta-analizy w celu syntezy wyników z wielu badań",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#równoważenie-rzetelności-trafności-i-spójności",
    "href": "rozdzial3b.html#równoważenie-rzetelności-trafności-i-spójności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.8 Równoważenie Rzetelności, Trafności i Spójności",
    "text": "8.8 Równoważenie Rzetelności, Trafności i Spójności\nChociaż rzetelność, trafność i spójność są kluczowe dla wysokiej jakości badań, czasami wiążą się z kompromisami:\n\nWysoce rzetelna miara może nie mieć trafności, jeśli konsekwentnie mierzy niewłaściwą rzecz.\nDążenie do idealnej trafności wewnętrznej (np. w ściśle kontrolowanych eksperymentach laboratoryjnych) może zmniejszyć trafność zewnętrzną.\nZapewnienie wysokiej spójności w różnych kontekstach może wymagać poświęcenia pewnego stopnia precyzji lub głębi w konkretnych sytuacjach.\n\nBadacze muszą starannie równoważyć te aspekty w oparciu o swoje pytania badawcze i charakter badania. Kompleksowe zrozumienie rzetelności, trafności i spójności pomaga w projektowaniu solidnych badań, dokładnej interpretacji wyników i znaczącym wkładzie do korpusu wiedzy naukowej.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#kompromis-między-obciążeniem-a-wariancją",
    "href": "rozdzial3b.html#kompromis-między-obciążeniem-a-wariancją",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.9 Kompromis między Obciążeniem a Wariancją",
    "text": "8.9 Kompromis między Obciążeniem a Wariancją\nPojęcia rzetelności i trafności są ściśle związane ze statystycznym pojęciem kompromisu między obciążeniem a wariancją. Ten kompromis jest fundamentalny w uczeniu maszynowym i modelowaniu statystycznym.\n\nObciążenie odnosi się do błędu wprowadzonego przez przybliżenie problemu ze świata rzeczywistego uproszczonym modelem. Wysokie obciążenie może prowadzić do niedopasowania.\nWariancja odnosi się do błędu wprowadzonego przez wrażliwość modelu na małe fluktuacje w zbiorze treningowym. Wysoka wariancja może prowadzić do przeuczenia.\n\nZobrazujmy to pojęcie za pomocą uproszczonego wykresu:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_prawdziwa &lt;- sin(x)\ny_niskie_obciazenie_wysoka_wariancja &lt;- y_prawdziwa + rnorm(100, 0, 0.3)\ny_wysokie_obciazenie_niska_wariancja &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_prawdziwa, y_niskie_obciazenie_wysoka_wariancja, y_wysokie_obciazenie_niska_wariancja),\n                 typ = rep(c(\"Prawdziwa Funkcja\", \"Niskie Obciążenie, Wysoka Wariancja\", \"Wysokie Obciążenie, Niska Wariancja\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = typ)) +\n  geom_line() +\n  geom_point(data = subset(df, typ != \"Prawdziwa Funkcja\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Kompromis między Obciążeniem a Wariancją\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Typ Modelu\") +\n  theme_minimal()\n\n\n\n\nWizualizacja kompromisu między obciążeniem a wariancją\n\n\n\n\nNa tym wykresie: - Czarna linia reprezentuje prawdziwą funkcję bazową. - Niebieskie punkty reprezentują model z niskim obciążeniem, ale wysoką wariancją. Średnio podąża blisko prawdziwej funkcji, ale ma dużo szumu. - Czerwona linia reprezentuje model z wysokim obciążeniem, ale niską wariancją. Konsekwentnie niedoszacowuje prawdziwej funkcji, ale ma mniej szumu.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#dokładność-i-precyzja",
    "href": "rozdzial3b.html#dokładność-i-precyzja",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.10 Dokładność i Precyzja",
    "text": "8.10 Dokładność i Precyzja\nPojęcia dokładności i precyzji są ściśle związane z trafnością i rzetelnością:\n\nDokładność odnosi się do tego, jak blisko pomiar jest prawdziwej wartości (podobnie do trafności).\nPrecyzja odnosi się do tego, jak spójne lub powtarzalne są pomiary (podobnie do rzetelności).\n\nMożemy zobrazować te pojęcia za pomocą uproszczonej analogii do tarczy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"Wysoka Dokładność\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Niska Dokładność\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"Wysoka Dokładność\\nNiska Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Niska Dokładność\\nNiska Precyzja\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Dokładność vs Precyzja\")\n\n\n\n\nWizualizacja Dokładności vs Precyzji\n\n\n\n\nW tej wizualizacji: - Wysoka dokładność oznacza, że punkty są blisko środka (dziesiątki). - Wysoka precyzja oznacza, że punkty są ściśle zgrupowane. - Każdy panel reprezentuje inną kombinację dokładności i precyzji.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#podsumowanie",
    "href": "rozdzial3b.html#podsumowanie",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.11 Podsumowanie",
    "text": "8.11 Podsumowanie\nZrozumienie rzetelności i trafności jest kluczowe dla prowadzenia solidnych badań. Pojęcia te pomagają nam zapewnić, że nasze pomiary są zarówno spójne, jak i dokładne. Łącząc je z ideami takimi jak kompromis między obciążeniem a wariancją oraz dokładnością i precyzją, zyskujemy głębsze zrozumienie wyzwań związanych z pomiarem i modelowaniem w badaniach naukowych. Jako badacze musimy dążyć do opracowania miar i modeli, które są zarówno rzetelne, jak i trafne, równoważąc kompromisy między obciążeniem a wariancją oraz między dokładnością a precyzją. Wymaga to starannego projektowania metodologii badań, rygorystycznego testowania naszych instrumentów pomiarowych i przemyślanej interpretacji naszych wyników.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#zrozumienie-obciążenia-vs.-wariancji-w-pomiarach-statystycznych",
    "href": "rozdzial3b.html#zrozumienie-obciążenia-vs.-wariancji-w-pomiarach-statystycznych",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.12 Zrozumienie Obciążenia vs. Wariancji w Pomiarach Statystycznych",
    "text": "8.12 Zrozumienie Obciążenia vs. Wariancji w Pomiarach Statystycznych\n\n8.12.1 Wprowadzenie\nW statystyce i uczeniu maszynowym dwa ważne pojęcia, które wpływają na wydajność naszych modeli, to obciążenie (bias) i wariancja (variance). Zrozumienie tych pojęć jest kluczowe dla budowania efektywnych modeli predykcyjnych i unikania typowych pułapek, takich jak przeuczenie i niedouczenie.\n\nObciążenie odnosi się do błędu wprowadzonego przez przybliżenie rzeczywistego problemu, który może być złożony, za pomocą uproszczonego modelu. Wysokie obciążenie może prowadzić do niedouczenia.\n\nWyobraź sobie obciążenie jako średnią odległość naszych przewidywań od prawdziwych wartości.\nW kontekście trafności, wysokie obciążenie oznacza, że nasz model nie uchwycił prawdziwej zależności w danych.\n\nWariancja odnosi się do tego, jak bardzo nasz model zmieniłby się, gdybyśmy oszacowali go przy użyciu innego zbioru treningowego. Wysoka wariancja może prowadzić do przeuczenia.\n\nWyobraź sobie wariancję jako to, jak bardzo nasze przewidywania wahałyby się, gdybyśmy użyli różnych zbiorów danych.\nW kontekście rzetelności, wysoka wariancja oznacza, że nasz model jest zbyt wrażliwy na konkretne dane, na których został wytrenowany.\n\n\nZbadamy cztery scenariusze, aby zilustrować różne kombinacje obciążenia i wariancji przy użyciu syntetycznych danych i modeli regresji.\n\n\n8.12.2 Funkcja Generowania Danych i Dopasowywania Modelu\nNajpierw stwórzmy funkcję, która pomoże nam generować dane i dopasowywać modele dla każdego scenariusza:\n\ngeneruj_i_dopasuj &lt;- function(n, wyraz_wolny, nachylenie, odch_szumu, stopien_modelu) {\n  # Generowanie syntetycznych danych\n  x &lt;- runif(n, 0, 10)\n  y &lt;- wyraz_wolny + nachylenie * x + rnorm(n, 0, odch_szumu)\n  \n  # Dopasowanie modelu\n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", stopien_modelu, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  # Generowanie przewidywań\n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  # Wykres\n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    geom_abline(intercept = wyraz_wolny, slope = nachylenie, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Stopień Modelu:\", stopien_modelu),\n         subtitle = paste(\"Odchylenie Standardowe Szumu:\", odch_szumu),\n         x = \"X (Zmienna Wejściowa)\",\n         y = \"Y (Zmienna Docelowa)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nTa funkcja wykonuje następujące czynności: 1. Generuje syntetyczne dane na podstawie naszych parametrów 2. Dopasowuje model regresji wielomianowej 3. Tworzy wykres pokazujący prawdziwą zależność (niebieska przerywana linia), przewidywania naszego modelu (czerwona ciągła linia) i punkty danych\nTeraz zbadajmy nasze cztery scenariusze!\n\n\n8.12.3 Scenariusz 1: Niskie Obciążenie, Niska Wariancja\nW tym idealnym scenariuszu używamy modelu liniowego do dopasowania danych liniowych z niskim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 1, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model (czerwona linia) ściśle podąża za prawdziwą zależnością (niebieska przerywana linia). - Punkty danych są skupione blisko linii, co wskazuje na niski szum. - Ten scenariusz reprezentuje dobre dopasowanie: model uchwycił podstawowy trend bez nadmiernej złożoności.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#scenariusz-2-niskie-obciążenie-wysoka-wariancja",
    "href": "rozdzial3b.html#scenariusz-2-niskie-obciążenie-wysoka-wariancja",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.13 Scenariusz 2: Niskie Obciążenie, Wysoka Wariancja",
    "text": "8.13 Scenariusz 2: Niskie Obciążenie, Wysoka Wariancja\nTutaj używamy modelu liniowego do dopasowania danych liniowych, ale z wysokim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 5, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model nadal uchwycił ogólny trend, ale punkty danych są bardziej rozproszone. - Ta wysoka wariancja oznacza, że przewidywania naszego modelu byłyby mniej wiarygodne. - W rzeczywistych warunkach mogłoby to reprezentować sytuację, w której nasze pomiary są średnio poprawne, ale mają dużo losowego błędu.\n\n8.13.1 Scenariusz 3: Wysokie Obciążenie, Niska Wariancja\nW tym przypadku używamy modelu liniowego do dopasowania danych kwadratowych (zakrzywionych) z niskim szumem.\n\ndane_kwadratowe &lt;- function(n, wyraz_wolny, nachylenie, odch_szumu, stopien_modelu) {\n  x &lt;- runif(n, 0, 10)\n  y &lt;- wyraz_wolny + nachylenie * x^2 + rnorm(n, 0, odch_szumu)\n  \n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", stopien_modelu, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    stat_function(fun = function(x) wyraz_wolny + nachylenie * x^2, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Stopień Modelu:\", stopien_modelu),\n         subtitle = paste(\"Odchylenie Standardowe Szumu:\", odch_szumu),\n         x = \"X (Zmienna Wejściowa)\",\n         y = \"Y (Zmienna Docelowa)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\ndane_kwadratowe(n = 100, wyraz_wolny = 1, nachylenie = 0.2, odch_szumu = 1, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model liniowy (czerwona linia) nie uchwycił krzywizny prawdziwej zależności (niebieska przerywana linia). - To wysokie obciążenie oznacza, że nasz model konsekwentnie myli się w swoich przewidywaniach. - W rzeczywistych warunkach mogłoby to reprezentować użycie zbyt uproszczonego modelu dla złożonego zjawiska.\n\n\n8.13.2 Scenariusz 4: Wysokie Obciążenie, Wysoka Wariancja\nNa koniec używamy wielomianu wysokiego stopnia do dopasowania danych liniowych z wysokim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 5, stopien_modelu = 5)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model (czerwona linia) jest zbyt złożony, próbując dopasować się do szumu zamiast do podstawowego trendu. - Ta kombinacja wysokiego obciążenia i wysokiej wariancji prowadzi do słabej generalizacji. - W rzeczywistych warunkach mogłoby to reprezentować nadmierne skomplikowanie naszej analizy i wyciąganie fałszywych wniosków z losowych fluktuacji w naszych danych.\n\n\n8.13.3 Podsumowanie\nZrozumienie kompromisu między obciążeniem a wariancją jest kluczowe w modelowaniu statystycznym:\n\nNiskie Obciążenie, Niska Wariancja: Idealny scenariusz, w którym nasz model dokładnie uchwycił podstawową zależność bez nadmiernej wrażliwości na szum.\nNiskie Obciążenie, Wysoka Wariancja: Nasz model jest średnio poprawny, ale niewiarygodny ze względu na wysoką wrażliwość na pojedyncze punkty danych.\nWysokie Obciążenie, Niska Wariancja: Nasz model jest konsekwentnie błędny z powodu nadmiernego uproszczenia, ale daje stabilne przewidywania.\nWysokie Obciążenie, Wysoka Wariancja: Najgorszy scenariusz, w którym nasz model jest zarówno niedokładny, jak i niewiarygodny.\n\nW praktyce często musimy zrównoważyć obciążenie i wariancję. Techniki takie jak walidacja krzyżowa, regularyzacja i metody zespołowe mogą pomóc w znalezieniu tej równowagi.\nPamiętaj: - Model z wysokim obciążeniem jest zbyt prosty i pomija ważne wzorce w danych. - Model z wysoką wariancją jest zbyt złożony i dopasowuje się do szumu w danych treningowych. - Celem jest znalezienie złotego środka, który uchwyci prawdziwe wzorce bez nadmiernego dopasowania do szumu.\nZrozumienie tych pojęć pomoże ci lepiej wybierać odpowiednie modele, unikać przeuczenia i niedouczenia oraz budować bardziej efektywne modele predykcyjne w przyszłych analizach statystycznych!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "",
    "text": "9.1 Introduction\nResearch designs are fundamental to the scientific process, providing structured approaches to investigate hypotheses and answer research questions. This chapter explores two main categories of research designs: experimental and non-experimental, with a focus on the Neyman-Rubin potential outcome framework. We’ll delve into various design types, their characteristics, and provide practical examples using R for data analysis and visualization.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#experimental-designs",
    "href": "chapter4.html#experimental-designs",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.2 Experimental Designs",
    "text": "9.2 Experimental Designs\nExperimental designs are characterized by the researcher’s control over the independent variable(s) and random assignment of subjects to different conditions. These designs are considered the gold standard for establishing causal relationships.\n\n9.2.1 Randomized Controlled Trials (RCTs)\nRCTs are the most rigorous form of experimental design. They involve:\n\nRandom assignment of subjects to treatment and control groups\nManipulation of the independent variable\nMeasurement of the dependent variable\n\nLet’s visualize a simple RCT design:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Create sample data\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  group = factor(rep(c(\"Control\", \"Treatment\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Simulate treatment effect\ndata$post_test &lt;- ifelse(data$group == \"Treatment\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Reshape data for plotting\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"time\", values_to = \"score\")\n\n# Create plot\nggplot(data_long, aes(x = time, y = score, color = group, group = interaction(id, group))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = group), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Pre-test and Post-test Scores in RCT\",\n       x = \"Time\", y = \"Score\", color = \"Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nRandomized Controlled Trial Design\n\n\n\n\nThis plot shows individual trajectories and group means for pre-test and post-test scores in a hypothetical RCT. The treatment group shows a clear increase in scores compared to the control group.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "href": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.3 A/B Testing: An Example and Comparison with RCTs",
    "text": "9.3 A/B Testing: An Example and Comparison with RCTs\nA/B testing is a widely used experimental method in digital marketing, user experience design, and product development. This chapter will present an example of A/B testing, explain its methodology, and discuss how it differs from Randomized Controlled Trials (RCTs).\n\n9.3.1 Example: Website Landing Page Conversion Rate\nLet’s consider an example where an e-commerce company wants to improve the conversion rate of their landing page. They decide to test two different layouts: the current layout (A) and a new layout (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Simulate data\nn_visitors &lt;- 10000\ndata &lt;- data.frame(\n  Version = sample(c(\"A\", \"B\"), n_visitors, replace = TRUE),\n  Converted = rbinom(n_visitors, 1, ifelse(sample(c(\"A\", \"B\"), n_visitors, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Calculate conversion rates\nconversion_rates &lt;- data %&gt;%\n  group_by(Version) %&gt;%\n  summarise(\n    Visitors = n(),\n    Conversions = sum(Converted),\n    ConversionRate = mean(Converted)\n  )\n\n# Visualize results\nggplot(conversion_rates, aes(x = Version, y = ConversionRate, fill = Version)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", ConversionRate * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"A/B Test: Landing Page Conversion Rates\",\n       x = \"Page Version\", y = \"Conversion Rate\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure 9.1: A/B Test Results: Landing Page Conversion Rates\n\n\n\n\n\nIn this example, we simulated data for 10,000 visitors randomly assigned to either version A or B of the landing page. The results show that version B has a slightly higher conversion rate (11.44%) compared to version A (10.94%).\n\n\n9.3.2 A/B Testing Methodology\nA/B testing typically follows these steps:\n\nIdentify the element to be tested (e.g., landing page layout).\nCreate two versions: the control (A) and the variant (B).\nRandomly assign visitors to either version.\nCollect data on the metric of interest (e.g., conversion rate).\nAnalyze the results using statistical methods.\nMake a decision based on the results.\n\n\n\n9.3.3 Differences between A/B Testing and RCTs\nWhile A/B testing and Randomized Controlled Trials (RCTs) share some similarities, they have several key differences:\n\nScope and Context:\n\nA/B Testing: Typically used in digital environments for quick, iterative improvements.\nRCTs: Used in various fields, including medicine, psychology, and social sciences, often for more complex interventions.\n\nDuration:\n\nA/B Testing: Usually shorter, often running for days or weeks.\nRCTs: Can last months or years, especially in medical research.\n\nSample Size:\n\nA/B Testing: Can involve very large sample sizes due to ease of implementation in digital platforms.\nRCTs: Sample sizes are often smaller due to practical and cost constraints.\n\nBlinding:\n\nA/B Testing: Participants are usually unaware they’re part of a test.\nRCTs: May involve single, double, or triple blinding to reduce bias.\n\nEthical Considerations:\n\nA/B Testing: Generally involves low-risk changes with minimal ethical concerns.\nRCTs: Often require extensive ethical review, especially in medical contexts.\n\nOutcome Measures:\n\nA/B Testing: Typically focuses on a single, easily measurable outcome (e.g., click-through rate).\nRCTs: Often measure multiple outcomes, including potential side effects or long-term impacts.\n\nGeneralizability:\n\nA/B Testing: Results are often specific to the platform or context tested.\nRCTs: Aim for broader generalizability, though this can vary.\n\nAnalysis Complexity:\n\nA/B Testing: Often uses simpler statistical analyses.\nRCTs: May involve more complex statistical methods to account for various factors.\n\n\nA/B testing is a powerful tool for making data-driven decisions in digital environments. While it shares the fundamental principle of randomization with RCTs, it is typically simpler, faster, and more focused on specific, measurable outcomes in digital contexts. Understanding these differences helps researchers and practitioners choose the most appropriate method for their specific needs and constraints.\n\n\n9.3.4 Example 1: Effect of Sleep Duration on Cognitive Performance\nResearch Question: Does increasing sleep duration improve cognitive performance in college students?\n\n# Generating sample data\nset.seed(456)\nn &lt;- 100\npre_experimental &lt;- rnorm(n, mean = 70, sd = 10)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = 8, sd = 5)\npre_control &lt;- rnorm(n, mean = 70, sd = 10)\npost_control &lt;- pre_control + rnorm(n, mean = 1, sd = 5)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Experimental\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  Score = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = Score, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Effect of Increased Sleep Duration on Cognitive Performance\") +\n  xlab(\"Time\") +\n  ylab(\"Cognitive Performance Score\")\n\n\n\n\n\n\n\nFigure 9.2: Effect of Sleep Duration on Cognitive Performance\n\n\n\n\n\n\n9.3.4.1 Interpretation\nThis plot demonstrates the effect of increased sleep duration on cognitive performance. The experimental group, which increased their sleep duration, shows a more substantial improvement in cognitive performance compared to the control group. This suggests that increasing sleep duration may positively impact cognitive abilities in college students.\n\n\n\n9.3.5 Example 2: Impact of Mindfulness Training on Stress Levels\nResearch Question: Can a short-term mindfulness training program reduce stress levels in healthcare workers?\n\n# Generating sample data\nset.seed(789)\nn &lt;- 120\npre_experimental &lt;- rnorm(n, mean = 60, sd = 15)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = -12, sd = 8)\npre_control &lt;- rnorm(n, mean = 60, sd = 15)\npost_control &lt;- pre_control + rnorm(n, mean = -2, sd = 6)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Mindfulness\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  StressScore = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = StressScore, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Impact of Mindfulness Training on Stress Levels\") +\n  xlab(\"Time\") +\n  ylab(\"Stress Score\")\n\n\n\n\n\n\n\nFigure 9.3: Impact of Mindfulness Training on Stress Levels\n\n\n\n\n\n\n9.3.5.1 Interpretation\nThis visualization illustrates the impact of a mindfulness training program on stress levels in healthcare workers. The mindfulness group shows a more significant decrease in stress scores compared to the control group. This suggests that the mindfulness training program may be effective in reducing stress levels among healthcare workers.\nWhen interpreting such results, it’s important to consider:\n\nThe magnitude of the change in each group\nThe difference in change between the experimental and control groups\nThe variability within each group\nAny potential confounding factors not accounted for in the experimental design\n\nThese examples provide a template for visualizing and interpreting similar experimental designs across different research contexts.\n\n\n\n9.3.6 Factorial Designs\nFactorial designs allow researchers to study the effects of multiple independent variables simultaneously. They are efficient and can reveal interaction effects between variables.\nExample of a 2x2 factorial design:\n\n# Create sample data for 2x2 factorial design\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  factor_a = rep(rep(c(\"Low\", \"High\"), each = n_per_group), 2),\n  factor_b = rep(c(\"Control\", \"Treatment\"), each = n_per_group * 2),\n  outcome = NA\n)\n\n# Generate outcomes\nfactorial_data$outcome &lt;- ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Control\",\n                                 rnorm(n_per_group, 40, 5),\n                                 ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Treatment\",\n                                        rnorm(n_per_group, 45, 5),\n                                        ifelse(factorial_data$factor_a == \"High\" & factorial_data$factor_b == \"Control\",\n                                               rnorm(n_per_group, 50, 5),\n                                               rnorm(n_per_group, 60, 5))))\n\n# Create plot\nggplot(factorial_data, aes(x = factor_b, y = outcome, fill = factor_a)) +\n  geom_boxplot() +\n  facet_wrap(~factor_a, scales = \"free_x\") +\n  labs(title = \"2x2 Factorial Design\",\n       x = \"Factor B\", y = \"Outcome\", fill = \"Factor A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n2x2 Factorial Design\n\n\n\n\nThis plot illustrates a 2x2 factorial design, showing the effects of two factors (A and B) on the outcome variable. We can observe main effects for both factors and a potential interaction effect.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#non-experimental-designs",
    "href": "chapter4.html#non-experimental-designs",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.4 Non-Experimental Designs",
    "text": "9.4 Non-Experimental Designs\nNon-experimental designs are used when randomization or manipulation of variables is not possible or ethical. They include observational/descriptive studies and quasi-experimental designs.\n\n9.4.1 Observational Studies\nObservational studies involve collecting data without manipulating variables. They are useful for exploring relationships and generating hypotheses.\nExample: Correlation study\n\nset.seed(789)\nn &lt;- 100\nstudy_time &lt;- runif(n, 0, 10)\nexam_score &lt;- 50 + 5 * study_time + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(study_time, exam_score)\n\nggplot(correlation_data, aes(x = study_time, y = exam_score)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Correlation between Study Time and Exam Score\",\n       x = \"Study Time (hours)\", y = \"Exam Score\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCorrelation between Study Time and Exam Score\n\n\n\n\nThis scatter plot shows the relationship between study time and exam scores, illustrating a positive correlation typical in observational studies.\n\n\n9.4.2 Quasi-Experimental Designs\nQuasi-experimental designs lack random assignment but attempt to establish causal relationships. Common types include:\n\nDifference-in-Differences (DiD)\nRegression Discontinuity Design (RDD)\n\n\n9.4.2.1 Difference-in-Differences (DiD)\nDiD is used to estimate treatment effects by comparing the average change over time in the outcome variable for the treatment group to the average change over time for the control group.\nLet’s simulate a DiD analysis using the plm package:\n\nlibrary(plm)\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\nDifference-in-Differences Analysis\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nThe plot shows the average outcomes for treatment and control groups over time. The vertical dashed line indicates the intervention point. The DiD estimate is the difference between the two groups’ changes from pre- to post-intervention periods.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt’s important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\n9.4.2.2 Regression Discontinuity Design (RDD)\nRDD is used when treatment assignment is determined by a cutoff value on a continuous variable. It compares observations just above and below the cutoff to estimate the treatment effect.\nLet’s implement an RDD analysis using the rdrobust package:\n\nlibrary(rdrobust)\n\n# Generate synthetic RDD data\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# RDD analysis\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     4.092     0.231    17.723     0.000     [3.640 , 4.545]     \n        Robust         -         -    15.013     0.000     [3.600 , 4.680]     \n=============================================================================\n\n# Visualize RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regression Discontinuity Design\",\n       x = \"Running Variable\", y = \"Outcome\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nRegression Discontinuity Design Analysis\n\n\n\n\nThe plot shows the discontinuity at the cutoff point (x = 0), with separate regression lines fitted on either side. The treatment effect is estimated by the gap between these lines at the cutoff.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "href": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.5 The Neyman-Rubin Potential Outcome Framework",
    "text": "9.5 The Neyman-Rubin Potential Outcome Framework\nThe Neyman-Rubin potential outcome framework provides a formal approach to causal inference. It introduces the concept of potential outcomes: for each unit, we consider the outcome under treatment and the outcome under control, even though we can only observe one in reality.\nKey concepts:\n\nPotential Outcomes: \\(Y_i(1)\\) and \\(Y_i(0)\\) for treatment and control, respectively.\nObserved Outcome: \\(Y_i = Y_i(1)T_i + Y_i(0)(1-T_i)\\), where \\(T_i\\) is the treatment indicator.\nIndividual Treatment Effect: \\(\\tau_i = Y_i(1) - Y_i(0)\\)\nAverage Treatment Effect (ATE): \\(E[\\tau_i] = E[Y_i(1) - Y_i(0)]\\)\n\nThe framework emphasizes the “fundamental problem of causal inference”: we can never observe both potential outcomes for a single unit simultaneously.\n\n9.5.1 Example: Estimating ATE in an RCT\nIn an RCT, random assignment ensures that treatment is independent of potential outcomes, allowing unbiased estimation of the ATE:\n\\[\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\\]\nWhere \\(n_1\\) and \\(n_0\\) are the numbers of treated and control units, respectively.\n\n# Using the RCT data from earlier\nate_estimate &lt;- mean(data$post_test[data$group == \"Treatment\"]) - \n                mean(data$post_test[data$group == \"Control\"])\n\nWarning in mean.default(data$post_test[data$group == \"Treatment\"]): argument is\nnot numeric or logical: returning NA\n\n\nWarning in mean.default(data$post_test[data$group == \"Control\"]): argument is\nnot numeric or logical: returning NA\n\ncat(\"Estimated Average Treatment Effect:\", round(ate_estimate, 2))\n\nEstimated Average Treatment Effect: NA\n\n\nThis estimate represents the causal effect of the treatment under the assumptions of the potential outcome framework.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#conclusion",
    "href": "chapter4.html#conclusion",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.6 Conclusion",
    "text": "9.6 Conclusion\nThis chapter has explored various research designs, from experimental approaches like RCTs and factorial designs to non-experimental methods such as observational studies and quasi-experimental designs. We’ve demonstrated how to implement and visualize these designs using R, and introduced the Neyman-Rubin potential outcome framework for causal inference.\nUnderstanding these designs and their appropriate use is crucial for conducting rigorous research and drawing valid causal conclusions. Each design has its strengths and limitations, and the choice of design should be guided by the research question, ethical considerations, and practical constraints.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#references",
    "href": "chapter4.html#references",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.7 References",
    "text": "9.7 References\n\nImbens, G. W., & Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press.\nAngrist, J. D., & Pischke, J. S. (2008). Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton University Press.\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and Quasi-Experimental Designs for Generalized Causal Inference. Houghton Mifflin.\nCunningham, S. (2021). Causal Inference: The Mixtape. Yale University Press.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html",
    "href": "rozdzial4.html",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "",
    "text": "10.1 Wstęp\nProjekty badawcze stanowią fundament procesu naukowego, zapewniając ustrukturyzowane podejście do badania hipotez i odpowiadania na pytania badawcze. Ten rozdział analizuje dwie główne kategorie projektów badawczych: eksperymentalne i nieeksperymentalne, ze szczególnym uwzględnieniem modelu potencjalnych wyników Neymana-Rubina. Zagłębimy się w różne typy projektów, ich charakterystykę i przedstawimy praktyczne przykłady wykorzystania R do analizy danych i wizualizacji.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-eksperymentalne",
    "href": "rozdzial4.html#projekty-eksperymentalne",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.2 Projekty Eksperymentalne",
    "text": "10.2 Projekty Eksperymentalne\nProjekty eksperymentalne charakteryzują się kontrolą badacza nad zmienną(ymi) niezależną(ymi) oraz losowym przydziałem uczestników do różnych warunków. Te projekty są uważane za złoty standard w ustalaniu związków przyczynowych.\n\n10.2.1 Randomizowane Badania Kontrolowane (RCT)\nRCT są najbardziej rygorystyczną formą projektu eksperymentalnego. Obejmują one:\n\nLosowy przydział uczestników do grup eksperymentalnej i kontrolnej\nManipulację zmienną niezależną\nPomiar zmiennej zależnej\n\nZobaczmy wizualizację prostego projektu RCT:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Tworzenie przykładowych danych\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  grupa = factor(rep(c(\"Kontrolna\", \"Eksperymentalna\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Symulacja efektu leczenia\ndata$post_test &lt;- ifelse(data$grupa == \"Eksperymentalna\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Przekształcenie danych do formatu długiego\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"czas\", values_to = \"wynik\")\n\n# Tworzenie wykresu\nggplot(data_long, aes(x = czas, y = wynik, color = grupa, group = interaction(id, grupa))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = grupa), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Wyniki Pre-test i Post-test w RCT\",\n       x = \"Czas\", y = \"Wynik\", color = \"Grupa\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_x_discrete(labels = c(\"pre_test\" = \"Pre-test\", \"post_test\" = \"Post-test\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nProjekt Randomizowanego Badania Kontrolowanego\n\n\n\n\nTen wykres pokazuje indywidualne trajektorie i średnie grupowe dla wyników pre-test i post-test w hipotetycznym RCT. Grupa eksperymentalna wykazuje wyraźny wzrost wyników w porównaniu do grupy kontrolnej.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#testy-ab-przykład-i-porównanie-z-rct",
    "href": "rozdzial4.html#testy-ab-przykład-i-porównanie-z-rct",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.3 Testy A/B: Przykład i Porównanie z RCT",
    "text": "10.3 Testy A/B: Przykład i Porównanie z RCT\nTesty A/B to szeroko stosowana metoda eksperymentalna w marketingu cyfrowym, projektowaniu doświadczeń użytkownika i rozwoju produktów. Ten rozdział przedstawi przykład testu A/B, wyjaśni jego metodologię i omówi, czym różni się od Randomizowanych Badań Kontrolowanych (RCT).\n\n10.3.1 Przykład: Współczynnik Konwersji Strony Docelowej\nRozważmy przykład, w którym firma e-commerce chce poprawić współczynnik konwersji swojej strony docelowej. Decydują się przetestować dwa różne układy: obecny układ (A) i nowy układ (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Symulacja danych\nn_odwiedzajacych &lt;- 10000\ndane &lt;- data.frame(\n  Wersja = sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE),\n  Konwersja = rbinom(n_odwiedzajacych, 1, ifelse(sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Obliczenie współczynników konwersji\nwspolczynniki_konwersji &lt;- dane %&gt;%\n  group_by(Wersja) %&gt;%\n  summarise(\n    Odwiedzajacy = n(),\n    Konwersje = sum(Konwersja),\n    WspolczynnikKonwersji = mean(Konwersja)\n  )\n\n# Wizualizacja wyników\nggplot(wspolczynniki_konwersji, aes(x = Wersja, y = WspolczynnikKonwersji, fill = Wersja)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", WspolczynnikKonwersji * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"Test A/B: Współczynniki Konwersji Strony Docelowej\",\n       x = \"Wersja Strony\", y = \"Współczynnik Konwersji\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure 10.1: Wyniki Testu A/B: Współczynniki Konwersji Strony Docelowej\n\n\n\n\n\nW tym przykładzie zasymulowaliśmy dane dla 10 000 odwiedzających losowo przypisanych do wersji A lub B strony docelowej. Wyniki pokazują, że wersja B ma nieco wyższy współczynnik konwersji (11,44%) w porównaniu do wersji A (10,94%).\n\n\n10.3.2 Metodologia Testów A/B\nTesty A/B zazwyczaj przebiegają według następujących kroków:\n\nZidentyfikowanie elementu do przetestowania (np. układ strony docelowej).\nStworzenie dwóch wersji: kontrolnej (A) i wariantu (B).\nLosowe przypisanie odwiedzających do jednej z wersji.\nZbieranie danych o interesującej nas metryce (np. współczynniku konwersji).\nAnaliza wyników przy użyciu metod statystycznych.\nPodjęcie decyzji na podstawie wyników.\n\n\n\n10.3.3 Różnice między Testami A/B a RCT\nChoć testy A/B i Randomizowane Badania Kontrolowane (RCT) mają pewne podobieństwa, istnieje kilka kluczowych różnic:\n\nZakres i Kontekst:\n\nTesty A/B: Zazwyczaj stosowane w środowiskach cyfrowych do szybkich, iteracyjnych ulepszeń.\nRCT: Stosowane w różnych dziedzinach, w tym medycynie, psychologii i naukach społecznych, często dla bardziej złożonych interwencji.\n\nCzas Trwania:\n\nTesty A/B: Zwykle krótsze, często trwające dni lub tygodnie.\nRCT: Mogą trwać miesiące lub lata, szczególnie w badaniach medycznych.\n\nWielkość Próby:\n\nTesty A/B: Mogą obejmować bardzo duże próby ze względu na łatwość implementacji na platformach cyfrowych.\nRCT: Wielkości prób są często mniejsze ze względu na praktyczne i kosztowe ograniczenia.\n\nZaślepienie:\n\nTesty A/B: Uczestnicy zazwyczaj nie są świadomi, że biorą udział w teście.\nRCT: Mogą obejmować pojedyncze, podwójne lub potrójne zaślepienie w celu zmniejszenia błędu systematycznego.\n\nWzględy Etyczne:\n\nTesty A/B: Generalnie obejmują zmiany niskiego ryzyka z minimalnymi obawami etycznymi.\nRCT: Często wymagają obszernej oceny etycznej, szczególnie w kontekście medycznym.\n\nMiary Wyników:\n\nTesty A/B: Zazwyczaj skupiają się na pojedynczym, łatwo mierzalnym wyniku (np. współczynnik klikalności).\nRCT: Często mierzą wiele wyników, w tym potencjalne skutki uboczne lub długoterminowe efekty.\n\nMożliwość Uogólnienia:\n\nTesty A/B: Wyniki są często specyficzne dla testowanej platformy lub kontekstu.\nRCT: Dążą do szerszej możliwości uogólnienia, choć może to się różnić.\n\nZłożoność Analizy:\n\nTesty A/B: Często wykorzystują prostsze analizy statystyczne.\nRCT: Mogą obejmować bardziej złożone metody statystyczne, aby uwzględnić różne czynniki.\n\n\nTesty A/B są potężnym narzędziem do podejmowania decyzji opartych na danych w środowiskach cyfrowych. Choć dzielą podstawową zasadę randomizacji z RCT, są zazwyczaj prostsze, szybsze i bardziej skoncentrowane na konkretnych, mierzalnych wynikach w kontekstach cyfrowych. Zrozumienie tych różnic pomaga badaczom i praktykom wybrać najbardziej odpowiednią metodę do ich konkretnych potrzeb i ograniczeń.\nTesty A/B są szczególnie przydatne w optymalizacji stron internetowych, aplikacji mobilnych i kampanii marketingowych, gdzie szybkie iteracje i ciągłe ulepszenia są kluczowe. Z kolei RCT pozostają złotym standardem w badaniach naukowych, szczególnie w dziedzinach takich jak medycyna, gdzie rygorystyczna kontrola i długoterminowa obserwacja są niezbędne.\nNiezależnie od wybranej metody, kluczowe jest staranne planowanie, precyzyjne wykonanie i ostrożna interpretacja wyników. Zarówno testy A/B, jak i RCT, gdy są odpowiednio stosowane, mogą dostarczyć cennych informacji i przyczynić się do podejmowania lepszych decyzji opartych na danych.\n\n\n10.3.4 Przykład 1: Wpływ Długości Snu na Wydajność Poznawczą\nPytanie Badawcze: Czy zwiększenie długości snu poprawia wydajność poznawczą u studentów?\n\n# Generowanie przykładowych danych\nset.seed(456)\nn &lt;- 100\npre_eksperymentalna &lt;- rnorm(n, mean = 70, sd = 10)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = 8, sd = 5)\npre_kontrolna &lt;- rnorm(n, mean = 70, sd = 10)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = 1, sd = 5)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Eksperymentalna\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  Wynik = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = Wynik, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wpływ Zwiększonej Długości Snu na Wydajność Poznawczą\") +\n  xlab(\"Czas\") +\n  ylab(\"Wynik Wydajności Poznawczej\")\n\n\n\n\n\n\n\nFigure 10.2: Wpływ Długości Snu na Wydajność Poznawczą\n\n\n\n\n\n\n10.3.4.1 Interpretacja\nTen wykres pokazuje wpływ zwiększonej długości snu na wydajność poznawczą. Grupa eksperymentalna, która zwiększyła długość snu, wykazuje znacznie większą poprawę w wydajności poznawczej w porównaniu do grupy kontrolnej. Sugeruje to, że zwiększenie długości snu może pozytywnie wpływać na zdolności poznawcze studentów.\n\n\n\n10.3.5 Przykład 2: Wpływ Treningu Uważności na Poziom Stresu\nPytanie Badawcze: Czy krótkoterminowy program treningu uważności może obniżyć poziom stresu u pracowników służby zdrowia?\n\n# Generowanie przykładowych danych\nset.seed(789)\nn &lt;- 120\npre_eksperymentalna &lt;- rnorm(n, mean = 60, sd = 15)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = -12, sd = 8)\npre_kontrolna &lt;- rnorm(n, mean = 60, sd = 15)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = -2, sd = 6)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Uważność\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  PoziomStresu = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = PoziomStresu, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wpływ Treningu Uważności na Poziom Stresu\") +\n  xlab(\"Czas\") +\n  ylab(\"Poziom Stresu\")\n\n\n\n\n\n\n\nFigure 10.3: Wpływ Treningu Uważności na Poziom Stresu\n\n\n\n\n\n\n10.3.5.1 Interpretacja\nTa wizualizacja ilustruje wpływ programu treningu uważności na poziom stresu u pracowników służby zdrowia. Grupa uważności wykazuje znacznie większy spadek poziomu stresu w porównaniu do grupy kontrolnej. Sugeruje to, że program treningu uważności może być skuteczny w redukcji poziomu stresu wśród pracowników służby zdrowia.\n\n\n\n10.3.6 Projekty Czynnikowe\nProjekty czynnikowe pozwalają badaczom na jednoczesne badanie efektów wielu zmiennych niezależnych. Są one efektywne i mogą ujawniać efekty interakcji między zmiennymi.\nPrzykład projektu czynnikowego 2x2:\n\n# Tworzenie przykładowych danych dla projektu czynnikowego 2x2\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  czynnik_a = rep(rep(c(\"Niski\", \"Wysoki\"), each = n_per_group), 2),\n  czynnik_b = rep(c(\"Kontrola\", \"Interwencja\"), each = n_per_group * 2),\n  wynik = NA\n)\n\n# Generowanie wyników\nfactorial_data$wynik &lt;- ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Kontrola\",\n                               rnorm(n_per_group, 40, 5),\n                               ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Interwencja\",\n                                      rnorm(n_per_group, 45, 5),\n                                      ifelse(factorial_data$czynnik_a == \"Wysoki\" & factorial_data$czynnik_b == \"Kontrola\",\n                                             rnorm(n_per_group, 50, 5),\n                                             rnorm(n_per_group, 60, 5))))\n\n# Tworzenie wykresu\nggplot(factorial_data, aes(x = czynnik_b, y = wynik, fill = czynnik_a)) +\n  geom_boxplot() +\n  facet_wrap(~czynnik_a, scales = \"free_x\") +\n  labs(title = \"Projekt Czynnikowy 2x2\",\n       x = \"Czynnik B\", y = \"Wynik\", fill = \"Czynnik A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nProjekt Czynnikowy 2x2\n\n\n\n\nTen wykres ilustruje projekt czynnikowy 2x2, pokazując efekty dwóch czynników (A i B) na zmienną wynikową. Możemy zaobserwować główne efekty dla obu czynników oraz potencjalny efekt interakcji.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-nieeksperymentalne",
    "href": "rozdzial4.html#projekty-nieeksperymentalne",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.4 Projekty Nieeksperymentalne",
    "text": "10.4 Projekty Nieeksperymentalne\nProjekty nieeksperymentalne są stosowane, gdy randomizacja lub manipulacja zmiennymi nie jest możliwa lub etyczna. Obejmują one badania obserwacyjne/opisowe i quasi-eksperymentalne.\n\n10.4.1 Badania Obserwacyjne\nBadania obserwacyjne polegają na zbieraniu danych bez manipulowania zmiennymi. Są one przydatne do eksploracji relacji i generowania hipotez.\nPrzykład: Badanie korelacyjne\n\nset.seed(789)\nn &lt;- 100\nczas_nauki &lt;- runif(n, 0, 10)\nwynik_egzaminu &lt;- 50 + 5 * czas_nauki + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(czas_nauki, wynik_egzaminu)\n\nggplot(correlation_data, aes(x = czas_nauki, y = wynik_egzaminu)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Korelacja między Czasem Nauki a Wynikiem Egzaminu\",\n       x = \"Czas Nauki (godziny)\", y = \"Wynik Egzaminu\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nKorelacja między Czasem Nauki a Wynikiem Egzaminu\n\n\n\n\nTen wykres punktowy pokazuje relację między czasem nauki a wynikami egzaminu, ilustrując pozytywną korelację typową dla badań obserwacyjnych.\n\n\n10.4.2 Projekty Quasi-Eksperymentalne\nProjekty quasi-eksperymentalne nie mają losowego przydziału, ale próbują ustalić związki przyczynowe. Popularne typy to:\n\nRóżnica w Różnicach (DiD)\nRegresja Nieciągła (RDD)\n\n\n10.4.2.1 Różnica w Różnicach (DiD)\nDiD jest używana do oszacowania efektów interwencji poprzez porównanie średniej zmiany w czasie w zmiennej wynikowej dla grupy eksperymentalnej ze średnią zmianą w czasie dla grupy kontrolnej.\nPrzeprowadźmy symulację analizy DiD przy użyciu pakietu plm:\n\nlibrary(plm)\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nWykres pokazuje średnie wyniki dla grup interwencji i kontrolnej w czasie. Pionowa przerywana linia wskazuje punkt interwencji. Oszacowanie DiD to różnica między zmianami obu grup od okresu przed do po interwencji.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt’s important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\n10.4.2.2 Regresja Nieciągła (RDD)\nRDD jest stosowana, gdy przydział do interwencji jest określony przez wartość graniczną na ciągłej zmiennej. Porównuje obserwacje tuż powyżej i poniżej punktu granicznego, aby oszacować efekt interwencji.\nPrzeprowadźmy analizę RDD przy użyciu pakietu rdrobust:\n\nlibrary(rdrobust)\n\n# Generowanie syntetycznych danych RDD\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# Analiza RDD\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     4.092     0.231    17.723     0.000     [3.640 , 4.545]     \n        Robust         -         -    15.013     0.000     [3.600 , 4.680]     \n=============================================================================\n\n# Wizualizacja RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regresja Nieciągła\",\n       x = \"Zmienna Bieżąca\", y = \"Wynik\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAnaliza Regresji Nieciągłej\n\n\n\n\nWykres pokazuje nieciągłość w punkcie granicznym (x = 0), z oddzielnymi liniami regresji dopasowanymi po obu stronach. Efekt interwencji jest szacowany przez różnicę między tymi liniami w punkcie granicznym.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#model-potencjalnych-wyników-neymana-rubina",
    "href": "rozdzial4.html#model-potencjalnych-wyników-neymana-rubina",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.5 Model Potencjalnych Wyników Neymana-Rubina",
    "text": "10.5 Model Potencjalnych Wyników Neymana-Rubina\nModel potencjalnych wyników Neymana-Rubina zapewnia formalne podejście do wnioskowania przyczynowego. Wprowadza on koncepcję potencjalnych wyników: dla każdej jednostki rozważamy wynik w warunkach interwencji i w warunkach kontrolnych, mimo że w rzeczywistości możemy zaobserwować tylko jeden z nich.\nKluczowe pojęcia:\n\nPotencjalne Wyniki: \\(Y_i(1)\\) i \\(Y_i(0)\\) odpowiednio dla interwencji i kontroli.\nObserwowany Wynik: \\(Y_i = Y_i(1)T_i + Y_i(0)(1-T_i)\\), gdzie \\(T_i\\) to wskaźnik interwencji.\nIndywidualny Efekt Interwencji: \\(\\tau_i = Y_i(1) - Y_i(0)\\)\nPrzeciętny Efekt Interwencji (ATE): \\(E[\\tau_i] = E[Y_i(1) - Y_i(0)]\\)\n\nModel podkreśla “fundamentalny problem wnioskowania przyczynowego”: nigdy nie możemy zaobserwować obu potencjalnych wyników dla pojedynczej jednostki jednocześnie.\n\n10.5.1 Przykład: Szacowanie ATE w RCT\nW RCT, losowy przydział zapewnia, że interwencja jest niezależna od potencjalnych wyników, umożliwiając nieobciążone oszacowanie ATE:\n\\[\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\\]\nGdzie \\(n_1\\) i \\(n_0\\) to odpowiednio liczby jednostek w grupie interwencji i kontrolnej.\n\n# Używając danych RCT z wcześniejszego przykładu\nate_estimate &lt;- mean(data$post_test[data$grupa == \"Eksperymentalna\"]) - \n                mean(data$post_test[data$grupa == \"Kontrolna\"])\n\ncat(\"Oszacowany Przeciętny Efekt Interwencji:\", round(ate_estimate, 2))\n\nOszacowany Przeciętny Efekt Interwencji: 9.66",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "11.1 Understanding Outliers\nBefore diving into specific measures, it’s crucial to understand the concept of outliers, as they can significantly impact many descriptive statistics.\nOutliers are data points that differ significantly from other observations in the dataset. They can occur due to:\nOutliers can have a substantial effect on many statistical measures, especially those based on means or sums of squared deviations. Therefore, it’s essential to:\nThroughout this chapter, we’ll discuss how different descriptive measures are affected by outliers.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-outliers",
    "href": "chapter5.html#understanding-outliers",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "Measurement or recording errors\nGenuine extreme values in the population\nSampling from a different population\n\n\n\nIdentify outliers through both statistical methods and domain knowledge\nInvestigate the cause of outliers\nMake informed decisions about whether to include or exclude them in analyses",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-central-tendency",
    "href": "chapter5.html#measures-of-central-tendency",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.2 Measures of Central Tendency",
    "text": "11.2 Measures of Central Tendency\nMeasures of central tendency aim to identify the “typical” or “central” value in a dataset. The three primary measures are mean, median, and mode.\n\n11.2.1 Arithmetic Mean\nThe arithmetic mean is the sum of all values divided by the number of values.\nFormula: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\)\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(data)\n\n[1] 5.142857\n\n\nPros:\n\nEasy to calculate and understand\nUses all data points\nUseful for further statistical calculations\n\nCons:\n\nSensitive to outliers\nNot ideal for skewed distributions\n\nExample with outlier:\n\ndata_with_outlier &lt;- c(2, 4, 4, 5, 5, 7, 100)\nmean(data_with_outlier)\n\n[1] 18.14286\n\n\nAs we can see, the outlier (100) drastically affects the mean.\n\n\n11.2.2 Median\nThe median is the middle value when the data is ordered.\nR calculation:\n\nmedian(data)\n\n[1] 5\n\nmedian(data_with_outlier)\n\n[1] 5\n\n\nPros:\n\nNot affected by extreme outliers\nBetter for skewed distributions\n\nCons:\n\nDoesn’t use all data points\nLess useful for further statistical calculations\n\n\n\n11.2.3 Mode\nThe mode is the most frequently occurring value.\nR calculation:\n\nlibrary(modeest)\nmfv(data)  # Most frequent value\n\n[1] 4 5\n\n\nPros:\n\nOnly measure of central tendency for nominal data\nCan identify multiple peaks in the data\n\nCons:\n\nNot always uniquely defined\nNot useful for continuous data\n\n\n\n11.2.4 Weighted Mean\nThe weighted mean is used when some data points are more important than others. There are two types of weighted means: with not normalized weights and with normalized weights.\n\n11.2.4.1 Weighted Mean with Not Normalized Weights\nThis is the standard form of the weighted mean, where weights can be any positive numbers representing the importance of each data point.\nFormula: \\(\\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\\)\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\n11.2.4.2 Weighted Mean with Normalized Weights (Fractions)\nIn this case, the weights are fractions that sum to 1, representing the proportion of importance for each data point.\nFormula: \\(\\bar{x}_w = \\sum_{i=1}^n w_i x_i\\), where \\(\\sum_{i=1}^n w_i = 1\\)\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Note: these sum to 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nPros of Weighted Means:\n\nAccount for varying importance of data points\nUseful in survey analysis with different sample sizes or importance levels\nCan adjust for unequal probabilities in sampling designs\n\nCons of Weighted Means:\n\nRequire justification for weights\nCan be misused to manipulate results\nMay be less intuitive to interpret than simple arithmetic mean\n\nComparison:\nThe not normalized weights are often easier to assign based on real-world importance or sample sizes, but require the additional step of normalization in the calculation. Normalized weights (fractions) make the calculation simpler but may be less intuitive to assign directly.\nExample in Social Science:\nSuppose we’re calculating the average income for a region with three cities:\nCity A: Average income $50,000, population 100,000 City B: Average income $60,000, population 200,000 City C: Average income $70,000, population 300,000\nWe can use population as weights:\n\nincomes &lt;- c(50000, 60000, 70000)\npopulations &lt;- c(100000, 200000, 300000)\n\n# Not normalized weights\nweighted.mean(incomes, populations)\n\n[1] 63333.33\n\n# Normalized weights\npop_normalized &lt;- populations / sum(populations)\nsum(incomes * pop_normalized)\n\n[1] 63333.33\n\n\nThis weighted average gives a more accurate representation of the region’s average income than a simple arithmetic mean of the three city averages would.\nPros:\n\nAccounts for varying importance of data points\nUseful in survey analysis with different sample sizes\n\nCons:\n\nRequires justification for weights\nCan be misused to manipulate results",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-variability",
    "href": "chapter5.html#measures-of-variability",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.3 Measures of Variability",
    "text": "11.3 Measures of Variability\nThese measures describe how spread out the data is. They are crucial for understanding the dispersion of data points around the central tendency.\n\n11.3.1 Range\nThe range is the difference between the maximum and minimum values.\nFormula: \\(R = x_{max} - x_{min}\\)\nR calculation:\n\nrange(data)\n\n[1] 2 9\n\nmax(data) - min(data)\n\n[1] 7\n\n\nPros:\n\nSimple to calculate and understand\nGives an immediate sense of data spread\n\nCons:\n\nExtremely sensitive to outliers\nDoesn’t provide information about the distribution between extremes\n\n\n\n11.3.2 Interquartile Range (IQR)\nThe IQR is the difference between the 75th and 25th percentiles.\nFormula: \\(IQR = Q_3 - Q_1\\)\nR calculation:\n\nIQR(data)\n\n[1] 2\n\n\nPros:\n\nRobust to outliers\nProvides information about the spread of the middle 50% of the data\n\nCons:\n\nIgnores the tails of the distribution\nLess efficient than standard deviation for normal distributions\n\n\n\n11.3.3 Variance\nVariance measures the average squared deviation from the mean.\nFormula: \\(s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\\)\nR calculation:\n\nvar(data)\n\n[1] 5.142857\n\n\nPros:\n\nUses all data points\nFoundation for many statistical tests\n\nCons:\n\nUnits are squared, making interpretation less intuitive\nSensitive to outliers\n\n\n\n11.3.4 Standard Deviation\nThe standard deviation is the square root of the variance.\nFormula: \\(s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\\)\nR calculation:\n\nsd(data)\n\n[1] 2.267787\n\n\nPros:\n\nIn same units as original data\nWidely used and understood\n\nCons:\n\nStill sensitive to outliers\nAssumes data is roughly normally distributed\n\n\n\n11.3.5 Coefficient of Variation\nThe coefficient of variation is the standard deviation divided by the mean, often expressed as a percentage.\nFormula: \\(CV = \\frac{s}{\\bar{x}} \\times 100\\%\\)\nR calculation:\n\n(sd(data) / mean(data)) * 100\n\n[1] 44.09586\n\n\nPros:\n\nAllows comparison of variability between datasets with different units or means\nUseful in fields like finance for risk assessment\n\nCons:\n\nNot meaningful for data with both positive and negative values\nCan be misleading when mean is close to zero",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-relative-position",
    "href": "chapter5.html#measures-of-relative-position",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.4 Measures of Relative Position",
    "text": "11.4 Measures of Relative Position\nThese measures help us understand where a particular value falls in relation to the entire dataset.\n\n11.4.1 Percentiles\nPercentiles divide the data into 100 equal parts.\nFormula: For the kth percentile: \\(P_k = L + \\frac{k(n+1)}{100}\\), where L is the lower limit of the interval\nR calculation:\n\nquantile(data, probs = seq(0, 1, 0.25))\n\n  0%  25%  50%  75% 100% \n   2    4    5    6    9 \n\n\n\n\n11.4.2 Quartiles\nQuartiles divide the data into four equal parts.\n\nQ1: 25th percentile\nQ2: Median (50th percentile)\nQ3: 75th percentile\n\nR calculation:\n\nsummary(data)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   4.000   5.000   5.143   6.000   9.000 \n\n\nPros:\n\nRobust to outliers\nProvide information about data spread and skewness\n\nCons:\n\nLess precise than using all data points\nMultiple methods of calculation can lead to slightly different results",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-shape",
    "href": "chapter5.html#measures-of-shape",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.5 Measures of Shape",
    "text": "11.5 Measures of Shape\nThese measures describe the shape of the probability distribution of the data.\n\n11.5.1 Skewness\nSkewness measures the asymmetry of the probability distribution.\nFormula: \\(SK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3\\)\nR calculation:\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nskewness(data)\n\n[1] 0.4592793\n\n\nInterpretation:\n\nPositive skewness: right tail is longer (mean &gt; median)\nNegative skewness: left tail is longer (mean &lt; median)\nZero skewness: symmetrical distribution\n\nPros:\n\nProvides information about distribution shape\nUseful for checking assumptions of normality\n\nCons:\n\nSensitive to outliers\nCan be misleading for multimodal distributions\n\n\n\n11.5.2 Kurtosis\nKurtosis measures the “tailedness” of the probability distribution.\nFormula: \\(K = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\\)\nR calculation:\n\nkurtosis(data)\n\n[1] 2.457047\n\n\nInterpretation:\n\nPositive kurtosis: heavy tails, peaked distribution\nNegative kurtosis: light tails, flat distribution\nZero kurtosis: normal distribution\n\nPros:\n\nProvides information about extreme values in the distribution\nUseful for financial modeling and risk assessment\n\nCons:\n\nSensitive to outliers\nCan be difficult to interpret practically",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-bivariate-and-multivariate-descriptive-statistics",
    "href": "chapter5.html#introduction-to-bivariate-and-multivariate-descriptive-statistics",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.6 Introduction to Bivariate and Multivariate Descriptive Statistics",
    "text": "11.6 Introduction to Bivariate and Multivariate Descriptive Statistics\nWhile univariate statistics describe single variables, bivariate and multivariate statistics explore relationships between variables.\n\n11.6.1 Bivariate Statistics\n\n11.6.1.1 Correlation\nCorrelation measures the strength and direction of the linear relationship between two variables.\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 5, 4, 5)\ncor(x, y)\n\n[1] 0.7745967\n\n\n\n\n11.6.1.2 Covariance\nCovariance measures how two variables vary together.\n\ncov(x, y)\n\n[1] 1.5\n\n\n\n\n11.6.1.3 Cross-tabulation\nCross-tabulation (contingency table) shows the relationship between two categorical variables.\n\ntable(cut(x, 2), cut(y, 2))\n\n           \n            (2,3.5] (3.5,5]\n  (0.996,3]       1       2\n  (3,5]           0       2\n\n\n\n\n\n11.6.2 Multivariate Statistics\n\nMultiple Correlation: Correlation between a dependent variable and multiple independent variables.\nPartial Correlation: Correlation between two variables while controlling for others.\nFactor Analysis: Technique to reduce many variables to a few underlying factors.\n\nThese topics will be covered in more detail in later chapters.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#conclusion",
    "href": "chapter5.html#conclusion",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.7 Conclusion",
    "text": "11.7 Conclusion\nDescriptive statistics are essential tools for summarizing and understanding data in social science research. However, it’s crucial to:\n\nChoose appropriate measures based on data type and distribution\nBe aware of the limitations of each measure, especially regarding outliers\nUse multiple measures to gain a comprehensive understanding\n\nRemember, descriptive statistics are just the starting point. They provide the foundation for inferential statistics, which allow us to draw conclusions about populations based on samples.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#references",
    "href": "chapter5.html#references",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.8 References",
    "text": "11.8 References\n\nAgresti, A., & Finlay, B. (2009). Statistical methods for the social sciences (4th ed.). Pearson Prentice Hall.\nField, A. (2013). Discovering statistics using IBM SPSS statistics. Sage.\nHair, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2019). Multivariate data analysis (8th ed.). Cengage Learning.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "href": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.9 Exercise 1. Center and dispersion of data",
    "text": "11.9 Exercise 1. Center and dispersion of data\n\n11.9.1 Data\nWe have salary data (in thousands of euros) from two small European companies:\n\nCompany X: X = {2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35};\nCompany Y: Y = {3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8}\n\n\n\n11.9.2 Measures of Central Tendency\n\n11.9.2.1 Mean\nThe mean is the average of all values in a dataset.\nFormula: \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\)\n\n11.9.2.1.1 Manual Calculation for Company X\n\n\n\nValue (\\(x_i\\))\nFrequency (\\(f_i\\))\n\\(x_i \\cdot f_i\\)\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nTotal\nn = 20\nSum = 119\n\n\n\n\\(\\bar{x} = \\frac{119}{20} = 5.95\\)\n\n\n11.9.2.1.2 Manual Calculation for Company Y\n\n\n\nValue (\\(x_i\\))\nFrequency (\\(f_i\\))\n\\(x_i \\cdot f_i\\)\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nTotal\nn = 20\nSum = 100\n\n\n\n\\(\\bar{y} = \\frac{100}{20} = 5\\)\n\n\n11.9.2.1.3 R Verification\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\n11.9.2.2 Median\nThe median is the middle value when the data is ordered.\n\n11.9.2.2.1 Manual Calculation for Company X\nOrdered data: 2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\(\\frac{4 + 4}{2} = 4\\)\n\n\n11.9.2.2.2 Manual Calculation for Company Y\nOrdered data: 3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\(\\frac{5 + 5}{2} = 5\\)\n\n\n11.9.2.2.3 R Verification\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\n11.9.2.3 Mode\nThe mode is the most frequent value in the dataset.\nFor Company X, the mode is 3 (appears 6 times). For Company Y, there are two modes: 4 and 5 (both appear 6 times).\n\n# Function to calculate mode\nget_mode &lt;- function(x) {\n  unique_x &lt;- unique(x)\n  unique_x[which.max(tabulate(match(x, unique_x)))]\n}\n\nget_mode(X)\n\n[1] 3\n\nget_mode(Y)\n\n[1] 4\n\n\n\n\n\n11.9.3 Measures of Dispersion\n\n11.9.3.1 Variance\nThe variance measures the average squared deviation from the mean.\nFormula: \\(s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\)\n\n11.9.3.1.1 Manual Calculation for Company X\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(f_i\\)\n\\(x_i - \\bar{x}\\)\n\\((x_i - \\bar{x})^2\\)\n\\(f_i(x_i - \\bar{x})^2\\)\n\n\n\n\n2\n3\n-3.95\n15.6025\n46.8075\n\n\n3\n6\n-2.95\n8.7025\n52.215\n\n\n4\n5\n-1.95\n3.8025\n19.0125\n\n\n5\n4\n-0.95\n0.9025\n3.61\n\n\n20\n1\n14.05\n197.4025\n197.4025\n\n\n35\n1\n29.05\n843.9025\n843.9025\n\n\nTotal\n20\n\n\n1162.95\n\n\n\n\\(s^2 = \\frac{1162.95}{19} = 61.21\\)\n\n\n11.9.3.1.2 Manual Calculation for Company Y\n\n\n\n\n\n\n\n\n\n\n\\(y_i\\)\n\\(f_i\\)\n\\(y_i - \\bar{y}\\)\n\\((y_i - \\bar{y})^2\\)\n\\(f_i(y_i - \\bar{y})^2\\)\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nTotal\n20\n\n\n34\n\n\n\n\\(s^2 = \\frac{34}{19} = 1.79\\)\n\n\n11.9.3.1.3 R Verification\n\nvar(X)\n\n[1] 61.20789\n\nvar(Y)\n\n[1] 1.789474\n\n\n\n\n\n11.9.3.2 Standard Deviation\nThe standard deviation is the square root of the variance.\nFormula: \\(s = \\sqrt{s^2}\\)\nFor Company X: \\(s = \\sqrt{61.21} = 7.82\\) For Company Y: \\(s = \\sqrt{1.79} = 1.34\\)\n\n11.9.3.2.1 R Verification\n\nsd(X)\n\n[1] 7.823547\n\nsd(Y)\n\n[1] 1.337712\n\n\n\n\n\n\n11.9.4 Quartiles\nQuartiles divide the dataset into four equal parts.\n\n11.9.4.1 Manual Calculation for Company X\nOrdered data: 2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35\n\nQ1 (25th percentile): median of first 10 numbers = 3\nQ2 (50th percentile, median): 4\nQ3 (75th percentile): median of last 10 numbers = 5\n\n\n\n11.9.4.2 Manual Calculation for Company Y\nOrdered data: 3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8\n\nQ1 (25th percentile): median of first 10 numbers = 4\nQ2 (50th percentile, median): 5\nQ3 (75th percentile): median of last 10 numbers = 6\n\n\n\n11.9.4.3 R Verification\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\n\n11.9.5 Tukey Box Plot\nA Tukey box plot visually represents the distribution of data based on quartiles. We’ll use ggplot2 to create the plot.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Prepare the data\ndata &lt;- data.frame(\n  Company = rep(c(\"X\", \"Y\"), each = 20),\n  Salary = c(X, Y)\n)\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot() +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\n11.9.5.1 Interpreting the Box Plot\n\nThe box represents the interquartile range (IQR) from Q1 to Q3.\nThe line inside the box is the median (Q2).\nWhiskers extend to the smallest and largest values within 1.5 * IQR.\nPoints beyond the whiskers are considered outliers.\n\n\n\n\n11.9.6 Comparison of Results\n\n\n\nMeasure\nCompany X\nCompany Y\n\n\n\n\nMean\n5.95\n5.00\n\n\nMedian\n4\n5\n\n\nMode\n3\n4 and 5\n\n\nVariance\n61.21\n1.79\n\n\nStandard Deviation\n7.82\n1.34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\n11.9.6.1 Key Observations:\n\nCentral Tendency: Company X has a higher mean but lower median than Company Y, indicating a right-skewed distribution for Company X.\nDispersion: Company X shows much higher variance and standard deviation, suggesting greater salary disparities.\nDistribution Shape: Company Y’s salaries are more tightly clustered, while Company X has extreme values (potential outliers) that significantly affect its mean and variance.\nQuartiles: Company Y’s interquartile range (Q3 - Q1) is slightly larger, but its overall range is much smaller than Company X’s.\n\n\n\n\n11.9.7 Conclusion\nThis comparative analysis reveals significant differences in salary structures between the two companies. Company X shows greater variability and potential inequality in its pay scale, while Company Y demonstrates a more consistent and narrowly distributed salary range. These insights can be valuable for understanding compensation strategies and potential areas for policy review in each company.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-2.-comparing-district-magnitude-variability-between-countries",
    "href": "chapter5.html#exercise-2.-comparing-district-magnitude-variability-between-countries",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.10 Exercise 2. Comparing District Magnitude Variability Between Countries",
    "text": "11.10 Exercise 2. Comparing District Magnitude Variability Between Countries\n\n11.10.1 Data\nWe have data on the size of electoral districts from two countries:\n\nCountry with high variability (X): 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nCountry with low variability (Y): 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\n\n\n\n11.10.2 Measures of Central Tendency\n\n11.10.2.1 Arithmetic Mean\nThe arithmetic mean is the sum of all values divided by their count.\nFormula: \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\)\n\n11.10.2.1.1 Calculations for Country X\n\n\n\nValues\nSum\n\n\n\n\n1 + 3 + 5 + 7 + 9 + 11 + 13 + 15 + 17 + 19\n100\n\n\n\n\\(\\bar{x} = \\frac{100}{10} = 10\\)\n\n\n11.10.2.1.2 Calculations for Country Y\n\n\n\nValues\nSum\n\n\n\n\n8 + 9 + 9 + 10 + 10 + 11 + 11 + 12 + 12 + 13\n105\n\n\n\n\\(\\bar{y} = \\frac{105}{10} = 10.5\\)\n\n\n\n11.10.2.2 Median\nThe median is the middle value in an ordered set of data.\n\n11.10.2.2.1 Calculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMedian = \\(\\frac{9 + 11}{2} = 10\\)\n\n\n11.10.2.2.2 Calculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMedian = \\(\\frac{10 + 11}{2} = 10.5\\)\n\n\n\n11.10.2.3 Mode\nThe mode is the most frequently occurring value in a dataset.\nFor Country X, there is no mode (all values occur once). For Country Y, there are four modes: 9, 10, 11, and 12 (each occurs twice).\n\n\n\n11.10.3 Measures of Dispersion\n\n11.10.3.1 Range\nThe range is the difference between the maximum and minimum values.\n\n11.10.3.1.1 Calculations for Country X\nRange = 19 - 1 = 18\n\n\n11.10.3.1.2 Calculations for Country Y\nRange = 13 - 8 = 5\n\n\n\n11.10.3.2 Variance\nVariance measures the average squared deviation from the mean.\nFormula: \\(s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\)\n\n11.10.3.2.1 Calculations for Country X\n\n\n\n\\(x_i\\)\n\\((x_i - \\bar{x})\\)\n\\((x_i - \\bar{x})^2\\)\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSum\n\n330\n\n\n\n\\(s^2_X = \\frac{330}{9} = 36.67\\)\n\n\n11.10.3.2.2 Calculations for Country Y\n\n\n\n\\(x_i\\)\n\\((y_i - \\bar{y})\\)\n\\((y_i - \\bar{y})^2\\)\n\n\n\n\n8\n-2.5\n6.25\n\n\n9\n-1.5\n2.25\n\n\n9\n-1.5\n2.25\n\n\n10\n-0.5\n0.25\n\n\n10\n-0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n12\n1.5\n2.25\n\n\n12\n1.5\n2.25\n\n\n13\n2.5\n6.25\n\n\nSum\n\n22.5\n\n\n\n\\(s^2_Y = \\frac{22.5}{9} = 2.5\\)\n\n\n\n11.10.3.3 Standard Deviation\nThe standard deviation is the square root of the variance.\nFormula: \\(s = \\sqrt{s^2}\\)\n\n11.10.3.3.1 Calculations for Country X\n\\(s_X = \\sqrt{36.67} \\approx 6.06\\)\n\n\n11.10.3.3.2 Calculations for Country Y\n\\(s_Y = \\sqrt{2.5} \\approx 1.58\\)\n\n\n\n\n11.10.4 Quartiles and Interquartile Range (IQR)\nQuartiles divide the dataset into four equal parts.\n\n11.10.4.1 Calculations for Country X\n\nQ1 (25th percentile): \\(\\frac{3 + 5}{2} = 4\\)\nQ2 (50th percentile, median): 10\nQ3 (75th percentile): \\(\\frac{15 + 17}{2} = 16\\)\nIQR = Q3 - Q1 = 16 - 4 = 12\n\n\n\n11.10.4.2 Calculations for Country Y\n\nQ1 (25th percentile): 9\nQ2 (50th percentile, median): 10.5\nQ3 (75th percentile): 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n\n\n\n11.10.5 Coefficient of Variation (CV)\nThe coefficient of variation is the ratio of the standard deviation to the mean, expressed as a percentage.\nFormula: \\(CV = \\frac{s}{\\bar{x}} \\times 100\\%\\)\n\n11.10.5.1 Calculations for Country X\n\\(CV_X = \\frac{6.06}{10} \\times 100\\% = 60.6\\%\\)\n\n\n11.10.5.2 Calculations for Country Y\n\\(CV_Y = \\frac{1.58}{10.5} \\times 100\\% = 15.0\\%\\)\n\n\n\n11.10.6 Comparison of Results\n\n\n\nMeasure\nCountry X (High var.)\nCountry Y (Low var.)\n\n\n\n\nMean\n10\n10.5\n\n\nMedian\n10\n10.5\n\n\nMode\nNone\n9, 10, 11, 12\n\n\nRange\n18\n5\n\n\nVariance\n36.67\n2.5\n\n\nStandard Dev.\n6.06\n1.58\n\n\nIQR\n12\n3\n\n\nCoef. of Var.\n60.6%\n15.0%\n\n\n\n\n\n11.10.7 Boxplot Comparison\nTo visually compare the distribution of district sizes between the two countries, we can use a Tukey-style boxplot. This type of plot provides a concise summary of the data’s distribution, including the median, quartiles, and potential outliers.\n\n# Load necessary library\nlibrary(ggplot2)\n\n# Create data frames for each country\ncountry_x &lt;- data.frame(country = \"X\", size = c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19))\ncountry_y &lt;- data.frame(country = \"Y\", size = c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13))\n\n# Combine the data\nall_data &lt;- rbind(country_x, country_y)\n\n# Create the boxplot\nggplot(all_data, aes(x = country, y = size, fill = country)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(title = \"Comparison of District Magnitude Variability\",\n       x = \"Country\",\n       y = \"District Size\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\"))\n\n\n\n\nComparison of District Magnitude Variability\n\n\n\n\n\n11.10.7.1 Interpreting the Boxplot\nThe boxplot provides several key pieces of information:\n\nThe box represents the interquartile range (IQR), with the lower edge at Q1 and the upper edge at Q3.\nThe line inside the box represents the median (Q2).\nThe whiskers extend to the smallest and largest values within 1.5 times the IQR from the edges of the box.\nAny points beyond the whiskers are considered potential outliers and are plotted individually.\n\nFrom this plot, we can observe:\n\nThe median district size for Country Y is slightly higher than for Country X, consistent with our earlier calculations.\nThe box for Country X is much larger than for Country Y, indicating a greater spread of the middle 50% of the data and thus higher variability.\nCountry X’s data spans a much wider range, as shown by the longer whiskers, further confirming its higher variability.\nCountry Y’s data is more tightly clustered, with a smaller box and shorter whiskers, indicating lower variability.\nThe individual points for Country X are more spread out, while those for Country Y are more clustered, providing a visual representation of the difference in variability.\n\nThis boxplot visualization reinforces our earlier numerical analysis, clearly showing the contrast in district size variability between the two countries.\n\n\n\n11.10.8 Key Observations\n\nCentral Tendency: Both countries have similar means and medians, indicating that their average district sizes are close.\nDispersion:\n\nCountry X shows much higher variance and standard deviation, confirming its high variability.\nThe range for Country X (18) is more than three times larger than for Country Y (5).\nThe IQR for Country X (12) is four times larger than for Country Y (3), indicating a much wider spread of the middle 50% of the data.\n\nDistribution Shape:\n\nCountry X has a uniform distribution with no clear mode.\nCountry Y has a more clustered distribution with multiple modes, indicating common district sizes.\n\nCoefficient of Variation:\n\nThe CV of Country X (60.6%) is significantly higher than Country Y (15.0%), providing a standardized measure of the difference in variability.\n\nVisual Comparison:\n\nThe boxplot clearly illustrates the stark difference in variability between the two countries, supporting our numerical findings.\n\n\n\n\n11.10.9 Conclusions\nThis analysis clearly shows the contrast in electoral district size variability between the two countries:\n\nCountry X exhibits high variability, with district sizes spread widely from 1 to 19. This may indicate a diverse electoral system with a mix of small (possibly single-member) and large multi-member districts.\nCountry Y exhibits low variability, with district sizes tightly clustered between 8 and 13. This suggests a more uniform electoral system, likely consisting of medium-sized multi-member districts.\n\nThese differences in variability can have significant implications for political representation, party strategies, and electoral outcomes in each country. High variability (as in Country X) may lead to more diverse representation but could also result in more complex electoral strategies. Low variability (as in Country Y) may promote more consistent representation across districts but potentially limit the diversity of political voices.\nThe addition of the boxplot provides a powerful visual tool for understanding these differences, making the contrast between the two countries immediately apparent and complementing the detailed numerical analysis.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "href": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.11 Appendix: Summary Tables for Data Types and Applicable Statistical Measures",
    "text": "11.11 Appendix: Summary Tables for Data Types and Applicable Statistical Measures\n\n11.11.1 Table 1: Discrete vs. Continuous Data\n\n\n\n\n\n\n\n\nCharacteristic\nDiscrete Data\nContinuous Data\n\n\n\n\nDefinition\nCan only take specific values\nCan take any value within a range\n\n\nExamples\nNumber of children, Shoe size\nHeight, Weight, Time\n\n\nPros\n- Easy to categorize- Straightforward to count- Often simpler to analyze\n- More precise measurements- Allows for more sophisticated statistical analyses- Can be grouped into intervals\n\n\nCons\n- Limited precision- May not capture subtle differences- Some statistical methods may not be applicable\n- Can be more complex to analyze- May require larger sample sizes for meaningful analysis- Rounding errors can occur in measurement\n\n\nVisualization\nBar charts, Pie charts\nHistograms, Scatter plots, Line graphs\n\n\n\n\n\n11.11.2 Table 2: Stevens’ Typology of Measurement Scales\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nDefinition\nCategories with no order\nOrdered categories\nEqual intervals, no true zero\nEqual intervals with true zero\n\n\nExamples\nGender, Blood type\nEducation level, Likert scales\nTemperature (°C, °F), Calendar dates\nHeight, Weight, Age\n\n\nPros\n- Easy to collect- Simple to categorize\n- Captures order- Useful for rankings\n- Allows for meaningful differences- More sophisticated analyses possible\n- Most versatile- Allows all arithmetic operations\n\n\nCons\n- Limited analytical options- No arithmetic operations\n- Differences between categories not quantifiable- Limited arithmetic operations\n- No true zero point- Ratios not meaningful\n- Can be difficult to obtain true ratio measurements in some fields\n\n\n\n\n\n11.11.3 Table 3: Applicable Statistical Measures by Data Type\n\n\n\n\n\n\n\n\n\nData Type\nMeasures of Center\nMeasures of Variability\nCorrelation/Association\n\n\n\n\nNominal\nMode\n-\nCramér’s V, Chi-square\n\n\nOrdinal\nMedian, Mode\nInterquartile Range\nSpearman’s rho, Kendall’s tau\n\n\nInterval\nMean, Median, Mode\nStandard Deviation, Variance, Range\nPearson’s r\n\n\nRatio\nMean, Median, Mode\nStandard Deviation, Variance, Range, Coefficient of Variation\nPearson’s r\n\n\nDiscrete\nMean*, Median, Mode\nStandard Deviation, Variance, Range\nDepends on level of measurement\n\n\nContinuous\nMean, Median, Mode\nStandard Deviation, Variance, Range\nPearson’s r\n\n\n\n* Note: While these measures can be calculated for discrete data, interpretation should be careful, especially for data with few possible values.\nThese tables provide a comprehensive overview of different data types, their characteristics, and the appropriate statistical measures to use with each type. They can be easily copied and pasted into your Quarto book in RStudio. The markdown format should be compatible with Quarto, but you may need to adjust the formatting slightly depending on your specific Quarto settings.\n\n\n11.11.4 Table 4: Pros and Cons of Various Statistical Measures\n\n11.11.4.1 Measures of Center\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nMean\n- Uses all data points- Allows for further statistical calculations- Ideal for normally distributed data\n- Sensitive to outliers- Not ideal for skewed distributions- Not meaningful for nominal data\nInterval, Ratio, some Discrete, Continuous\n\n\nMedian\n- Not affected by outliers- Good for skewed distributions- Can be used with ordinal data\n- Ignores the actual values of most data points- Less useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nMode\n- Can be used with any data type- Good for finding most common category\n- May not be unique (multimodal)- Not useful for many types of analyses- Ignores magnitude of differences between values\nAll types\n\n\n\n\n\n11.11.4.2 Measures of Variability\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nRange\n- Simple to calculate and understand- Gives quick idea of data spread\n- Very sensitive to outliers- Ignores all data between extremes- Not useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nInterquartile Range (IQR)\n- Not affected by outliers- Good for skewed distributions\n- Ignores 50% of the data- Less intuitive than range\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nVariance\n- Uses all data points- Basis for many statistical procedures\n- Sensitive to outliers- Units are squared (less intuitive)\nInterval, Ratio, some Discrete, Continuous\n\n\nStandard Deviation\n- Uses all data points- Same units as original data- Widely used and understood\n- Sensitive to outliers- Assumes roughly normal distribution for interpretation\nInterval, Ratio, some Discrete, Continuous\n\n\nCoefficient of Variation\n- Allows comparison between datasets with different units or means\n- Can be misleading when means are close to zero- Not meaningful for data with negative values\nRatio, some Interval\n\n\n\n\n\n11.11.4.3 Measures of Correlation/Association\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nPearson’s r\n- Measures linear relationship- Widely used and understood\n- Assumes normal distribution- Sensitive to outliers- Only captures linear relationships\nInterval, Ratio, Continuous\n\n\nSpearman’s rho\n- Can be used with ordinal data- Captures monotonic relationships- Less sensitive to outliers\n- Loses information by converting to ranks- May miss some types of relationships\nOrdinal, Interval, Ratio\n\n\nKendall’s tau\n- Can be used with ordinal data- More robust than Spearman’s for small samples- Has nice interpretation (probability of concordance)\n- Loses information by only considering order- Computationally more intensive\nOrdinal, Interval, Ratio\n\n\nChi-square\n- Can be used with nominal data- Tests independence of categorical variables\n- Requires large sample sizes- Sensitive to sample size- Doesn’t measure strength of association\nNominal, Ordinal\n\n\nCramér’s V\n- Can be used with nominal data- Provides measure of strength of association- Normalized to [0,1] range\n- Interpretation can be subjective- May overestimate association in small samples\nNominal, Ordinal\n\n\n\nThis table provides a comprehensive overview of the pros and cons of various statistical measures, along with information about which types of data they can be applied to. You can add this to your existing tables in your Quarto book to provide a more complete reference for choosing appropriate statistical measures.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html",
    "href": "rozdzial5.html",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "12.1 Zrozumienie wartości odstających\nZanim zagłębimy się w konkretne miary, kluczowe jest zrozumienie koncepcji wartości odstających, ponieważ mogą one znacząco wpływać na wiele statystyk opisowych.\nWartości odstające to punkty danych, które znacznie różnią się od innych obserwacji w zbiorze danych. Mogą wystąpić z powodu:\nWartości odstające mogą mieć istotny wpływ na wiele miar statystycznych, szczególnie tych opartych na średnich lub sumach kwadratów odchyleń. Dlatego ważne jest, aby:\nW trakcie tego rozdziału omówimy, jak różne miary opisowe są dotknięte przez wartości odstające.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#zrozumienie-wartości-odstających",
    "href": "rozdzial5.html#zrozumienie-wartości-odstających",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "Błędów pomiaru lub rejestracji\nPrawdziwych skrajnych wartości w populacji\nPróbkowania z innej populacji\n\n\n\nIdentyfikować wartości odstające zarówno poprzez metody statystyczne, jak i wiedzę dziedzinową\nBadać przyczyny występowania wartości odstających\nPodejmować świadome decyzje o tym, czy włączyć je do analiz, czy nie",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-tendencji-centralnej",
    "href": "rozdzial5.html#miary-tendencji-centralnej",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.2 Miary tendencji centralnej",
    "text": "12.2 Miary tendencji centralnej\nMiary tendencji centralnej mają na celu identyfikację “typowej” lub “centralnej” wartości w zbiorze danych. Trzy podstawowe miary to średnia, mediana i moda.\n\n12.2.1 Średnia arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez liczbę wartości.\nWzór: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\)\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nŁatwa do obliczenia i zrozumienia\nWykorzystuje wszystkie punkty danych\nPrzydatna do dalszych obliczeń statystycznych\n\nWady:\n\nWrażliwa na wartości odstające\nNieidealna dla rozkładów skośnych\n\nPrzykład z wartością odstającą:\n\ndane_z_odstajaca &lt;- c(2, 4, 4, 5, 5, 7, 100)\nmean(dane_z_odstajaca)\n\n[1] 18.14286\n\n\nJak widać, wartość odstająca (100) drastycznie wpływa na średnią.\n\n\n12.2.2 Mediana\nMediana to środkowa wartość, gdy dane są uporządkowane.\nObliczenie w R:\n\nmedian(dane)\n\n[1] 5\n\nmedian(dane_z_odstajaca)\n\n[1] 5\n\n\nZalety:\n\nNie jest dotknięta przez skrajne wartości odstające\nLepsza dla rozkładów skośnych\n\nWady:\n\nNie wykorzystuje wszystkich punktów danych\nMniej przydatna do dalszych obliczeń statystycznych\n\n\n\n12.2.3 Moda\nModa to najczęściej występująca wartość.\nObliczenie w R:\n\nlibrary(modeest)\nmfv(dane)  # Najczęściej występująca wartość\n\n[1] 4 5\n\n\nZalety:\n\nJedyna miara tendencji centralnej dla danych nominalnych\nMoże identyfikować wiele szczytów w danych\n\nWady:\n\nNie zawsze jednoznacznie zdefiniowana\nNieprzydatna dla danych ciągłych\n\n\n\n12.2.4 Średnia ważona\nŚrednia ważona jest używana, gdy niektóre punkty danych są ważniejsze od innych. Rozróżniamy dwa typy średnich ważonych: z wagami nienormalizowanymi i z wagami znormalizowanymi.\n\n12.2.4.1 Średnia ważona z wagami nienormalizowanymi\nJest to standardowa forma średniej ważonej, gdzie wagi mogą być dowolnymi liczbami dodatnimi reprezentującymi ważność każdego punktu danych.\nWzór: \\(\\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\\)\nObliczenie w R:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\n12.2.4.2 Średnia ważona z wagami znormalizowanymi (ułamkami)\nW tym przypadku wagi są ułamkami sumującymi się do 1, reprezentującymi proporcję ważności dla każdego punktu danych.\nWzór: \\(\\bar{x}_w = \\sum_{i=1}^n w_i x_i\\), gdzie \\(\\sum_{i=1}^n w_i = 1\\)\nObliczenie w R:\n\nx &lt;- c(2, 4, 5, 7)\nw_znormalizowane &lt;- c(0.1, 0.3, 0.4, 0.2)  # Uwaga: te sumują się do 1\nsum(x * w_znormalizowane)\n\n[1] 4.8\n\n\nZalety średnich ważonych:\n\nUwzględniają różną ważność punktów danych\nPrzydatne w analizie badań z różnymi wielkościami próby lub poziomami ważności\nMogą korygować nierówne prawdopodobieństwa w projektach próbkowania\n\nWady średnich ważonych:\n\nWymagają uzasadnienia dla wag\nMogą być niewłaściwie użyte do manipulacji wynikami\nMogą być mniej intuicyjne w interpretacji niż prosta średnia arytmetyczna\n\nPorównanie:\nWagi nienormalizowane są często łatwiejsze do przypisania na podstawie rzeczywistej ważności lub wielkości próby, ale wymagają dodatkowego kroku normalizacji w obliczeniach. Wagi znormalizowane (ułamki) upraszczają obliczenia, ale mogą być mniej intuicyjne do bezpośredniego przypisania.\nPrzykład w naukach społecznych:\nZałóżmy, że obliczamy średni dochód dla regionu z trzema miastami:\nMiasto A: Średni dochód 50 000 zł, populacja 100 000 Miasto B: Średni dochód 60 000 zł, populacja 200 000 Miasto C: Średni dochód 70 000 zł, populacja 300 000\nMożemy użyć populacji jako wag:\n\ndochody &lt;- c(50000, 60000, 70000)\npopulacje &lt;- c(100000, 200000, 300000)\n\n# Wagi nienormalizowane\nweighted.mean(dochody, populacje)\n\n[1] 63333.33\n\n# Wagi znormalizowane\npop_znormalizowane &lt;- populacje / sum(populacje)\nsum(dochody * pop_znormalizowane)\n\n[1] 63333.33\n\n\nTa średnia ważona daje dokładniejsze przedstawienie średniego dochodu regionu niż prosta średnia arytmetyczna średnich dochodów trzech miast.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-zmienności",
    "href": "rozdzial5.html#miary-zmienności",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.3 Miary zmienności",
    "text": "12.3 Miary zmienności\nTe miary opisują, jak rozproszone są dane. Są kluczowe dla zrozumienia rozproszenia punktów danych wokół tendencji centralnej.\n\n12.3.1 Rozstęp\nRozstęp to różnica między wartością maksymalną a minimalną.\nWzór: \\(R = x_{max} - x_{min}\\)\nObliczenie w R:\n\nrange(dane)\n\n[1] 2 9\n\nmax(dane) - min(dane)\n\n[1] 7\n\n\nZalety:\n\nProsty do obliczenia i zrozumienia\nDaje natychmiastowe poczucie rozpiętości danych\n\nWady:\n\nNiezwykle wrażliwy na wartości odstające\nNie dostarcza informacji o rozkładzie między skrajnościami\n\n\n\n12.3.2 Rozstęp międzykwartylowy (IQR)\nIQR to różnica między 75. a 25. percentylem.\nWzór: \\(IQR = Q_3 - Q_1\\)\nObliczenie w R:\n\nIQR(dane)\n\n[1] 2\n\n\nZalety:\n\nOdporny na wartości odstające\nDostarcza informacji o rozpiętości środkowych 50% danych\n\nWady:\n\nIgnoruje ogony rozkładu\nMniej efektywny niż odchylenie standardowe dla rozkładów normalnych\n\n\n\n12.3.3 Wariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: \\(s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\\)\nObliczenie w R:\n\nvar(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nWykorzystuje wszystkie punkty danych\nPodstawa dla wielu testów statystycznych\n\nWady:\n\nJednostki są podniesione do kwadratu, co utrudnia interpretację\nWrażliwa na wartości odstające\n\n\n\n12.3.4 Odchylenie standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji.\nWzór: \\(s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\\)\nObliczenie w R:\n\nsd(dane)\n\n[1] 2.267787\n\n\nZalety:\n\nW tych samych jednostkach co oryginalne dane\nSzeroko stosowane i rozumiane\n\nWady:\n\nNadal wrażliwe na wartości odstające\nZakłada, że dane są w przybliżeniu normalnie rozłożone\n\n\n\n12.3.5 Współczynnik zmienności\nWspółczynnik zmienności to odchylenie standardowe podzielone przez średnią, często wyrażane jako procent.\nWzór: \\(CV = \\frac{s}{\\bar{x}} \\times 100\\%\\)\nObliczenie w R:\n\n(sd(dane) / mean(dane)) * 100\n\n[1] 44.09586\n\n\nZalety:\n\nPozwala na porównanie zmienności między zbiorami danych o różnych jednostkach lub średnich\nPrzydatny w dziedzinach takich jak finanse do oceny ryzyka\n\nWady:\n\nNie ma sensu dla danych z wartościami zarówno dodatnimi, jak i ujemnymi\nMoże być mylący, gdy średnia jest bliska zeru",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-położenia-względnego",
    "href": "rozdzial5.html#miary-położenia-względnego",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.4 Miary położenia względnego",
    "text": "12.4 Miary położenia względnego\nTe miary pomagają zrozumieć, gdzie konkretna wartość znajduje się w stosunku do całego zbioru danych.\n\n12.4.1 Percentyle\nPercentyle dzielą dane na 100 równych części.\nWzór: Dla k-tego percentyla: \\(P_k = L + \\frac{k(n+1)}{100}\\), gdzie L to dolna granica przedziału\nObliczenie w R:\n\nquantile(dane, probs = seq(0, 1, 0.25))\n\n  0%  25%  50%  75% 100% \n   2    4    5    6    9 \n\n\n\n\n12.4.2 Kwartyle\nKwartyle dzielą dane na cztery równe części.\n\nQ1: 25. percentyl\nQ2: Mediana (50. percentyl)\nQ3: 75. percentyl\n\nObliczenie w R:\n\nsummary(dane)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   4.000   5.000   5.143   6.000   9.000 \n\n\nZalety:\n\nOdporne na wartości odstające\nDostarczają informacji o rozpiętości i skośności danych\n\nWady:\n\nMniej precyzyjne niż użycie wszystkich punktów danych\nRóżne metody obliczania mogą prowadzić do nieznacznie różnych wyników",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-kształtu",
    "href": "rozdzial5.html#miary-kształtu",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.5 Miary kształtu",
    "text": "12.5 Miary kształtu\nTe miary opisują kształt rozkładu prawdopodobieństwa danych.\n\n12.5.1 Skośność\nSkośność mierzy asymetrię rozkładu prawdopodobieństwa.\nWzór: \\(SK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3\\)\nObliczenie w R:\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nskewness(dane)\n\n[1] 0.4592793\n\n\nInterpretacja:\n\nSkośność dodatnia: prawy ogon jest dłuższy (średnia &gt; mediana)\nSkośność ujemna: lewy ogon jest dłuższy (średnia &lt; mediana)\nSkośność zero: rozkład symetryczny\n\nZalety:\n\nDostarcza informacji o kształcie rozkładu\nPrzydatna do sprawdzania założeń normalności\n\nWady:\n\nWrażliwa na wartości odstające\nMoże być myląca dla rozkładów wielomodalnych\n\n\n\n12.5.2 Kurtoza\nKurtoza mierzy “grubość ogonów” rozkładu prawdopodobieństwa.\nWzór: \\(K = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\\)\nObliczenie w R:\n\nkurtosis(dane)\n\n[1] 2.457047\n\n\nInterpretacja:\n\nKurtoza dodatnia: ciężkie ogony, rozkład wysmukły\nKurtoza ujemna: lekkie ogony, rozkład płaski\nKurtoza zero: rozkład normalny\n\nZalety:\n\nDostarcza informacji o wartościach skrajnych w rozkładzie\nPrzydatna do modelowania finansowego i oceny ryzyka\n\nWady:\n\nWrażliwa na wartości odstające\nMoże być trudna do praktycznej interpretacji",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wprowadzenie-do-dwuwymiarowych-i-wielowymiarowych-statystyk-opisowych",
    "href": "rozdzial5.html#wprowadzenie-do-dwuwymiarowych-i-wielowymiarowych-statystyk-opisowych",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.6 Wprowadzenie do dwuwymiarowych i wielowymiarowych statystyk opisowych",
    "text": "12.6 Wprowadzenie do dwuwymiarowych i wielowymiarowych statystyk opisowych\nPodczas gdy statystyki jednowymiarowe opisują pojedyncze zmienne, statystyki dwuwymiarowe i wielowymiarowe badają relacje między zmiennymi.\n\n12.6.1 Statystyki dwuwymiarowe\n\n12.6.1.1 Korelacja\nKorelacja mierzy siłę i kierunek liniowego związku między dwiema zmiennymi.\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 5, 4, 5)\ncor(x, y)\n\n[1] 0.7745967\n\n\n\n\n12.6.1.2 Kowariancja\nKowariancja mierzy, jak dwie zmienne zmieniają się razem.\n\ncov(x, y)\n\n[1] 1.5\n\n\n\n\n12.6.1.3 Tabela krzyżowa\nTabela krzyżowa (tabela kontyngencji) pokazuje relację między dwiema zmiennymi kategorialnymi.\n\ntable(cut(x, 2), cut(y, 2))\n\n           \n            (2,3.5] (3.5,5]\n  (0.996,3]       1       2\n  (3,5]           0       2\n\n\n\n\n\n12.6.2 Statystyki wielowymiarowe\n\nKorelacja wielokrotna: Korelacja między zmienną zależną a wieloma zmiennymi niezależnymi.\nKorelacja cząstkowa: Korelacja między dwiema zmiennymi przy kontrolowaniu innych.\nAnaliza czynnikowa: Technika redukcji wielu zmiennych do kilku podstawowych czynników.\n\nTe tematy zostaną omówione bardziej szczegółowo w kolejnych rozdziałach.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#podsumowanie",
    "href": "rozdzial5.html#podsumowanie",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.7 Podsumowanie",
    "text": "12.7 Podsumowanie\nStatystyki opisowe są niezbędnymi narzędziami do podsumowywania i zrozumienia danych w badaniach nauk społecznych. Jednak ważne jest, aby:\n\nWybierać odpowiednie miary w oparciu o typ danych i rozkład\nByć świadomym ograniczeń każdej miary, szczególnie w odniesieniu do wartości odstających\nUżywać wielu miar, aby uzyskać kompleksowe zrozumienie\n\nPamiętaj, że statystyki opisowe są tylko punktem wyjścia. Stanowią one podstawę dla statystyk inferencyjnych, które pozwalają nam wyciągać wnioski o populacjach na podstawie próbek.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#bibliografia",
    "href": "rozdzial5.html#bibliografia",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.8 Bibliografia",
    "text": "12.8 Bibliografia\n\nAgresti, A., & Finlay, B. (2009). Metody statystyczne dla nauk społecznych (wyd. 4). Pearson Prentice Hall.\nField, A. (2013). Odkrywanie statystyki z wykorzystaniem IBM SPSS Statistics. Sage.\nHair, J. F., Black, W. C., Babin, B. J., & Anderson, R. E. (2019). Analiza danych wielowymiarowych (wyd. 8). Cengage Learning.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ćwiczenie-1.-porównanie-wynagrodzeń",
    "href": "rozdzial5.html#ćwiczenie-1.-porównanie-wynagrodzeń",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.9 Ćwiczenie 1. Porównanie wynagrodzeń",
    "text": "12.9 Ćwiczenie 1. Porównanie wynagrodzeń\n\n12.9.1 Dane\nMamy dane o wynagrodzeniach (w tysiącach euro) z dwóch małych firm europejskich:\n\nFirma X: X = {2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35};\nFirma Y: Y = {3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8}\n\n\n\n12.9.2 Miary tendencji centralnej\n\n12.9.2.1 Średnia arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez ich liczbę.\nWzór: \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\)\n\n12.9.2.1.1 Obliczenia ręczne dla Firmy X\n\n\n\nWartość (\\(x_i\\))\nCzęstość (\\(f_i\\))\n\\(x_i \\cdot f_i\\)\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nSuma\nn = 20\nSuma = 119\n\n\n\n\\(\\bar{x} = \\frac{119}{20} = 5,95\\)\n\n\n12.9.2.1.2 Obliczenia ręczne dla Firmy Y\n\n\n\nWartość (\\(x_i\\))\nCzęstość (\\(f_i\\))\n\\(x_i \\cdot f_i\\)\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nSuma\nn = 20\nSuma = 100\n\n\n\n\\(\\bar{y} = \\frac{100}{20} = 5\\)\n\n\n12.9.2.1.3 Weryfikacja w R\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\n12.9.2.2 Mediana\nMediana to wartość środkowa w uporządkowanym zbiorze danych.\n\n12.9.2.2.1 Obliczenia ręczne dla Firmy X\nUporządkowane dane: 2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35\nn = 20 (parzyste), więc bierzemy średnią z 10. i 11. wartości:\nMediana = \\(\\frac{4 + 4}{2} = 4\\)\n\n\n12.9.2.2.2 Obliczenia ręczne dla Firmy Y\nUporządkowane dane: 3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8\nn = 20 (parzyste), więc bierzemy średnią z 10. i 11. wartości:\nMediana = \\(\\frac{5 + 5}{2} = 5\\)\n\n\n12.9.2.2.3 Weryfikacja w R\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\n12.9.2.3 Dominanta (moda)\nDominanta to najczęściej występująca wartość w zbiorze danych.\nDla Firmy X dominanta wynosi 3 (występuje 6 razy). Dla Firmy Y są dwie dominanty: 4 i 5 (obie występują 6 razy).\n\n# Funkcja do obliczania dominanty\nznajdz_dominante &lt;- function(x) {\n  unikalne_x &lt;- unique(x)\n  unikalne_x[which.max(tabulate(match(x, unikalne_x)))]\n}\n\nznajdz_dominante(X)\n\n[1] 3\n\nznajdz_dominante(Y)\n\n[1] 4\n\n\n\n\n\n12.9.3 Miary rozproszenia\n\n12.9.3.1 Wariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: \\(s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\)\n\n12.9.3.1.1 Obliczenia ręczne dla Firmy X\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(f_i\\)\n\\(x_i - \\bar{x}\\)\n\\((x_i - \\bar{x})^2\\)\n\\(f_i(x_i - \\bar{x})^2\\)\n\n\n\n\n2\n3\n-3,95\n15,6025\n46,8075\n\n\n3\n6\n-2,95\n8,7025\n52,215\n\n\n4\n5\n-1,95\n3,8025\n19,0125\n\n\n5\n4\n-0,95\n0,9025\n3,61\n\n\n20\n1\n14,05\n197,4025\n197,4025\n\n\n35\n1\n29,05\n843,9025\n843,9025\n\n\nSuma\n20\n\n\n1162,95\n\n\n\n\\(s^2 = \\frac{1162,95}{19} = 61,21\\)\n\n\n12.9.3.1.2 Obliczenia ręczne dla Firmy Y\n\n\n\n\n\n\n\n\n\n\n\\(y_i\\)\n\\(f_i\\)\n\\(y_i - \\bar{x}\\)\n\\((y_i - \\bar{y})^2\\)\n\\(f_i(y_i - \\bar{y})^2\\)\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nSuma\n20\n\n\n34\n\n\n\n\\(s^2 = \\frac{34}{19} = 1,79\\)\n\n\n12.9.3.1.3 Weryfikacja w R\n\nvar(X)\n\n[1] 61.20789\n\nvar(Y)\n\n[1] 1.789474\n\n\n\n\n\n12.9.3.2 Odchylenie standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji.\nWzór: \\(s = \\sqrt{s^2}\\)\nDla Firmy X: \\(s = \\sqrt{61,21} = 7,82\\) Dla Firmy Y: \\(s = \\sqrt{1,79} = 1,34\\)\n\n12.9.3.2.1 Weryfikacja w R\n\nsd(X)\n\n[1] 7.823547\n\nsd(Y)\n\n[1] 1.337712\n\n\n\n\n\n\n12.9.4 Kwartyle\nKwartyle dzielą zbiór danych na cztery równe części.\n\n12.9.4.1 Obliczenia ręczne dla Firmy X\nUporządkowane dane: 2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 3\nQ2 (50. percentyl, mediana): 4\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 5\n\n\n\n12.9.4.2 Obliczenia ręczne dla Firmy Y\nUporządkowane dane: 3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 4\nQ2 (50. percentyl, mediana): 5\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 6\n\n\n\n12.9.4.3 Weryfikacja w R\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\n\n12.9.5 Wykres pudełkowy Tukeya\nWykres pudełkowy Tukeya wizualnie przedstawia rozkład danych na podstawie kwartyli. Użyjemy biblioteki ggplot2 do stworzenia wykresu.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Przygotowanie danych\ndane &lt;- data.frame(\n  Firma = rep(c(\"X\", \"Y\"), each = 20),\n  Wynagrodzenie = c(X, Y)\n)\n\n# Tworzenie wykresu pudełkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot() +\n  labs(title = \"Rozkład wynagrodzeń w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiące euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\n12.9.5.1 Interpretacja wykresu pudełkowego\n\nPudełko reprezentuje rozstęp międzykwartylowy (IQR) od Q1 do Q3.\nLinia wewnątrz pudełka to mediana (Q2).\nWąsy rozciągają się do najmniejszych i największych wartości w granicach 1,5 * IQR.\nPunkty poza wąsami są uznawane za wartości odstające.\n\n\n\n\n12.9.6 Porównanie wyników\n\n\n\nMiara\nFirma X\nFirma Y\n\n\n\n\nŚrednia\n5,95\n5,00\n\n\nMediana\n4\n5\n\n\nDominanta\n3\n4 i 5\n\n\nWariancja\n61,21\n1,79\n\n\nOdchylenie standard.\n7,82\n1,34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\n12.9.6.1 Kluczowe obserwacje:\n\nTendencja centralna: Firma X ma wyższą średnią, ale niższą medianę niż Firma Y, co wskazuje na prawostronnie skośny rozkład dla Firmy X.\nRozproszenie: Firma X wykazuje znacznie wyższą wariancję i odchylenie standardowe, sugerując większe dysproporcje w wynagrodzeniach.\nKształt rozkładu: Wynagrodzenia w Firmie Y są bardziej skupione, podczas gdy Firma X ma wartości ekstremalne (potencjalne wartości odstające), które znacząco wpływają na jej średnią i wariancję.\nKwartyle: Rozstęp międzykwartylowy (Q3 - Q1) Firmy Y jest nieznacznie większy, ale jej ogólny zakres jest znacznie mniejszy niż Firmy X.\n\n\n\n\n12.9.7 Wnioski\nTa analiza porównawcza ujawnia znaczące różnice w strukturach wynagrodzeń między dwiema firmami. Firma X wykazuje większą zmienność i potencjalną nierówność w swojej skali płac, podczas gdy Firma Y demonstruje bardziej spójny i wąsko rozłożony zakres wynagrodzeń. Te spostrzeżenia mogą być cenne dla zrozumienia strategii wynagrodzeń i potencjalnych obszarów do przeglądu polityki w każdej z firm.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ćwiczenie-2.-porównanie-zmienności-wielkości-okręgów-wyborczych-między-krajami",
    "href": "rozdzial5.html#ćwiczenie-2.-porównanie-zmienności-wielkości-okręgów-wyborczych-między-krajami",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.10 Ćwiczenie 2. Porównanie Zmienności Wielkości Okręgów Wyborczych Między Krajami",
    "text": "12.10 Ćwiczenie 2. Porównanie Zmienności Wielkości Okręgów Wyborczych Między Krajami\n\n12.10.1 Dane\nMamy dane o wielkości okręgów wyborczych z dwóch krajów:\n\nKraj o wysokiej zmienności (X): 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nKraj o niskiej zmienności (Y): 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\n\n\n\n12.10.2 Miary Tendencji Centralnej\n\n12.10.2.1 Średnia Arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez ich liczbę.\nWzór: \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\)\n\n12.10.2.1.1 Obliczenia dla Kraju X\n\n\n\nWartości\nSuma\n\n\n\n\n1 + 3 + 5 + 7 + 9 + 11 + 13 + 15 + 17 + 19\n100\n\n\n\n\\(\\bar{x} = \\frac{100}{10} = 10\\)\n\n\n12.10.2.1.2 Obliczenia dla Kraju Y\n\n\n\nWartości\nSuma\n\n\n\n\n8 + 9 + 9 + 10 + 10 + 11 + 11 + 12 + 12 + 13\n105\n\n\n\n\\(\\bar{y} = \\frac{105}{10} = 10,5\\)\n\n\n\n12.10.2.2 Mediana\nMediana to środkowa wartość w uporządkowanym zbiorze danych.\n\n12.10.2.2.1 Obliczenia dla Kraju X\nUporządkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMediana = \\(\\frac{9 + 11}{2} = 10\\)\n\n\n12.10.2.2.2 Obliczenia dla Kraju Y\nUporządkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMediana = \\(\\frac{10 + 11}{2} = 10,5\\)\n\n\n\n12.10.2.3 Dominanta\nDominanta to najczęściej występująca wartość w zbiorze danych.\nDla Kraju X nie ma dominanty (wszystkie wartości występują raz). Dla Kraju Y są cztery dominanty: 9, 10, 11 i 12 (każda występuje dwa razy).\n\n\n\n12.10.3 Miary Rozproszenia\n\n12.10.3.1 Rozstęp\nRozstęp to różnica między wartością maksymalną a minimalną.\n\n12.10.3.1.1 Obliczenia dla Kraju X\nRozstęp = 19 - 1 = 18\n\n\n12.10.3.1.2 Obliczenia dla Kraju Y\nRozstęp = 13 - 8 = 5\n\n\n\n12.10.3.2 Wariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: \\(s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\)\n\n12.10.3.2.1 Obliczenia dla Kraju X\n\n\n\n\\(x_i\\)\n\\((x_i - \\bar{x})\\)\n\\((x_i - \\bar{x})^2\\)\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSuma\n\n330\n\n\n\n\\(s^2_X = \\frac{330}{9} = 36,67\\)\n\n\n12.10.3.2.2 Obliczenia dla Kraju Y\n\n\n\n\\(x_i\\)\n\\((y_i - \\bar{y})\\)\n\\((y_i - \\bar{y})^2\\)\n\n\n\n\n8\n-2,5\n6,25\n\n\n9\n-1,5\n2,25\n\n\n9\n-1,5\n2,25\n\n\n10\n-0,5\n0,25\n\n\n10\n-0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n12\n1,5\n2,25\n\n\n12\n1,5\n2,25\n\n\n13\n2,5\n6,25\n\n\nSuma\n\n22,5\n\n\n\n\\(s^2_Y = \\frac{22,5}{9} = 2,5\\)\n\n\n\n12.10.3.3 Odchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji.\nWzór: \\(s = \\sqrt{s^2}\\)\n\n12.10.3.3.1 Obliczenia dla Kraju X\n\\(s_X = \\sqrt{36,67} \\approx 6,06\\)\n\n\n12.10.3.3.2 Obliczenia dla Kraju Y\n\\(s_Y = \\sqrt{2,5} \\approx 1,58\\)\n\n\n\n\n12.10.4 Kwartyle i Rozstęp Międzykwartylowy (IQR)\nKwartyle dzielą zbiór danych na cztery równe części.\n\n12.10.4.1 Obliczenia dla Kraju X\n\nQ1 (25. percentyl): \\(\\frac{3 + 5}{2} = 4\\)\nQ2 (50. percentyl, mediana): 10\nQ3 (75. percentyl): \\(\\frac{15 + 17}{2} = 16\\)\nIQR = Q3 - Q1 = 16 - 4 = 12\n\n\n\n12.10.4.2 Obliczenia dla Kraju Y\n\nQ1 (25. percentyl): 9\nQ2 (50. percentyl, mediana): 10,5\nQ3 (75. percentyl): 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n\n\n\n12.10.5 Współczynnik Zmienności (CV)\nWspółczynnik zmienności to stosunek odchylenia standardowego do średniej, wyrażony w procentach.\nWzór: \\(CV = \\frac{s}{\\bar{x}} \\times 100\\%\\)\n\n12.10.5.1 Obliczenia dla Kraju X\n\\(CV_X = \\frac{6,06}{10} \\times 100\\% = 60,6\\%\\)\n\n\n12.10.5.2 Obliczenia dla Kraju Y\n\\(CV_Y = \\frac{1,58}{10,5} \\times 100\\% = 15,0\\%\\)\n\n\n\n12.10.6 Porównanie Wyników\n\n\n\nMiara\nKraj X (Wysoka zm.)\nKraj Y (Niska zm.)\n\n\n\n\nŚrednia\n10\n10,5\n\n\nMediana\n10\n10,5\n\n\nDominanta\nBrak\n9, 10, 11, 12\n\n\nRozstęp\n18\n5\n\n\nWariancja\n36,67\n2,5\n\n\nOdch. Stand.\n6,06\n1,58\n\n\nIQR\n12\n3\n\n\nWsp. Zmienności\n60,6%\n15,0%\n\n\n\n\n\n12.10.7 Porównanie za pomocą Wykresu Pudełkowego\nAby wizualnie porównać rozkład wielkości okręgów wyborczych między dwoma krajami, możemy użyć wykresu pudełkowego w stylu Tukeya. Ten typ wykresu zapewnia zwięzłe podsumowanie rozkładu danych, w tym medianę, kwartyle i potencjalne wartości odstające.\n\n# Wczytaj niezbędną bibliotekę\nlibrary(ggplot2)\n\n# Utwórz ramki danych dla każdego kraju\nkraj_x &lt;- data.frame(kraj = \"X\", wielkosc = c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19))\nkraj_y &lt;- data.frame(kraj = \"Y\", wielkosc = c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13))\n\n# Połącz dane\nwszystkie_dane &lt;- rbind(kraj_x, kraj_y)\n\n# Utwórz wykres pudełkowy\nggplot(wszystkie_dane, aes(x = kraj, y = wielkosc, fill = kraj)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(title = \"Porównanie Zmienności Wielkości Okręgów Wyborczych\",\n       x = \"Kraj\",\n       y = \"Wielkość Okręgu\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\"))\n\n\n\n\nPorównanie Zmienności Wielkości Okręgów Wyborczych\n\n\n\n\n\n12.10.7.1 Interpretacja Wykresu Pudełkowego\nWykres pudełkowy dostarcza kilku kluczowych informacji:\n\nPudełko reprezentuje rozstęp międzykwartylowy (IQR), z dolną krawędzią na Q1 i górną na Q3.\nLinia wewnątrz pudełka reprezentuje medianę (Q2).\nWąsy rozciągają się do najmniejszej i największej wartości w zakresie 1,5 * IQR od krawędzi pudełka.\nPunkty poza wąsami są uznawane za potencjalne wartości odstające i są wykreślane indywidualnie.\n\nNa podstawie tego wykresu możemy zaobserwować:\n\nMediana wielkości okręgów dla Kraju Y jest nieznacznie wyższa niż dla Kraju X, co jest zgodne z naszymi wcześniejszymi obliczeniami.\nPudełko dla Kraju X jest znacznie większe niż dla Kraju Y, co wskazuje na większe rozproszenie środkowych 50% danych, a tym samym wyższą zmienność.\nDane Kraju X obejmują znacznie szerszy zakres, co widać po dłuższych wąsach, co dodatkowo potwierdza jego wyższą zmienność.\nDane Kraju Y są bardziej skupione, z mniejszym pudełkiem i krótszymi wąsami, co wskazuje na niższą zmienność.\nIndywidualne punkty dla Kraju X są bardziej rozproszone, podczas gdy dla Kraju Y są bardziej skupione, co stanowi wizualną reprezentację różnicy w zmienności.\n\nTa wizualizacja za pomocą wykresu pudełkowego wzmacnia naszą wcześniejszą analizę numeryczną, wyraźnie pokazując kontrast w zmienności wielkości okręgów wyborczych między dwoma krajami.\n\n\n\n12.10.8 Kluczowe Obserwacje\n\nTendencja Centralna: Oba kraje mają podobne średnie i mediany, co wskazuje, że ich przeciętne wielkości okręgów są zbliżone.\nRozproszenie:\n\nKraj X wykazuje znacznie wyższą wariancję i odchylenie standardowe, potwierdzając jego wysoką zmienność.\nRozstęp dla Kraju X (18) jest ponad trzy razy większy niż dla Kraju Y (5).\nIQR dla Kraju X (12) jest cztery razy większy niż dla Kraju Y (3), wskazując na znacznie szersze rozproszenie środkowych 50% danych.\n\nKształt Rozkładu:\n\nKraj X ma rozkład równomierny bez wyraźnej dominanty.\nKraj Y ma bardziej skupiony rozkład z wieloma dominantami, wskazując na powszechne wielkości okręgów.\n\nWspółczynnik Zmienności:\n\nCV Kraju X (60,6%) jest znacznie wyższy niż Kraju Y (15,0%), dostarczając standaryzowanej miary różnicy w zmienności.\n\nPorównanie Wizualne:\n\nWykres pudełkowy wyraźnie ilustruje znaczącą różnicę w zmienności między dwoma krajami, potwierdzając nasze numeryczne odkrycia.\n\n\n\n\n12.10.9 Wnioski\nTa analiza wyraźnie pokazuje kontrast w zmienności wielkości okręgów wyborczych między dwoma krajami:\n\nKraj X wykazuje wysoką zmienność, z wielkościami okręgów rozproszonymi szeroko od 1 do 19. Może to wskazywać na zróżnicowany system wyborczy z mieszanką małych (prawdopodobnie jednomandatowych) i dużych okręgów wielomandatowych.\nKraj Y wykazuje niską zmienność, z wielkościami okręgów ściśle skupionymi między 8 a 13. Sugeruje to bardziej jednolity system wyborczy, prawdopodobnie składający się z okręgów wielomandatowych średniej wielkości.\n\nTe różnice w zmienności mogą mieć istotne implikacje dla reprezentacji politycznej, strategii partyjnych i wyników wyborów w każdym kraju. Wysoka zmienność (jak w Kraju X) może prowadzić do bardziej zróżnicowanej reprezentacji, ale może również skutkować bardziej złożonymi strategiami wyborczymi. Niska zmienność (jak w Kraju Y) może sprzyjać bardziej spójnej reprezentacji w okręgach, ale potencjalnie ograniczać różnorodność głosów politycznych.\nDodanie wykresu pudełkowego dostarcza potężnego narzędzia wizualnego do zrozumienia tych różnic, czyniąc kontrast między dwoma krajami natychmiast widocznym i uzupełniając szczegółową analizę numeryczną.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#appendix-tabele-podsumowujące-typy-danych-i-odpowiednie-miary-statystyczne",
    "href": "rozdzial5.html#appendix-tabele-podsumowujące-typy-danych-i-odpowiednie-miary-statystyczne",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.11 Appendix: Tabele Podsumowujące Typy Danych i Odpowiednie Miary Statystyczne",
    "text": "12.11 Appendix: Tabele Podsumowujące Typy Danych i Odpowiednie Miary Statystyczne\n\n12.11.1 Tabela 1: Dane Dyskretne vs. Ciągłe\n\n\n\n\n\n\n\n\nCharakterystyka\nDane Dyskretne\nDane Ciągłe\n\n\n\n\nDefinicja\nMogą przyjmować tylko określone wartości\nMogą przyjmować dowolną wartość w danym zakresie\n\n\nPrzykłady\nLiczba dzieci, Rozmiar buta\nWzrost, Waga, Czas\n\n\nZalety\n- Łatwe do kategoryzacji- Proste do zliczenia- Często łatwiejsze do analizy\n- Bardziej precyzyjne pomiary- Umożliwiają bardziej zaawansowane analizy statystyczne- Mogą być grupowane w przedziały\n\n\nWady\n- Ograniczona precyzja- Mogą nie uchwycić subtelnych różnic- Niektóre metody statystyczne mogą nie mieć zastosowania\n- Mogą być bardziej złożone do analizy- Mogą wymagać większych próbek dla znaczącej analizy- Mogą wystąpić błędy zaokrągleń przy pomiarze\n\n\nWizualizacja\nWykresy słupkowe, Wykresy kołowe\nHistogramy, Wykresy punktowe, Wykresy liniowe\n\n\n\n\n\n12.11.2 Tabela 2: Typologia Skal Pomiarowych Stevensa\n\n\n\n\n\n\n\n\n\n\nCharakterystyka\nNominalna\nPorządkowa\nInterwałowa\nIlorazowa\n\n\n\n\nDefinicja\nKategorie bez ustalonego porządku\nUporządkowane kategorie\nRówne interwały, brak prawdziwego zera\nRówne interwały z prawdziwym zerem\n\n\nPrzykłady\nPłeć, Grupa krwi\nPoziom wykształcenia, Skale Likerta\nTemperatura (°C, °F), Daty kalendarzowe\nWzrost, Waga, Wiek\n\n\nZalety\n- Łatwe do zebrania- Proste do kategoryzacji\n- Uwzględnia porządek- Przydatne do rankingów\n- Pozwala na znaczące różnice- Możliwe bardziej zaawansowane analizy\n- Najbardziej wszechstronne- Pozwala na wszystkie operacje arytmetyczne\n\n\nWady\n- Ograniczone opcje analityczne- Brak operacji arytmetycznych\n- Różnice między kategoriami nie są kwantyfikowalne- Ograniczone operacje arytmetyczne\n- Brak prawdziwego punktu zerowego- Stosunki nie są znaczące\n- Może być trudno uzyskać prawdziwe pomiary ilorazowe w niektórych dziedzinach\n\n\n\n\n\n12.11.3 Tabela 3: Odpowiednie Miary Statystyczne dla Różnych Typów Danych\n\n\n\n\n\n\n\n\n\nTyp Danych\nMiary Tendencji Centralnej\nMiary Zmienności\nKorelacja/Asocjacja\n\n\n\n\nNominalne\nModa\n-\nV Craméra, Chi-kwadrat\n\n\nPorządkowe\nMediana, Moda\nRozstęp międzykwartylowy\nRho Spearmana, Tau Kendalla\n\n\nInterwałowe\nŚrednia, Mediana, Moda\nOdchylenie standardowe, Wariancja, Zakres\nr Pearsona\n\n\nIlorazowe\nŚrednia, Mediana, Moda\nOdchylenie standardowe, Wariancja, Zakres, Współczynnik zmienności\nr Pearsona\n\n\nDyskretne\nŚrednia*, Mediana, Moda\nOdchylenie standardowe, Wariancja, Zakres\nZależy od poziomu pomiaru\n\n\nCiągłe\nŚrednia, Mediana, Moda\nOdchylenie standardowe, Wariancja, Zakres\nr Pearsona\n\n\n\n* Uwaga: Chociaż te miary można obliczyć dla danych dyskretnych, interpretacja powinna być ostrożna, zwłaszcza dla danych z niewielką liczbą możliwych wartości.\n\n\n12.11.4 Tabela 4: Zalety i Wady Różnych Miar Statystycznych\n\n12.11.4.1 Miary Tendencji Centralnej\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nŚrednia\n- Wykorzystuje wszystkie punkty danych- Pozwala na dalsze obliczenia statystyczne- Idealna dla danych o rozkładzie normalnym\n- Wrażliwa na wartości odstające- Nieodpowiednia dla rozkładów skośnych- Bez znaczenia dla danych nominalnych\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nMediana\n- Niewrażliwa na wartości odstające- Dobra dla rozkładów skośnych- Może być stosowana do danych porządkowych\n- Ignoruje rzeczywiste wartości większości punktów danych- Mniej użyteczna do dalszych analiz statystycznych\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nModa\n- Może być stosowana do każdego typu danych- Dobra do znajdowania najczęstszej kategorii\n- Może nie być unikalna (rozkłady multimodalne)- Nieprzydatna do wielu typów analiz- Ignoruje wielkość różnic między wartościami\nWszystkie typy\n\n\n\n\n\n12.11.4.2 Miary Zmienności\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nZakres\n- Prosty do obliczenia i zrozumienia- Daje szybki obraz rozproszenia danych\n- Bardzo wrażliwy na wartości odstające- Ignoruje wszystkie dane między ekstremami- Nieprzydatny do dalszych analiz statystycznych\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nRozstęp międzykwartylowy (IQR)\n- Niewrażliwy na wartości odstające- Dobry dla rozkładów skośnych\n- Ignoruje 50% danych- Mniej intuicyjny niż zakres\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nWariancja\n- Wykorzystuje wszystkie punkty danych- Podstawa wielu procedur statystycznych\n- Wrażliwa na wartości odstające- Jednostki są podniesione do kwadratu (mniej intuicyjne)\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nOdchylenie standardowe\n- Wykorzystuje wszystkie punkty danych- Te same jednostki co oryginalne dane- Szeroko stosowane i zrozumiałe\n- Wrażliwe na wartości odstające- Zakłada w przybliżeniu rozkład normalny dla interpretacji\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nWspółczynnik zmienności\n- Pozwala na porównanie między zbiorami danych o różnych jednostkach lub średnich\n- Może być mylący, gdy średnie są bliskie zeru- Bez znaczenia dla danych z wartościami ujemnymi\nIlorazowe, niektóre Interwałowe\n\n\n\n\n\n12.11.4.3 Miary Korelacji/Asocjacji\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nr Pearsona\n- Mierzy zależność liniową- Szeroko stosowany i zrozumiały\n- Zakłada rozkład normalny- Wrażliwy na wartości odstające- Uchwytuje tylko zależności liniowe\nInterwałowe, Ilorazowe, Ciągłe\n\n\nRho Spearmana\n- Może być stosowany do danych porządkowych- Uchwytuje zależności monotoniczne- Mniej wrażliwy na wartości odstające\n- Traci informacje przez konwersję na rangi- Może pominąć niektóre typy zależności\nPorządkowe, Interwałowe, Ilorazowe\n\n\nTau Kendalla\n- Może być stosowany do danych porządkowych- Bardziej odporny niż Spearman dla małych próbek- Ma ładną interpretację (prawdopodobieństwo zgodności)\n- Traci informacje, biorąc pod uwagę tylko porządek- Bardziej intensywny obliczeniowo\nPorządkowe, Interwałowe, Ilorazowe\n\n\nChi-kwadrat\n- Może być stosowany do danych nominalnych- Testuje niezależność zmiennych kategorycznych\n- Wymaga dużych rozmiarów próbek- Wrażliwy na rozmiar próbki- Nie mierzy siły asocjacji\nNominalne, Porządkowe\n\n\nV Craméra\n- Może być stosowany do danych nominalnych- Dostarcza miarę siły asocjacji- Znormalizowany do zakresu [0,1]\n- Interpretacja może być subiektywna- Może przeszacować asocjację w małych próbkach\nNominalne, Porządkowe\n\n\n\nTe tabele dostarczają kompleksowego przeglądu różnych typów danych, ich charakterystyk oraz odpowiednich miar statystycznych do zastosowania dla każdego typu. Mogą być łatwo skopiowane i wklejone do Twojej książki Quarto w RStudio. Format markdown powinien być kompatybilny z Quarto, ale może być konieczne niewielkie dostosowanie formatowania w zależności od konkretnych ustawień Quarto.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "13  Data Visualization: with examples in R",
    "section": "",
    "text": "13.1 Introduction to Data Types and Visualization\nBefore diving into specific visualization techniques, it’s crucial to understand the different types of data you might encounter and how they influence your choice of visualization method. We’ll explore these concepts with practical examples using the ggplot2 library in R.\nFirst, let’s load the necessary libraries:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#bar-plots",
    "href": "chapter6.html#bar-plots",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.2 Bar Plots",
    "text": "13.2 Bar Plots\nBar plots are excellent for displaying categorical data or summarizing continuous data by groups.\n\n13.2.1 Understanding Bar Plots\nA bar plot represents data using rectangular bars with heights proportional to the values they represent. They are used to compare different categories or groups.\nKey components of a bar plot: 1. X-axis: Represents categories 2. Y-axis: Represents values (can be counts, percentages, or any numerical value) 3. Bars: Rectangle for each category, height corresponds to its value\n\n13.2.1.1 Example Data\nLet’s use a simple dataset of fruit sales:\n\nfruits &lt;- c(\"Apple\", \"Banana\", \"Orange\", \"Grape\")\nsales &lt;- c(120, 85, 70, 100)\n\n# Create a data frame\ndf &lt;- data.frame(fruit = fruits, sales = sales)\n\n\n\n\n13.2.2 Hand-Drawn Bar Plot\nTo create a bar plot by hand:\n\nDraw a horizontal line (x-axis) and a vertical line (y-axis) perpendicular to each other.\nLabel the x-axis with your categories (fruits), evenly spaced.\nLabel the y-axis with a suitable scale for your values (sales, 0 to 120 in increments of 20).\nFor each category, draw a rectangle (bar) whose height corresponds to its value on the y-axis scale.\nColor or shade each bar if desired.\nAdd a title and labels for both axes.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen drawing by hand, use graph paper for more precise measurements and straighter lines. Choose a scale that allows all your data to fit while maximizing the use of space.\n\n\n\n\n13.2.3 Bar Plot in Base R\n\n# Create bar plot\nbarplot(sales, names.arg = fruits, \n        main = \"Fruit Sales\",\n        xlab = \"Fruit Types\", ylab = \"Sales\")\n\n\n\n\n\n\n\n\n\n\n13.2.4 Bar Plot with ggplot2\n\n# Create bar plot with ggplot2\nggplot(df, aes(x = fruit, y = sales)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Fruit Sales\",\n       x = \"Fruit Types\", y = \"Sales\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n13.2.5 Interpreting Bar Plots\nWhen interpreting a bar plot, consider the following:\n\nRelative Heights: Compare the heights of the bars to understand which categories have higher or lower values.\nOrdering: Sometimes, bars are ordered by height to make comparisons easier.\nPatterns: Look for any patterns or trends across categories.\nOutliers: Identify any bars that are much taller or shorter than the others.\n\n\n13.2.5.1 Example Interpretation\nFor our fruit sales data:\n\nApples have the highest sales (120), followed by Grapes (100).\nOranges have the lowest sales (70).\nThere’s a considerable difference between the highest (Apples) and lowest (Oranges) sales.\nBananas and Grapes have similar sales figures, in the middle range.\n\nThis information could be useful for inventory management or marketing strategies in a fruit shop.\n\n\n\n\n\n\nNote\n\n\n\nBar plots are great for comparing categories, but they don’t show the distribution within each category. For that, you might need other plot types like box plots.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#histograms",
    "href": "chapter6.html#histograms",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.3 Histograms",
    "text": "13.3 Histograms\nHistograms visualize the distribution of a continuous variable by dividing it into intervals (bins) and showing the frequency or density of data points in each bin.\n\n13.3.1 Understanding Histograms\nKey components of a histogram: 1. X-axis: Represents the variable’s values, divided into bins 2. Y-axis: Represents frequency, relative frequency, or density 3. Bars: Rectangle for each bin, height corresponds to the y-axis measure\nThere are three main types of histograms:\n\nFrequency Histogram: The y-axis shows the count of data points in each bin.\nRelative Frequency Histogram: The y-axis shows the proportion of data points in each bin (frequency divided by total number of data points).\nDensity Histogram: The y-axis shows the density, which is the relative frequency divided by the bin width. The total area of all bars sums to 1.\n\n\n13.3.1.1 Example Data\nLet’s use a dataset of 50 student exam scores (out of 100):\n\nset.seed(123)  # for reproducibility\nscores &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\n13.3.2 Hand-Drawn Histogram\nTo create a frequency histogram by hand:\n\nFind the range of your data.\nChoose a number of bins (let’s use 7 bins).\nCreate a frequency table.\nDraw x and y axes.\nLabel x-axis with bin ranges and y-axis with frequency.\nDraw a rectangle for each bin, with height corresponding to its frequency.\nAdd a title and labels for both axes.\n\nFor a relative frequency histogram, divide each frequency by the total number of data points before drawing the bars.\nFor a density histogram, divide the relative frequency by the bin width before drawing the bars.\n\n\n\n\n\n\nTip\n\n\n\nThe number of bins can affect the interpretation. Too few bins may obscure important features, while too many may introduce noise. A common rule of thumb is to use the square root of the number of data points as the number of bins.\n\n\n\n\n13.3.3 Histograms in Base R\n\n# Frequency Histogram\nhist(scores, breaks = 7, \n     main = \"Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Relative Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Relative Frequency\")\n\n\n\n\n\n\n\n# Density Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Density Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Density\")\nlines(density(scores), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n13.3.4 Histograms with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(score = scores)\n\n# Frequency Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nggplot(df, aes(x = score, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Relative Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Relative Frequency\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Density Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Density Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Density\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n13.3.5 Interpreting Histograms\nWhen interpreting a histogram, consider:\n\nCentral Tendency: Where is the peak of the distribution?\nSpread: How wide is the distribution?\nShape: Is it symmetric, skewed, or multi-modal?\nOutliers: Are there any unusual values far from the main distribution?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#box-plots-and-tukey-box-plots",
    "href": "chapter6.html#box-plots-and-tukey-box-plots",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.4 Box Plots and Tukey Box Plots",
    "text": "13.4 Box Plots and Tukey Box Plots\nBox plots, also known as box-and-whisker plots, provide a concise summary of a distribution. We’ll focus on the Tukey-style box plot, named after the statistician John Tukey who popularized this type of plot.\n\n13.4.1 Understanding Box Plots\nA box plot represents five key statistics:\n\nMinimum value (excluding outliers)\nFirst quartile (Q1)\nMedian\nThird quartile (Q3)\nMaximum value (excluding outliers)\n\nAdditionally, box plots show:\n\nWhiskers: Lines extending from the box to the minimum and maximum values (excluding outliers)\nOutliers: Individual points beyond the whiskers\n\n\n13.4.1.1 Calculating Quartiles and Outliers\nTo create a box plot, follow these steps:\n\nOrder your data from smallest to largest.\nFind the median (middle value if odd number of data points, average of two middle values if even).\nFind Q1 (median of lower half of data) and Q3 (median of upper half of data).\nCalculate the Interquartile Range (IQR) = Q3 - Q1\nDetermine outliers using Tukey’s rule:\n\nLower outliers: &lt; Q1 - 1.5 * IQR\nUpper outliers: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe factor 1.5 in Tukey’s outlier rule is based on the properties of the normal distribution. For normally distributed data, this rule identifies about 0.7% of the data as potential outliers.\n\n\n\n\n13.4.1.2 Example Data\nLet’s use a small dataset to illustrate:\n\ndata &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\n13.4.2 Hand-Drawn Tukey Box Plot\nTo create a Tukey box plot by hand:\n\nDraw a vertical line representing the range from minimum to maximum (2 to 15 in our example, excluding the outlier).\nDraw a box from Q1 to Q3.\nDraw a horizontal line through the box at the median.\nDraw whiskers from the box to the minimum and maximum values (excluding outliers).\nRepresent the outlier (50) as an individual point beyond the whisker.\nAdd a scale to the vertical axis and label it.\n\n\n\n13.4.3 Box Plot in Base R\n\n# Create box plot\nboxplot(data, main = \"Box Plot of Sample Data\",\n        ylab = \"Values\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\n13.4.4 Tukey Box Plot with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(value = data)\n\n# Create Tukey box plot with ggplot2\nggplot(df, aes(x = \"\", y = value)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Tukey Box Plot of Sample Data\",\n       x = \"\", y = \"Values\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n13.4.5 Interpreting Box Plots\nWhen interpreting a box plot, consider the following:\n\nCentral Tendency: The median shows the center of the distribution.\nSpread: The box (IQR) represents the middle 50% of the data.\nSkewness: If the median line is closer to one end of the box, the distribution is skewed.\nOutliers: Points beyond the whiskers are potential outliers.\nComparisons: When comparing multiple box plots, look at relative positions of medians, box sizes, and presence of outliers.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#conclusion",
    "href": "chapter6.html#conclusion",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.5 Conclusion",
    "text": "13.5 Conclusion\nIn this chapter, we explored three fundamental types of data visualizations: bar plots, histograms, and box plots. We demonstrated how to create these plots by hand, using R’s base plotting system, and using the ggplot2 library.\nEach type of plot serves a different purpose: - Bar plots are excellent for comparing categories. - Histograms show the distribution of a continuous variable. - Box plots provide a concise summary of a distribution, highlighting central tendency, spread, and outliers.\nRemember, the choice of visualization depends on your data type and the insights you want to convey. Always consider your audience and the story you want to tell with your data when selecting and designing your visualizations.\nPractice creating these plots by hand to deepen your understanding of their construction and interpretation. Then, leverage the power of R and ggplot2 to quickly create and customize these visualizations for larger datasets and more complex analyses.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html",
    "href": "rozdzial6.html",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "",
    "text": "14.1 Wprowadzenie do Typów Danych i Wizualizacji\nPrzed zagłębieniem się w konkretne techniki wizualizacji, ważne jest zrozumienie różnych typów danych i ich wpływu na wybór metody wizualizacji. Przeanalizujemy te koncepcje na praktycznych przykładach z użyciem biblioteki ggplot2 w R.\nNajpierw załadujmy niezbędne biblioteki:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-słupkowe",
    "href": "rozdzial6.html#wykresy-słupkowe",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.2 Wykresy Słupkowe",
    "text": "14.2 Wykresy Słupkowe\nWykresy słupkowe doskonale nadają się do prezentacji danych kategorycznych lub podsumowania danych ciągłych w grupach.\n\n14.2.1 Zrozumienie Wykresów Słupkowych\nWykres słupkowy przedstawia dane za pomocą prostokątnych słupków, których wysokość jest proporcjonalna do reprezentowanych przez nie wartości. Służą do porównywania różnych kategorii lub grup.\nGłówne elementy wykresu słupkowego: 1. Oś X: Reprezentuje kategorie 2. Oś Y: Reprezentuje wartości (mogą to być liczebności, procenty lub dowolne wartości numeryczne) 3. Słupki: Prostokąt dla każdej kategorii, wysokość odpowiada jej wartości\n\n14.2.1.1 Przykładowe Dane\nUżyjmy prostego zestawu danych dotyczącego sprzedaży owoców:\n\nowoce &lt;- c(\"Jabłko\", \"Banan\", \"Pomarańcza\", \"Winogrono\")\nsprzedaz &lt;- c(120, 85, 70, 100)\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(owoc = owoce, sprzedaz = sprzedaz)\n\n\n\n\n14.2.2 Ręcznie Rysowany Wykres Słupkowy\nAby stworzyć wykres słupkowy ręcznie:\n\nNarysuj linię poziomą (oś X) i pionową (oś Y) prostopadłe do siebie.\nOznacz oś X swoimi kategoriami (owocami), równomiernie rozmieszczonymi.\nOznacz oś Y odpowiednią skalą dla Twoich wartości (sprzedaż, od 0 do 120 z przyrostami co 20).\nDla każdej kategorii narysuj prostokąt (słupek), którego wysokość odpowiada jej wartości na skali osi Y.\nJeśli chcesz, pokoloruj lub zacienuj każdy słupek.\nDodaj tytuł i etykiety dla obu osi.\n\n\n\n\n\n\n\nTip\n\n\n\nPrzy rysowaniu ręcznym użyj papieru milimetrowego dla dokładniejszych pomiarów i prostszych linii. Wybierz skalę, która pozwoli zmieścić wszystkie dane, maksymalnie wykorzystując dostępną przestrzeń.\n\n\n\n\n14.2.3 Wykres Słupkowy w Podstawowym R\n\n# Tworzenie wykresu słupkowego\nbarplot(sprzedaz, names.arg = owoce, \n        main = \"Sprzedaż Owoców\",\n        xlab = \"Rodzaje Owoców\", ylab = \"Sprzedaż\")\n\n\n\n\n\n\n\n\n\n\n14.2.4 Wykres Słupkowy z ggplot2\n\n# Tworzenie wykresu słupkowego z ggplot2\nggplot(df, aes(x = owoc, y = sprzedaz)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Sprzedaż Owoców\",\n       x = \"Rodzaje Owoców\", y = \"Sprzedaż\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n14.2.5 Interpretacja Wykresów Słupkowych\nPodczas interpretacji wykresu słupkowego zwróć uwagę na:\n\nWzględne Wysokości: Porównaj wysokości słupków, aby zrozumieć, które kategorie mają wyższe lub niższe wartości.\nKolejność: Czasami słupki są uporządkowane według wysokości, aby ułatwić porównania.\nWzorce: Poszukaj wzorców lub trendów między kategoriami.\nWartości Odstające: Zidentyfikuj słupki, które są znacznie wyższe lub niższe od pozostałych.\n\n\n14.2.5.1 Przykładowa Interpretacja\nDla naszych danych o sprzedaży owoców:\n\nJabłka mają najwyższą sprzedaż (120), następnie Winogrona (100).\nPomarańcze mają najniższą sprzedaż (70).\nIstnieje znaczna różnica między najwyższą (Jabłka) a najniższą (Pomarańcze) sprzedażą.\nBanany i Winogrona mają podobne wartości sprzedaży, w średnim zakresie.\n\nTa informacja może być przydatna dla zarządzania zapasami lub strategii marketingowych w sklepie owocowym.\n\n\n\n\n\n\nNote\n\n\n\nWykresy słupkowe są świetne do porównywania kategorii, ale nie pokazują rozkładu wewnątrz każdej kategorii. Do tego mogą być potrzebne inne typy wykresów, jak wykresy pudełkowe.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#histogramy",
    "href": "rozdzial6.html#histogramy",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.3 Histogramy",
    "text": "14.3 Histogramy\nHistogramy wizualizują rozkład zmiennej ciągłej poprzez podzielenie jej na przedziały (bins) i pokazanie częstości lub gęstości punktów danych w każdym przedziale.\n\n14.3.1 Zrozumienie Histogramów\nGłówne elementy histogramu: 1. Oś X: Reprezentuje wartości zmiennej, podzielone na przedziały 2. Oś Y: Reprezentuje częstość, względną częstość lub gęstość 3. Słupki: Prostokąt dla każdego przedziału, wysokość odpowiada mierze na osi Y\nIstnieją trzy główne typy histogramów:\n\nHistogram Częstości: Oś Y pokazuje liczbę punktów danych w każdym przedziale.\nHistogram Częstości Względnej: Oś Y pokazuje proporcję punktów danych w każdym przedziale (częstość podzielona przez całkowitą liczbę punktów danych).\nHistogram Gęstości: Oś Y pokazuje gęstość, która jest częstością względną podzieloną przez szerokość przedziału. Całkowita powierzchnia wszystkich słupków sumuje się do 1.\n\n\n14.3.1.1 Przykładowe Dane\nUżyjmy zbioru 50 wyników egzaminów studentów (na 100 punktów):\n\nset.seed(123)  # dla powtarzalności\nwyniki &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\n14.3.2 Ręcznie Rysowany Histogram\nAby stworzyć histogram częstości ręcznie:\n\nZnajdź zakres danych.\nWybierz liczbę przedziałów (użyjmy 7 przedziałów).\nUtwórz tabelę częstości.\nNarysuj osie X i Y.\nOznacz oś X zakresami przedziałów, a oś Y częstością.\nNarysuj prostokąt dla każdego przedziału, z wysokością odpowiadającą jego częstości.\nDodaj tytuł i etykiety dla obu osi.\n\nDla histogramu częstości względnej, podziel każdą częstość przez całkowitą liczbę punktów danych przed narysowaniem słupków.\nDla histogramu gęstości, podziel częstość względną przez szerokość przedziału przed narysowaniem słupków.\n\n\n\n\n\n\nTip\n\n\n\nLiczba przedziałów może wpłynąć na interpretację. Zbyt mało przedziałów może ukryć ważne cechy, podczas gdy zbyt wiele może wprowadzić szum. Powszechną regułą jest użycie pierwiastka kwadratowego z liczby punktów danych jako liczby przedziałów.\n\n\n\n\n14.3.3 Histogramy w Podstawowym R\n\n# Histogram Częstości\nhist(wyniki, breaks = 7, \n     main = \"Histogram Częstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość\")\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Częstości Względnej Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość Względna\")\n\n\n\n\n\n\n\n# Histogram Gęstości\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Gęstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Gęstość\")\nlines(density(wyniki), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n14.3.4 Histogramy z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wynik = wyniki)\n\n# Histogram Częstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nggplot(df, aes(x = wynik, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Względnej Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość Względna\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Histogram Gęstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Histogram Gęstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Gęstość\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n14.3.5 Interpretacja Histogramów\nPodczas interpretacji histogramu zwróć uwagę na:\n\nTendencję Centralną: Gdzie znajduje się szczyt rozkładu?\nRozrzut: Jak szeroki jest rozkład?\nKształt: Czy jest symetryczny, skośny, czy wielomodalny?\nWartości Odstające: Czy są nietypowe wartości daleko od głównego rozkładu?",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "href": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya",
    "text": "14.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya\nWykresy pudełkowe, znane również jako wykresy skrzynkowe, dostarczają zwięzłego podsumowania rozkładu. Skupimy się na wykresie pudełkowym w stylu Tukeya, nazwanym na cześć statystyka Johna Tukeya, który spopularyzował ten typ wykresu.\n\n14.4.1 Zrozumienie Wykresów Pudełkowych\nWykres pudełkowy przedstawia pięć kluczowych statystyk:\n\nWartość minimalna (z wyłączeniem wartości odstających)\nPierwszy kwartyl (Q1)\nMediana\nTrzeci kwartyl (Q3)\nWartość maksymalna (z wyłączeniem wartości odstających)\n\nDodatkowo wykresy pudełkowe pokazują:\n\nWąsy: Linie rozciągające się od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających)\nWartości odstające: Indywidualne punkty poza wąsami\n\n\n14.4.1.1 Obliczanie Kwartyli i Wartości Odstających\nAby stworzyć wykres pudełkowy, postępuj zgodnie z tymi krokami:\n\nUporządkuj dane od najmniejszej do największej wartości.\nZnajdź medianę (środkowa wartość dla nieparzystej liczby punktów danych, średnia z dwóch środkowych wartości dla parzystej).\nZnajdź Q1 (mediana dolnej połowy danych) i Q3 (mediana górnej połowy danych).\nOblicz Rozstęp Międzykwartylowy (IQR) = Q3 - Q1\nOkreśl wartości odstające używając reguły Tukeya:\n\nDolne wartości odstające: &lt; Q1 - 1.5 * IQR\nGórne wartości odstające: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nWspółczynnik 1.5 w regule Tukeya dla wartości odstających opiera się na właściwościach rozkładu normalnego. Dla danych o rozkładzie normalnym, ta reguła identyfikuje około 0.7% danych jako potencjalne wartości odstające.\n\n\n\n\n14.4.1.2 Przykładowe Dane\nUżyjmy małego zbioru danych do ilustracji:\n\ndane &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\n14.4.2 Ręcznie Rysowany Wykres Pudełkowy Tukeya\nAby stworzyć wykres pudełkowy Tukeya ręcznie:\n\nNarysuj linię pionową reprezentującą zakres od minimum do maksimum (2 do 15 w naszym przykładzie, z wyłączeniem wartości odstającej).\nNarysuj pudełko od Q1 do Q3.\nNarysuj poziomą linię przez pudełko na poziomie mediany.\nNarysuj wąsy od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających).\nPrzedstaw wartość odstającą (50) jako indywidualny punkt poza wąsem.\nDodaj skalę do osi pionowej i oznacz ją.\n\n\n\n14.4.3 Wykres Pudełkowy w Podstawowym R\n\n# Tworzenie wykresu pudełkowego\nboxplot(dane, main = \"Wykres Pudełkowy Przykładowych Danych\",\n        ylab = \"Wartości\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\n14.4.4 Wykres Pudełkowy Tukeya z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wartosc = dane)\n\n# Tworzenie wykresu pudełkowego Tukeya z ggplot2\nggplot(df, aes(x = \"\", y = wartosc)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Wykres Pudełkowy Tukeya Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n14.4.5 Interpretacja Wykresów Pudełkowych\nPodczas interpretacji wykresu pudełkowego zwróć uwagę na następujące elementy:\n\nTendencja Centralna: Mediana pokazuje środek rozkładu.\nRozrzut: Pudełko (IQR) reprezentuje środkowe 50% danych.\nSkośność: Jeśli linia mediany jest bliżej jednego końca pudełka, rozkład jest skośny.\nWartości Odstające: Punkty poza wąsami są potencjalnymi wartościami odstającymi.\nPorównania: Przy porównywaniu wielu wykresów pudełkowych, zwróć uwagę na względne położenie median, rozmiary pudełek i obecność wartości odstających.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "href": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.5 Zaawansowane Techniki Wizualizacji",
    "text": "14.5 Zaawansowane Techniki Wizualizacji\nOprócz podstawowych typów wykresów, warto poznać kilka bardziej zaawansowanych technik wizualizacji, które mogą być przydatne w analizie danych.\n\n14.5.1 Wykresy Skrzypcowe\nWykresy skrzypcowe łączą cechy wykresów pudełkowych i wykresów gęstości, dając bardziej kompletny obraz rozkładu danych.\n\n# Tworzenie wykresu skrzypcowego\nggplot(df, aes(x = \"\", y = wartosc)) +\n  geom_violin(fill = \"lightblue\") +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Wykres Skrzypcowy Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n14.5.2 Wykresy Rozrzutu z Marginesami\nŁączenie wykresów rozrzutu z histogramami na marginesach może dostarczyć więcej informacji o rozkładzie danych w dwóch wymiarach.\n\n# Generowanie danych do wykresu rozrzutu\nset.seed(123)\ndf_scatter &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100)\n)\n\n# Tworzenie wykresu rozrzutu z marginesami\nlibrary(ggExtra)\np &lt;- ggplot(df_scatter, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal()\nggMarginal(p, type = \"histogram\", fill = \"lightblue\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wnioski",
    "href": "rozdzial6.html#wnioski",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.6 Wnioski",
    "text": "14.6 Wnioski\nW tym rozdziale poznaliśmy trzy podstawowe typy wizualizacji danych: wykresy słupkowe, histogramy i wykresy pudełkowe. Pokazaliśmy, jak tworzyć te wykresy ręcznie, używając podstawowego systemu wykresów R oraz biblioteki ggplot2.\nKażdy typ wykresu służy innemu celowi: - Wykresy słupkowe doskonale nadają się do porównywania kategorii. - Histogramy pokazują rozkład zmiennej ciągłej. - Wykresy pudełkowe dostarczają zwięzłego podsumowania rozkładu, podkreślając tendencję centralną, rozrzut i wartości odstające.\nPamiętaj, że wybór wizualizacji zależy od typu danych i wniosków, które chcesz przekazać. Zawsze bierz pod uwagę swoją docelową grupę odbiorców i historię, którą chcesz opowiedzieć za pomocą swoich danych, wybierając i projektując wizualizacje.\nĆwicz tworzenie tych wykresów ręcznie, aby pogłębić zrozumienie ich konstrukcji i interpretacji. Następnie wykorzystaj moc R i ggplot2, aby szybko tworzyć i dostosowywać te wizualizacje dla większych zbiorów danych i bardziej złożonych analiz.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#ćwiczenia-praktyczne",
    "href": "rozdzial6.html#ćwiczenia-praktyczne",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.7 Ćwiczenia Praktyczne",
    "text": "14.7 Ćwiczenia Praktyczne\n\nZbierz dane o popularności różnych gatunków muzycznych wśród Twoich znajomych. Stwórz wykres słupkowy przedstawiający te dane.\nZmierz czas reakcji 30 osób na bodziec dźwiękowy (w milisekundach). Utwórz histogram tych danych.\nZbierz dane o wzroście 50 osób w Twojej społeczności. Stwórz wykres pudełkowy dla tych danych, osobno dla mężczyzn i kobiet.\nZnajdź zestaw danych online (np. na Kaggle) i stwórz trzy różne wizualizacje dla tych danych. Opisz, jakie wnioski można wyciągnąć z każdej wizualizacji.\nStwórz wykres skrzypcowy dla danych o cenach domów w różnych dzielnicach miasta. Porównaj go z wykresem pudełkowym tych samych danych. Jakie dodatkowe informacje dostarcza wykres skrzypcowy?\n\nPamiętaj, że praktyka jest kluczem do opanowania sztuki wizualizacji danych. Eksperymentuj z różnymi typami wykresów i parametrami, aby znaleźć najlepszy sposób przedstawienia swoich danych.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\n\n\n\nBlair, G., Coppock, A., & Humphreys, M. (2023). Research design in the social sciences: declaration, diagnosis, and redesign. Princeton University Press. https://book.declaredesign.org/\nBryman, A., 2016. Social research methods. Oxford University Press.\nBueno de Mesquita, Ethan and Anthony Fowler. 2021. Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis. Princeton University Press.\nCetinkaya-Rundel, M., Diez, D.M. and Barr, C.D., 2019 (4th ed.). OpenIntro Statistics: an Open-source Textbook: https://www.openintro.org/book/os/\nClaude [Large language model], 2024. https://www.anthropic.com\nConcepts and Computation: An Introduction to Political Methodology. https://pos3713.github.io/notes/\nHannay, K. (2019). Introduction to statistics and data science. http://khannay.com/StatsBook/\nIsmay, C. and Kim, A.Y., 2019. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. https://moderndive.com/index.html\nNavarro, D.J. and Foxcroft, D.R. (2019). Learning statistics with Jamovi: a tutorial for psychology students and other beginners. (Version 0.70). DOI: 10.24384/hgc3-7p15\nRemler, D.K. and Van Ryzin, G.G., 2014. Research methods in practice: Strategies for description and causation. Sage Publications.\nSanchez, G., Marzban, E. (2020) All Models Are Wrong: Concepts of Statistical Learning. https://allmodelsarewrong.github.io\nSchneider, W. J. (2023). Psycheval: A psychological evaluation toolkit. https://github.com/wjschne/psycheval\nTimbers, T., Campbell, T., & Lee, M. (2022). Data science: A first introduction. Chapman and Hall/CRC. https://datasciencebook.ca/",
    "crumbs": [
      "References"
    ]
  }
]