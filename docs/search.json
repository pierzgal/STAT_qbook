[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Data Analysis: An Introduction (Wprowadzenie do analizy danych społecznych)",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nImportant\n\n\n\nThis is a preliminary draft of a Quarto class notes on social data analysis. Please do not cite or reproduce its contents, as it may contain errors!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "",
    "text": "1.1 What is Data Science?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#what-is-data-science",
    "href": "chapter1.html#what-is-data-science",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "",
    "text": "Important\n\n\n\nStatistics and Data Science are The Art and Science of Learning from Data.\n\n\n\nData science and statistics are powerful tools that help us understand complex phenomena across various social sciences, including political science, economics, and sociology. These complementary fields provide researchers and practitioners with the means to analyze trends, behaviors, and outcomes in society, offering insights that can shape policy and advance our understanding of human interaction.\nStatistics provides the mathematical foundation for analyzing societal trends and outcomes, offering methods for designing studies, summarizing data, and making inferences. Data science expands on this foundation by incorporating computational methods and domain expertise to handle larger datasets and perform more complex analyses.\nTogether, these disciplines allow us to collect and process large datasets, visualize complex information, uncover patterns in social interactions, evaluate policy impacts, and support evidence-based decision-making. Their applications are vast and varied, from studying voting patterns and analyzing economic indicators to researching social inequalities and examining human behavior.\nAs our world becomes increasingly data-driven, the importance of data science and statistics in social sciences continues to grow.\n\n\n\n\n\n\n\nNote\n\n\n\nIn social sciences, data science combines statistical methods, computational tools, and domain expertise to analyze complex social phenomena and human behavior.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-relationship-between-statistics-and-data-science",
    "href": "chapter1.html#the-relationship-between-statistics-and-data-science",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.2 The Relationship Between Statistics and Data Science",
    "text": "1.2 The Relationship Between Statistics and Data Science\nStatistics and data science are closely interrelated fields with significant overlap, especially in social sciences. Rather than strict divisions, it’s more accurate to view them as complementary approaches on a continuum:\n\nTraditional StatisticsModern Data Science\n\n\n\nRooted in mathematical theories and methods for data analysis\nEmphasizes statistical inference, hypothesis testing, and probability theory\nHistorically central to social sciences for analyzing surveys, experiments, and observational studies\n\n\n\n\nIntegrates statistical methods with computer science and domain expertise\nExpands focus to include machine learning, big data processing, and predictive modeling\nIn social sciences, often tackles large-scale digital data and complex behavioral datasets\n\n\n\n\nData science can be seen as an evolution and expansion of traditional statistics, incorporating new technologies and methodologies to handle larger and more complex social science datasets.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#essential-concepts-in-data-science-and-statistics",
    "href": "chapter1.html#essential-concepts-in-data-science-and-statistics",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.3 Essential Concepts in Data Science and Statistics",
    "text": "1.3 Essential Concepts in Data Science and Statistics\n\n1.3.1 Population, Sample and related concepts\n\n\n\n\n\n\nImportant\n\n\n\n\nData: Observations or measurements collected from a sample or population.\nPopulation: The entire set of individuals or items under study at a specific time.\n\nExample: All eligible voters in a country during a specific election year.\n\nSample: A subset of the population that is actually measured. A representative sample is a subset of a larger population that accurately reflects the characteristics of that population. The sample should mirror the population in terms of important traits like age, gender, socioeconomic status, etc. Often uses random sampling methods to avoid bias. Large enough to be statistically significant, but smaller than the full population.\n\nExample: 1500 randomly selected eligible voters surveyed in a pre-election poll.\n\n\n\n\n\n\n\n\n\n\nData Generating Process (DGP) and Superpopulation: Extending Traditional Concepts\n\n\n\nIn traditional statistics, we work with two key concepts:\n\nPopulation: The entire group we want to study.\nSample: A subset of the population that we actually observe and analyze.\n\nModern research often requires thinking beyond this dichotomous division. This is where the concepts of Data Generating Process (DGP) and superpopulation come in, deepening our understanding of data and their structure.\n\n1.3.2 Data Generating Process (DGP)\nFormal definition:\nDGP is a set of statistical and causal mechanisms responsible for producing the observed values of variables in a system, most often described using mathematical functions and probability distributions.\nIntuitive explanation:\nDGP can be viewed as a “black box” that transforms causes into effects. It’s the fundamental mechanism that produces data observed in the real world - both in our sample and in the entire population, as well as beyond it.\nExample:\nIn a study of voting behaviors, the DGP would include factors such as:\n\nDemographic characteristics of voters\nEconomic conditions\nPolitical events\nSocial media influence\nHistorical voting trends\n\nAll these factors shape voting behaviors regardless of whether a given voter was included in the study or not.\n\n\n1.3.3 Superpopulation\nFormal definition:\nA superpopulation is a hypothetical, infinite population from which the observed population can be viewed as a random sample. It represents all potential units and outcomes that could be generated by the same DGP.\nIntuitive explanation:\nSuperpopulation extends beyond both the sample and the observable population, encompassing all potential outcomes that could occur under similar conditions or processes - both now and in the future.\n\n\n1.3.4 Comparison of Approaches\n\n1.3.4.1 1. Traditional approach vs. superpopulation approach\n\nTraditional:\n\nPopulation: all registered voters in a region\nSample: 1000 surveyed voters\n\nSuperpopulation:\n\nObserved data: 1000 surveyed voters\nPopulation: all registered voters\nSuperpopulation: All possible voters and voting scenarios, including future elections and hypothetical political contexts\n\n\n\n\n1.3.4.2 2. When the sample equals the population\nIn studies of all 16 provinces of Poland:\n\nTraditional view: No distinction between sample and population\nSuperpopulation view: Treats these 16 provinces as a “sample” from a theoretical set of all possible interactions between provinces and politics\n\n\n\n\n1.3.5 Practical Application\nStudy of the impact of a new urban planning policy:\n\nTraditional approach:\n\nPopulation: All cities in the country\nSample: Cities included in the study\n\nSuperpopulation approach:\n\nObserved data: Cities in the study\nPopulation: All present cities\nSuperpopulation: All cities (existing or potential) where similar planning principles could be applied\n\n\nThe DGP in this case would be a complex set of factors that determine how urban planning policies affect the development of cities.\n\n\n1.3.6 Key Methodological Aspects\n\nScope and limitations:\nResearchers should clearly define what units or processes they are trying to understand, going beyond merely describing the sample and population.\nGeneralizability:\nWhen formulating conclusions about the superpopulation, one should clearly define the boundaries within which the findings apply.\nContext specificity:\nAlthough the concept of superpopulation allows for broader inference, it’s important to understand that the DGP may vary depending on the context.\n\n\n\n1.3.7 Example: Pizza Quality in New York\nPopulation:\nAll currently operating pizzerias in New York.\nSample:\n50 randomly selected pizzerias from different districts.\nSuperpopulation:\nAll possible pizzerias that could exist in New York:\n\nCurrently operating\nFuture (not yet opened)\nHistorical (closed)\nHypothetical (in alternative conditions)\n\nData Generating Process (DGP):\nFactors affecting pizza quality:\n\nIngredients and their quality\nSkills and experience of the chefs\nEquipment and infrastructure\nPreparation methods and recipes\nEnvironmental factors (e.g., water quality)\nCultural influences and traditions\nEconomic conditions (costs, rents)\n\nThe DGP is like a “recipe for pizza quality” that applies to all potential pizzerias in the superpopulation, not just currently existing establishments.\n\n\n\n\n\n\n\n\ngraph TD\n    A[Data Generating Process DGP]\n    B(Population)\n    C[Sample]\n    A --&gt;|Generates| B\n    B --&gt;|Sampled from| C\n    C -.-&gt;|Inference| B\n    C -.-&gt;|Inference| A\n    B -.-&gt;|Inference| A\n    \n    classDef dgp fill:#f9f,stroke:#333,stroke-width:4px;\n    classDef pop fill:#bbf,stroke:#333,stroke-width:2px;\n    classDef sam fill:#bfb,stroke:#333,stroke-width:2px;\n    \n    class A dgp;\n    class B pop;\n    class C sam;\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation of the DGP, Population, and Sample Diagram\n\n\n\nThis diagram illustrates the relationships between the Data Generating Process (DGP), population, and sample, including paths of inference:\n\nDirect relationships (solid arrows):\n\nThe DGP generates the population\nSamples are drawn from the population\n\nInference paths (dashed arrows):\n\nFrom Sample to Population: Traditional statistical inference\nFrom Sample to DGP: Inferring about the underlying process from sample data\nFrom Population to DGP: Inferring about the DGP using complete population data\n\n\n\n\n\n\n\nPopulation vs. sample. Retrieved from: https://allmodelsarewrong.github.io/mse.html\n\n\nData forms the foundation of statistical analysis. It can be:\n\nPrimary data: Collected firsthand for a specific purpose\nSecondary data: Obtained from existing sources\n\nExample: In a study of university students’ heights, the population is all university students in the country, while a sample might be 1000 randomly selected students.\n\n\n1.3.8 Variables and Constants\nVariables are characteristics that can take different values across a dataset. They can be:\n\nQuantitative (numeric):\n\nContinuous: Height, weight, temperature\nDiscrete: Number of children, count of errors in a program\n\nQualitative (categorical):\n\nNominal: Blood type, eye color\nOrdinal: Education level, customer satisfaction rating\n\n\nConstants are fixed values that remain unchanged throughout an analysis.\n\n1.3.8.1 Types of Data in Social Sciences\nSocial science research deals with various types of data:\n\nQuantitative Data: Numerical data (e.g., survey responses, economic indicators)\nQualitative Data: Non-numerical data (e.g., interview transcripts, open-ended survey responses)\nBig Data: Large-scale digital traces (e.g., social media posts, online behavior logs)\n\n\n\n\n1.3.9 Population Parameters and Estimands\nPopulation parameters are numerical characteristics of a population. Key points:\n\nThey describe the entire population, not just a sample.\nThey are usually denoted by Greek letters.\nIn most cases, they cannot be directly calculated because we can’t measure the entire population.\nThey are determined by the underlying Data Generating Process (DGP).\n\nCommon population parameters include:\n\nPopulation mean (\\mu): The average value of a variable in the population.\nPopulation variance (\\sigma^2): A measure of variability in the population.\nPopulation proportion (p): The proportion of individuals in the population with a certain characteristic.\n\nAn estimand is the target of estimation - the specific population parameter or function of parameters that we aim to estimate. It defines what we want to know about the population.\n\n\n\n\n\n\nExample: Height of University Students\n\n\n\nConsider the height of all university students in a country:\n\n\\mu (estimand): The true average height of all university students (population mean)\n\\sigma^2 (estimand): The true variance of heights in the population\n\nThese parameters are unknown estimands that we aim to estimate using sample data.\n\n\n\n\n1.3.10 Statistic(s) and Estimators\nA statistic (singular) or sample statistic is any quantity computed from values in a sample, which is considered for a statistical purpose.\nWhen a statistic is used for estimating an estimand (population parameter), it is called an estimator. Estimators are functions of sample data that provide approximate values for unknown population parameters.\nExamples of statistics/estimators:\n\nSample mean: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i (estimates \\mu)\nSample variance: s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2 (estimates \\sigma^2)\nSample proportion: \\hat{p} = \\frac{x}{n} (estimates p)\n\n\n\n1.3.11 Estimates\nAn estimate is the specific value obtained by applying an estimator to a particular sample. It is a point value that approximates the true estimand (population parameter).\nExample: If we calculate a sample mean height of 68 inches from our data, then 68 inches is our estimate of the estimand \\mu (population mean height).\n\n\n1.3.12 Statistical Models\n\n\n\n\n\n\nNote\n\n\n\nA model in science is a simplified representation of a complex system or phenomenon. It’s designed to help us understand, explain, and make predictions about the real world. Models can take various forms, including mathematical equations, computer simulations, or conceptual frameworks. They allow scientists to focus on key aspects of a system while ignoring less relevant details, making complex problems more manageable and easier to study.\n\n\nStatistical models represent relationships between variables and help in making predictions or inferences about estimands (population parameters).\nExample: A linear regression model y = \\beta_0 + \\beta_1x + \\epsilon describes the relationship between an independent variable x and a dependent variable y, where:\n\ny is the dependent variable (e.g. quantity demanded)\nx is the independent variable (e.g. price, income level of the consumer)\n\\beta_0 and \\beta_1 are parameters, estimands to be estimated\n\\epsilon is the error term, representing unexplained variation\n\nI’ll help you create a callout note about causal inference and counterfactuals for Quarto.\n\n\n\n\n\n\nCausal Inference and Counterfactuals\n\n\n\nIn social sciences, we often want to understand what would have happened if we had done something differently - this hypothetical scenario is called a counterfactual. For instance:\n\nWhat would a person’s income be if they had attended college vs. if they hadn’t?\nHow would voter turnout change if voting was mandatory?\n\nSince we can’t observe both scenarios simultaneously, statistical models help us estimate these counterfactuals by: 1. Controlling for confounding variables 2. Comparing similar groups that differ only in the treatment 3. Using techniques like propensity score matching or instrumental variables\n\n\n\nFundamental problem of causal inference: We can think of causal inference as a PREDICTION problem. How could we predict the counterfactual given that we never observe it?\n\n\nRemember: Correlation ≠ Causation, but careful research design and statistical methods can help us make causal claims.\n\n\n\nConfounding bias and spurious correlation (https://www.bradyneal.com/causal-inference-course) drinking the night before is a common cause of sleeping with shoes on and waking up with a headache :-)\n\n\n\n\n\nReverse causality: https://ff13.fastforwardlabs.com/\n\n\n\n\nThis creates a note-type callout in Quarto that explains the concept succinctly while highlighting key points about counterfactuals and their estimation.\n\n\n1.3.13 Inference\nStatistical inference is the process of drawing conclusions about estimands (population parameters) based on sample data. It involves two main types:\n\nEstimation: Using sample statistics (estimators) to estimate estimands (population parameters)\nHypothesis testing: Making decisions about estimands based on sample evidence\n\n\n\n\n\n\n\nEstimation and Hypothesis Testing\n\n\n\n\nEstimation\n\nEstimation is about determining the likely value of a population parameter based on sample data. In the context of a binomial distribution, we might be interested in estimating the probability of success (p) for a certain event.\nExample: Coin Flipping\nLet’s say we’re flipping a coin 100 times and want to estimate the probability of getting heads.\n\nWe flip the coin 100 times and observe 55 heads.\nOur point estimate for p (probability of heads) would be 55/100 = 0.55\nWe might also calculate a confidence interval, e.g., a 95% confidence interval might be (0.45, 0.65).\n\nThe confidence interval tells us a range where we think the true probability might lie. In plain English, this means: “We’re 95% confident that the true probability of getting heads is between 45% and 65%.”\nThe goal here is to provide our best guess of the true probability of heads, along with a range of plausible values.\nImportant Concepts in Estimation:\n\nBias\n\nBias refers to the tendency of an estimator to systematically overestimate or underestimate the true value of a population parameter (estimand).\n\nAn unbiased estimator is one whose average value (when estimation is repeated multiple times) equals the true value of the parameter.\nBias can be understood as the difference between the average value of the estimator and the true value of the parameter.\n\n\nEfficiency\n\nEfficiency refers to the precision of an estimator. A more efficient estimator produces results closer to the true parameter value, i.e., it has less dispersion in its results.\n\nIt is most often measured by the variance of the estimator (lower variance means higher efficiency)\nFor unbiased estimators, efficiency is often compared using Mean Squared Error (MSE)\n\n\nHypothesis Testing\n\nHypothesis testing, on the other hand, is about making a decision between two competing claims about a population parameter. We typically have a null hypothesis (H0) and an alternative hypothesis (H1).\nExample: Is the Coin Fair?\nUsing the same coin-flipping scenario, let’s say we want to test if the coin is fair (p = 0.5) or biased towards heads (p &gt; 0.5).\n\nNull hypothesis (H0): p = 0.5 (the coin is fair)\nAlternative hypothesis (H1): p &gt; 0.5 (the coin is biased towards heads)\nWe observe 55 heads out of 100 flips\n\nIntroducing p-values and “Probabilistic Proof by Contradiction”\nNow, let’s dive into the concept of p-values and how hypothesis testing works as a kind of “probabilistic proof by contradiction”:\n\nWe start by assuming the null hypothesis (H0) is true. In this case, we assume the coin is fair.\nWe then ask: “If the coin were truly fair, how likely would it be to observe 55 or more heads out of 100 flips?”\nThis probability is called the p-value. It’s the probability of observing our data (or more extreme data) assuming the null hypothesis is true.\nIf this probability (the p-value) is very small, we have a contradiction: we’ve observed something that should be very rare if our assumption (H0) were true.\nWe typically set a threshold called the significance level (often 0.05 or 5%) for what we consider “very small.”\nIf the p-value is less than our chosen significance level, we reject H0. We conclude that our observation is too unlikely under H0, so we favor the alternative hypothesis instead.\nIf the p-value is greater than our significance level, we fail to reject H0. We don’t have enough evidence to conclude the coin is biased.\n\nThis process is like a “probabilistic proof by contradiction” because:\n\nWe start by assuming H0 (like assuming the opposite of what we want to prove in a proof by contradiction).\nWe see if this assumption leads to a very unlikely situation (our observed data).\nIf it does, we reject the assumption (H0) and favor the alternative.\n\nThe p-value quantifies exactly how unlikely our observation is under H0. A very small p-value (like 0.01) means: “If H0 were true, we’d only expect to see data this extreme about 1% of the time.”\nHypothesis testing and estimation are related but distinct statistical procedures; hypothesis testing can be used to make inferences about estimates and can complement estimation in several ways, e.g.:\n\nTesting Point Estimates: Hypothesis testing can be used to evaluate whether a point estimate is significantly different from a hypothesized value. For example, if we estimate that a coin has a 0.55 probability of landing heads, we could use a hypothesis test to determine if this is significantly different from 0.5 (a fair coin).\nParameter Significance: In multivariate models, hypothesis tests (like t-tests in regression) can help determine which estimated parameters are significantly different from zero, providing insight into which variables are important in the model.\n\n\n\n\n\n1.3.14 Relationships Between Concepts\n\nThe Data Generating Process (DGP) determines the actual values of population parameters (estimands).\nEstimands are estimated using statistics calculated from the sample (estimators).\nThe quality of estimators is assessed based on properties such as bias and efficiency in estimating the estimand.\nStatistical models use estimated parameters to describe relationships between variables in the population.\nStatistical inference involves drawing conclusions about estimands based on sample data, utilizing the properties of estimators.\n\n\n\n\n\n\n\nExample: Studying Voting Behavior\n\n\n\n\nPopulation: All eligible voters in a country\nEstimand: p = true proportion of voters supporting a given candidate\nSample: 1000 randomly selected eligible voters\nEstimator: \\hat{p} = proportion of voters in the sample supporting the candidate\nEstimate: Specific value of \\hat{p} calculated from the sample (e.g., 0.52)\nDGP: Complex interaction of factors influencing voting decisions, such as political beliefs, economic conditions, media exposure, and social networks.\n\nUnderstanding the DGP helps researchers interpret why the estimand p has a certain value and how it might change over time. For example, a sudden change in the economy might affect voters’ preferences, thereby changing the value of p.\nBias and efficiency in the context of the example:\n\nIf \\hat{p} is an unbiased estimator, it means that when the survey is repeated multiple times with different samples, the average value of \\hat{p} will be close to the true value of p.\nThe efficiency of \\hat{p} determines how dispersed the results of individual surveys are around this average. The less dispersion, the more efficient the estimator.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#core-components-of-data-science",
    "href": "chapter1.html#core-components-of-data-science",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.4 Core Components of Data Science",
    "text": "1.4 Core Components of Data Science\n\nData CollectionData ProcessingExploratory Data Analysis (EDA)Statistical InferenceMachine LearningData Visualization and CommunicationReproducibility and Open Science\n\n\n\nExperimental methods: Controlled studies where researchers manipulate variables to observe effects\nObservational studies: Gathering data by watching and recording without interfering\nSurveys and interviews: Collecting information directly from people through questions\nDigital data collection: Gathering data from online sources, sensors, or computer systems\nEthical considerations: Ensuring research respects participants’ rights and well-being\n\n\n\n\nData cleaning: Removing errors and inconsistencies from raw data\nHandling missing values: Addressing gaps in the dataset that could affect analysis\nData transformation: Converting data into formats suitable for analysis, like changing text to numbers\n\n\n\n\nDescriptive statistics: Summarizing data with measures like mean, median, and standard deviation\nData visualization: Creating graphs and charts to visually represent data patterns\nPattern identification: Discovering trends or relationships in the data\n\n\n\n\nHypothesis testing: Using data to evaluate claims about populations\nRegression analysis: Examining relationships between variables and making predictions\nCausal inference: Determining if one variable directly influences another\n\n\n\n\nSupervised learning: Training models to predict outcomes using data with known answers\nUnsupervised learning: Finding hidden patterns in data without predefined categories\nNatural Language Processing (NLP): Teaching computers to understand and analyze human language\n\n\n\n\nEffective visualizations: Creating clear, informative graphics to represent complex data\nScience communication: Explaining findings to different audiences, from experts to the public\nScientific writing: Preparing research papers and reports to share results\n\n\n\n\nVersion control: Tracking changes in data and code throughout the research process\nOpen data practices: Sharing research data and methods for verification and further study\nReproducible workflows: Documenting research steps so others can repeat the study",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#tools-for-data-science-in-social-sciences",
    "href": "chapter1.html#tools-for-data-science-in-social-sciences",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.5 Tools for Data Science in Social Sciences",
    "text": "1.5 Tools for Data Science in Social Sciences\nIn this course, we’ll use R for our data analysis, as it’s widely used in social science research.\n\n1.5.1 R for Social Science Data Analysis\nR offers powerful capabilities for social science research, from data manipulation to advanced statistical modeling.\n\n\nKliknij, aby pokazać/ukryć kod R\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nKliknij, aby pokazać/ukryć kod R\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate example data with a Simpson's Paradox\nn &lt;- 1000\ndata &lt;- tibble(\n  age_group = sample(c(\"Young\", \"Middle\", \"Old\"), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),\n  education_years = case_when(\n    age_group == \"Young\" ~ rnorm(n, mean = 10, sd = 1),\n    age_group == \"Middle\" ~ rnorm(n, mean = 13, sd = 1),\n    age_group == \"Old\" ~ rnorm(n, mean = 16, sd = 1)\n  ),\n  income = case_when(\n    age_group == \"Young\" ~ 70000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Middle\" ~ 50000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Old\" ~ 30000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000)\n  )\n)\n\n# Basic data summary\nsummary(data)\n\n\n  age_group         education_years      income     \n Length:1000        Min.   : 6.628   Min.   :34068  \n Class :character   1st Qu.:10.913   1st Qu.:51508  \n Mode  :character   Median :13.004   Median :63376  \n                    Mean   :12.986   Mean   :63307  \n                    3rd Qu.:14.934   3rd Qu.:75023  \n                    Max.   :18.861   Max.   :96620  \n\n\nKliknij, aby pokazać/ukryć kod R\n# Correlation analysis\ncor(data %&gt;% select(education_years, income))\n\n\n                education_years     income\neducation_years       1.0000000 -0.8152477\nincome               -0.8152477  1.0000000\n\n\nKliknij, aby pokazać/ukryć kod R\n# Overall trend (Simpson's Paradox)\noverall_plot &lt;- ggplot(data, aes(x = education_years, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Overall Relationship between Education and Income\",\n       subtitle = \"Simpson's Paradox: Appears negative\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Trend by age group (Resolving Simpson's Paradox)\ngrouped_plot &lt;- ggplot(data, aes(x = education_years, y = income, color = age_group)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Relationship between Education and Income by Age Group\",\n       subtitle = \"Resolving Simpson's Paradox: Positive relationship within groups\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Statistical analysis\nmodel_overall &lt;- lm(income ~ education_years, data = data)\nmodel_by_age &lt;- lm(income ~ education_years + age_group, data = data)\n\n# Print results\nprint(overall_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(grouped_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_overall))\n\n\n\nCall:\nlm(formula = income ~ education_years, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24451  -5439    235   5262  34328 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     121814.7     1339.5   90.94   &lt;2e-16 ***\neducation_years  -4505.4      101.3  -44.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7976 on 998 degrees of freedom\nMultiple R-squared:  0.6646,    Adjusted R-squared:  0.6643 \nF-statistic:  1978 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_by_age))\n\n\n\nCall:\nlm(formula = income ~ education_years + age_group, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-14827  -3369    118   3356  16388 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      48270.8     2028.4  23.797  &lt; 2e-16 ***\neducation_years   1135.5      154.6   7.345 4.26e-13 ***\nage_groupOld    -19942.8      593.2 -33.619  &lt; 2e-16 ***\nage_groupYoung   20461.1      600.7  34.064  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4950 on 996 degrees of freedom\nMultiple R-squared:  0.8711,    Adjusted R-squared:  0.8707 \nF-statistic:  2244 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\n# Calculate and print correlations\noverall_cor &lt;- cor(data$education_years, data$income)\ngroup_cors &lt;- data %&gt;%\n  group_by(age_group) %&gt;%\n  summarize(correlation = cor(education_years, income))\n\nprint(\"Overall correlation:\")\n\n\n[1] \"Overall correlation:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(overall_cor)\n\n\n[1] -0.8152477\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(\"Correlations by age group:\")\n\n\n[1] \"Correlations by age group:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(group_cors)\n\n\n# A tibble: 3 × 2\n  age_group correlation\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Middle          0.185\n2 Old             0.291\n3 Young           0.223\n\n\nThis example demonstrates basic data manipulation, summary statistics, and visualization using R, which are common tasks in social science research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#causal-inference-vs.-observational-studies",
    "href": "chapter1.html#causal-inference-vs.-observational-studies",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.6 Causal Inference vs. Observational Studies",
    "text": "1.6 Causal Inference vs. Observational Studies\nIn social sciences and beyond, understanding the relationship between variables is crucial. Two key approaches to this are causal inference and observational studies, each with its own strengths and limitations.\n\nCausal InferenceObservational StudiesKey Distinction: Correlation vs. Causation\n\n\n\nAims to establish cause-and-effect relationships\nOften involves experimental designs or advanced statistical techniques\nSeeks to answer “What if?” questions and determine the impact of interventions\nExamples: Randomized controlled trials, quasi-experimental designs, instrumental variables\n\n\n\n\nExamine relationships between variables without direct intervention\nRely on data collected from natural settings or existing datasets\nCan identify correlations and patterns but struggle to establish causation\nExamples: Cohort studies, case-control studies, cross-sectional surveys\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember: Correlation Does Not Imply Causation\n\n\n\nA fundamental principle in research is that correlation between two variables does not necessarily imply a causal relationship. This concept is crucial when interpreting results from observational studies.\n\nCorrelation: Measures the strength and direction of a relationship between variables\nCausation: Indicates that changes in one variable directly cause changes in another\n\nWhile strong correlations can suggest potential causal links, additional evidence and rigorous methods are required to establish causality.\n\n\n\nChallenges in Establishing CausalityMethods to Strengthen Causal ClaimsImportance in Social Sciences\n\n\n\nConfounding variables: Unmeasured factors that affect both the presumed cause and effect\nReverse causality: The presumed effect might actually be causing the presumed cause\nSelection bias: Non-random selection of subjects into study groups\n\n\n\n\nRandomized controlled trials (when ethical and feasible)\nNatural experiments or quasi-experimental designs\nPropensity score matching\nDifference-in-differences analysis\nInstrumental variable approaches\nDirected acyclic graphs (DAGs) for visualizing causal relationships\n\n\n\nUnderstanding the distinction between causal inference and observational studies is crucial in social sciences, where ethical considerations often limit experimental manipulation. Researchers must carefully design studies and interpret results to avoid misleading conclusions about causality.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#models-in-science-from-deterministic-to-stochastic",
    "href": "chapter1.html#models-in-science-from-deterministic-to-stochastic",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.7 Models in Science: From Deterministic to Stochastic (*)",
    "text": "1.7 Models in Science: From Deterministic to Stochastic (*)\nModels are essential tools in scientific research, helping scientists to represent, understand, and predict complex phenomena. This section explores the main types of models used in science, along with examples of their applications. It’s important to note that these categories often overlap, and many scientific models incorporate multiple aspects.\n\n1.7.1 Mathematical Models\nMathematical models use equations and mathematical concepts to describe and analyze systems or phenomena. They can be further divided into several subcategories, though it’s important to note that some complex models may incorporate elements from multiple categories:\n\n1.7.1.1 a. Deterministic Models\nDeterministic models provide precise predictions based on a set of variables, without incorporating randomness at the macroscopic level.\nExample: Newton’s laws of motion, which can precisely predict the motion of objects under known forces in classical mechanics.\n\n\n1.7.1.2 b. Stochastic Models\nStochastic models incorporate randomness and probability. However, it’s crucial to distinguish between two fundamentally different types of stochastic models:\n\n1.7.1.2.1 i. Classical Stochastic Models\nThese models deal with randomness arising from incomplete information or complex interactions in classical systems. The underlying system is deterministic, but practical limitations in measurement or computation lead to the use of probabilistic descriptions.\nExample: Regression models in statistics, where the randomness represents unexplained variation or measurement error:\ny = β_0 + β_1x + ε\nWhere:\n\ny is the dependent variable (e.g. quantity demanded)\nx is the independent variable (e.g. price, income level of the consumer)\nβ_0 and β_1 are parameters\nε is the error term, representing unexplained variation\n\n\n\n1.7.1.2.2 ii. Quantum Stochastic Models\nThese models deal with the fundamental, irreducible randomness inherent in quantum mechanical systems. This randomness is not due to lack of information, but is a core feature of quantum reality.\nExample: The Standard Model in particle physics, which describes particle interactions using quantum field theory. For instance, the decay of a particle is inherently probabilistic:\nP(t) = e^{-t/τ}\nWhere:\n\nP(t) is the probability that the particle has not decayed after time t\nτ is the mean lifetime of the particle\n\n\n\n\n1.7.1.3 c. Computer Simulation Models\nComputer simulations use algorithms and computational methods based on mathematical models to simulate complex systems and predict their behavior over time. These can be deterministic or stochastic.\nExample: Climate models that simulate the Earth’s climate system, incorporating factors such as atmospheric composition, ocean currents, and solar radiation to project future climate scenarios.\n\n\n\n1.7.2 Conceptual Models\nConceptual models are abstract representations of systems or processes, often using diagrams or flowcharts to illustrate relationships between components.\nExample: The water cycle model in Earth sciences, which illustrates the continuous movement of water within the Earth and atmosphere through processes such as evaporation, precipitation, and runoff.\n\n\n1.7.3 Physical Models\nPhysical models are tangible representations of objects or systems, often scaled down or simplified versions of the real thing.\nExample: Wind tunnel models in aerodynamics research, used to study the effects of air moving past solid objects and optimize designs for aircraft, vehicles, or buildings.\n\n\n1.7.4 Theoretical Models\nTheoretical models are abstract frameworks based on fundamental principles and hypotheses, often used to explain observed phenomena or predict new ones. These models frequently employ mathematical formulations and can be deterministic or stochastic in nature.\nExample: The theory of evolution by natural selection, which provides a framework for understanding the diversity and adaptation of life forms over time.\n\n\n1.7.5 Conclusion\nThese various forms of models play crucial roles in scientific research, each offering unique advantages for understanding and predicting natural phenomena. Scientists often use multiple types of models in conjunction to gain comprehensive insights into complex systems and processes.\nIt’s important to recognize that these categories are not mutually exclusive and often overlap:\n\nMathematical models form the foundation for many other types of models, including computer simulations and some theoretical models.\nComputer simulation models are essentially mathematical models implemented through computational methods, and can be either deterministic or stochastic.\nTheoretical models often employ mathematical formulations and may be implemented as computer simulations.\nPhysical models may be designed based on mathematical models and can be used to validate computer simulations.\n\nThe choice of model type often depends on the specific research question, the nature of the system being studied, the available data, and the computational resources at hand. As science progresses, the boundaries between these model types continue to blur, leading to increasingly sophisticated and interdisciplinary approaches to modeling complex phenomena.\nIt’s crucial to distinguish between different types of stochastic models. Classical stochastic models, such as those used in regression analysis, deal with randomness arising from incomplete information or complex interactions in otherwise deterministic systems. In contrast, quantum stochastic models, like those in particle physics, deal with fundamental, irreducible randomness inherent in quantum mechanical systems. This distinction reflects the profound differences between classical and quantum paradigms in physics and highlights the diverse ways in which probability is used in scientific modeling.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#understanding-spurious-correlations-confounders-and-colliders",
    "href": "chapter1.html#understanding-spurious-correlations-confounders-and-colliders",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.8 Understanding Spurious Correlations, Confounders, and Colliders (*)",
    "text": "1.8 Understanding Spurious Correlations, Confounders, and Colliders (*)\nIn this tutorial, we’ll explore three important concepts in statistical analysis: spurious correlations, confounders, and colliders. Understanding these concepts is crucial for avoiding misinterpretation of data and drawing incorrect conclusions from statistical analyses.\nLet’s start by loading the necessary libraries:\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nset.seed(123) # for reproducibility\n\n\n1.8.1 Spurious Correlations\nSpurious correlations are relationships between variables that appear to be causal but are actually coincidental or caused by an unseen third factor.\n\n\n1.8.2 Example: Ice Cream Sales and Drowning Incidents\nLet’s create a dataset that shows a spurious correlation between ice cream sales and drowning incidents:\n\nn &lt;- 100\nspurious_data &lt;- tibble(\n  temperature = rnorm(n, mean = 25, sd = 5),\n  ice_cream_sales = 100 + 5 * temperature + rnorm(n, sd = 10),\n  drowning_incidents = 1 + 0.5 * temperature + rnorm(n, sd = 2)\n)\n\nggplot(spurious_data, aes(x = ice_cream_sales, y = drowning_incidents)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Spurious Correlation: Ice Cream Sales vs. Drowning Incidents\",\n       x = \"Ice Cream Sales\", y = \"Drowning Incidents\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis plot shows a positive correlation between ice cream sales and drowning incidents. However, this relationship is spurious. The real cause for both is the temperature:\n\nggplot(spurious_data, aes(x = temperature)) +\n  geom_point(aes(y = ice_cream_sales), color = \"blue\") +\n  geom_point(aes(y = drowning_incidents * 10), color = \"red\") +\n  geom_smooth(aes(y = ice_cream_sales), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(aes(y = drowning_incidents * 10), method = \"lm\", se = FALSE, color = \"red\") +\n  scale_y_continuous(\n    name = \"Ice Cream Sales\",\n    sec.axis = sec_axis(~./10, name = \"Drowning Incidents\")\n  ) +\n  labs(title = \"Temperature as the Common Cause\",\n       x = \"Temperature\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n1.8.3 Confounders\nA confounder is a variable that influences both the dependent variable and independent variable, causing a spurious association.\n\n\n1.8.4 Example: Education, Income, and Age\n\nlibrary(tidyverse)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nn &lt;- 1000\nconfounder_data &lt;- tibble(\n  age = runif(n, 25, 65),\n  education = round(10 + 0.1 * age + rnorm(n, sd = 2)),\n  income = 20000 + 1000 * education + 500 * age + rnorm(n, sd = 5000)\n)\n\n# Without controlling for age\nmodel_naive &lt;- lm(income ~ education, data = confounder_data)\n# Controlling for age\nmodel_adjusted &lt;- lm(income ~ education + age, data = confounder_data)\n\n# Create age groups for visualization\nconfounder_data &lt;- confounder_data %&gt;%\n  mutate(age_group = cut(age, breaks = 3, labels = c(\"Young\", \"Middle\", \"Old\")))\n\n# Visualize\nggplot(confounder_data, aes(x = education, y = income)) +\n  geom_point(aes(color = age), alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1.2) +\n  geom_smooth(aes(group = age_group, color = as.numeric(age_group)), \n              method = \"lm\", se = FALSE, linewidth = 1) +\n  scale_color_viridis_c(name = \"Age\", \n                        breaks = c(30, 45, 60), \n                        labels = c(\"Young\", \"Middle\", \"Old\")) +\n  labs(title = \"Education vs Income, Confounded by Age\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCompare the coefficients:\n\nsummary(model_naive)$coefficients[\"education\", \"Estimate\"]\n\n[1] 2328.718\n\nsummary(model_adjusted)$coefficients[\"education\", \"Estimate\"]\n\n[1] 1101.783\n\n\nThe effect of education on income is overestimated when we don’t control for age.\n\n\n1.8.5 Colliders\nA collider is a variable that is influenced by both the independent variable and the dependent variable. Controlling for a collider can introduce a spurious correlation.\n\n\n1.8.6 Example: Job Satisfaction, Salary, and Work-Life Balance\nLet’s create a dataset where work-life balance is a collider between job satisfaction and salary:\n\nn &lt;- 1000\ncollider_data &lt;- tibble(\n  job_satisfaction = rnorm(n),\n  salary = rnorm(n),\n  work_life_balance = -0.5 * job_satisfaction - 0.5 * salary + rnorm(n, sd = 0.5)\n)\n\n# Without controlling for work-life balance\nmodel_correct &lt;- lm(salary ~ job_satisfaction, data = collider_data)\n\n# Incorrectly controlling for work-life balance\nmodel_collider &lt;- lm(salary ~ job_satisfaction + work_life_balance, data = collider_data)\n\n# Visualize\nggplot(collider_data, aes(x = job_satisfaction, y = salary, color = work_life_balance)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_color_viridis_c() +\n  labs(title = \"Job Satisfaction vs Salary, Work-Life Balance as Collider\",\n       x = \"Job Satisfaction\", y = \"Salary\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCompare the coefficients:\n\nsummary(model_correct)$coefficients[\"job_satisfaction\", \"Estimate\"]\n\n[1] 0.02063487\n\nsummary(model_collider)$coefficients[\"job_satisfaction\", \"Estimate\"]\n\n[1] -0.4794016\n\n\nControlling for the collider (work-life balance) introduces a spurious correlation between job satisfaction and salary.\n\n\n1.8.7 Conclusion\nUnderstanding spurious correlations, confounders, and colliders is crucial for proper statistical analysis and causal inference. Always consider the underlying causal structure of your data and be cautious about which variables you control for in your analyses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#further-reading",
    "href": "chapter1.html#further-reading",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.9 Further Reading",
    "text": "1.9 Further Reading\n\nPearl, J. (2009). Causality: Models, Reasoning, and Inference. Cambridge University Press.\nHernán, M. A., & Robins, J. M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#ethical-considerations-in-social-science-data-analysis",
    "href": "chapter1.html#ethical-considerations-in-social-science-data-analysis",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.10 Ethical Considerations in Social Science Data Analysis",
    "text": "1.10 Ethical Considerations in Social Science Data Analysis\nEthics play a crucial role in social science research:\n\nPrivacy and Consent: Ensuring participant privacy and informed consent\nData Protection: Securely storing and managing sensitive personal data\nBias and Representation: Addressing sampling bias and ensuring diverse representation\nTransparency: Clearly communicating research methods and limitations\nSocial Impact: Considering the potential societal implications of research findings\n\n\n\n\n\n\n\nImportant\n\n\n\nSocial scientists must carefully consider the ethical implications of their data collection, analysis, and dissemination practices.\n\n\n\n1.10.1 Key Takeaways\n\nData science in social sciences builds upon traditional statistical methods, incorporating new technologies to analyze complex social phenomena.\nUnderstanding concepts like population, sample, and data generating processes is crucial for valid social science research.\nThe data science process in social research involves multiple steps from ethical data collection to the communication of insights.\nR is a powerful tool for social science data analysis, offering a wide range of capabilities.\nEthical considerations should be at the forefront of any social science data project.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-a-classical-vs-quantum-randomness-understanding-the-fundamental-differences",
    "href": "chapter1.html#appendix-a-classical-vs-quantum-randomness-understanding-the-fundamental-differences",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.11 Appendix A: Classical vs Quantum Randomness: Understanding the Fundamental Differences",
    "text": "1.11 Appendix A: Classical vs Quantum Randomness: Understanding the Fundamental Differences\nTo understand how the randomness in quantum mechanics differs from the randomness represented by the error term in regression models, we need to examine their origins, nature, and implications.\n\n1.11.1 Origin of Randomness\n\n1.11.1.1 Classical Randomness (Regression Models)\n\nSource: Incomplete information or complex interactions in an otherwise deterministic system.\nNature: Epistemic uncertainty (due to lack of knowledge).\nExample: In a regression model, y = β_0 + β_1x + ε, the error term ε represents unexplained variation.\n\n\n\n1.11.1.2 Quantum Randomness\n\nSource: Fundamental property of quantum systems.\nNature: Ontic uncertainty (inherent to the system, not due to lack of knowledge).\nExample: The exact time of decay of a radioactive atom cannot be predicted, only its probability.\n\n\n\n\n1.11.2 Philosophical Implications\n\n1.11.2.1 Classical Randomness\n\nDeterminism: Underlying reality is deterministic; randomness reflects our ignorance.\nHidden Variables: In principle, if we had complete information, we could predict outcomes precisely.\n\n\n\n1.11.2.2 Quantum Randomness\n\nIndeterminism: Randomness is a fundamental feature of reality, not just our description of it.\nNo Hidden Variables: Even with complete information about a quantum system, some outcomes remain unpredictable (as suggested by Bell’s theorem).\n\n\n\n\n1.11.3 Mathematical Treatment\n\n1.11.3.1 Classical Randomness\n\nProbability Theory: Based on classical probability theory.\nDistribution: Often assumed to follow known distributions (e.g., normal distribution in many regression models).\nCentral Limit Theorem: Applies to large samples of random variables.\n\n\n\n1.11.3.2 Quantum Randomness\n\nQuantum Probability: Based on the mathematical framework of quantum mechanics.\nWave Function: Describes the quantum state and its evolution.\nBorn Rule: Gives probabilities of measurement outcomes from the wave function.\n\n\n\n\n1.11.4 Predictability and Control\n\n1.11.4.1 Classical Randomness\n\nReducible: In principle, can be reduced by gathering more data or improving measurement precision.\nControllable: Systematic errors can be identified and corrected.\n\n\n\n1.11.4.2 Quantum Randomness\n\nIrreducible: Cannot be eliminated even with perfect measurements.\nFundamentally Uncontrollable: The act of measurement itself affects the system (measurement problem).\n\n\n\n\n1.11.5 Practical Implications\n\n1.11.5.1 Classical Randomness\n\nError Reduction: Focus on improving measurement techniques and data collection.\nModel Refinement: Aim to explain more variance and reduce the error term.\n\n\n\n1.11.5.2 Quantum Randomness\n\nInherent Limitation: Accept fundamental limits on predictability.\nProbabilistic Predictions: Focus on accurate probability distributions rather than exact outcomes.\n\n\n\n\n1.11.6 Examples to Understand the Difference\n\n1.11.6.1 Classical Randomness Example\nImagine flipping a coin. Classical physics says the outcome is determined by initial conditions (force applied, air resistance, etc.). The “randomness” comes from our inability to precisely measure and account for all these factors.\n\n\n1.11.6.2 Quantum Randomness Example\nIn the double-slit experiment, individual particles show interference patterns as if they went through both slits simultaneously. The exact path of any individual particle is fundamentally undetermined until measured, and this indeterminacy cannot be resolved by more precise measurements.\n\n\n\n1.11.7 Conclusion\nWhile both types of randomness lead to probabilistic predictions, their fundamental natures are quite different:\n\nClassical randomness in regression models is a reflection of our incomplete knowledge or measurement limitations in an otherwise deterministic system.\nQuantum randomness is a fundamental property of quantum systems, representing an inherent indeterminacy in nature that persists even with perfect knowledge and measurement.\n\nUnderstanding these differences is crucial for correctly interpreting and applying statistical models in different scientific contexts, from social sciences using regression analysis to quantum physics experiments.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-b-large-language-models---understanding-their-stochastic-nature",
    "href": "chapter1.html#appendix-b-large-language-models---understanding-their-stochastic-nature",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.12 Appendix B: Large Language Models - Understanding Their Stochastic Nature",
    "text": "1.12 Appendix B: Large Language Models - Understanding Their Stochastic Nature\nLarge Language Models (LLMs) like GPT-3, BERT, and Claude have revolutionized natural language processing but can make puzzling mistakes, especially in mathematical tasks. This appendix explains LLMs’ functioning, stochastic nature, and compares them to classical statistical models.\n\n1.12.1 LLM Basics and Stochastic Nature\nLLMs are trained on vast text data to predict the probability distribution of the next token in a sequence. They use transformer architectures for processing and generating text. Key aspects of their stochastic nature include:\n\nProbabilistic token selection: LLMs choose each word based on calculated probabilities, not fixed rules.\nTemperature-controlled randomness: A “temperature” parameter adjusts the randomness of selections, balancing creativity and coherence.\nNon-deterministic outputs: The same input can produce different outputs in separate runs.\nContextual ambiguity: LLMs interpret context probabilistically, sometimes leading to misunderstandings.\n\n\n\n1.12.2 Comparison to Classical Statistical Models\nTo understand LLMs better, let’s compare them to Ordinary Least Squares (OLS) regression:\n\n\n\n\n\n\n\n\nAspect\nOLS Regression\nLarge Language Models\n\n\n\n\nBasic Function\nPredicts continuous outcomes based on input variables\nPredicts probability distribution of next token based on previous tokens\n\n\nInput-Output\nContinuous variables, linear relationships\nDiscrete tokens, non-linear relationships\n\n\nPrediction Type\nPoint predictions with confidence intervals\nProbability distributions over possible tokens\n\n\nModel Complexity\nFew parameters\nBillions of parameters\n\n\nInterpretability\nClear coefficient interpretations\nLargely opaque internal workings\n\n\nNoise Handling\nAssumes random noise in outcome variable\nDeals with natural language variability\n\n\nExtrapolation\nLess reliable outside training range\nLess reliable on unfamiliar topics\n\n\n\nBoth models aim to learn input-output mappings based on training data patterns.\n\n\n1.12.3 Implications for Mathematical Tasks\nLLMs’ stochastic nature affects mathematical operations:\n\nVariable outputs for repeated calculations: Each attempt might yield a different result due to probabilistic token selection.\nConfidence doesn’t guarantee correctness: High model confidence can occur even for incorrect answers.\nApproximation rather than exact computation: LLMs pattern-match rather than perform precise calculations.\n\nLimitations in mathematical tasks stem from:\n\nTraining objective mismatch: LLMs are trained for language prediction, not mathematical accuracy.\nLack of explicit mathematical reasoning: They don’t have built-in mathematical rules or operations.\nAbsence of working memory: LLMs can’t reliably store and manipulate intermediate results.\nLimited context window: They may lose track of relevant information in long problems.\nTraining data limitations: Underrepresentation of certain math concepts can lead to poor performance.\nLack of consistency checks: LLMs don’t verify the logical consistency of their outputs.\n\n\n\n1.12.4 Best Practices and Conclusion\nWhen using LLMs for mathematical tasks:\n\nFocus on conceptual explanations, not precise calculations: LLMs excel at explaining concepts but may falter on exact computations.\nVerify results with dedicated software: Always double-check LLM calculations with proper math tools.\nBreak down complex problems: Splitting tasks into smaller steps can improve LLM performance.\nBe aware of rephrasing effects: Different phrasings of the same problem may yield different results.\nUse as assistive tools, not replacements for expertise: LLMs should complement, not substitute, mathematical expertise.\n\nUnderstanding LLMs’ probabilistic nature helps leverage their strengths in language tasks while recognizing their limitations in domains requiring deterministic precision, like mathematics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-c-deterministic-and-stochastic-models",
    "href": "chapter1.html#appendix-c-deterministic-and-stochastic-models",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.13 Appendix C: Deterministic and Stochastic Models (*)",
    "text": "1.13 Appendix C: Deterministic and Stochastic Models (*)\n\n1.13.1 Deterministic Models\nDeterministic models are those where the output is fully determined by the parameter values and the initial conditions. These models are often used in physics and engineering.\n\n\n1.13.2 Example: Uniformly Accelerated Motion\nA classic example of a deterministic model is uniformly accelerated motion, described by the equation:\nx(t) = x_0 + v_0t + \\frac{1}{2}at^2\nWhere:\n\nx(t) is the position at time t\nx_0 is the initial position\nv_0 is the initial velocity\na is the acceleration\nt is time\n\nLet’s simulate this in R:\n\n# Uniformly accelerated motion\nsimulate_accelerated_motion &lt;- function(x0, v0, a, t) {\n  x0 + v0 * t + 0.5 * a * t^2\n}\n\n# Generating data\nt &lt;- seq(0, 10, by = 0.1)\nx &lt;- simulate_accelerated_motion(x0 = 0, v0 = 2, a = 1, t = t)\n\n# Plot\nplot(t, x, type = \"l\", xlab = \"Time\", ylab = \"Position\", \n     main = \"Uniformly Accelerated Motion\")\n\n\n\n\n\n\n\n\nThis code will generate a plot of uniformly accelerated motion, which is an intuitive example from Newtonian dynamics. In this case, an object starts moving with an initial velocity and accelerates uniformly, resulting in a parabolic trajectory on the position-time graph.\n\n\n1.13.3 Stochastic Models in Social Sciences\nStochastic models incorporate randomness and are often used in social sciences where there’s inherent uncertainty in the systems being studied.\n\n\n1.13.4 Example: Ordinary Least Squares (OLS) Regression\nOLS is a fundamental stochastic model in social sciences. It’s represented as:\nY = \\beta_0 + \\beta_1X + \\epsilon\nWhere:\n\nY is the dependent variable\nX is the independent variable\n\\beta_0 and \\beta_1 are parameters\n\\epsilon is the error term (stochastic component)\n\nLet’s demonstrate OLS in R:\n\n# Generate some sample data\nset.seed(123)\nX &lt;- rnorm(100)\nY &lt;- 2 + 3*X + rnorm(100, sd = 0.5)\n\n# Fit OLS model\nmodel &lt;- lm(Y ~ X)\n\n# Summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95367 -0.34175 -0.04375  0.29032  1.64520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.94860    0.04878   39.95   &lt;2e-16 ***\nX            2.97376    0.05344   55.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4854 on 98 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.969 \nF-statistic:  3097 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n# Plot\nplot(X, Y, main = \"OLS Regression\")\nabline(model, col = \"red\")\n\n\n\n\n\n\n\n\nThis will fit an OLS model to some simulated data and plot the results.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff\n\n\n\n\n1.13.5 Advanced Stochastic Models: Large Language Models\nLarge Language Models (LLMs) like GPT-3 are complex stochastic models used in natural language processing. While we can’t implement a full LLM in this tutorial, we can discuss its principles.\nLLMs are based on the transformer architecture and use self-attention mechanisms. They’re trained on vast amounts of text data and learn to predict the next token in a sequence.\nThe core of an LLM can be thought of as a conditional probability distribution:\nP(x_t | x_{&lt;t}, \\theta)\nWhere: - x_t is the current token - x_{&lt;t} represents all previous tokens - \\theta are the model parameters\n\n\n\n\n\n\nNote\n\n\n\nTokens in Large Language Models (LLMs) are the basic units of text that the model processes. They can be thought of as pieces of words or punctuation marks. Here are key points about tokens:\nDefinition: Tokens are the smallest units of text that an LLM processes. They can be whole words, parts of words, or even individual characters or punctuation marks. Tokenization: The process of breaking text into tokens is called tokenization. LLMs use specific algorithms to perform this task. Examples:\nThe word “cat” might be a single token. A longer word like “understanding” might be broken into multiple tokens, e.g., “under” and “standing”. Punctuation marks like “.” or “?” are often individual tokens. Common prefixes or suffixes might be their own tokens.\nVocabulary: LLMs have a fixed vocabulary of tokens they recognize. This vocabulary typically ranges from tens of thousands to hundreds of thousands of tokens. Significance: The way text is tokenized can affect how the model understands and generates language. It’s particularly important for handling different languages, rare words, or specialized vocabulary. Context: In the equation for LLMs: P(x_t | x_{&lt;t}, \\theta) Where:\nx_t represents the current token x_{&lt;t} represents all previous tokens in the sequence \\theta represents the model parameters\n\n\nUnlike deterministic models, LLMs produce different outputs even for the same input due to their stochastic nature.\n\n\n1.13.6 Conclusion\nWe’ve explored a range of models from deterministic to highly complex stochastic ones. Each type of model has its place in science, depending on the system being studied and the level of uncertainty involved.\nRemember, the choice between deterministic and stochastic models often depends on the nature of the system you’re studying and the questions you’re trying to answer. Deterministic models are great for systems with well-understood mechanics, while stochastic models shine when dealing with inherent randomness or complex, not fully understood systems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-d-introduction-to-r-rstudio-and-tidyverse",
    "href": "chapter1.html#appendix-d-introduction-to-r-rstudio-and-tidyverse",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.14 Appendix D: Introduction to R, RStudio, and tidyverse",
    "text": "1.14 Appendix D: Introduction to R, RStudio, and tidyverse\nR is a powerful programming language and environment for statistical computing and graphics. It’s widely used in academia, especially in fields like social sciences, for data analysis and visualization.\n\n1.14.0.1 Key features of R:\n\nOpen-source and free\nExtensive package ecosystem\nStrong community support\nExcellent for statistical analysis and data visualization\n\n\n\n1.14.1 Getting Started with RStudio\nRStudio is an Integrated Development Environment (IDE) for R that makes it easier to work with R.\n\n1.14.1.1 Installing R and RStudio\n\nDownload and install R from CRAN\nDownload and install RStudio from RStudio’s website\n\n\n\n1.14.1.2 RStudio Interface\nRStudio has four main panes:\n\nSource Editor: Where you write and edit your R scripts\nConsole: Where you can type R commands and see output\nEnvironment/History: Shows all objects in your workspace and command history\nFiles/Plots/Packages/Help: Multipurpose pane for file management, viewing plots, managing packages, and accessing help\n\n\n\n1.14.1.3 Basic RStudio Features\n\nCreating a new R script: File &gt; New File &gt; R Script\nRunning code: Select code and press Ctrl+Enter (Cmd+Enter on Mac)\nInstalling packages: Tools &gt; Install Packages\nGetting help: Type ?function_name in the console\n\n\n\n\n1.14.2 R Basics\n\n1.14.2.1 Data Types in R\n\n# Numeric\nx &lt;- 10.5\nclass(x)\n\n[1] \"numeric\"\n\n# Integer\ny &lt;- 1L\nclass(y)\n\n[1] \"integer\"\n\n# Character\nname &lt;- \"Alice\"\nclass(name)\n\n[1] \"character\"\n\n# Logical\nis_student &lt;- TRUE\nclass(is_student)\n\n[1] \"logical\"\n\n\n\n\n1.14.2.2 Data Structures\n\n1.14.2.2.1 Vectors\n\n# Create a vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\n\n# Vector operations\nnumbers + 2\n\n[1] 3 4 5 6 7\n\nnumbers * 2\n\n[1]  2  4  6  8 10\n\nmean(numbers)\n\n[1] 3\n\nlength(fruits)\n\n[1] 3\n\n\n\n\n1.14.2.2.2 Matrices\n\n# Create a matrix\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\nprint(m)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n# Matrix operations\nt(m)  # transpose\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nm * 2  # scalar multiplication\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    4    8   12\n\n\n\n\n1.14.2.2.3 Data Frames\n\n# Create a data frame\ndf &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  student = c(TRUE, FALSE, TRUE)\n)\nprint(df)\n\n     name age student\n1   Alice  25    TRUE\n2     Bob  30   FALSE\n3 Charlie  35    TRUE\n\n# Accessing data frame elements\ndf$name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\ndf[1, 2]\n\n[1] 25\n\ndf[df$age &gt; 25, ]\n\n     name age student\n2     Bob  30   FALSE\n3 Charlie  35    TRUE\n\n\n\n\n\n1.14.2.3 Functions\n\n# Define a function\ngreet &lt;- function(name) {\n  paste(\"Hello,\", name, \"!\")\n}\n\n# Use the function\ngreet(\"Alice\")\n\n[1] \"Hello, Alice !\"\n\n# Function with multiple arguments\ncalculate_bmi &lt;- function(weight, height) {\n  bmi &lt;- weight / (height^2)\n  return(bmi)\n}\n\ncalculate_bmi(70, 1.75)\n\n[1] 22.85714\n\n\n\n\n1.14.2.4 Control Structures\n\n# If-else statement\nx &lt;- 10\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n} else {\n  print(\"x is not greater than 5\")\n}\n\n[1] \"x is greater than 5\"\n\n# For loop\nfor (i in 1:5) {\n  print(paste(\"Iteration\", i))\n}\n\n[1] \"Iteration 1\"\n[1] \"Iteration 2\"\n[1] \"Iteration 3\"\n[1] \"Iteration 4\"\n[1] \"Iteration 5\"\n\n# While loop\ncounter &lt;- 1\nwhile (counter &lt;= 5) {\n  print(paste(\"Counter:\", counter))\n  counter &lt;- counter + 1\n}\n\n[1] \"Counter: 1\"\n[1] \"Counter: 2\"\n[1] \"Counter: 3\"\n[1] \"Counter: 4\"\n[1] \"Counter: 5\"\n\n\n\n\n\n1.14.3 Introduction to tidyverse\nThe tidyverse is a collection of R packages designed for data science. These packages share a common philosophy and are designed to work together seamlessly.\n\n1.14.3.1 Key tidyverse Packages\n\nggplot2: for data visualization\ndplyr: for data manipulation\ntidyr: for tidying data\nreadr: for reading rectangular data\npurrr: for functional programming\ntibble: modern reimagining of data frames\n\n\n\n1.14.3.2 Getting Started with tidyverse\n\n# Install tidyverse (run once)\n# install.packages(\"tidyverse\")\n\n# Load tidyverse\nlibrary(tidyverse)\n\n\n\n1.14.3.3 Data Import with readr\n\n# Reading CSV files\ndata &lt;- read_csv(\"social_data.csv\")\n\n# Reading other file formats\nread_tsv(\"data.tsv\")  # Tab-separated values\nread_delim(\"data.txt\", delim = \"|\")  # Custom delimiter\n\n\n\n1.14.3.4 Data Manipulation with dplyr\n\n# Let's use the built-in mtcars dataset\ndata(\"mtcars\")\n\n# Selecting columns\nmtcars %&gt;% \n  select(mpg, cyl, hp)\n\n                     mpg cyl  hp\nMazda RX4           21.0   6 110\nMazda RX4 Wag       21.0   6 110\nDatsun 710          22.8   4  93\nHornet 4 Drive      21.4   6 110\nHornet Sportabout   18.7   8 175\nValiant             18.1   6 105\nDuster 360          14.3   8 245\nMerc 240D           24.4   4  62\nMerc 230            22.8   4  95\nMerc 280            19.2   6 123\nMerc 280C           17.8   6 123\nMerc 450SE          16.4   8 180\nMerc 450SL          17.3   8 180\nMerc 450SLC         15.2   8 180\nCadillac Fleetwood  10.4   8 205\nLincoln Continental 10.4   8 215\nChrysler Imperial   14.7   8 230\nFiat 128            32.4   4  66\nHonda Civic         30.4   4  52\nToyota Corolla      33.9   4  65\nToyota Corona       21.5   4  97\nDodge Challenger    15.5   8 150\nAMC Javelin         15.2   8 150\nCamaro Z28          13.3   8 245\nPontiac Firebird    19.2   8 175\nFiat X1-9           27.3   4  66\nPorsche 914-2       26.0   4  91\nLotus Europa        30.4   4 113\nFord Pantera L      15.8   8 264\nFerrari Dino        19.7   6 175\nMaserati Bora       15.0   8 335\nVolvo 142E          21.4   4 109\n\n# Filtering rows\nmtcars %&gt;% \n  filter(cyl == 4)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n# Arranging data\nmtcars %&gt;% \n  arrange(desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n# Creating new variables\nmtcars %&gt;% \n  mutate(kpl = mpg * 0.425)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb     kpl\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  8.9250\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  8.9250\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1  9.6900\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  9.0950\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  7.9475\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  7.6925\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  6.0775\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 10.3700\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  9.6900\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  8.1600\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  7.5650\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  6.9700\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  7.3525\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  6.4600\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  4.4200\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  4.4200\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  6.2475\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 13.7700\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 12.9200\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 14.4075\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  9.1375\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  6.5875\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  6.4600\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  5.6525\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  8.1600\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 11.6025\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 11.0500\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 12.9200\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  6.7150\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  8.3725\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  6.3750\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2  9.0950\n\n# Summarizing data\nmtcars %&gt;% \n  group_by(cyl) %&gt;% \n  summarize(mean_mpg = mean(mpg),\n            count = n())\n\n# A tibble: 3 × 3\n    cyl mean_mpg count\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1     4     26.7    11\n2     6     19.7     7\n3     8     15.1    14\n\n\n\n\n1.14.3.5 Data Visualization with ggplot2\n\n# Scatter plot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Car Weight vs. Fuel Efficiency\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles per Gallon\")\n\n\n\n\nCar Weight vs. Fuel Efficiency\n\n\n\n\n\n# Bar chart\nmtcars %&gt;% \n  count(cyl) %&gt;% \n  ggplot(aes(x = factor(cyl), y = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Number of Cars by Cylinder Count\",\n       x = \"Number of Cylinders\",\n       y = \"Count\")\n\n\n\n\nNumber of Cars by Cylinder Count\n\n\n\n\n\n# Box plot\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Fuel Efficiency by Number of Cylinders\",\n       x = \"Number of Cylinders\",\n       y = \"Miles per Gallon\")\n\n\n\n\nFuel Efficiency by Number of Cylinders\n\n\n\n\n\n\n\n1.14.4 Additional Resources\n\nR for Data Science\ntidyverse documentation\nRStudio Cheat Sheets\nQuarto Guide\nR Cookbook\n\nRemember to experiment with the code, modify examples, and don’t hesitate to use the built-in R help system (accessed by typing ?function_name in the console) when you encounter unfamiliar functions or concepts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html",
    "href": "rozdzial1.html",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "",
    "text": "2.1 Czym są Statystyka i Nauka o Danych?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#czym-są-statystyka-i-nauka-o-danych",
    "href": "rozdzial1.html#czym-są-statystyka-i-nauka-o-danych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "",
    "text": "Important\n\n\n\nStatystyka i data science to sztuka i nauka (o metodach, technikach lub narzędziach) uczenia się z danych.\n\n\n\nNauka o danych i statystyka to potężne narzędzia, które pomagają nam zrozumieć złożone zjawiska w różnych naukach społecznych, w tym w politologii, ekonomii i socjologii. Te uzupełniające się dziedziny dostarczają badaczom i praktykom środków do analizy trendów, zachowań i wyników w społeczeństwie, oferując wgląd, który może kształtować politykę i pogłębiać nasze zrozumienie ludzkich zachowań.\nStatystyka dostarcza matematycznych podstaw do analizy trendów i wyników społecznych, oferując metody projektowania badań, podsumowywania danych i wyciągania wniosków. Nauka o danych rozszerza tę podstawę, włączając metody obliczeniowe i wiedzę dziedzinową, aby radzić sobie z większymi zbiorami danych i przeprowadzać bardziej złożone analizy.\nRazem te dyscypliny pozwalają nam zbierać i przetwarzać duże zbiory danych, wizualizować złożone informacje, odkrywać wzorce w interakcjach społecznych, oceniać wpływ polityk i wspierać podejmowanie decyzji opartych na dowodach. Ich zastosowania są rozległe i zróżnicowane, od badania wzorców głosowania i analizy wskaźników ekonomicznych po badanie nierówności społecznych i analizę zachowań ludzkich.\nW miarę jak nasz świat staje się coraz bardziej oparty na danych, znaczenie nauki o danych i statystyki w naukach społecznych nadal rośnie.\n\n\n\n\n\n\n\nNote\n\n\n\nW naukach społecznych nauka o danych łączy metody statystyczne, narzędzia obliczeniowe i wiedzę dziedzinową do analizy złożonych zjawisk społecznych i zachowań ludzkich.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#związek-między-statystyką-a-nauką-o-danych",
    "href": "rozdzial1.html#związek-między-statystyką-a-nauką-o-danych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.2 Związek Między Statystyką a Nauką o Danych",
    "text": "2.2 Związek Między Statystyką a Nauką o Danych\nStatystyka i data science to ściśle powiązane dziedziny o znaczącym nakładaniu się, szczególnie w naukach społecznych. Zamiast ścisłego podziału, trafniej jest postrzegać je jako komplementarne podejścia na pewnym kontinuum:\n\nTradycyjna StatystykaData Science\n\n\n\nZakorzeniona w teoriach matematycznych i metodach analizy danych\nKładzie nacisk na wnioskowanie statystyczne, testowanie hipotez i teorię prawdopodobieństwa\nHistorycznie kluczowa w naukach społecznych do analizy badań ankietowych, eksperymentów i badań obserwacyjnych\n\n\n\n\nIntegruje metody statystyczne z nauką o komputerach i wiedzą dziedzinową\nPoszerza fokus o uczenie maszynowe, przetwarzanie big data i modelowanie predykcyjne\nW naukach społecznych często zajmuje się wielkoskalowymi danymi cyfrowymi i złożonymi zbiorami danych behawioralnych\n\n\n\n\nNauka o danych może być postrzegana jako wynik ewolucji i rozszerzenie tradycyjnej statystyki, włączając nowe technologie i metody do obsługi większych i bardziej złożonych zbiorów danych w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#podstawowe-koncepcje-w-nauce-o-danych-i-statystyce",
    "href": "rozdzial1.html#podstawowe-koncepcje-w-nauce-o-danych-i-statystyce",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.3 Podstawowe Koncepcje w Nauce o Danych i Statystyce",
    "text": "2.3 Podstawowe Koncepcje w Nauce o Danych i Statystyce\n\n2.3.1 Dane i Populacje (Data and Populations) oraz pojęcia pokrewne\n\n\n\n\n\n\nImportant\n\n\n\n\nDane: Obserwacje lub pomiary zebrane z próby lub populacji.\nPopulacja: Cały zbiór osób lub elementów badanych w określonym czasie.\n\nPrzykład: Wszyscy uprawnieni wyborcy w kraju podczas konkretnego roku wyborczego.\n\nPróba: Podzbiór populacji, który jest faktycznie mierzony. Reprezentatywna próba to podzbiór większej populacji, który dokładnie odzwierciedla cechy tej populacji. Próba powinna odzwierciedlać populację pod względem ważnych cech, takich jak wiek, płeć, status społeczno-ekonomiczny itp. Często wykorzystuje metody losowego doboru próby, aby uniknąć stronniczości. Jest wystarczająco duża, aby być statystycznie istotna, ale mniejsza niż cała populacja.\n\nPrzykład: 1500 losowo wybranych uprawnionych wyborców ankietowanych w przedwyborczym sondażu.\n\n\n\n\n\n\n\n\n\n\nProces Generowania Danych (PGD) i Superpopulacja: Rozszerzenie Tradycyjnych Koncepcji\n\n\n\nW tradycyjnej statystyce pracujemy z dwoma kluczowymi pojęciami:\n\nPopulacja: Cała grupa, którą chcemy badać.\nPróba: Podzbiór populacji, który faktycznie obserwujemy i analizujemy.\n\nWspółczesne badania często wymagają myślenia wykraczającego poza ten dychotomiczny podział. Tu wkraczają koncepcje Procesu Generowania Danych (PGD) i superpopulacji, pogłębiając nasze rozumienie danych i ich struktury.\n\n2.3.2 Proces Generowania Danych (PGD; Data Generating Process, DGP)\nDefinicja formalna:\nPGD to zbiór mechanizmów statystycznych i przyczynowych odpowiedzialnych za wytwarzanie obserwowanych wartości zmiennych w systemie, opisany najczęściej za pomocą funkcji matematycznych i rozkładów prawdopodobieństwa.\nIntuicyjne wyjaśnienie:\nPGD można postrzegać jako “czarną skrzynkę”, która przekształca przyczyny w skutki. To fundamentalny mechanizm, który produkuje dane obserwowane w rzeczywistym świecie - zarówno w naszej próbie, jak i w całej populacji, a także poza nią.\nPrzykład:\nW badaniu zachowań wyborczych, PGD obejmowałby czynniki takie jak:\n\nCechy demograficzne wyborców\nWarunki ekonomiczne\nWydarzenia polityczne\nWpływ mediów społecznościowych\nHistoryczne trendy wyborcze\n\nWszystkie te czynniki kształtują zachowania wyborcze niezależnie od tego, czy dany wyborca został uwzględniony w badaniu, czy nie.\n\n\n2.3.3 Superpopulacja\nDefinicja formalna:\nSuperpopulacja to hipotetyczna, nieskończona populacja, z której obserwowana populacja może być postrzegana jako próba losowa. Reprezentuje wszystkie potencjalne jednostki i wyniki, które mogłyby zostać wygenerowane przez ten sam PGD.\nIntuicyjne wyjaśnienie:\nSuperpopulacja wykracza zarówno poza próbę, jak i obserwowalną populację, obejmując wszystkie potencjalne wyniki, które mogłyby wystąpić w podobnych warunkach lub procesach - zarówno teraz, jak i w przyszłości.\n\n\n2.3.4 Porównanie podejść\n\n2.3.4.1 1. Podejście tradycyjne vs. podejście superpopulacyjne\n\nTradycyjne:\n\nPopulacja: wszyscy zarejestrowani wyborcy w województwie\nPróba: 1000 ankietowanych wyborców\n\nSuperpopulacyjne:\n\nObserwowane dane: 1000 ankietowanych wyborców\nPopulacja: wszyscy zarejestrowani wyborcy\nSuperpopulacja: Wszyscy możliwi wyborcy i scenariusze głosowania, w tym przyszłe wybory i hipotetyczne konteksty polityczne\n\n\n\n\n2.3.4.2 2. Gdy próba równa się populacji\nW badaniach wszystkich 16 województw Polski:\n\nTradycyjne spojrzenie: Brak rozróżnienia między próbą a populacją\nSpojrzenie superpopulacyjne: Traktuje te 16 województw jako “próbę” z teoretycznego zbioru wszystkich możliwych interakcji między województwami a polityką\n\n\n\n\n2.3.5 Zastosowanie w praktyce\nBadanie wpływu nowej polityki planowania urbanistycznego:\n\nPodejście tradycyjne:\n\nPopulacja: Wszystkie miasta w kraju\nPróba: Miasta uwzględnione w badaniu\n\nPodejście superpopulacyjne:\n\nObserwowane dane: Miasta w badaniu\nPopulacja: Wszystkie obecne miasta\nSuperpopulacja: Wszystkie miasta (istniejące lub potencjalne), w których można by zastosować podobne zasady planowania\n\n\nPGD w tym przypadku byłby złożonym zestawem czynników, które determinują, jak polityki planowania urbanistycznego wpływają na rozwój miast.\n\n\n2.3.6 Kluczowe aspekty metodologiczne\n\nZakres i ograniczenia:\nBadacze powinni jasno określić, jakie jednostki lub procesy starają się zrozumieć, wykraczając poza samo opisanie próby i populacji.\nMożliwość uogólnienia:\nPrzy formułowaniu wniosków dotyczących superpopulacji, należy wyraźnie określić granice, w których ustalenia mają zastosowanie.\nSpecyfika kontekstu:\nChoć koncepcja superpopulacji pozwala na szersze wnioskowanie, ważne jest zrozumienie, że PGD może się różnić w zależności od kontekstu.\n\n\n\n2.3.7 Przykład: Jakość Pizzy w Nowym Jorku\nPopulacja:\nWszystkie obecnie działające pizzerie w Nowym Jorku.\nPróba:\n50 losowo wybranych pizzerii z różnych dzielnic.\nSuperpopulacja:\nWszystkie możliwe pizzerie, które mogłyby istnieć w Nowym Jorku:\n\nObecnie działające\nPrzyszłe (nieotwarte)\nHistoryczne (zamknięte)\nHipotetyczne (w alternatywnych warunkach)\n\nProces Generowania Danych (PGD):\nCzynniki wpływające na jakość pizzy:\n\nSkładniki i ich jakość\nUmiejętności i doświadczenie szefów kuchni\nSprzęt i infrastruktura\nMetody przygotowania i przepisy\nCzynniki środowiskowe (np. jakość wody)\nWpływy kulturowe i tradycje\nUwarunkowania ekonomiczne (koszty, czynsze)\n\nPGD jest jak “przepis na jakość pizzy”, który ma zastosowanie do wszystkich potencjalnych pizzerii w superpopulacji, nie tylko do obecnie istniejących lokali.\n\n\n\n\n\n\n\n\ngraph TD\n    A[Data Generating Process DGP]\n    B(Population)\n    C[Sample]\n    A --&gt;|Generates| B\n    B --&gt;|Sampled from| C\n    C -.-&gt;|Inference| B\n    C -.-&gt;|Inference| A\n    B -.-&gt;|Inference| A\n    \n    classDef dgp fill:#f9f,stroke:#333,stroke-width:4px;\n    classDef pop fill:#bbf,stroke:#333,stroke-width:2px;\n    classDef sam fill:#bfb,stroke:#333,stroke-width:2px;\n    \n    class A dgp;\n    class B pop;\n    class C sam;\n\n\n\n\n\n\n\n\n\n\n\n\nObjaśnienie diagramu PGD, Populacji i Próby\n\n\n\nDiagram przedstawia relacje między Procesem Generującym Dane (PGD), populacją i próbą, wraz ze ścieżkami wnioskowania:\n\nRelacje bezpośrednie (ciągłe strzałki):\n\nPGD generuje populację\nZ populacji pobierane są próby\n\nŚcieżki wnioskowania (przerywane strzałki):\n\nOd Próby do Populacji: Tradycyjne wnioskowanie statystyczne\nOd Próby do PGD: Wnioskowanie o podstawowym procesie na podstawie danych z próby\nOd Populacji do PGD: Wnioskowanie o PGD przy użyciu pełnych danych populacji\n\n\n\n\n\n\n\nPopulacja vs. próba. Retrieved from: https://allmodelsarewrong.github.io/mse.html\n\n\nDane stanowią podstawę analizy statystycznej. Mogą być:\n\nDane pierwotne (Primary data): Zebrane bezpośrednio w określonym celu\nDane wtórne (Secondary data): Uzyskane z istniejących źródeł\n\nPrzykład: W badaniu wzrostu studentów uniwersyteckich, populacją są wszyscy studenci uniwersyteccy w kraju, podczas gdy próba może składać się z 1000 losowo wybranych studentów.\n\n\n2.3.8 Zmienne i Stałe (Variables and Constants)\nZmienne to cechy, które mogą przyjmować różne wartości w zbiorze danych. Mogą być:\n\nIlościowe (Quantitative):\n\nCiągłe (Continuous): Wzrost, waga, temperatura\nDyskretne (Discrete): Liczba dzieci, liczba błędów w programie\n\nJakościowe (Qualitative):\n\nNominalne (Nominal): Grupa krwi, kolor oczu\nPorządkowe (Ordinal): Poziom wykształcenia, ocena satysfakcji klienta\n\n\nStałe to wartości, które pozostają niezmienne w trakcie analizy.\n\n2.3.8.1 Rodzaje Danych w Naukach Społecznych\nBadania w naukach społecznych zajmują się różnymi rodzajami danych:\n\nDane Ilościowe: Dane liczbowe (np. odpowiedzi z ankiet, wskaźniki ekonomiczne)\nDane Jakościowe: Dane nieliczbowe (np. transkrypcje wywiadów, odpowiedzi na pytania otwarte w ankietach)\nBig Data: Dane cyfrowe na dużą skalę (np. posty w mediach społecznościowych, logi zachowań online)\n\n\n\n\n2.3.9 Parametry Populacji i Estymanda (Population Parameters and Estimands)\nParametry populacji to liczbowe charakterystyki populacji. Kluczowe punkty:\n\nOpisują całą populację, nie tylko próbę.\nZwykle oznaczane są greckimi literami.\nW większości przypadków nie mogą być bezpośrednio obliczone, ponieważ nie możemy zmierzyć całej populacji.\nSą determinowane przez podstawowy Proces Generujący Dane (DGP).\n\nTypowe parametry populacji to:\n\nŚrednia populacji (Population mean) (\\mu): Średnia/oczekiwana wartość zmiennej w populacji.\nWariancja populacji (Population variance) (\\sigma^2): Miara zmienności w populacji.\nProporcja populacji (Population proportion) (p): Proporcja osób w populacji posiadających daną cechę.\n\nEstymand (Estimand) to cel estymacji - konkretny parametr populacji lub funkcja parametrów, którą chcemy oszacować. Definiuje to, co chcemy wiedzieć o populacji.\n\n\n\n\n\n\nPrzykład: Wzrost Studentów Uniwersyteckich\n\n\n\nRozważmy wzrost wszystkich studentów uniwersyteckich w kraju:\n\n\\mu (estymand): Prawdziwa średnia wysokość wszystkich studentów uniwersyteckich (średnia populacji)\n\\sigma^2 (estymand): Prawdziwa wariancja wysokości w populacji\n\nTe parametry są nieznanymi estymandami, które chcemy oszacować na podstawie danych z próby.\n\n\n\n\n2.3.10 Statystyki i Estymatory (Statistic(s) and Estimators)\nStatystyka (pojedyncza) lub statystyka z próby to dowolna wielkość obliczona na podstawie wartości z próby, która jest rozważana w celu statystycznym.\nGdy statystyka jest używana do oszacowania estymandy (parametru populacji), nazywana jest estymatorem. Estymatory są funkcjami danych z próby, które dostarczają przybliżonych wartości dla nieznanych parametrów populacji.\nPrzykłady statystyk/estymatorów:\n\nŚrednia z próby (Sample mean): \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i (szacuje \\mu)\nWariancja z próby (Sample variance): s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2 (szacuje \\sigma^2)\nProporcja z próby (Sample proportion): \\hat{p} = \\frac{x}{n} (szacuje p)\n\n\n\n2.3.11 Oszacowania (Estimates)\nOszacowanie to konkretna wartość uzyskana przez zastosowanie estymatora do konkretnej próby. Jest to wartość punktowa, która przybliża prawdziwą estymandę (parametr populacji).\nPrzykład: Jeśli obliczamy średnią wysokość z próby wynoszącą 173 cm, to 173 cm jest naszym oszacowaniem estymandy \\mu (średniej wysokości populacji).\n\n\n2.3.12 Modele Statystyczne (Statistical Models)\n\n\n\n\n\n\nNote\n\n\n\nModel w nauce to uproszczona reprezentacja złożonego systemu lub zjawiska. Jest on ta zaprojektowany, aby pomóc nam zrozumieć, wyjaśnić i przewidywać zjawiska zachodzące w rzeczywistym świecie. Modele mogą przybierać różne formy, w tym równania matematyczne, symulacje komputerowe lub ramy koncepcyjne. Pozwalają naukowcom skupić się na kluczowych aspektach systemu, ignorując mniej istotne szczegóły, co sprawia, że złożone problemy stają się łatwiejsze do zrozumienia i badania.\n\n\nModele statystyczne reprezentują relacje między zmiennymi i pomagają w przewidywaniu lub wnioskowaniu o estymandach (parametrach populacji).\nPrzykład: Model regresji liniowej y = \\beta_0 + \\beta_1x + \\epsilon opisuje relację między zmienną niezależną x a zmienną zależną y, gdzie:\n\ny to zmienna zależna (np. wielkość popytu na dobro)\nx to zmienna niezależna (np. cena lub dochód konsumenta)\n\\beta_0 i \\beta_1 to parametry, estymandy do oszacowania\n\\epsilon to składnik błędu, reprezentujący niewyjaśnioną zmienność\n\n\n\n\n\n\n\nWnioskowanie przyczynowe i kontrfakty\n\n\n\nW naukach społecznych często chcemy zrozumieć co by się stało, gdybyśmy podjęli inne działanie - ten hipotetyczny scenariusz nazywamy kontrfaktem/wynikiem kontrfaktycznym. Na przykład:\n\nJakie byłyby zarobki danej osoby, gdyby poszła na studia vs. gdyby nie poszła?\nJak zmieniłaby się frekwencja wyborcza, gdyby głosowanie było obowiązkowe?\n\nPonieważ nie możemy obserwować obu scenariuszy jednocześnie, modele statystyczne pomagają nam oszacować te kontrfakty poprzez:\n\nKontrolowanie zmiennych zakłócających (confounders)\nPorównywanie podobnych grup, które różnią się tylko badanym czynnikiem\nWykorzystanie technik takich jak dopasowanie według współczynnika skłonności czy zmienne instrumentalne\n\n\n\n\nFundamentalny problem wnioskowania przyczynowego: We can think of causal inference as a PREDICTION problem. How could we predict the counterfactual given that we never observe it?\n\n\nPamiętaj: Korelacja ≠ Przyczynowość, ale staranny projekt badawczy i metody statystyczne mogą pomóc nam formułować wnioski przyczynowe.\n\n\n\nConfounding bias and spurious correlation (https://www.bradyneal.com/causal-inference-course) drinking the night before is a common cause of sleeping with shoes on and waking up with a headache :-)\n\n\n\n\n\nReverse causality: https://ff13.fastforwardlabs.com/\n\n\n\n\n\n\n2.3.13 Wnioskowanie (Inference)\nWnioskowanie statystyczne to proces wyciągania wniosków o estymandach (parametrach populacji) na podstawie danych z próby. Obejmuje dwa główne typy:\n\nEstymacja (Estimation): Używanie statystyk z próby (estymatorów) do oszacowania estymand (parametrów populacji)\nTestowanie hipotez (Hypothesis testing): Podejmowanie decyzji o estymandach na podstawie dowodów z próby\n\n\n\n\n\n\n\nEstymacja i testowanie hipotez: wstęp\n\n\n\n\nEstymacja\n\nEstymacja polega na określeniu prawdopodobnej wartości parametru populacji na podstawie danych z próby. W kontekście rozkładu dwumianowego możemy być zainteresowani oszacowaniem prawdopodobieństwa sukcesu (p) dla określonego zdarzenia.\nPrzykład: Rzucanie monetą\nPowiedzmy, że rzucamy monetą 100 razy i chcemy oszacować prawdopodobieństwo wypadnięcia orła.\n\nRzucamy monetą 100 razy i obserwujemy 55 orłów.\nNasze punktowe oszacowanie p (prawdopodobieństwo wypadnięcia orła) wynosi 55/100 = 0,55\nMożemy również obliczyć przedział ufności, np. 95% przedział ufności może wynosić (0,45; 0,65).\n\nPrzedział ufności mówi nam o zakresie, w którym może leżeć prawdziwe prawdopodobieństwo. Mówiąc prościej: “Jesteśmy w 95% pewni, że prawdziwe prawdopodobieństwo wypadnięcia orła mieści się między 45% a 65%.”\nCelem jest tutaj dostarczenie naszego najlepszego oszacowania prawdziwego prawdopodobieństwa wypadnięcia orła, wraz z zakresem prawdopodobnych wartości.\nWażne Pojęcia Teorii Estymacji:\n\nObciążenie (Bias)\n\nObciążenie odnosi się do tendencji estymatora do systematycznego przeszacowania lub niedoszacowania prawdziwej wartości parametru populacji (estymandy).\n\nEstymator nieobciążony to taki, którego średnia wartość (przy wielokrotnym powtórzeniu estymacji) jest równa prawdziwej wartości parametru.\nObciążenie można rozumieć jako różnicę między średnią wartością estymatora a prawdziwą wartością parametru.\n\n\nEfektywność (Efficiency)\n\nEfektywność odnosi się do precyzji estymatora. Bardziej efektywny estymator daje wyniki bliższe prawdziwej wartości parametru, czyli ma mniejsze rozproszenie wyników.\n\nMierzona jest najczęściej wariancją estymatora (im mniejsza wariancja, tym większa efektywność)\nDla nieobciążonych estymatorów efektywność często porównuje się za pomocą Błędu Średniokwadratowego (Mean Squared Error, MSE)\n\n\nTestowanie hipotez\n\nTestowanie hipotez z kolei polega na podejmowaniu decyzji między dwoma konkurencyjnymi twierdzeniami dotyczącymi parametru populacji. Zazwyczaj mamy hipotezę zerową (H0) i hipotezę alternatywną (H1).\nPrzykład: Czy moneta jest uczciwa?\nKorzystając z tego samego scenariusza rzucania monetą, powiedzmy, że chcemy sprawdzić, czy moneta jest uczciwa (p = 0,5), czy też stronnicza na korzyść orła (p &gt; 0,5).\n\nHipoteza zerowa (H0): p = 0,5 (moneta jest uczciwa)\nHipoteza alternatywna (H1): p &gt; 0,5 (moneta jest stronnicza na korzyść orła)\nObserwujemy 55 orłów na 100 rzutów\n\nP-wartość i jak testowanie hipotez działa jako rodzaj “probabilistycznego dowodu nie wprost”:\n\nZaczynamy od założenia, że hipoteza zerowa (H0) jest prawdziwa. W tym przypadku zakładamy, że moneta jest uczciwa.\nNastępnie pytamy: “Jeśli moneta byłaby naprawdę uczciwa, jakie byłoby prawdopodobieństwo zaobserwowania 55 lub więcej orłów na 100 rzutów?”\nTo prawdopodobieństwo nazywa się wartością p. Jest to prawdopodobieństwo zaobserwowania naszych danych (lub bardziej ekstremalnych) przy założeniu, że hipoteza zerowa jest prawdziwa.\nJeśli to prawdopodobieństwo (wartość p) jest bardzo małe, mamy sprzeczność: zaobserwowaliśmy coś, co powinno być bardzo rzadkie, gdyby nasze założenie (H0) było prawdziwe.\nZwykle ustalamy próg zwany poziomem istotności (często 0,05 lub 5%) dla tego, co uważamy za “bardzo małe”.\nJeśli wartość p jest mniejsza niż wybrany poziom istotności, odrzucamy H0. Wnioskujemy, że nasza obserwacja jest zbyt mało prawdopodobna przy H0, więc faworyzujemy hipotezę alternatywną.\nJeśli wartość p jest większa niż nasz poziom istotności, nie odrzucamy H0. Nie mamy wystarczających dowodów, aby stwierdzić, że moneta jest stronnicza.\n\nTen proces jest jak “probabilistyczny dowód nie wprost”, ponieważ:\n\nZaczynamy od założenia H0 (podobnie jak zakładamy przeciwieństwo tego, co chcemy udowodnić w dowodzie nie wprost).\nSprawdzamy, czy to założenie prowadzi do bardzo mało prawdopodobnej sytuacji (naszych zaobserwowanych danych).\nJeśli tak, odrzucamy założenie (H0) i faworyzujemy alternatywę.\n\nWartość p dokładnie określa, jak mało prawdopodobna jest nasza obserwacja przy założeniu H0. Bardzo mała wartość p (np. 0,01) oznacza: “Gdyby H0 była prawdziwa, spodziewalibyśmy się zobaczyć tak ekstremalne dane tylko około 1% czasu.”\nTestowanie hipotez i estymacja to powiązane, ale odrębne procedury statystyczne; testowanie hipotez może być wykorzystane do wyciągania wniosków o oszacowaniach i może uzupełniać estymację na kilka sposobów, np.:\n\nTestowanie oszacowań punktowych: Testowanie hipotez może być wykorzystane do oceny, czy oszacowanie punktowe różni się istotnie od hipotetycznej wartości. Na przykład, jeśli oszacujemy, że moneta ma prawdopodobieństwo 0,55 wypadnięcia orłem, możemy użyć testu hipotezy, aby określić, czy ta wartość różni się istotnie od 0,5 (uczciwa moneta).\nIstotność parametrów: W modelach wielowymiarowych, testy hipotez (takie jak testy t w regresji) mogą pomóc określić, które oszacowane parametry różnią się istotnie od zera, dając wgląd w to, które zmienne są ważne w modelu.\n\n\n\n\n\n2.3.14 Relacje Między Pojęciami\n\nProces Generujący Dane (DGP - Data Generating Process) określa rzeczywiste wartości parametrów populacji (estymand).\nEstymandy są szacowane za pomocą statystyk obliczonych na podstawie próby (estymatorów).\nJakość estymatorów ocenia się na podstawie właściwości takich jak obciążenie i efektywność w szacowaniu estymandy.\nModele statystyczne wykorzystują oszacowane parametry do opisania relacji między zmiennymi w populacji.\nWnioskowanie statystyczne polega na wyciąganiu wniosków o estymandach na podstawie danych z próby, wykorzystując właściwości estymatorów.\n\n\n\n\n\n\n\nPrzykład: Badanie Zachowań Wyborczych\n\n\n\n\nPopulacja: Wszyscy uprawnieni wyborcy w kraju\nEstymanda: p = rzeczywista proporcja wyborców popierających danego kandydata\nPróba: 1000 losowo wybranych uprawnionych wyborców\nEstymator: \\hat{p} = proporcja wyborców z próby popierających kandydata\nOszacowanie: Konkretna wartość \\hat{p} obliczona z próby (np. 0,52)\nDGP: Złożona interakcja czynników wpływających na decyzje wyborcze, takich jak przekonania polityczne, warunki ekonomiczne, ekspozycja na media i sieci społeczne.\n\nZrozumienie DGP pomaga badaczom interpretować, dlaczego estymanda p ma określoną wartość i jak może się zmieniać w czasie. Na przykład, nagła zmiana w gospodarce może wpłynąć na preferencje wyborców, zmieniając tym samym wartość p.\nObciążenie i efektywność w kontekście przykładu:\n\nJeśli \\hat{p} jest nieobciążonym estymatorem, oznacza to, że przy wielokrotnym powtórzeniu badania na różnych próbach, średnia wartość \\hat{p} będzie bliska rzeczywistej wartości p.\nEfektywność \\hat{p} określa, jak bardzo rozproszone są wyniki poszczególnych badań wokół tej średniej. Im mniejsze rozproszenie, tym estymator jest bardziej efektywny.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#główne-komponenty-nauki-o-danych",
    "href": "rozdzial1.html#główne-komponenty-nauki-o-danych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.4 Główne Komponenty Nauki o Danych",
    "text": "2.4 Główne Komponenty Nauki o Danych\n\nZbieranie DanychPrzetwarzanie DanychEksploracyjna Analiza Danych (EDA)Wnioskowanie StatystyczneUczenie MaszynoweWizualizacja Danych i KomunikacjaPowtarzalność i Otwarta Nauka\n\n\n\nMetody eksperymentalne: Kontrolowane badania, w których naukowcy manipulują zmiennymi, aby obserwować efekty\nBadania obserwacyjne: Gromadzenie danych poprzez obserwację i rejestrację bez ingerencji\nAnkiety i wywiady: Zbieranie informacji bezpośrednio od ludzi poprzez zadawanie pytań\nCyfrowe zbieranie danych: Gromadzenie danych ze źródeł internetowych, czujników lub systemów komputerowych\nAspekty etyczne: Zapewnienie, że badania respektują prawa i dobro uczestników\n\n\n\n\nCzyszczenie danych: Usuwanie błędów i niespójności z surowych danych\nObsługa brakujących wartości: Radzenie sobie z lukami w zbiorze danych, które mogłyby wpłynąć na analizę\nTransformacja danych: Konwertowanie danych na formaty odpowiednie do analizy, np. zmiana tekstu na liczby\n\n\n\n\nStatystyki opisowe: Podsumowanie danych za pomocą miar takich jak średnia, mediana i odchylenie standardowe\nWizualizacja danych: Tworzenie wykresów i diagramów do wizualnego przedstawienia wzorców w danych\nIdentyfikacja wzorców: Odkrywanie trendów lub zależności w danych\n\n\n\n\nTestowanie hipotez: Wykorzystanie danych do oceny twierdzeń o populacjach\nAnaliza regresji: Badanie zależności między zmiennymi i dokonywanie przewidywań\nWnioskowanie przyczynowe: Określanie, czy jedna zmienna bezpośrednio wpływa na inną\n\n\n\n\nUczenie nadzorowane: Trenowanie modeli do przewidywania wyników przy użyciu danych ze znanymi odpowiedziami\nUczenie nienadzorowane: Znajdowanie ukrytych wzorców w danych bez predefiniowanych kategorii\nPrzetwarzanie języka naturalnego (NLP): Nauczanie komputerów rozumienia i analizy ludzkiego języka\n\n\n\n\nEfektywne wizualizacje: Tworzenie czytelnych, informatywnych grafik do przedstawiania złożonych danych\nKomunikacja naukowa: Wyjaśnianie wyników różnym odbiorcom, od ekspertów po ogół społeczeństwa\nPisanie naukowe: Przygotowywanie artykułów i raportów naukowych w celu dzielenia się wynikami\n\n\n\n\nKontrola wersji: Śledzenie zmian w danych i kodzie w trakcie procesu badawczego\nPraktyki otwartych danych: Udostępnianie danych i metod badawczych do weryfikacji i dalszych badań\nPowtarzalne procesy badawcze: Dokumentowanie kroków badawczych, aby inni mogli powtórzyć badanie",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#narzędzia-do-nauki-o-danych-w-naukach-społecznych",
    "href": "rozdzial1.html#narzędzia-do-nauki-o-danych-w-naukach-społecznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.5 Narzędzia do Nauki o Danych w Naukach Społecznych",
    "text": "2.5 Narzędzia do Nauki o Danych w Naukach Społecznych\nW tym kursie będziemy głównie używać R do naszej analizy danych, ponieważ jest on szeroko stosowany w badaniach nauk społecznych.\n\n2.5.1 R w Analizie Danych Nauk Społecznych\nR oferuje potężne możliwości dla badań w naukach społecznych, od manipulacji danymi po zaawansowane modelowanie statystyczne.\n\n\nKliknij, aby pokazać/ukryć kod R\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nKliknij, aby pokazać/ukryć kod R\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate example data with a Simpson's Paradox\nn &lt;- 1000\ndata &lt;- tibble(\n  age_group = sample(c(\"Young\", \"Middle\", \"Old\"), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),\n  education_years = case_when(\n    age_group == \"Young\" ~ rnorm(n, mean = 10, sd = 1),\n    age_group == \"Middle\" ~ rnorm(n, mean = 13, sd = 1),\n    age_group == \"Old\" ~ rnorm(n, mean = 16, sd = 1)\n  ),\n  income = case_when(\n    age_group == \"Young\" ~ 70000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Middle\" ~ 50000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Old\" ~ 30000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000)\n  )\n)\n\n# Basic data summary\nsummary(data)\n\n\n  age_group         education_years      income     \n Length:1000        Min.   : 6.628   Min.   :34068  \n Class :character   1st Qu.:10.913   1st Qu.:51508  \n Mode  :character   Median :13.004   Median :63376  \n                    Mean   :12.986   Mean   :63307  \n                    3rd Qu.:14.934   3rd Qu.:75023  \n                    Max.   :18.861   Max.   :96620  \n\n\nKliknij, aby pokazać/ukryć kod R\n# Correlation analysis\ncor(data %&gt;% select(education_years, income))\n\n\n                education_years     income\neducation_years       1.0000000 -0.8152477\nincome               -0.8152477  1.0000000\n\n\nKliknij, aby pokazać/ukryć kod R\n# Overall trend (Simpson's Paradox)\noverall_plot &lt;- ggplot(data, aes(x = education_years, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Overall Relationship between Education and Income\",\n       subtitle = \"Simpson's Paradox: Appears negative\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Trend by age group (Resolving Simpson's Paradox)\ngrouped_plot &lt;- ggplot(data, aes(x = education_years, y = income, color = age_group)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Relationship between Education and Income by Age Group\",\n       subtitle = \"Resolving Simpson's Paradox: Positive relationship within groups\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Statistical analysis\nmodel_overall &lt;- lm(income ~ education_years, data = data)\nmodel_by_age &lt;- lm(income ~ education_years + age_group, data = data)\n\n# Print results\nprint(overall_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(grouped_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_overall))\n\n\n\nCall:\nlm(formula = income ~ education_years, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24451  -5439    235   5262  34328 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     121814.7     1339.5   90.94   &lt;2e-16 ***\neducation_years  -4505.4      101.3  -44.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7976 on 998 degrees of freedom\nMultiple R-squared:  0.6646,    Adjusted R-squared:  0.6643 \nF-statistic:  1978 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_by_age))\n\n\n\nCall:\nlm(formula = income ~ education_years + age_group, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-14827  -3369    118   3356  16388 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      48270.8     2028.4  23.797  &lt; 2e-16 ***\neducation_years   1135.5      154.6   7.345 4.26e-13 ***\nage_groupOld    -19942.8      593.2 -33.619  &lt; 2e-16 ***\nage_groupYoung   20461.1      600.7  34.064  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4950 on 996 degrees of freedom\nMultiple R-squared:  0.8711,    Adjusted R-squared:  0.8707 \nF-statistic:  2244 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\n# Calculate and print correlations\noverall_cor &lt;- cor(data$education_years, data$income)\ngroup_cors &lt;- data %&gt;%\n  group_by(age_group) %&gt;%\n  summarize(correlation = cor(education_years, income))\n\nprint(\"Overall correlation:\")\n\n\n[1] \"Overall correlation:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(overall_cor)\n\n\n[1] -0.8152477\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(\"Correlations by age group:\")\n\n\n[1] \"Correlations by age group:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(group_cors)\n\n\n# A tibble: 3 × 2\n  age_group correlation\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Middle          0.185\n2 Old             0.291\n3 Young           0.223\n\n\nTen przykład demonstruje podstawowe operacje na danych, statystyki opisowe i wizualizację danych przy użyciu R.\nCertainly. Here’s the Polish version of the section on causal inference versus observational studies:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wnioskowanie-przyczynowe-a-badania-obserwacyjne",
    "href": "rozdzial1.html#wnioskowanie-przyczynowe-a-badania-obserwacyjne",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.6 Wnioskowanie przyczynowe a badania obserwacyjne",
    "text": "2.6 Wnioskowanie przyczynowe a badania obserwacyjne\nW naukach społecznych i nie tylko, zrozumienie relacji między zmiennymi jest kluczowe. Dwa główne podejścia to wnioskowanie przyczynowe i badania obserwacyjne, każde z własnymi mocnymi stronami i ograniczeniami.\n\nWnioskowanie przyczynoweBadania obserwacyjneKluczowe rozróżnienie: Korelacja vs. Przyczynowość\n\n\n\nDąży do ustalenia związków przyczynowo-skutkowych\nCzęsto obejmuje plany eksperymentalne lub zaawansowane techniki statystyczne\nStara się odpowiedzieć na pytania “Co by było, gdyby?” i określić wpływ interwencji\nPrzykłady: Randomizowane badania kontrolowane, projekty quasi-eksperymentalne, zmienne instrumentalne\n\n\n\n\nBadają relacje między zmiennymi bez bezpośredniej interwencji\nOpierają się na danych zebranych w naturalnych warunkach lub z istniejących zbiorów danych\nMogą identyfikować korelacje i wzorce, ale mają trudności z ustaleniem przyczynowości\nPrzykłady: Badania kohortowe, badania kliniczno-kontrolne, przekrojowe badania ankietowe\n\n\n\n\n\n\n\n\n\n\n\n\n\nPamiętaj: Korelacja nie implikuje przyczynowości\n\n\n\nFundamentalna zasada w badaniach głosi, że korelacja między dwiema zmiennymi niekoniecznie implikuje związek przyczynowy. Ta koncepcja jest kluczowa przy interpretacji wyników badań obserwacyjnych.\n\nKorelacja: Mierzy siłę i kierunek związku między zmiennymi\nPrzyczynowość: Wskazuje, że zmiany w jednej zmiennej bezpośrednio powodują zmiany w drugiej\n\nChociaż silne korelacje mogą sugerować potencjalne związki przyczynowe, do ustalenia przyczynowości wymagane są dodatkowe dowody i rygorystyczne metody.\n\n\n\nWyzwania w ustalaniu przyczynowościMetody wzmacniania twierdzeń przyczynowychZnaczenie w naukach społecznych\n\n\n\nZmienne zakłócające: Niezmierzone czynniki wpływające zarówno na domniemaną przyczynę, jak i skutek\nOdwrotna przyczynowość: Domniemany skutek może w rzeczywistości powodować domniemaną przyczynę\nBłąd selekcji: Nielosowy dobór uczestników do grup badawczych\n\n\n\n\nRandomizowane badania kontrolowane (gdy są etyczne i wykonalne)\nNaturalne eksperymenty lub projekty quasi-eksperymentalne\nDopasowanie według propensity score\nAnaliza różnicy w różnicach\nPodejścia oparte na zmiennych instrumentalnych\nSkierowane grafy acykliczne (DAG) do wizualizacji relacji przyczynowych\n\n\n\nZrozumienie różnicy między wnioskowaniem przyczynowym a badaniami obserwacyjnymi jest kluczowe w naukach społecznych, gdzie względy etyczne często ograniczają manipulacje eksperymentalne. Badacze muszą starannie projektować badania i interpretować wyniki, aby uniknąć wprowadzających w błąd wniosków dotyczących przyczynowości.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#modele-w-nauce-od-deterministycznych-do-stochastycznych",
    "href": "rozdzial1.html#modele-w-nauce-od-deterministycznych-do-stochastycznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.7 Modele w Nauce: Od Deterministycznych do Stochastycznych",
    "text": "2.7 Modele w Nauce: Od Deterministycznych do Stochastycznych\nModele są niezbędnymi narzędziami w badaniach naukowych, pomagając naukowcom reprezentować, rozumieć i przewidywać złożone zjawiska. Ta sekcja omawia główne typy modeli stosowanych w nauce, wraz z przykładami ich zastosowań. Należy pamiętać, że te kategorie często się nakładają, a wiele modeli naukowych łączy w sobie różne aspekty.\n\n2.7.1 Modele Matematyczne\nModele matematyczne wykorzystują równania i koncepcje matematyczne do opisywania i analizowania systemów lub zjawisk. Można je podzielić na kilka podkategorii, choć należy pamiętać, że niektóre złożone modele mogą zawierać elementy z wielu kategorii:\n\n2.7.1.1 a. Modele Deterministyczne\nModele deterministyczne dostarczają precyzyjnych przewidywań na podstawie zestawu zmiennych, bez uwzględniania losowości na poziomie makroskopowym.\nPrzykład: Prawa ruchu Newtona, które mogą precyzyjnie przewidzieć ruch obiektów pod wpływem znanych sił w mechanice klasycznej.\n\n\n2.7.1.2 b. Modele Stochastyczne\nModele stochastyczne uwzględniają losowość i prawdopodobieństwo. Jednak kluczowe jest rozróżnienie dwóch fundamentalnie różnych typów modeli stochastycznych:\n\n2.7.1.2.1 i. Klasyczne Modele Stochastyczne\nTe modele zajmują się losowością wynikającą z niepełnej informacji lub złożonych interakcji w systemach klasycznych. Podstawowy system jest deterministyczny, ale praktyczne ograniczenia w pomiarach lub obliczeniach prowadzą do użycia opisów probabilistycznych.\nPrzykład: Modele regresji w statystyce, gdzie losowość reprezentuje niewyjaśnioną zmienność lub błąd pomiaru:\ny = β_0 + β_1x + ε\nGdzie:\n\ny to zmienna zależna (np. wielkość popytu na dobro)\nx to zmienna niezależna (np. cena lub dochód konsumenta)\nβ_0 i β_1 to parametry\nε to składnik błędu, reprezentujący niewyjaśnioną zmienność\n\n\n\n2.7.1.2.2 ii. Kwantowe Modele Stochastyczne\nTe modele zajmują się fundamentalną, nieredukowalną losowością nieodłącznie związaną z systemami mechaniki kwantowej. Ta losowość nie wynika z braku informacji, ale jest podstawową cechą rzeczywistości kwantowej.\nPrzykład: Model Standardowy w fizyce cząstek elementarnych, który opisuje interakcje cząstek za pomocą kwantowej teorii pola. Na przykład, rozpad cząstki jest z natury probabilistyczny:\nP(t) = e^{-t/τ}\nGdzie:\n\nP(t) to prawdopodobieństwo, że cząstka nie rozpadła się po czasie t\nτ to średni czas życia cząstki\n\n\n\n\n2.7.1.3 c. Modele Symulacji Komputerowych\nSymulacje komputerowe wykorzystują algorytmy i metody obliczeniowe oparte na modelach matematycznych do symulowania złożonych systemów i przewidywania ich zachowania w czasie. Mogą być deterministyczne lub stochastyczne.\nPrzykład: Modele klimatyczne symulujące system klimatyczny Ziemi, uwzględniające czynniki takie jak skład atmosfery, prądy oceaniczne i promieniowanie słoneczne do prognozowania przyszłych scenariuszy klimatycznych.\n\n\n\n2.7.2 Modele Koncepcyjne\nModele koncepcyjne to abstrakcyjne reprezentacje systemów lub procesów, często wykorzystujące diagramy lub schematy blokowe do ilustrowania relacji między komponentami.\nPrzykład: Model obiegu wody w naukach o Ziemi, który ilustruje ciągły ruch wody w obrębie Ziemi i atmosfery poprzez procesy takie jak parowanie, opady i spływ powierzchniowy.\n\n\n2.7.3 Modele Fizyczne\nModele fizyczne to namacalne reprezentacje obiektów lub systemów, często w formie pomniejszonej lub uproszczonej wersji rzeczywistego obiektu.\nPrzykład: Modele tunelu aerodynamicznego w badaniach aerodynamiki, używane do badania efektów przepływu powietrza wokół obiektów stałych i optymalizacji projektów samolotów, pojazdów lub budynków.\n\n\n2.7.4 Modele Teoretyczne\nModele teoretyczne to abstrakcyjne ramy oparte na fundamentalnych zasadach i hipotezach, często używane do wyjaśniania obserwowanych zjawisk lub przewidywania nowych. Te modele często wykorzystują równania matematyczne i mogą być deterministyczne lub stochastyczne.\nPrzykład: Teoria ewolucji poprzez dobór naturalny, która dostarcza ram do zrozumienia różnorodności i adaptacji form życia w czasie.\n\n\n2.7.5 Podsumowanie\nTe różne formy modeli odgrywają kluczową rolę w badaniach naukowych, każda oferując unikalne zalety dla zrozumienia i przewidywania zjawisk naturalnych. Naukowcy często używają wielu typów modeli jednocześnie, aby uzyskać kompleksowy wgląd w złożone systemy i procesy.\nWażne jest, aby zdawać sobie sprawę, że te kategorie nie są wzajemnie wykluczające i często się nakładają:\n\nModele matematyczne stanowią podstawę dla wielu innych typów modeli, w tym symulacji komputerowych i niektórych modeli teoretycznych.\nModele symulacji komputerowych są zasadniczo modelami matematycznymi implementowanymi za pomocą metod obliczeniowych i mogą być deterministyczne lub stochastyczne.\nModele teoretyczne często wykorzystują sformułowania matematyczne i mogą być implementowane jako symulacje komputerowe.\nModele fizyczne mogą być projektowane na podstawie modeli matematycznych i mogą być używane do walidacji symulacji komputerowych.\n\nWybór typu modelu często zależy od konkretnego pytania badawczego, natury badanego systemu, dostępnych danych oraz zasobów obliczeniowych. W miarę postępu nauki granice między tymi typami modeli coraz bardziej się zacierają, prowadząc do coraz bardziej wyrafinowanych i interdyscyplinarnych podejść do modelowania złożonych zjawisk.\nKluczowe jest rozróżnienie różnych typów modeli stochastycznych. Klasyczne modele stochastyczne, takie jak te używane w analizie regresji, zajmują się losowością wynikającą z niepełnej informacji lub złożonych interakcji w systemach, które są zasadniczo deterministyczne. Z drugiej strony, kwantowe modele stochastyczne, jak te w fizyce cząstek, zajmują się fundamentalną, nieredukowalną losowością nieodłącznie związaną z systemami mechaniki kwantowej. To rozróżnienie odzwierciedla głębokie różnice między klasycznymi a kwantowymi paradygmatami w fizyce i podkreśla różnorodne sposoby, w jakie prawdopodobieństwo jest wykorzystywane w modelowaniu naukowym.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zrozumienie-pozornych-korelacji-zmiennych-zakłócających-i-kolizyjnych",
    "href": "rozdzial1.html#zrozumienie-pozornych-korelacji-zmiennych-zakłócających-i-kolizyjnych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.8 Zrozumienie Pozornych Korelacji, Zmiennych Zakłócających i Kolizyjnych (*)",
    "text": "2.8 Zrozumienie Pozornych Korelacji, Zmiennych Zakłócających i Kolizyjnych (*)\nW tej sekcji zbadamy trzy ważne pojęcia w analizie statystycznej: pozorne korelacje, zmienne zakłócające i zmienne kolizyjne. Zrozumienie tych pojęć jest kluczowe dla uniknięcia błędnej interpretacji danych i wyciągania nieprawidłowych wniosków z analiz statystycznych.\nZacznijmy od załadowania niezbędnych bibliotek:\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nset.seed(123) # dla powtarzalności\n\n\n2.8.1 Pozorne Korelacje\nPozorne korelacje to związki między zmiennymi, które wydają się przyczynowe, ale w rzeczywistości są przypadkowe lub spowodowane przez niewidoczny trzeci czynnik.\n\n2.8.1.1 Przykład: Sprzedaż lodów a przypadki utonięć\nStwórzmy zbiór danych, który pokazuje pozorną korelację między sprzedażą lodów a przypadkami utonięć:\n\nn &lt;- 100\ndane_pozorne &lt;- tibble(\n  temperatura = rnorm(n, mean = 25, sd = 5),\n  sprzedaz_lodow = 100 + 5 * temperatura + rnorm(n, sd = 10),\n  przypadki_utoniec = 1 + 0.5 * temperatura + rnorm(n, sd = 2)\n)\n\nggplot(dane_pozorne, aes(x = sprzedaz_lodow, y = przypadki_utoniec)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Pozorna Korelacja: Sprzedaż Lodów vs Przypadki Utonięć\",\n       x = \"Sprzedaż Lodów\", y = \"Przypadki Utonięć\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTen wykres pokazuje pozytywną korelację między sprzedażą lodów a przypadkami utonięć. Jednak ta relacja jest pozorna. Prawdziwą przyczyną obu zjawisk jest temperatura:\n\nggplot(dane_pozorne, aes(x = temperatura)) +\n  geom_point(aes(y = sprzedaz_lodow), color = \"blue\") +\n  geom_point(aes(y = przypadki_utoniec * 10), color = \"red\") +\n  geom_smooth(aes(y = sprzedaz_lodow), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(aes(y = przypadki_utoniec * 10), method = \"lm\", se = FALSE, color = \"red\") +\n  scale_y_continuous(\n    name = \"Sprzedaż Lodów\",\n    sec.axis = sec_axis(~./10, name = \"Przypadki Utonięć\")\n  ) +\n  labs(title = \"Temperatura jako Wspólna Przyczyna\",\n       x = \"Temperatura\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n2.8.2 Zmienne Zakłócające\nZmienna zakłócająca to zmienna, która wpływa zarówno na zmienną zależną, jak i niezależną, powodując pozorny związek.\n\n2.8.2.1 Przykład: Edukacja, Dochód i Wiek\nStwórzmy zbiór danych, w którym wiek zakłóca relację między edukacją a dochodem:\n\nlibrary(tidyverse)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nn &lt;- 1000\nconfounder_data &lt;- tibble(\n  age = runif(n, 25, 65),\n  education = round(10 + 0.1 * age + rnorm(n, sd = 2)),\n  income = 20000 + 1000 * education + 500 * age + rnorm(n, sd = 5000)\n)\n\n# Without controlling for age\nmodel_naive &lt;- lm(income ~ education, data = confounder_data)\n# Controlling for age\nmodel_adjusted &lt;- lm(income ~ education + age, data = confounder_data)\n\n# Create age groups for visualization\nconfounder_data &lt;- confounder_data %&gt;%\n  mutate(age_group = cut(age, breaks = 3, labels = c(\"Young\", \"Middle\", \"Old\")))\n\n# Visualize\nggplot(confounder_data, aes(x = education, y = income)) +\n  geom_point(aes(color = age), alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1.2) +\n  geom_smooth(aes(group = age_group, color = as.numeric(age_group)), \n              method = \"lm\", se = FALSE, linewidth = 1) +\n  scale_color_viridis_c(name = \"Age\", \n                        breaks = c(30, 45, 60), \n                        labels = c(\"Young\", \"Middle\", \"Old\")) +\n  labs(title = \"Education vs Income, Confounded by Age\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPorównajmy współczynniki:\n\nsummary(model_naive)$coefficients[\"education\", \"Estimate\"]\n\n[1] 2328.718\n\nsummary(model_adjusted)$coefficients[\"education\", \"Estimate\"]\n\n[1] 1101.783\n\n\nEfekt edukacji na dochód jest przeszacowany, gdy nie kontrolujemy wieku.\n\n\n\n2.8.3 Zmienne Kolizyjne\nZmienna kolizyjna to zmienna, na którą wpływają zarówno zmienna niezależna, jak i zmienna zależna. Kontrolowanie zmiennej kolizyjnej może wprowadzić pozorną korelację.\n\n2.8.3.1 Przykład: Satysfakcja z pracy, Wynagrodzenie i Równowaga między pracą a życiem prywatnym\nStwórzmy zbiór danych, w którym równowaga między pracą a życiem prywatnym jest zmienną kolizyjną między satysfakcją z pracy a wynagrodzeniem:\n\nn &lt;- 1000\ndane_kolizyjne &lt;- tibble(\n  satysfakcja_z_pracy = rnorm(n),\n  wynagrodzenie = rnorm(n),\n  rownowaga_praca_zycie = -0.5 * satysfakcja_z_pracy - 0.5 * wynagrodzenie + rnorm(n, sd = 0.5)\n)\n\n# Bez kontrolowania równowagi praca-życie\nmodel_poprawny &lt;- lm(wynagrodzenie ~ satysfakcja_z_pracy, data = dane_kolizyjne)\n\n# Błędne kontrolowanie równowagi praca-życie\nmodel_kolizyjny &lt;- lm(wynagrodzenie ~ satysfakcja_z_pracy + rownowaga_praca_zycie, data = dane_kolizyjne)\n\n# Wizualizacja\nggplot(dane_kolizyjne, aes(x = satysfakcja_z_pracy, y = wynagrodzenie, color = rownowaga_praca_zycie)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_color_viridis_c() +\n  labs(title = \"Satysfakcja z Pracy vs Wynagrodzenie, Równowaga Praca-Życie jako Zmienna Kolizyjna\",\n       x = \"Satysfakcja z Pracy\", y = \"Wynagrodzenie\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPorównajmy współczynniki:\n\nsummary(model_poprawny)$coefficients[\"satysfakcja_z_pracy\", \"Estimate\"]\n\n[1] 0.02063487\n\nsummary(model_kolizyjny)$coefficients[\"satysfakcja_z_pracy\", \"Estimate\"]\n\n[1] -0.4794016\n\n\nKontrolowanie zmiennej kolizyjnej (równowaga praca-życie) wprowadza pozorną korelację między satysfakcją z pracy a wynagrodzeniem.\n\n\n\n2.8.4 Podsumowanie\nZrozumienie pozornych korelacji, zmiennych zakłócających i kolizyjnych jest kluczowe dla prawidłowej analizy statystycznej i wnioskowania przyczynowego. Zawsze rozważ podstawową strukturę przyczynową swoich danych i bądź ostrożny w kwestii tego, które zmienne kontrolujesz w swoich analizach.\n\n\n2.8.5 Dalsza Lektura\n\nPearl, J. (2009). Causality: Models, Reasoning, and Inference. Cambridge University Press.\nHernán, M. A., & Robins, J. M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#etyczne-aspekty-w-analizie-danych-nauk-społecznych",
    "href": "rozdzial1.html#etyczne-aspekty-w-analizie-danych-nauk-społecznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.9 Etyczne Aspekty w Analizie Danych Nauk Społecznych",
    "text": "2.9 Etyczne Aspekty w Analizie Danych Nauk Społecznych\nEtyka odgrywa kluczową rolę w badaniach nauk społecznych:\n\nPrywatność i Zgoda: Zapewnienie prywatności uczestników i świadomej zgody\nOchrona Danych: Bezpieczne przechowywanie i zarządzanie wrażliwymi danymi osobowymi\nBłędy i Reprezentacja: Adresowanie błędów próbkowania i zapewnienie różnorodnej reprezentacji\nPrzejrzystość: Jasne komunikowanie metod badawczych i ograniczeń\nWpływ Społeczny: Rozważanie potencjalnych społecznych implikacji wyników badań\n\n\n\n\n\n\n\nWarning\n\n\n\nNaukowcy społeczni muszą starannie rozważyć etyczne implikacje swoich praktyk zbierania, analizy i rozpowszechniania danych.\n\n\n\n2.9.1 Kluczowe Wnioski\n\nNauka o danych w naukach społecznych bazuje na tradycyjnych metodach statystycznych, włączając nowe technologie do analizy złożonych zjawisk społecznych.\nZrozumienie koncepcji takich jak populacja, próba i procesy generowania danych jest kluczowe dla prawidłowych badań w naukach społecznych.\nProces nauki o danych w badaniach społecznych obejmuje wiele etapów, od etycznego zbierania danych po komunikację wniosków.\nR jest potężnym narzędziem do analizy danych w naukach społecznych, oferującym szeroki zakres możliwości.\nAspekty etyczne powinny być na pierwszym planie każdego projektu związanego z danymi w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-a-losowość-klasyczna-a-kwantowa-zrozumienie-fundamentalnych-różnic",
    "href": "rozdzial1.html#appendix-a-losowość-klasyczna-a-kwantowa-zrozumienie-fundamentalnych-różnic",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.10 Appendix A: Losowość Klasyczna a Kwantowa: Zrozumienie Fundamentalnych Różnic",
    "text": "2.10 Appendix A: Losowość Klasyczna a Kwantowa: Zrozumienie Fundamentalnych Różnic\nAby zrozumieć, jak losowość w mechanice kwantowej różni się od losowości reprezentowanej przez składnik błędu w modelach regresji, musimy przeanalizować ich pochodzenie, naturę i implikacje.\n\n2.10.1 Pochodzenie Losowości\n\n2.10.1.1 Losowość Klasyczna (Modele Regresji)\n\nŹródło: Niekompletna informacja lub złożone interakcje w systemie, który w zasadzie jest deterministyczny.\nNatura: Niepewność epistemiczna (wynikająca z braku wiedzy).\nPrzykład: W modelu regresji, y = β_0 + β_1x + ε, składnik błędu ε reprezentuje niewyjaśnioną zmienność.\n\n\n\n2.10.1.2 Losowość Kwantowa\n\nŹródło: Fundamentalna właściwość systemów kwantowych.\nNatura: Niepewność ontyczna (nieodłączna cecha systemu, nie wynika z braku wiedzy).\nPrzykład: Dokładny moment rozpadu atomu radioaktywnego nie może być przewidziany, można określić jedynie jego prawdopodobieństwo.\n\n\n\n\n2.10.2 Implikacje Filozoficzne\n\n2.10.2.1 Losowość Klasyczna\n\nDeterminizm: Podstawowa rzeczywistość jest deterministyczna; losowość odzwierciedla naszą niewiedzę.\nUkryte Zmienne: W zasadzie, gdybyśmy mieli pełną informację, moglibyśmy dokładnie przewidzieć wyniki.\n\n\n\n2.10.2.2 Losowość Kwantowa\n\nIndeterminizm: Losowość jest fundamentalną cechą rzeczywistości, nie tylko naszego jej opisu.\nBrak Ukrytych Zmiennych: Nawet przy pełnej informacji o systemie kwantowym, niektóre wyniki pozostają nieprzewidywalne (co sugeruje twierdzenie Bella).\n\n\n\n\n2.10.3 Ujęcie Matematyczne\n\n2.10.3.1 Losowość Klasyczna\n\nTeoria Prawdopodobieństwa: Oparta na klasycznej teorii prawdopodobieństwa.\nRozkład: Często zakłada się znane rozkłady (np. rozkład normalny w wielu modelach regresji).\nCentralne Twierdzenie Graniczne: Stosuje się do dużych prób zmiennych losowych.\n\n\n\n2.10.3.2 Losowość Kwantowa\n\nPrawdopodobieństwo Kwantowe: Oparte na matematycznych podstawach mechaniki kwantowej.\nFunkcja Falowa: Opisuje stan kwantowy i jego ewolucję.\nReguła Borna: Określa prawdopodobieństwa wyników pomiarów na podstawie funkcji falowej.\n\n\n\n\n2.10.4 Przewidywalność i Kontrola\n\n2.10.4.1 Losowość Klasyczna\n\nRedukowalna: W zasadzie można ją zmniejszyć, zbierając więcej danych lub poprawiając dokładność pomiarów.\nKontrolowalna: Błędy systematyczne można zidentyfikować i skorygować.\n\n\n\n2.10.4.2 Losowość Kwantowa\n\nNieredukowalna: Nie można jej wyeliminować nawet przy idealnych pomiarach.\nFundamentalnie Niekontrolowalna: Sam akt pomiaru wpływa na system (problem pomiaru).\n\n\n\n\n2.10.5 Praktyczne Implikacje\n\n2.10.5.1 Losowość Klasyczna\n\nRedukcja Błędów: Koncentracja na udoskonalaniu technik pomiarowych i zbierania danych.\nUdoskonalanie Modelu: Dążenie do wyjaśnienia większej wariancji i zmniejszenia składnika błędu.\n\n\n\n2.10.5.2 Losowość Kwantowa\n\nNieodłączne Ograniczenie: Akceptacja fundamentalnych granic przewidywalności.\nPrzewidywania Probabilistyczne: Skupienie na dokładnych rozkładach prawdopodobieństwa zamiast na dokładnych wynikach.\n\n\n\n\n2.10.6 Przykłady Pomagające Zrozumieć Różnicę\n\n2.10.6.1 Przykład Losowości Klasycznej\nWyobraź sobie rzut monetą. Fizyka klasyczna mówi, że wynik jest zdeterminowany przez warunki początkowe (przyłożona siła, opór powietrza itp.). “Losowość” wynika z naszej niezdolności do precyzyjnego zmierzenia i uwzględnienia wszystkich tych czynników.\n\n\n2.10.6.2 Przykład Losowości Kwantowej\nW eksperymencie z podwójną szczeliną pojedyncze cząstki wykazują wzory interferencyjne, jakby przechodziły przez obie szczeliny jednocześnie. Dokładna ścieżka każdej pojedynczej cząstki jest fundamentalnie nieokreślona do momentu pomiaru, a tej nieokreśloności nie można rozwiązać przez bardziej precyzyjne pomiary.\n\n\n\n2.10.7 Podsumowanie\nChociaż oba rodzaje losowości prowadzą do probabilistycznych przewidywań, ich fundamentalne natury są zupełnie różne:\n\nLosowość klasyczna w modelach regresji jest odzwierciedleniem naszej niepełnej wiedzy lub ograniczeń pomiarowych w systemie, który w zasadzie jest deterministyczny.\nLosowość kwantowa jest fundamentalną właściwością systemów kwantowych, reprezentującą nieodłączną nieokreśloność w naturze, która utrzymuje się nawet przy doskonałej wiedzy i pomiarze.\n\nZrozumienie tych różnic jest kluczowe dla prawidłowej interpretacji i stosowania modeli statystycznych w różnych kontekstach naukowych, od nauk społecznych wykorzystujących analizę regresji po eksperymenty z fizyki kwantowej.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-b-duże-modele-językowe---zrozumienie-ich-stochastycznej-natury",
    "href": "rozdzial1.html#appendix-b-duże-modele-językowe---zrozumienie-ich-stochastycznej-natury",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.11 Appendix B: Duże Modele Językowe - Zrozumienie Ich Stochastycznej Natury",
    "text": "2.11 Appendix B: Duże Modele Językowe - Zrozumienie Ich Stochastycznej Natury\nDuże Modele Językowe (LLM), takie jak GPT-3, BERT i Claude, zrewolucjonizowały przetwarzanie języka naturalnego, ale mogą popełniać zagadkowe błędy, szczególnie w zadaniach matematycznych. Ten dodatek wyjaśnia funkcjonowanie LLM, ich stochastyczną naturę i porównuje je z klasycznymi modelami statystycznymi.\n\n2.11.1 Podstawy LLM i Ich Stochastyczna Natura\nLLM są trenowane na ogromnych zbiorach danych tekstowych, aby przewidywać rozkład prawdopodobieństwa następnego tokenu w sekwencji. Wykorzystują architektury transformerowe do przetwarzania i generowania tekstu. Kluczowe aspekty ich stochastycznej natury obejmują:\n\nProbabilistyczny wybór tokenów: LLM wybierają każde słowo na podstawie obliczonych prawdopodobieństw, a nie stałych reguł.\nLosowość kontrolowana temperaturą: Parametr “temperatury” dostosowuje losowość wyborów, równoważąc kreatywność i spójność.\nNiedeterministyczne wyniki: Te same dane wejściowe mogą prowadzić do różnych wyników w oddzielnych uruchomieniach.\nKontekstowa niejednoznaczność: LLM interpretują kontekst probabilistycznie, co czasami prowadzi do nieporozumień.\n\n\n\n2.11.2 Porównanie z Klasycznymi Modelami Statystycznymi\nAby lepiej zrozumieć LLM, porównajmy je z regresją Najmniejszych Kwadratów (OLS):\n\n\n\n\n\n\n\n\nAspekt\nRegresja OLS\nDuże Modele Językowe\n\n\n\n\nPodstawowa funkcja\nPrzewiduje ciągłe wyniki na podstawie zmiennych wejściowych\nPrzewiduje rozkład prawdopodobieństwa następnego tokenu na podstawie poprzednich tokenów\n\n\nWejście-Wyjście\nZmienne ciągłe, relacje liniowe\nDyskretne tokeny, relacje nieliniowe\n\n\nTyp predykcji\nPredykcje punktowe z przedziałami ufności\nRozkłady prawdopodobieństwa dla możliwych tokenów\n\n\nZłożoność modelu\nNiewiele parametrów\nMiliardy parametrów\n\n\nInterpretowalność\nJasne interpretacje współczynników\nLargely nieprzejrzyste działanie wewnętrzne\n\n\nObsługa szumu\nZakłada losowy szum w zmiennej wynikowej\nRadzi sobie ze zmiennością języka naturalnego\n\n\nEkstrapolacja\nMniej wiarygodna poza zakresem treningu\nMniej wiarygodna dla nieznanych tematów\n\n\n\nOba modele dążą do nauczenia się mapowania wejścia-wyjścia na podstawie wzorców w danych treningowych.\n\n\n2.11.3 Implikacje dla Zadań Matematycznych\nStochastyczna natura LLM wpływa na operacje matematyczne:\n\nZmienne wyniki dla powtarzanych obliczeń: Każda próba może dać inny wynik ze względu na probabilistyczny wybór tokenów.\nPewność nie gwarantuje poprawności: Wysoka pewność modelu może wystąpić nawet dla niepoprawnych odpowiedzi.\nAproksymacja zamiast dokładnych obliczeń: LLM dopasowują wzorce zamiast wykonywać precyzyjne obliczenia.\n\nOgraniczenia w zadaniach matematycznych wynikają z:\n\nNiedopasowania celu treningu: LLM są trenowane do przewidywania języka, nie dokładności matematycznej.\nBraku jawnego rozumowania matematycznego: Nie mają wbudowanych reguł czy operacji matematycznych.\nBraku pamięci roboczej: LLM nie mogą niezawodnie przechowywać i manipulować wynikami pośrednimi.\nOgraniczonego okna kontekstowego: Mogą tracić istotne informacje w długich problemach.\nOgraniczeń danych treningowych: Niedoreprezentowanie pewnych koncepcji matematycznych może prowadzić do słabych wyników.\nBraku kontroli spójności: LLM nie weryfikują logicznej spójności swoich wyników.\n\n\n\n2.11.4 Najlepsze Praktyki i Wnioski\nPrzy korzystaniu z LLM do zadań matematycznych:\n\nSkup się na wyjaśnieniach koncepcyjnych, nie na dokładnych obliczeniach: LLM doskonale wyjaśniają koncepcje, ale mogą zawodzić w dokładnych obliczeniach.\nWeryfikuj wyniki dedykowanym oprogramowaniem: Zawsze sprawdzaj obliczenia LLM odpowiednimi narzędziami matematycznymi.\nRozbijaj złożone problemy: Podział zadań na mniejsze kroki może poprawić wydajność LLM.\nBądź świadomy efektów przeformułowania: Różne sformułowania tego samego problemu mogą dawać różne wyniki.\nUżywaj jako narzędzi wspomagających, nie zamienników dla ekspertyzy: LLM powinny uzupełniać, a nie zastępować wiedzę matematyczną.\n\nZrozumienie probabilistycznej natury LLM pomaga wykorzystać ich mocne strony w zadaniach językowych, jednocześnie uznając ich ograniczenia w dziedzinach wymagających deterministycznej precyzji, takich jak matematyka.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-c-modele-deterministyczne-a-modele-stochastyczne",
    "href": "rozdzial1.html#appendix-c-modele-deterministyczne-a-modele-stochastyczne",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.12 Appendix C: Modele Deterministyczne a Modele Stochastyczne (*)",
    "text": "2.12 Appendix C: Modele Deterministyczne a Modele Stochastyczne (*)\n\n2.12.1 Modele Deterministyczne\nModele deterministyczne to te, w których wynik jest w pełni określony przez wartości parametrów i warunki początkowe. Modele te są często używane w fizyce i inżynierii.\n\n\n2.12.2 Przykład: Ruch Jednostajnie Przyspieszony\nKlasycznym przykładem modelu deterministycznego jest ruch jednostajnie przyspieszony, opisany równaniem:\nx(t) = x_0 + v_0t + \\frac{1}{2}at^2\nGdzie:\n\nx(t) to położenie w czasie t\nx_0 to położenie początkowe\nv_0 to prędkość początkowa\na to przyspieszenie\nt to czas\n\nZasymulujmy to w R:\n\n# Ruch jednostajnie przyspieszony\nsymuluj_ruch_przyspieszony &lt;- function(x0, v0, a, t) {\n  x0 + v0 * t + 0.5 * a * t^2\n}\n\n# Generowanie danych\nt &lt;- seq(0, 10, by = 0.1)\nx &lt;- symuluj_ruch_przyspieszony(x0 = 0, v0 = 2, a = 1, t = t)\n\n# Wykres\nplot(t, x, type = \"l\", xlab = \"Czas\", ylab = \"Położenie\", \n     main = \"Ruch Jednostajnie Przyspieszony\")\n\n\n\n\n\n\n\n\nTen kod wygeneruje wykres ruchu jednostajnie przyspieszonego, który jest intuicyjnym przykładem z dynamiki Newtona. W tym przypadku obiekt zaczyna ruch z początkową prędkością i przyspiesza jednostajnie, co prowadzi do parabolicznej trajektorii na wykresie położenia w funkcji czasu.\n\n\n2.12.3 Modele Stochastyczne w Naukach Społecznych\nModele stochastyczne uwzględniają losowość i są często używane w naukach społecznych, gdzie istnieje nieodłączna niepewność w badanych systemach.\n\n\n2.12.4 Przykład: Regresja Metodą Najmniejszych Kwadratów (OLS)\nOLS to podstawowy model stochastyczny w naukach społecznych. Jest reprezentowany jako:\nY = \\beta_0 + \\beta_1X + \\epsilon\nGdzie:\n\nY to zmienna zależna\nX to zmienna niezależna\n\\beta_0 i \\beta_1 to parametry\n\\epsilon to składnik błędu (komponent stochastyczny)\n\nZademonstrujmy OLS w R:\n\n# Generowanie przykładowych danych\nset.seed(123)\nX &lt;- rnorm(100)\nY &lt;- 2 + 3*X + rnorm(100, sd = 0.5)\n\n# Dopasowanie modelu OLS\nmodel &lt;- lm(Y ~ X)\n\n# Podsumowanie modelu\nsummary(model)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95367 -0.34175 -0.04375  0.29032  1.64520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.94860    0.04878   39.95   &lt;2e-16 ***\nX            2.97376    0.05344   55.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4854 on 98 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.969 \nF-statistic:  3097 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n# Wykres\nplot(X, Y, main = \"Regresja OLS\")\nabline(model, col = \"red\")\n\n\n\n\n\n\n\n\nTo dopasuje model OLS do symulowanych danych i wykreśli wyniki.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff\n\n\n\n\n2.12.5 Zaawansowane Modele Stochastyczne: Duże Modele Językowe\nDuże Modele Językowe (LLM), takie jak GPT-3, to złożone modele stochastyczne używane w przetwarzaniu języka naturalnego. Chociaż nie możemy zaimplementować pełnego LLM w tym tutorialu, możemy omówić jego zasady.\nLLM opierają się na architekturze transformatora i wykorzystują mechanizmy samouwagi. Są trenowane na ogromnych ilościach danych tekstowych i uczą się przewidywać następny token w sekwencji.\nRdzeń LLM można postrzegać jako warunkowy rozkład prawdopodobieństwa:\nP(x_t | x_{&lt;t}, \\theta)\nGdzie:\n\nx_t to aktualny token\nx_{&lt;t} reprezentuje wszystkie poprzednie tokeny\n\\theta to parametry modelu\n\n\n\n\n\n\n\nNote\n\n\n\nTokeny w Dużych Modelach Językowych (LLM) to podstawowe jednostki tekstu, które model przetwarza. Można je postrzegać jako części słów lub znaki interpunkcyjne. Oto kluczowe informacje o tokenach:\nDefinicja: Tokeny to najmniejsze jednostki tekstu, które LLM przetwarza. Mogą to być całe słowa, części słów, a nawet pojedyncze znaki lub znaki interpunkcyjne. Tokenizacja: Proces dzielenia tekstu na tokeny nazywa się tokenizacją. LLM używają specyficznych algorytmów do wykonania tego zadania. Przykłady:\nSłowo “kot” może być pojedynczym tokenem. Dłuższe słowo jak “zrozumienie” może być podzielone na wiele tokenów, np. “zrozum” i “ienie”. Znaki interpunkcyjne jak “.” czy “?” są często oddzielnymi tokenami. Powszechne przedrostki lub przyrostki mogą być własnymi tokenami.\nSłownictwo: LLM mają ustalone słownictwo tokenów, które rozpoznają. To słownictwo zazwyczaj obejmuje od dziesiątek tysięcy do setek tysięcy tokenów. Znaczenie: Sposób tokenizacji tekstu może wpływać na to, jak model rozumie i generuje język. Jest to szczególnie ważne przy obsłudze różnych języków, rzadkich słów lub specjalistycznego słownictwa. Kontekst: W równaniu dla LLM: P(x_t | x_{&lt;t}, \\theta) Gdzie:\nx_t reprezentuje bieżący token x_{&lt;t} reprezentuje wszystkie poprzednie tokeny w sekwencji \\theta reprezentuje parametry modelu\n\n\nW przeciwieństwie do modeli deterministycznych, LLM produkują różne wyniki nawet dla tego samego wejścia ze względu na ich stochastyczną naturę.\n\n\n2.12.6 Podsumowanie\nKażdy rodzaj modelu ma swoje miejsce w nauce, w zależności od badanego systemu i poziomu niepewności.\nPamiętaj, że wybór między modelami deterministycznymi a stochastycznymi często zależy od natury badanego systemu i pytań, na które próbujesz odpowiedzieć. Modele deterministyczne są świetne dla systemów o dobrze zrozumiałej mechanice, podczas gdy modele stochastyczne sprawdzają się przy radzeniu sobie z nieodłączną losowością lub złożonymi, nie w pełni zrozumiałymi systemami.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-d-wprowadzenie-do-r-rstudio-i-tidyverse",
    "href": "rozdzial1.html#appendix-d-wprowadzenie-do-r-rstudio-i-tidyverse",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.13 Appendix D: Wprowadzenie do R, RStudio i tidyverse",
    "text": "2.13 Appendix D: Wprowadzenie do R, RStudio i tidyverse\nR to potężny język programowania i środowisko do obliczeń statystycznych i grafiki. Jest szeroko stosowany w środowisku akademickim, szczególnie w naukach społecznych, do analizy danych i wizualizacji.\n\n2.13.0.1 Kluczowe cechy R:\n\nOtwarty kod źródłowy i darmowy\nRozbudowany ekosystem pakietów\nSilne wsparcie społeczności\nDoskonały do analizy statystycznej i wizualizacji danych\n\n\n\n2.13.1 Pierwsze kroki z RStudio\nRStudio to zintegrowane środowisko programistyczne (IDE) dla R, które ułatwia pracę z R.\n\n2.13.1.1 Instalacja R i RStudio\n\nPobierz i zainstaluj R ze strony CRAN\nPobierz i zainstaluj RStudio ze strony RStudio\n\n\n\n2.13.1.2 Interfejs RStudio\nRStudio ma cztery główne panele:\n\nEdytor źródłowy: Gdzie piszesz i edytujesz skrypty R\nKonsola: Gdzie możesz wpisywać polecenia R i widzieć wyniki\nŚrodowisko/Historia: Pokazuje wszystkie obiekty w twoim obszarze roboczym i historię poleceń\nPliki/Wykresy/Pakiety/Pomoc: Wielofunkcyjny panel do zarządzania plikami, przeglądania wykresów, zarządzania pakietami i dostępu do pomocy\n\n\n\n2.13.1.3 Podstawowe funkcje RStudio\n\nTworzenie nowego skryptu R: Plik &gt; Nowy plik &gt; Skrypt R\nUruchamianie kodu: Zaznacz kod i naciśnij Ctrl+Enter (Cmd+Enter na Macu)\nInstalowanie pakietów: Narzędzia &gt; Instaluj pakiety\nUzyskiwanie pomocy: Wpisz ?nazwa_funkcji w konsoli\n\n\n\n\n2.13.2 Podstawy R\n\n2.13.2.1 Typy danych w R\n\n# Numeryczny\nx &lt;- 10.5\nclass(x)\n\n[1] \"numeric\"\n\n# Całkowity\ny &lt;- 1L\nclass(y)\n\n[1] \"integer\"\n\n# Znakowy\nimie &lt;- \"Alicja\"\nclass(imie)\n\n[1] \"character\"\n\n# Logiczny\njest_studentem &lt;- TRUE\nclass(jest_studentem)\n\n[1] \"logical\"\n\n\n\n\n2.13.2.2 Struktury danych\n\n2.13.2.2.1 Wektory\n\n# Tworzenie wektora\nliczby &lt;- c(1, 2, 3, 4, 5)\nowoce &lt;- c(\"jabłko\", \"banan\", \"wiśnia\")\n\n# Operacje na wektorach\nliczby + 2\n\n[1] 3 4 5 6 7\n\nliczby * 2\n\n[1]  2  4  6  8 10\n\nmean(liczby)\n\n[1] 3\n\nlength(owoce)\n\n[1] 3\n\n\n\n\n2.13.2.2.2 Macierze\n\n# Tworzenie macierzy\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\nprint(m)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n# Operacje na macierzach\nt(m)  # transpozycja\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nm * 2  # mnożenie skalarne\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    4    8   12\n\n\n\n\n2.13.2.2.3 Ramki danych\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(\n  imie = c(\"Alicja\", \"Bartek\", \"Celina\"),\n  wiek = c(25, 30, 35),\n  student = c(TRUE, FALSE, TRUE)\n)\nprint(df)\n\n    imie wiek student\n1 Alicja   25    TRUE\n2 Bartek   30   FALSE\n3 Celina   35    TRUE\n\n# Dostęp do elementów ramki danych\ndf$imie\n\n[1] \"Alicja\" \"Bartek\" \"Celina\"\n\ndf[1, 2]\n\n[1] 25\n\ndf[df$wiek &gt; 25, ]\n\n    imie wiek student\n2 Bartek   30   FALSE\n3 Celina   35    TRUE\n\n\n\n\n\n2.13.2.3 Funkcje\n\n# Definiowanie funkcji\npowitaj &lt;- function(imie) {\n  paste(\"Cześć,\", imie, \"!\")\n}\n\n# Użycie funkcji\npowitaj(\"Alicja\")\n\n[1] \"Cześć, Alicja !\"\n\n# Funkcja z wieloma argumentami\noblicz_bmi &lt;- function(waga, wzrost) {\n  bmi &lt;- waga / (wzrost^2)\n  return(bmi)\n}\n\noblicz_bmi(70, 1.75)\n\n[1] 22.85714\n\n\n\n\n2.13.2.4 Struktury kontrolne\n\n# Instrukcja if-else\nx &lt;- 10\nif (x &gt; 5) {\n  print(\"x jest większe niż 5\")\n} else {\n  print(\"x nie jest większe niż 5\")\n}\n\n[1] \"x jest większe niż 5\"\n\n# Pętla for\nfor (i in 1:5) {\n  print(paste(\"Iteracja\", i))\n}\n\n[1] \"Iteracja 1\"\n[1] \"Iteracja 2\"\n[1] \"Iteracja 3\"\n[1] \"Iteracja 4\"\n[1] \"Iteracja 5\"\n\n# Pętla while\nlicznik &lt;- 1\nwhile (licznik &lt;= 5) {\n  print(paste(\"Licznik:\", licznik))\n  licznik &lt;- licznik + 1\n}\n\n[1] \"Licznik: 1\"\n[1] \"Licznik: 2\"\n[1] \"Licznik: 3\"\n[1] \"Licznik: 4\"\n[1] \"Licznik: 5\"\n\n\n\n\n\n2.13.3 Wprowadzenie do tidyverse\nTidyverse to kolekcja pakietów R zaprojektowanych do nauki o danych. Te pakiety mają wspólną filozofię i są zaprojektowane do bezproblemowej współpracy.\n\n2.13.3.1 Kluczowe pakiety tidyverse\n\nggplot2: do wizualizacji danych\ndplyr: do manipulacji danymi\ntidyr: do porządkowania danych\nreadr: do odczytu danych prostokątnych\npurrr: do programowania funkcyjnego\ntibble: nowoczesne ujęcie ramek danych\n\n\n\n2.13.3.2 Rozpoczęcie pracy z tidyverse\n\n# Instalacja tidyverse (uruchom raz)\n# install.packages(\"tidyverse\")\n\n# Wczytanie tidyverse\nlibrary(tidyverse)\n\n\n\n2.13.3.3 Import danych z readr\n\n# Odczyt plików CSV\ndane &lt;- read_csv(\"dane_spoleczne.csv\")\n\n# Odczyt innych formatów plików\nread_tsv(\"dane.tsv\")  # Wartości oddzielone tabulatorem\nread_delim(\"dane.txt\", delim = \"|\")  # Niestandardowy separator\n\n\n\n2.13.3.4 Manipulacja danymi z dplyr\n\n# Użyjmy wbudowanego zbioru danych mtcars\ndata(\"mtcars\")\n\n# Wybieranie kolumn\nmtcars %&gt;% \n  select(mpg, cyl, hp)\n\n                     mpg cyl  hp\nMazda RX4           21.0   6 110\nMazda RX4 Wag       21.0   6 110\nDatsun 710          22.8   4  93\nHornet 4 Drive      21.4   6 110\nHornet Sportabout   18.7   8 175\nValiant             18.1   6 105\nDuster 360          14.3   8 245\nMerc 240D           24.4   4  62\nMerc 230            22.8   4  95\nMerc 280            19.2   6 123\nMerc 280C           17.8   6 123\nMerc 450SE          16.4   8 180\nMerc 450SL          17.3   8 180\nMerc 450SLC         15.2   8 180\nCadillac Fleetwood  10.4   8 205\nLincoln Continental 10.4   8 215\nChrysler Imperial   14.7   8 230\nFiat 128            32.4   4  66\nHonda Civic         30.4   4  52\nToyota Corolla      33.9   4  65\nToyota Corona       21.5   4  97\nDodge Challenger    15.5   8 150\nAMC Javelin         15.2   8 150\nCamaro Z28          13.3   8 245\nPontiac Firebird    19.2   8 175\nFiat X1-9           27.3   4  66\nPorsche 914-2       26.0   4  91\nLotus Europa        30.4   4 113\nFord Pantera L      15.8   8 264\nFerrari Dino        19.7   6 175\nMaserati Bora       15.0   8 335\nVolvo 142E          21.4   4 109\n\n# Filtrowanie wierszy\nmtcars %&gt;% \n  filter(cyl == 4)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n# Sortowanie danych\nmtcars %&gt;% \n  arrange(desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n# Tworzenie nowych zmiennych\nmtcars %&gt;% \n  mutate(kpl = mpg * 0.425)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb     kpl\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  8.9250\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  8.9250\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1  9.6900\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  9.0950\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  7.9475\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  7.6925\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  6.0775\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 10.3700\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  9.6900\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  8.1600\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  7.5650\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  6.9700\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  7.3525\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  6.4600\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  4.4200\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  4.4200\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  6.2475\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 13.7700\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 12.9200\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 14.4075\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  9.1375\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  6.5875\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  6.4600\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  5.6525\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  8.1600\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 11.6025\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 11.0500\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 12.9200\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  6.7150\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  8.3725\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  6.3750\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2  9.0950\n\n# Podsumowywanie danych\nmtcars %&gt;% \n  group_by(cyl) %&gt;% \n  summarize(srednie_mpg = mean(mpg),\n            liczba = n())\n\n# A tibble: 3 × 3\n    cyl srednie_mpg liczba\n  &lt;dbl&gt;       &lt;dbl&gt;  &lt;int&gt;\n1     4        26.7     11\n2     6        19.7      7\n3     8        15.1     14\n\n\n\n\n2.13.3.5 Wizualizacja danych z ggplot2\n\n# Wykres rozrzutu\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Waga samochodu vs. Zużycie paliwa\",\n       x = \"Waga (1000 funtów)\",\n       y = \"Mile na galon\")\n\n\n\n\nWaga samochodu vs. Zużycie paliwa\n\n\n\n\n\n# Wykres słupkowy\nmtcars %&gt;% \n  count(cyl) %&gt;% \n  ggplot(aes(x = factor(cyl), y = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Liczba samochodów według liczby cylindrów\",\n       x = \"Liczba cylindrów\",\n       y = \"Liczba\")\n\n\n\n\nLiczba samochodów według liczby cylindrów\n\n\n\n\n\n# Wykres pudełkowy\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Zużycie paliwa według liczby cylindrów\",\n       x = \"Liczba cylindrów\",\n       y = \"Mile na galon\")\n\n\n\n\nZużycie paliwa według liczby cylindrów\n\n\n\n\n\n\n\n2.13.4 Dodatkowe zasoby\n\nR for Data Science\nDokumentacja tidyverse\nŚciągawki RStudio\nPrzewodnik Quarto\nR Cookbook\n\nPamiętaj, aby eksperymentować z kodem, modyfikować przykłady i nie wahaj się korzystać z wbudowanego systemu pomocy R (dostępnego przez wpisanie ?nazwa_funkcji w konsoli), gdy napotkasz nieznane funkcje lub koncepcje.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1 Foundations in Number Sets\nBefore diving into data types, it’s essential to understand the basic number sets that form the foundation of our understanding of data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#foundations-in-number-sets",
    "href": "chapter2.html#foundations-in-number-sets",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1.1 Basic Number Sets\n\nNatural Numbers (ℕ): The counting numbers {0, 1, 2, 3, …}\nIntegers (ℤ): Includes natural numbers, their negatives, and zero {…, -2, -1, 0, 1, 2, …}\nRational Numbers (ℚ): Numbers that can be expressed as a fraction of two integers\nReal Numbers (ℝ): All numbers on the number line, including rationals and irrationals\n\n\n\n3.1.2 Properties of Sets\n\nCountable Sets: Sets whose elements can be put in a one-to-one correspondence with the natural numbers. For example, the set of integers is countable.\nUncountable Sets: Sets that are not countable. The set of real numbers is uncountable.\nDiscrete Sets: Sets where each element is separated from other elements by a finite gap. The integers form a discrete set.\nDense Sets: Sets where between any two elements, there is always another element of the set. The rational numbers and real numbers are dense sets.\n\n\n\n\n\n\n\nNote\n\n\n\nUnderstanding these set properties is crucial for grasping the nature of different data types in social sciences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#discrete-vs.-continuous-data",
    "href": "chapter2.html#discrete-vs.-continuous-data",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.2 Discrete vs. Continuous Data",
    "text": "3.2 Discrete vs. Continuous Data\nIn data science and statistics, we often categorize variables as either discrete or continuous. However, the distinction is not always clear-cut, and some variables exhibit characteristics of both types. This section explores the concepts of discrete and continuous data, their differences, and the interesting cases of variables that can be treated as both or challenge our intuitive understanding.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n3.2.1 Discrete Data\nDiscrete data can only take on specific, countable values. These values are often (but not always) integers.\n\n3.2.1.1 Characteristics of Discrete Data:\n\nCountable\nOften represented by integers\nCan be finite or infinite\nNo values between two adjacent data points\n\n\n\n3.2.1.2 Examples:\n\nNumber of students in a class\nNumber of cars sold by a dealership\nShoe sizes\n\n\n\n\n3.2.2 Continuous Data\nContinuous data can take on any value within a given range, including fractional and decimal values. It’s important to note that continuity is not solely determined by uncountability, but also by density.\n\n3.2.2.1 Characteristics of Continuous Data:\n\nCan be uncountable (like real numbers) or dense (like rational numbers)\nCan be measured to any level of precision (theoretically)\nRepresented by real numbers or dense subsets of real numbers\nThere are always values between any two data points\n\n\n\n3.2.2.2 Examples:\n\nHeight\nWeight\nTemperature\nPercentages (explained further below)\n\n\n\n\n3.2.3 The Discrete-Continuous Spectrum\nIn practice, some variables that are mathematically discrete are often treated as if they are continuous. This dual nature provides flexibility in how these variables can be analyzed and interpreted.\n\n3.2.3.1 Reasons for Treating Discrete Data as Continuous:\n\nDense Granularity\n\nWhen a discrete variable has a large number of possible values within a range, it can approximate continuity.\nExample: Income measured in individual cents. While technically discrete, the large number of possible values makes it behave similarly to a continuous variable.\n\nAnalytical Convenience\n\nContinuous methods often yield reasonable and useful results even for dense discrete variables.\nIt’s often easier to use existing statistical tools if continuity is assumed, as this allows the use of calculus-based methods.\n\nApproximation of Underlying Phenomena\n\nIn some cases, a discrete measurement might be an approximation of an underlying continuous process.\nExample: While we measure time in discrete units (seconds, minutes, hours), time itself is continuous.\n\n\n\n\n3.2.3.2 Examples of Variables with Dual Discrete-Continuous Nature:\n\nAge\n\nDiscrete: Typically measured in whole years\nContinuous: Can be considered as a continuous variable in many analyses, especially when dealing with large populations\n\nPrice and Income\n\nDiscrete: Prices and incomes are actually measured in discrete units (e.g., cents or smallest currency unit)\nContinuous: In economic models and many analyses, prices and incomes are treated as continuous variables due to their dense nature and analytical convenience\n\nTest Scores\n\nDiscrete: Often given as whole numbers\nContinuous: In statistical analyses, test scores might be treated as continuous, especially when the range of possible scores is large\n\n\n\n\n\n3.2.4 Special Case: Percentages and Rational Numbers\nPercentages present an interesting case in the discrete-continuous spectrum:\n\nRational Nature: Percentages are essentially fractions (m/100), making them rational numbers.\nDense but Countable: The set of rational numbers is dense (between any two rationals, there’s another rational) but also countable.\nPractical Continuity: In most practical applications, percentages are treated as continuous due to their dense nature.\nFinite Precision: In reality, percentages are often reported to a limited number of decimal places, creating a finite set of possible values.\n\n\n\n\n\n\n\nPercentages: Bridging Discrete and Continuous\n\n\n\nVariables measured in percentages, such as unemployment rates or voter turnout, challenge our intuitive understanding of discreteness and continuity:\n\nThey are rational numbers (fractions with denominator 100), which are technically countable.\nThey form a dense set within their range (0% to 100%), allowing for values between any two percentages.\nIn practice, they are often treated as continuous variables due to their dense nature and analytical convenience.\nThe precision of measurement (e.g., reporting to one or two decimal places) can impose a discrete structure on what is conceptually a dense set.\n\nThis duality allows for flexible analytical approaches, depending on the specific research context and required precision.\n\n\n\n\n3.2.5 Implications for Data Analysis\nUnderstanding the nuanced nature of variables as discrete, continuous, or somewhere in between has important implications for data analysis:\n\nFlexibility in Modeling: It allows for the use of a wider range of statistical techniques.\nSimplified Calculations: Treating dense discrete data as continuous can simplify calculations and make certain analyses more tractable.\nImproved Interpretability: In some cases, treating discrete data as continuous can lead to more intuitive or useful interpretations of results.\nPotential for Error: It’s important to be aware of when approximations are appropriate and when they might lead to misleading results.\nTheoretical vs. Practical Considerations: While the mathematical nature of the data is important, practical considerations in measurement and analysis often guide how we treat variables.\n\n\n\n3.2.6 Conclusion\nThe distinction between discrete and continuous data is not always rigid in social sciences. Many variables, including those involving money, percentages, or dense measurements, can be viewed through both discrete and continuous lenses. The choice of treatment should be guided by the nature of the data, the goals of the analysis, and the potential implications of the choice. This flexibility, when used thoughtfully, provides powerful tools for social science researchers to gain insights from their data.\n\n\n\n\n\n\nDiscrete vs. Continuous Numerical Data: A Language-Based Analogies\n\n\n\n\n3.2.6.1 The Language Connection\nThink about how you naturally ask questions about quantities:\n\n“How many cookies are in the jar?” (counting)\n“How much water is in the glass?” (measuring)\n\nThis natural language distinction reflects the two fundamental types of numerical data:\n\n\n3.2.6.2 Discrete Data = “How Many?” Questions\n\nLike counting whole objects (countable nouns)\nTakes specific values with gaps between them\nExamples:\n\nNumber of pets: 0, 1, 2, 3… (can’t have 2.5 pets)\nDice rolls: 1, 2, 3, 4, 5, 6\nStudents in a class: 20, 21, 22…\n\n\n🤔 Self-Check: Can you find a value between 2 and 3 students? Why not?\n\n\n3.2.6.3 Continuous Data = “How Much?” Questions\n\nLike measuring quantities (uncountable nouns)\nCan take any value within a range\nExamples:\n\nHeight: 1.7231… meters\nTemperature: 36.8325… °C\nTime: 3.5792… hours\n\n\n🤔 Self-Check: Write down three different values between 1.72 and 1.73 meters\n\n\n3.2.6.4 Quick Recognition Guide\n\nIf you naturally ask “How many?” → Discrete\nIf you naturally ask “How much?” → Continuous\nIf you can measure it more precisely → Continuous\nIf you can only use whole numbers → Discrete\n\n✍️ Practice: Classify these quantities as discrete or continuous\n\nYour age in years: _____\nYour height: _____\nNumber of songs in a playlist: _____\nVolume of water: _____\n\n\n\n\n\n\n3.2.7 R Code Example\nHere’s a simple R code example to illustrate how we might analyze variables treated as continuous:\n\n# Generate some sample data\nset.seed(123)\nages &lt;- round(runif(1000, min = 18, max = 80))\nunemployment_rates &lt;- round(runif(1000, min = 3, max = 10), 1)\n\n# Compare means and medians\ncat(\"Mean age:\", mean(ages), \"\\n\")\n\nMean age: 48.848 \n\ncat(\"Median age:\", median(ages), \"\\n\")\n\nMedian age: 48 \n\ncat(\"Mean unemployment rate:\", mean(unemployment_rates), \"\\n\")\n\nMean unemployment rate: 6.4871 \n\ncat(\"Median unemployment rate:\", median(unemployment_rates), \"\\n\")\n\nMedian unemployment rate: 6.5 \n\n# Linear regression (treating both as continuous)\nincome &lt;- 20000 + 500 * ages + 1000 * unemployment_rates + rnorm(1000, 0, 5000)\nmodel &lt;- lm(income ~ ages + unemployment_rates)\nsummary(model)\n\n\nCall:\nlm(formula = income ~ ages + unemployment_rates)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15354  -3471     54   3485  16684 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        20116.194    717.231   28.05   &lt;2e-16 ***\nages                 503.385      8.983   56.04   &lt;2e-16 ***\nunemployment_rates   989.333     80.004   12.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5053 on 997 degrees of freedom\nMultiple R-squared:  0.7637,    Adjusted R-squared:  0.7632 \nF-statistic:  1611 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n# Plot regression plane\nlibrary(plotly)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nplot_ly(x = ages, y = unemployment_rates, z = income, type = \"scatter3d\", mode = \"markers\") %&gt;%\n  add_trace(x = ages, y = unemployment_rates, z = fitted(model), type = \"scatter3d\", mode = \"lines\")\n\n\n\n\n\nThe 3D plot illustrates how both variables can be treated as continuous in a regression model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#introduction-to-stevens-data-typology",
    "href": "chapter2.html#introduction-to-stevens-data-typology",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.3 Introduction to Stevens’ Data Typology",
    "text": "3.3 Introduction to Stevens’ Data Typology\nStanley S. Stevens, an American psychologist, introduced a classification system for scales of measurement in his 1946 paper “On the Theory of Scales of Measurement.” This system, known as Stevens’ data typology or levels of measurement, has become fundamental in understanding how different types of data should be analyzed and interpreted.\nStevens proposed four levels of measurement:\n\nNominal\nOrdinal\nInterval\nRatio\n\nEach level has specific properties and allows for different types of statistical operations and analyses.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n3.3.1 Nominal Scale\n\n3.3.1.1 Definition\nThe nominal scale is the most basic level of measurement. It uses labels or categories to classify data without any quantitative value or order.\n\n\n3.3.1.2 Properties\n\nCategories are mutually exclusive\nNo inherent order among categories\nNo meaningful arithmetic operations can be performed\n\n\n\n3.3.1.3 Examples\n\nNationality (Polish, English, …)\nBlood types (A, B, AB, O)\nEye color (Blue, Brown, Green, Hazel)\nBinary variables (“Success” versus “Failure”)\n\n\n\n\n3.3.2 Ordinal Scale\n\n3.3.2.1 Definition\nThe ordinal scale categorizes data into ordered categories, but the intervals between categories are not necessarily equal or meaningful.\n\n\n3.3.2.2 Properties\n\nCategories have a defined order\nDifferences between categories are not quantifiable\nArithmetic operations on the numbers are not meaningful\n\n\n\n3.3.2.3 Examples\n\nEducation levels (High School, Bachelor’s, Master’s, PhD)\nLikert scales (Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree)\nSocioeconomic status (Low, Medium, High)\n\n\n\n\n3.3.3 Interval Scale\n\n3.3.3.1 Definition\nThe interval scale has ordered categories with equal intervals between adjacent categories. However, it lacks a true zero point.\n\n\n3.3.3.2 Properties\n\nEqual intervals between adjacent categories\nNo true zero point (zero is arbitrary)\nRatios between values are not meaningful\n\n\n\n3.3.3.3 Examples\n\nTemperature in Celsius or Fahrenheit\nCalendar years\npH scale (the difference between pH 4 and 5 represents the same change in hydrogen ion concentration as between pH 6 and 7)\nElevation above sea level\n\n\n\n\n3.3.4 Ratio Scale\n\n3.3.4.1 Definition\nThe ratio scale is the highest level of measurement. It has all the properties of the interval scale plus a true zero point, making ratios between values meaningful.\n\n\n3.3.4.2 Properties\n\nAll properties of interval scales\nTrue zero point\nRatios between values are meaningful\n\n\n\n3.3.4.3 Examples\n\nHeight\nWeight\nAge\nIncome\n\n\n\n\n\n\n\nUnderstanding Operations with Data Types\n\n\n\n\n3.3.5 Fundamental Concepts\n\n3.3.5.1 Temperature Scale Conversions\nBasic conversion formula: F = \\frac{9}{5}C + 32\n\n\n3.3.5.2 Data Type Classifications\n\nInterval Data (e.g., temperature): Has equal intervals but no true zero\nRatio Data (e.g., length): Has equal intervals and a true zero point\n\n\n\n\n3.3.6 Operations Analysis\n\n3.3.6.1 Differences (Works for Both Types)\n\n3.3.6.1.1 Temperature Proof\nFor any two temperatures C_1 and C_2: * F_1 = \\frac{9}{5}C_1 + 32 * F_2 = \\frac{9}{5}C_2 + 32\nTemperature difference: * \\Delta F = F_2 - F_1 * = (\\frac{9}{5}C_2 + 32) - (\\frac{9}{5}C_1 + 32) * = \\frac{9}{5}(C_2 - C_1) * = \\frac{9}{5}\\Delta C\n\n\n3.3.6.1.2 Why Differences Work\n\nInterval Data: Constant terms (like +32) cancel out\nRatio Data: Conversion factors preserve both differences and ratios\n\nExample: 20m - 10m = 10m converts to 65.6ft - 32.8ft = 32.8ft\nRatio preservation: 20m/10m = 65.6ft/32.8ft = 2\n\n\n\n\n\n3.3.6.2 Multiplication (Only Works for Ratio Data)\n\n3.3.6.2.1 Temperature Example (Fails)\nMethod A: Convert after multiplication\n\n10°C × 2 = 20°C → 68°F\n\nMethod B: Convert before multiplication\n\n10°C → 50°F → 100°F\n\nResults differ: 68°F ≠ 100°F\n\n\n3.3.6.2.2 Length Example (Works)\nMethod A: Convert after multiplication\n\n10m × 2 = 20m → 65.6ft\n\nMethod B: Convert before multiplication\n\n10m → 32.8ft × 2 = 65.6ft\n\nResults match: 65.6ft = 65.6ft\n\n\n\n\n3.3.7 Statistical Measures\n\n3.3.7.1 Mean (Works for Both Types)\n\nUses only differences\nTemperature example:\n\nCelsius: (10°C + 20°C + 30°C)/3 = 20°C\nFahrenheit: (50°F + 68°F + 86°F)/3 = 68°F\n\nMaintains proportional relationships\n\n\n\n3.3.7.2 Variance (Only Works for Ratio Data)\n\n3.3.7.2.1 Temperature Example (Fails)\nCelsius calculation:\n\nValues: 10°C, 20°C, 30°C\nMean = 20°C\nVariance = 66.67°C²\n\nFahrenheit calculation:\n\nValues: 50°F, 68°F, 86°F\nMean = 68°F\nVariance = 216°F²\n\nProblem: No consistent relationship between variances\n\n\n3.3.7.2.2 Length Example (Works)\nMeters:\n\nValues: 1.5m, 1.6m, 1.7m\nVariance = 0.01m²\n\nFeet:\n\nValues: 4.92ft, 5.25ft, 5.58ft\nVariance = 0.108ft²\n\nRelationship preserved: 0.108ft² = 0.01m² × (3.28084)²\n\n\n\n\n3.3.8 Key Principles\n\n3.3.8.1 Differences\n\nWork for both types because equal steps remain equal after conversion\nConstant terms cancel out\n\n\n\n3.3.8.2 Multiplication\n\nOnly works for ratio data because it requires true zero point\nMust preserve proportional relationships\n\n\n\n3.3.8.3 Statistical Measures\n\nMean: Valid for both types (uses differences)\nVariance: Only valid for ratio data (uses squares)\n\n\n\n3.3.8.4 Unit Conversions\n\nInterval: Preserves differences but not ratios\nRatio: Preserves both differences and ratios\n\n\n\n\n\n\n\n\n3.3.9 Importance in Research and Analysis\nUnderstanding Stevens’ data typology is crucial for several reasons:\n\nChoosing appropriate statistical tests: The level of measurement determines which statistical analyses are appropriate for a given dataset.\nInterpreting results: The meaning of statistical results depends on the level of measurement of the variables involved.\nDesigning measurement instruments: When creating surveys or other measurement tools, researchers must consider the level of measurement they want to achieve.\nData transformation: Sometimes, data can be transformed from one level to another, but this must be done carefully to avoid misinterpretation.\n\n\n\n3.3.10 Controversies and Limitations\nWhile Stevens’ typology is widely used, it has faced some criticisms:\n\nRigidity: Some argue that the typology is too rigid and that many real-world measurements fall between these categories.\nTreatment of ordinal data: There’s ongoing debate about when it’s appropriate to treat ordinal data as interval for certain analyses.\nPsychological scaling: Some psychological constructs (like intelligence) are difficult to categorize definitively within this system.\n\n\n\n3.3.11 Conclusion\nStevens’ data typology provides a fundamental framework for understanding different types of data and their properties. By recognizing the level of measurement of their variables, researchers can make informed decisions about data collection, analysis, and interpretation. However, it’s important to remember that while this typology is a useful guide, real-world data often requires nuanced consideration and may not always fit neatly into these categories.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not “twice as acidic” as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "href": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.4 Common Ordinal Scales in Behavioural Research",
    "text": "3.4 Common Ordinal Scales in Behavioural Research\n\n3.4.1 Likert Scales\nLikert scales are widely used in psychology and social sciences to measure attitudes, opinions, and perceptions. Named after psychologist Rensis Likert, these scales typically consist of a series of statements or questions that respondents rate on a scale, often from “Strongly Disagree” to “Strongly Agree.”\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n3.4.1.1 Why Likert Scales are Ordinal Variables\nLikert scales are considered ordinal variables for several reasons:\n\nOrder without equal intervals: While the responses have a clear order (e.g., “Strongly Disagree” &lt; “Disagree” &lt; “Neutral” &lt; “Agree” &lt; “Strongly Agree”), the intervals between these categories are not necessarily equal.\nSubjective interpretation: The difference between “Strongly Disagree” and “Disagree” may not be the same as the difference between “Agree” and “Strongly Agree” for all respondents.\nLack of true zero point: Likert scales typically don’t have a true zero point, which is a characteristic of interval or ratio scales.\n\n\n\n\n3.4.2 IQ and Other Psychological Variables as Ordinal Measures\nMany psychological measures, including IQ, are often treated as interval scales but are, in fact, ordinal. Here’s why:\n\nIQ Scores:\n\nWhile IQ scores are presented as numbers, the difference between an IQ of 100 and 110 may not represent the same cognitive difference as between 130 and 140.\nThe scale is normalized and adjusted over time, making it difficult to claim true interval properties.\n\nOther Psychological Measures:\n\nDepression scales (e.g., Beck Depression Inventory)\nAnxiety measures (e.g., State-Trait Anxiety Inventory)\nPersonality assessments (e.g., Big Five Inventory)\n\n\nThese measures often use summed Likert-type items or other scoring methods that don’t guarantee equal intervals between scores.\n\n\n3.4.3 Implications for Analysis\nRecognizing these measures as ordinal has important implications for data analysis:\n\nAppropriate statistical tests: Use non-parametric tests (e.g., Mann-Whitney U, Kruskal-Wallis) instead of parametric ones.\nCorrelation analysis: Use Spearman’s rank correlation instead of Pearson’s correlation.\nCentral tendency: Report median and mode rather than mean.\nData visualization: Use methods appropriate for ordinal data, such as bar plots or stacked bar charts.\n\n\n\n3.4.4 Conclusion\nWhile Likert scales and many behavioural measures are often treated as interval data for practical reasons, it’s crucial to remember their ordinal nature.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\nExercise: Identifying Measurement Scales\n\n\n\nFor each of the following variables, determine the most appropriate scale of measurement (Nominal, Ordinal, Interval, or Ratio). Also evaluate whether the variable is discrete or continuous.\n\nGender: nominal level of measurement, and discrete;\nCustomer satisfaction: Poor, Fair, Good, Excellent\nHeight (questionnaire): “I am: very short, short, average, tall, very tall”\nHeight (inches)\nReaction time (milliseconds)\nPostal codes: e.g., 61548, 61761, 62461, 47424, 65233\nAge (years)\nNationality\nStreet addresses\nMilitary ranks\nLeft-Right political scale placement\nFamily size: 1 child, 2 children, 3 children, …\nIQ score\nShirt size (S, M, L, …)\nMovie ratings (1 star, 2 stars, 3 stars)\nTemperature (Celsius)\nTemperature (Kelvin)\nBlood types: A, B, AB, O\nIncome categories: low, medium, high\nVoter turnout\nPolitical party affiliation\nElectoral district magnitude\n\nRemember to justify your choices for each variable.\nFor instance: In Stevens’ typology of measurement scales, street addresses are nominal data. This is because:\nThey serve purely as labels/identifiers. They have no inherent ordering (123 Main St isn’t “more than” 23 Oak St). You can’t perform meaningful mathematical operations on them.The only valid operation is testing for equality/inequality (is this the same address or different?)\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\n3.4.5 Notes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html",
    "href": "rozdzial2.html",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "",
    "text": "4.1 Podstawy Zbiorów Liczbowych\nZanim zagłębimy się w typy danych, istotne jest zrozumienie podstawowych zbiorów liczbowych, które stanowią fundament naszego rozumienia danych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "href": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "",
    "text": "4.1.1 Podstawowe Zbiory Liczbowe\n\nLiczby Naturalne (ℕ): Liczby używane do liczenia obiektów {0, 1, 2, 3, …}\nLiczby Całkowite (ℤ): Obejmują liczby naturalne, ich przeciwności i zero {…, -2, -1, 0, 1, 2, …}\nLiczby Wymierne (ℚ): Liczby, które można wyrazić jako ułamek dwóch liczb całkowitych\nLiczby Rzeczywiste (ℝ): Wszystkie liczby na osi liczbowej, w tym wymierne i niewymierne\n\n\n\n4.1.2 Właściwości Zbiorów\n\nZbiory Przeliczalne: Zbiory, których elementy można ustawić w relacji jeden do jednego z liczbami naturalnymi. Na przykład, zbiór liczb całkowitych jest przeliczalny.\nZbiory Nieprzeliczalne: Zbiory, które nie są przeliczalne. Zbiór liczb rzeczywistych jest nieprzeliczalny.\nZbiory Dyskretne: Zbiory, w których każdy element jest oddzielony od innych elementów skończoną przerwą. Liczby całkowite tworzą zbiór dyskretny.\nZbiory Gęste: Zbiory, w których między dowolnymi dwoma elementami zawsze znajduje się inny element zbioru. Liczby wymierne i rzeczywiste są zbiorami gęstymi.\n\n\n\n\n\n\n\nNote\n\n\n\nZrozumienie tych właściwości zbiorów jest kluczowe dla uchwycenia natury różnych typów danych w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#dane-dyskretne-vs.-ciągłe",
    "href": "rozdzial2.html#dane-dyskretne-vs.-ciągłe",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.2 Dane Dyskretne vs. Ciągłe",
    "text": "4.2 Dane Dyskretne vs. Ciągłe\nW nauce o danych i statystyce często kategoryzujemy zmienne jako dyskretne lub ciągłe. Jednak rozróżnienie to nie zawsze jest jednoznaczne, a niektóre zmienne wykazują cechy obu typów. Ta sekcja bada koncepcje danych dyskretnych i ciągłych, ich różnice oraz interesujące przypadki zmiennych, które można traktować jako oba typy lub które kwestionują nasze intuicyjne rozumienie.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n4.2.1 Dane Dyskretne\nDane dyskretne mogą przyjmować tylko określone, przeliczalne wartości. Te wartości często (ale nie zawsze) są liczbami całkowitymi.\n\n4.2.1.1 Cechy Danych Dyskretnych:\n\nPrzeliczalne\nCzęsto reprezentowane przez liczby całkowite\nMogą być skończone lub nieskończone\nBrak wartości między dwoma sąsiednimi punktami danych\n\n\n\n4.2.1.2 Przykłady:\n\nLiczba studentów w klasie\nLiczba samochodów sprzedanych przez dealera\nRozmiary butów\n\n\n\n\n4.2.2 Dane Ciągłe\nDane ciągłe mogą przyjmować dowolną wartość w danym zakresie, w tym wartości ułamkowe i dziesiętne. Ważne jest, aby zauważyć, że ciągłość nie jest określona wyłącznie przez nieprzeliczalność, ale również przez gęstość.\n\n4.2.2.1 Cechy Danych Ciągłych:\n\nMogą być nieprzeliczalne (jak liczby rzeczywiste) lub gęste (jak liczby wymierne)\nMogą być mierzone z dowolną precyzją (teoretycznie)\nReprezentowane przez liczby rzeczywiste lub gęste podzbiory liczb rzeczywistych\nZawsze istnieją wartości między dowolnymi dwoma punktami danych\n\n\n\n4.2.2.2 Przykłady:\n\nWzrost\nWaga\nTemperatura\nProcenty (wyjaśnione dalej poniżej)\n\n\n\n\n4.2.3 Spektrum Dyskretno-Ciągłe\nW praktyce niektóre zmienne, które matematycznie są dyskretne, często są traktowane tak, jakby były ciągłe. Ta dwoista natura zapewnia elastyczność w analizie i interpretacji tych zmiennych.\n\n4.2.3.1 Powody Traktowania Danych Dyskretnych jako Ciągłych:\n\nGęsta Granularność\n\nGdy zmienna dyskretna ma dużą liczbę możliwych wartości w danym zakresie, może przybliżać ciągłość.\nPrzykład: Dochód mierzony w pojedynczych groszach. Choć technicznie dyskretny, duża liczba możliwych wartości sprawia, że zachowuje się podobnie do zmiennej ciągłej.\n\nWygoda Analityczna\n\nMetody ciągłe często dają rozsądne i użyteczne wyniki nawet dla gęstych zmiennych dyskretnych.\nCzęsto łatwiej jest używać istniejących narzędzi statystycznych, jeśli założymy ciągłość, ponieważ pozwala to na stosowanie metod opartych na rachunku różniczkowym.\n\nPrzybliżenie Zjawisk Bazowych\n\nW niektórych przypadkach dyskretny pomiar może być przybliżeniem bazowego procesu ciągłego.\nPrzykład: Chociaż mierzymy czas w dyskretnych jednostkach (sekundy, minuty, godziny), sam czas jest ciągły.\n\n\n\n\n4.2.3.2 Przykłady Zmiennych o Dwoistej Naturze Dyskretno-Ciągłej:\n\nWiek\n\nDyskretny: Typowo mierzony w pełnych latach\nCiągły: Może być uznany za zmienną ciągłą w wielu analizach, szczególnie przy dużych populacjach\n\nCena i Dochód\n\nDyskretne: Ceny i dochody są w rzeczywistości mierzone w dyskretnych jednostkach (np. grosze lub najmniejsza jednostka waluty)\nCiągłe: W modelach ekonomicznych i wielu analizach ceny i dochody są traktowane jako zmienne ciągłe ze względu na ich gęstą naturę i wygodę analityczną\n\nWyniki Testów\n\nDyskretne: Często podawane jako liczby całkowite\nCiągłe: W analizach statystycznych wyniki testów mogą być traktowane jako ciągłe, szczególnie gdy zakres możliwych wyników jest duży\n\n\n\n\n\n4.2.4 Przypadek Szczególny: Procenty i Liczby Wymierne\nProcenty przedstawiają interesujący przypadek w spektrum dyskretno-ciągłym:\n\nNatura Wymierna: Procenty są zasadniczo ułamkami (m/100), co czyni je liczbami wymiernymi.\nGęste, ale Przeliczalne: Zbiór liczb wymiernych jest gęsty (między dowolnymi dwoma wymiernymi jest inny wymierny), ale także przeliczalny.\nPraktyczna Ciągłość: W większości praktycznych zastosowań procenty są traktowane jako ciągłe ze względu na ich gęstą naturę.\nSkończona Precyzja: W rzeczywistości procenty są często podawane z ograniczoną liczbą miejsc po przecinku, tworząc skończony zbiór możliwych wartości.\n\n\n\n\n\n\n\nProcenty: Łączenie Dyskretnego i Ciągłego\n\n\n\nZmienne mierzone w procentach, takie jak stopy bezrobocia czy frekwencja wyborcza, kwestionują nasze intuicyjne rozumienie dyskretności i ciągłości:\n\nSą liczbami wymiernymi (ułamki z mianownikiem 100), które technicznie są przeliczalne.\nTworzą zbiór gęsty w swoim zakresie (od 0% do 100%), pozwalając na wartości między dowolnymi dwoma procentami.\nW praktyce są często traktowane jako zmienne ciągłe ze względu na ich gęstą naturę i wygodę analityczną.\nPrecyzja pomiaru (np. podawanie do jednego lub dwóch miejsc po przecinku) może narzucić dyskretną strukturę na to, co koncepcyjnie jest zbiorem gęstym.\n\nTa dwoistość pozwala na elastyczne podejścia analityczne, w zależności od konkretnego kontekstu badawczego i wymaganej precyzji.\n\n\n\n\n4.2.5 Implikacje dla Analizy Danych\nZrozumienie zniuansowanej natury zmiennych jako dyskretnych, ciągłych lub gdzieś pomiędzy ma ważne implikacje dla analizy danych:\n\nElastyczność w Modelowaniu: Pozwala na wykorzystanie szerszego zakresu technik statystycznych.\nUproszczone Obliczenia: Traktowanie gęstych danych dyskretnych jako ciągłych może uprościć obliczenia i uczynić niektóre analizy bardziej wykonalnymi.\nLepsza Interpretowalność: W niektórych przypadkach traktowanie danych dyskretnych jako ciągłych może prowadzić do bardziej intuicyjnych lub użytecznych interpretacji wyników.\nPotencjał Błędu: Ważne jest, aby być świadomym, kiedy przybliżenia są odpowiednie, a kiedy mogą prowadzić do mylących wyników.\nRozważania Teoretyczne vs. Praktyczne: Choć matematyczna natura danych jest ważna, praktyczne względy w pomiarze i analizie często kierują tym, jak traktujemy zmienne.\n\n\n\n4.2.6 Wnioski\nRozróżnienie między danymi dyskretnymi a ciągłymi nie zawsze jest sztywne w naukach społecznych. Wiele zmiennych, w tym te dotyczące pieniędzy, procentów czy gęstych pomiarów, można oglądać przez pryzmat zarówno dyskretny, jak i ciągły. Wybór sposobu traktowania powinien być kierowany naturą danych, celami analizy i potencjalnymi implikacjami tego wyboru. Ta elastyczność, gdy jest używana rozważnie, zapewnia potężne narzędzia dla badaczy nauk społecznych do uzyskiwania wglądu w ich dane.\n\n\n\n\n\n\nDane Dyskretne vs. Ciągłe: Analogia Językowa\n\n\n\n\n4.2.6.1 Kluczowe Rozróżnienie Językowe\nW języku polskim mamy precyzyjne rozróżnienie:\n\n“Liczba” → używamy dla rzeczy policzalnych\n“Ilość” → używamy dla rzeczy niepoliczalnych\n\nTo rozróżnienie doskonale odzwierciedla dwa podstawowe typy danych liczbowych:\n\n\n4.2.6.2 Dane Dyskretne = “Liczba czegoś”\n\nUżywamy słowa “liczba” (tak jak mówimy “liczba studentów”)\nWartości są rozdzielone jak pojedyncze elementy\nPrzykłady:\n\nLiczba książek: 0, 1, 2, 3…\nLiczba punktów w teście: 0, 1, 2…\nLiczba mieszkańców: 100, 101, 102…\n\n\n🤔 Czy poprawne jest powiedzenie “ilość studentów” czy “liczba studentów”? (Poprawna forma pomoże Ci rozpoznać typ danych)\n\n\n4.2.6.3 Dane Ciągłe = “Ilość czegoś”\n\nUżywamy słowa “ilość” (tak jak mówimy “ilość wody”)\nWartości płynnie przechodzą jedna w drugą\nPrzykłady:\n\nIlość cieczy: 1,5231… litra\nIlość czasu: 2,3891… godziny\nIlość energii: 5,7123… kWh\n\n\n🤔 Czy mówimy “ilość wody” czy “liczba wody”? (Poprawna forma wskazuje na typ danych)\n\n\n4.2.6.4 Sposób Rozpoznawania\n\nCzy użyłbyś słowa “liczba”? → Dane dyskretne\nCzy użyłbyś słowa “ilość”? → Dane ciągłe\n\n✍️ Ćwiczenie: Uzupełnij poprawnym słowem i określ typ danych\n\n_____ uczniów w klasie (liczba/ilość): typ _____\n_____ deszczu (liczba/ilość): typ _____\n_____ piosenek (liczba/ilość): typ _____\n_____ temperatury (liczba/ilość): typ _____\n\n\n\n\n\n\n4.2.7 Przykład Kodu R\nOto prosty przykład kodu R ilustrujący, jak możemy analizować zmienne traktowane jako ciągłe:\n\n# Generowanie przykładowych danych\nset.seed(123)\nwiek &lt;- round(runif(1000, min = 18, max = 80))\nstopy_bezrobocia &lt;- round(runif(1000, min = 3, max = 10), 1)\n\n# Traktowanie wieku jako dyskretnego\ntabela_wieku &lt;- table(wiek)\nbarplot(tabela_wieku, main = \"Rozkład Wieku (Dyskretny)\", xlab = \"Wiek\", ylab = \"Częstotliwość\")\n\n\n\n\n\n\n\n# Traktowanie wieku jako ciągłego\nhist(wiek, main = \"Rozkład Wieku (Ciągły)\", xlab = \"Wiek\", ylab = \"Częstotliwość\")\n\n\n\n\n\n\n\n# Traktowanie stopy bezrobocia jako dyskretnej\ntabela_stopy &lt;- table(stopy_bezrobocia)\nbarplot(tabela_stopy, main = \"Stopy Bezrobocia (Dyskretne)\", xlab = \"Stopa (%)\", ylab = \"Częstotliwość\")\n\n\n\n\n\n\n\n# Traktowanie stopy bezrobocia jako ciągłej\nhist(stopy_bezrobocia, breaks = 30, main = \"Stopy Bezrobocia (Ciągłe)\", xlab = \"Stopa (%)\", ylab = \"Częstotliwość\")\n\n# Porównanie średnich i median\ncat(\"Średni wiek:\", mean(wiek), \"\\n\")\n\nŚredni wiek: 48.848 \n\ncat(\"Mediana wieku:\", median(wiek), \"\\n\")\n\nMediana wieku: 48 \n\ncat(\"Średnia stopa bezrobocia:\", mean(stopy_bezrobocia), \"\\n\")\n\nŚrednia stopa bezrobocia: 6.4871 \n\ncat(\"Mediana stopy bezrobocia:\", median(stopy_bezrobocia), \"\\n\")\n\nMediana stopy bezrobocia: 6.5 \n\n# Regresja liniowa (traktowanie obu jako ciągłych)\ndochod &lt;- 20000 + 500 * wiek + 1000 * stopy_bezrobocia + rnorm(1000, 0, 5000)\nmodel &lt;- lm(dochod ~ wiek + stopy_bezrobocia)\nsummary(model)\n\n\nCall:\nlm(formula = dochod ~ wiek + stopy_bezrobocia)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15354  -3471     54   3485  16684 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      20116.194    717.231   28.05   &lt;2e-16 ***\nwiek               503.385      8.983   56.04   &lt;2e-16 ***\nstopy_bezrobocia   989.333     80.004   12.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5053 on 997 degrees of freedom\nMultiple R-squared:  0.7637,    Adjusted R-squared:  0.7632 \nF-statistic:  1611 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n# Wykres płaszczyzny regresji\nlibrary(plotly)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n\n\n\n\n\n\nplot_ly(x = wiek, y = stopy_bezrobocia, z = dochod, type = \"scatter3d\", mode = \"markers\") %&gt;%\n  add_trace(x = wiek, y = stopy_bezrobocia, z = fitted(model), type = \"scatter3d\", mode = \"lines\")\n\n\n\n\n\nWykres 3D ilustruje, jak obie zmienne mogą być traktowane jako ciągłe w modelu regresji.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "href": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.3 Wprowadzenie do Typologii Danych Stevensa",
    "text": "4.3 Wprowadzenie do Typologii Danych Stevensa\nStanley S. Stevens, amerykański psycholog, wprowadził system klasyfikacji skal pomiarowych w swoim artykule z 1946 roku “On the Theory of Scales of Measurement”. Ten system, znany jako typologia danych Stevensa lub poziomy pomiaru, stał się fundamentalny dla zrozumienia, jak różne typy danych powinny być analizowane i interpretowane.\nStevens zaproponował cztery poziomy pomiaru:\n\nNominalny\nPorządkowy\nInterwałowy\nIlorazowy\n\nKażdy poziom ma specyficzne właściwości i pozwala na różne rodzaje operacji statystycznych i analiz.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n4.3.1 Skala Nominalna\n\n4.3.1.1 Definicja\nSkala nominalna jest najbardziej podstawowym poziomem pomiaru. Używa etykiet lub kategorii do klasyfikacji danych bez żadnej wartości ilościowej ani porządku.\n\n\n4.3.1.2 Właściwości\n\nKategorie są wzajemnie wykluczające się\nBrak inherentnego porządku między kategoriami\nNie można wykonywać znaczących operacji arytmetycznych\n\n\n\n4.3.1.3 Przykłady\n\nNarodowość (Polak, Niemiec, …)\nGrupy krwi (A, B, AB, O)\nKolor oczu (Niebieskie, Brązowe, Zielone, Piwne)\nZmienne binarne (“Sukces” versus “Niepowodzenie”)\n\n\n\n\n4.3.2 Skala Porządkowa\n\n4.3.2.1 Definicja\nSkala porządkowa kategoryzuje dane w uporządkowane kategorie, ale odstępy między kategoriami niekoniecznie są równe lub znaczące.\n\n\n4.3.2.2 Właściwości\n\nKategorie mają zdefiniowany porządek\nRóżnice między kategoriami nie są kwantyfikowalne\nOperacje arytmetyczne na liczbach nie są znaczące\n\n\n\n4.3.2.3 Przykłady\n\nPoziomy wykształcenia (Szkoła Średnia, Licencjat, Magister, Doktorat)\nSkale Likerta (Zdecydowanie się nie zgadzam, Nie zgadzam się, Neutralnie, Zgadzam się, Zdecydowanie się zgadzam)\nStatus społeczno-ekonomiczny (Niski, Średni, Wysoki)\n\n\n\n\n4.3.3 Skala Interwałowa\n\n4.3.3.1 Definicja\nSkala interwałowa ma uporządkowane kategorie z równymi odstępami między sąsiednimi kategoriami. Jednak brakuje jej prawdziwego punktu zerowego.\n\n\n4.3.3.2 Właściwości\n\nRówne odstępy między sąsiednimi kategoriami\nBrak prawdziwego punktu zerowego (zero jest umowne)\nStosunki między wartościami nie są znaczące\n\n\n\n4.3.3.3 Przykłady\n\nTemperatura w stopniach Celsjusza lub Fahrenheita\nLata kalendarzowe\nSkala pH\nWysokość nad poziomem morza\n\n\n\n\n4.3.4 Skala Ilorazowa\n\n4.3.4.1 Definicja\nSkala ilorazowa jest najwyższym poziomem pomiaru. Ma wszystkie właściwości skali interwałowej plus prawdziwy punkt zerowy, co sprawia, że stosunki między wartościami są znaczące.\n\n\n4.3.4.2 Właściwości\n\nWszystkie właściwości skal interwałowych\nPrawdziwy punkt zerowy\nStosunki między wartościami są znaczące\n\n\n\n4.3.4.3 Przykłady\n\nWzrost\nWaga\nWiek\nDochód\n\n\n\n\n\n\n\nDlaczego Niektóre Statystyki Działają (a Inne Nie) dla Skal Interwałowych\n\n\n\n\n4.3.4.4 Kluczowa Idea\nW przypadku skal interwałowych (np. temperatury):\n\nDozwolone jest dodawanie/odejmowanie wartości oraz dzielenie przez liczby\nNiedozwolone jest mnożenie/dzielenie wartości przez siebie\n\n\n\n4.3.4.5 Własności Skali Interwałowej\n\nRówne interwały reprezentują takie same różnice:\n\nRóżnica między 20°C a 25°C (5°C) reprezentuje taką samą zmianę jak między 30°C a 35°C\nProporcje różnic są zachowane: 10°C to dwa razy większa zmiana niż 5°C\n\nPunkt zero jest umowny:\n\n0°C to punkt zamarzania wody, a nie brak temperatury\nTen sam stan fizyczny ma różne wartości: 0°C = 32°F\n\nTransformacja liniowa:\n\nWzór ogólny: y = ax + b, gdzie a \\neq 0\nDla temperatury: F = C \\times \\frac{9}{5} + 32\n\n\n\n\n4.3.4.6 Dlaczego Średnia Arytmetyczna Działa\nPrzykład analizy dla dwóch temperatur:\nDane: 20°C i 30°C\n\nMetoda 1: Średnia w Celsjuszach, potem konwersja\n1. Średnia: (20°C + 30°C) ÷ 2 = 25°C\n2. Konwersja: 25°C × (9/5) + 32 = 77°F\n\nMetoda 2: Konwersja na °F, potem średnia\n1. Konwersja: 20°C → 68°F, 30°C → 86°F\n2. Średnia: (68°F + 86°F) ÷ 2 = 77°F\n\nObie metody dają ten sam wynik! ✓\nMatematyczny dowód poprawności:\n\\begin{align*}\n\\bar{F} &= \\frac{F_1 + F_2}{2} = \\frac{(C_1 \\times \\frac{9}{5} + 32) + (C_2 \\times \\frac{9}{5} + 32)}{2} \\\\\n&= \\frac{(C_1 + C_2) \\times \\frac{9}{5} + 64}{2} \\\\\n&= (\\frac{C_1 + C_2}{2}) \\times \\frac{9}{5} + 32 \\\\\n&= \\bar{C} \\times \\frac{9}{5} + 32\n\\end{align*}\nTo działa ponieważ:\n\nUżywamy tylko dozwolonych operacji\nZachowana jest liniowość transformacji\nUmowny punkt zero nie wpływa na wynik\n\n\n\n4.3.4.7 Dlaczego Wariancja Jest Problematyczna\nAnaliza na tym samym przykładzie:\nTe same temperatury: 20°C i 30°C\n\nMetoda 1: Wariancja w Celsjuszach\n1. Średnia: 25°C\n2. Odchylenia: (20 - 25)°C = -5°C, (30 - 25)°C = 5°C\n3. Kwadraty odchyleń: (-5°C)² = 25(°C)², (5°C)² = 25(°C)²\n4. Średnia: (25 + 25)(°C)² ÷ 2 = 25(°C)²\n\nMetoda 2: Wariancja w Fahrenheitach\n1. Konwersja: 20°C → 68°F, 30°C → 86°F\n2. Średnia: 77°F\n3. Odchylenia: (68 - 77)°F = -9°F, (86 - 77)°F = 9°F\n4. Kwadraty odchyleń: (-9°F)² = 81(°F)², (9°F)² = 81(°F)²\n5. Średnia: (81 + 81)(°F)² ÷ 2 = 81(°F)²\n\nProblem: 25(°C)² i 81(°F)² nie są porównywalne!\nMatematyczna analiza problemu:\nDla odchylenia w skali Fahrenheita: \\begin{align*}\n(F_i - \\bar{F})^2 &= [(C_i \\times \\frac{9}{5} + 32) - (\\bar{C} \\times \\frac{9}{5} + 32)]^2 \\\\\n&= [(C_i - \\bar{C}) \\times \\frac{9}{5}]^2 \\\\\n&= (C_i - \\bar{C})^2 \\times (\\frac{9}{5})^2\n\\end{align*}\nTo nie działa dobrze ponieważ:\n\nMnożymy temperatury przez siebie (niedozwolone!)\nPowstają jednostki kwadratowe (°C² lub °F²) bez interpretacji fizycznej\nWyniki w różnych skalach nie są porównywalne\nKwadrat współczynnika skalowania ((\\frac{9}{5})^2) nie ma interpretacji fizycznej\n\n\n\n4.3.4.8 Wnioski Teoretyczne\n\nOperacje dozwolone:\n\nDodawanie/odejmowanie (zachowuje różnice)\nMnożenie przez stałe (skalowanie)\nŚrednie arytmetyczne\nPorównywanie różnic temperatur\n\nOperacje niedozwolone:\n\nMnożenie temperatur\nDzielenie temperatur\nŚrednie geometryczne\nWspółczynnik zmienności\n\nImplikacje praktyczne:\n\nDla wariancji i odchylenia standardowego potrzebna ostrożna interpretacja\nLepiej używać miar opartych na różnicach (np. MAD)\nPrzy porównywaniu zmienności warto standaryzować dane\n\n\n\n\n4.3.4.9 Zasada Praktyczna\nJeśli w obliczeniach pojawia się mnożenie wartości ze skali interwałowej przez siebie, należy zachować ostrożność w interpretacji wyników!\n\n\n\n\n\n\n\n\n\nProporcje w Skalach Pomiarowych: Przypadek Temperatury\n\n\n\n\n4.3.4.10 Dwa Rodzaje Proporcji\n\n4.3.4.10.1 A) Proporcje wartości (NIE zachowują się w skali interwałowej):\nWeźmy 80°C i 20°C:\nW Celsjuszach: 80°C/20°C = 4\nW Fahrenheitach: 176°F/68°F ≈ 2.59\nW Kelwinach: 353.15K/293.15K ≈ 1.20\n\nTe same temperatury dają różne proporcje! \n→ Proporcje wartości NIE mają sensu na skalach interwałowych; sens mają tylko na skali ilorazowej\n\n\n4.3.4.10.2 B) Proporcje różnic (zachowują się w skali interwałowej):\nWeźmy dwie pary różnic:\nPara 1: 30°C - 20°C = 10°C\nPara 2: 80°C - 60°C = 20°C\n\nProporcja różnic w Celsjuszach:\n20°C/10°C = 2\n\nTe same temperatury w Fahrenheitach:\nPara 1: 86°F - 68°F = 18°F\nPara 2: 176°F - 140°F = 36°F\n\nProporcja różnic w Fahrenheitach:\n36°F/18°F = 2\n\nProporcja różnic jest taka sama! ✓\n\n\n\n4.3.4.11 Matematyczne Wyjaśnienie\nDla transformacji F = \\frac{9}{5}C + 32:\n\nProporcje wartości NIE zachowują się: \\frac{F_1}{F_2} = \\frac{\\frac{9}{5}C_1 + 32}{\\frac{9}{5}C_2 + 32} \\neq \\frac{C_1}{C_2}\nProporcje różnic zachowują się: \\frac{F_1 - F_2}{F_3 - F_4} = \\frac{(\\frac{9}{5}C_1 + 32) - (\\frac{9}{5}C_2 + 32)}{(\\frac{9}{5}C_3 + 32) - (\\frac{9}{5}C_4 + 32)} = \\frac{\\frac{9}{5}(C_1 - C_2)}{\\frac{9}{5}(C_3 - C_4)} = \\frac{C_1 - C_2}{C_3 - C_4}\n\n\n\n4.3.4.12 Dlaczego To Jest Ważne?\n\nDla wartości:\n\nW skali Celsjusza: 40°C nie jest “dwa razy cieplejsze” niż 20°C\nW skali Fahrenheita: 100°F nie jest “dwa razy cieplejsze” niż 50°F\nTylko w Kelwinach proporcje wartości mają sens fizyczny\n\nDla różnic:\n\nWzrost o 20°C jest zawsze dwa razy większy niż wzrost o 10°C\nWzrost o 36°F jest zawsze dwa razy większy niż wzrost o 18°F\nProporcje różnic są niezależne od skali\n\n\n\n\n4.3.4.13 Implikacje dla Statystyk\n\nOperacje bazujące na różnicach (DZIAŁAJĄ):\n\nŚrednia arytmetyczna\nOdchylenie bezwzględne\nRozstęp\n\nOperacje bazujące na proporcjach wartości (NIE DZIAŁAJĄ):\n\nŚrednia geometryczna\nWspółczynnik zmienności\nWariancja (bo używa kwadratu wartości)\n\n\n\n\n\n\n\n\n4.3.5 Znaczenie w Badaniach i Analizie\nZrozumienie typologii danych Stevensa jest kluczowe z kilku powodów:\n\nWybór odpowiednich testów statystycznych: Poziom pomiaru determinuje, które analizy statystyczne są odpowiednie dla danego zbioru danych.\nInterpretacja wyników: Znaczenie wyników statystycznych zależy od poziomu pomiaru zaangażowanych zmiennych.\nProjektowanie narzędzi pomiarowych: Przy tworzeniu ankiet lub innych narzędzi pomiarowych badacze muszą wziąć pod uwagę poziom pomiaru, który chcą osiągnąć.\nTransformacja danych: Czasami dane mogą być przekształcane z jednego poziomu na drugi, ale musi to być robione ostrożnie, aby uniknąć błędnej interpretacji.\n\n\n\n4.3.6 Kontrowersje i Ograniczenia\nChociaż typologia Stevensa jest szeroko stosowana, spotkała się z pewnymi krytykami:\n\nSztywność: Niektórzy twierdzą, że typologia jest zbyt sztywna i że wiele rzeczywistych pomiarów mieści się pomiędzy tymi kategoriami.\nTraktowanie danych porządkowych: Trwa debata na temat tego, kiedy właściwe jest traktowanie danych porządkowych jako interwałowych dla pewnych analiz.\nSkalowanie psychologiczne: Niektóre konstrukty psychologiczne (jak inteligencja) są trudne do jednoznacznego skategoryzowania w ramach tego systemu.\n\n\n\n4.3.7 Podsumowanie\nTypologia danych Stevensa dostarcza fundamentalnych ram dla zrozumienia różnych rodzajów danych i ich właściwości. Rozpoznając poziom pomiaru swoich zmiennych, badacze mogą podejmować świadome decyzje dotyczące gromadzenia danych, analizy i interpretacji. Jednak ważne jest, aby pamiętać, że chociaż ta typologia jest użytecznym przewodnikiem, rzeczywiste dane często wymagają niuansowego podejścia i nie zawsze pasują idealnie do tych kategorii.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not “twice as acidic” as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#popularne-skale-porządkowe-w-badaniach-behawioralnych",
    "href": "rozdzial2.html#popularne-skale-porządkowe-w-badaniach-behawioralnych",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.4 Popularne Skale Porządkowe w Badaniach Behawioralnych",
    "text": "4.4 Popularne Skale Porządkowe w Badaniach Behawioralnych\n\n4.4.1 Skale Likerta\nSkale Likerta są szeroko stosowane w psychologii i naukach społecznych do pomiaru postaw, opinii i percepcji. Nazwane na cześć psychologa Rensisa Likerta, skale te zazwyczaj składają się z serii stwierdzeń lub pytań, które respondenci oceniają na skali, często od “Zdecydowanie się nie zgadzam” do “Zdecydowanie się zgadzam”.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n4.4.1.1 Dlaczego Skale Likerta są Zmiennymi Porządkowymi\nSkale Likerta są uważane za zmienne porządkowe z kilku powodów:\n\nPorządek bez równych odstępów: Chociaż odpowiedzi mają wyraźną kolejność (np. “Zdecydowanie się nie zgadzam” &lt; “Nie zgadzam się” &lt; “Neutralnie” &lt; “Zgadzam się” &lt; “Zdecydowanie się zgadzam”), odstępy między tymi kategoriami niekoniecznie są równe.\nSubiektywna interpretacja: Różnica między “Zdecydowanie się nie zgadzam” a “Nie zgadzam się” może nie być taka sama jak różnica między “Zgadzam się” a “Zdecydowanie się zgadzam” dla wszystkich respondentów.\nBrak prawdziwego punktu zerowego: Skale Likerta zazwyczaj nie mają prawdziwego punktu zerowego, co jest cechą charakterystyczną skal interwałowych lub ilorazowych.\n\n\n\n\n4.4.2 IQ i Inne Zmienne Psychologiczne jako Miary Porządkowe\nWiele miar psychologicznych, w tym IQ, jest często traktowanych jako skale interwałowe, ale w rzeczywistości są to skale porządkowe. Oto dlaczego:\n\nWyniki IQ:\n\nChociaż wyniki IQ są przedstawiane jako liczby, różnica między IQ 100 a 110 może nie reprezentować takiej samej różnicy poznawczej jak między 130 a 140.\nSkala jest normalizowana i dostosowywana w czasie, co utrudnia stwierdzenie, że ma właściwości prawdziwie interwałowe.\n\nInne Miary Psychologiczne:\n\nSkale depresji (np. Inwentarz Depresji Becka)\nMiary lęku (np. Inwentarz Stanu i Cechy Lęku)\nOceny osobowości (np. Inwentarz Wielkiej Piątki)\n\n\nTe miary często wykorzystują sumowane pozycje typu Likerta lub inne metody punktacji, które nie gwarantują równych odstępów między wynikami.\n\n\n4.4.3 Implikacje dla Analizy\nUznanie tych miar za porządkowe ma ważne implikacje dla analizy danych:\n\nOdpowiednie testy statystyczne: Używaj testów nieparametrycznych (np. test U Manna-Whitneya, test Kruskala-Wallisa) zamiast parametrycznych.\nAnaliza korelacji: Używaj korelacji rangowej Spearmana zamiast korelacji Pearsona.\nTendencja centralna: Raportuj medianę i dominantę zamiast średniej.\nWizualizacja danych: Stosuj metody odpowiednie dla danych porządkowych, takie jak wykresy słupkowe lub skumulowane wykresy słupkowe.\n\n\n\n4.4.4 Podsumowanie\nChociaż skale Likerta i wiele miar psychologicznych jest często traktowanych jako dane interwałowe ze względów praktycznych, ważne jest, aby pamiętać o ich porządkowym charakterze.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\nĆwiczenie: Identyfikacja Skal Pomiarowych\n\n\n\nDla każdej z poniższych zmiennych określ najbardziej odpowiednią skalę pomiaru (Nominalna, Porządkowa, Przedziałowa lub Stosunkowa). Czy zmienna jest dyskretna, czy ciągła?\n\nPłeć: skala nominalna; zmienna dyskretna;\nSatysfakcja klienta: Niska, Średnia, Dobra, Doskonała\nWzrost (ankieta): “Jestem: bardzo niski, niski, przeciętnego wzrostu, wysoki, bardzo wysoki”\nWzrost mierzony w centymetrach\nCzas reakcji (w milisekundach)\nKody pocztowe: np. 00-001, 00-950, 80-452, 31-072\nWiek (w latach)\nMarki samochodów\nNarodowość\nLiczba dzieci w rodzinie: 1 dziecko, 2 dzieci, 3 dzieci, …\nWynik testu IQ\nTemperatura (skala Celsjusza)\nTemperatura (skala Kelvina)\nFrekwencja wyborcza\nPrzynależność partyjna\nWielkość okręgu wyborczego\nWspółrzędne w układzie kartezjańskim\nData (względem określonej epoki, np. n.e.)\nWysokość nad poziomem morza\nGrupy krwi: A, B, AB, 0\nKategorie dochodów: niskie, średnie, wysokie\nStopnie wojskowe\n\nPamiętaj, aby uzasadnić swój wybór skali dla każdej zmiennej.\nDla przykładu: W typologii skal pomiarowych Stevensa, adresy uliczne są danymi nominalnymi. Dlaczego?\nPełnią wyłącznie funkcję etykiet/identyfikatorów Nie mają naturalnego uporządkowania (ul. Mickiewicza 5 nie jest “większa” niż ul. Słowackiego 10) Nie można wykonywać na nich sensownych operacji matematycznych Jedyna dozwolona operacja to sprawdzanie równości/nierówności (czy to ten sam adres czy inny?)\nMimo że numery domów są liczbami, w systemie adresowym funkcjonują jako etykiety, a nie wartości ilościowe. Liczba 100 w adresie “ul. Kilińskiego 100” nie jest używana matematycznie - równie dobrze mogłaby to być “ul. Jabłkowa” czy “ul. Zeusa”, jeśli chodzi o jej funkcję w adresie.\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\n4.4.5 Notes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "",
    "text": "5.1 Introduction to Randomness\nRandomness is a cornerstone concept in statistics and scientific research. It refers to the unpredictability of individual outcomes, even when the overall pattern may be predictable. In the social sciences, understanding randomness is crucial for designing studies, collecting data, and interpreting results.\nConsider flipping a fair coin. While we know that the probability of getting heads is 50%, we can’t predict with certainty the outcome of any single flip. This unpredictability is the essence of randomness.\nExamples of random phenomena in social sciences include:\nUnderstanding randomness helps researchers distinguish between genuine effects and chance occurrences. For instance, if we observe a slight difference in test scores between two groups, randomness helps us determine whether this difference is likely due to a real effect or just chance variation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#introduction-to-randomness",
    "href": "chapter3.html#introduction-to-randomness",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "",
    "text": "Participant Selection: In a psychology experiment studying reaction times, the order in which participants arrive at the lab may be random.\nEconomic Behavior: The daily fluctuations in stock prices often exhibit random patterns, influenced by countless unpredictable factors.\nSocial Interactions: The occurrence of chance encounters between individuals in a community can be considered random events.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-bridging-sample-and-population",
    "href": "chapter3.html#sampling-bridging-sample-and-population",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.2 Sampling: Bridging Sample and Population",
    "text": "5.2 Sampling: Bridging Sample and Population\nSampling is the process of selecting a subset (sample) from a larger group (population) to make inferences about the population. It’s a critical skill in social science research, as studying entire populations is often impractical, too expensive, or sometimes impossible.\nKey Terms:\n\nPopulation: The entire group about which we want to draw conclusions.\nSample: A subset of the population that we actually study.\nSampling Frame: The list or procedure used to identify all members of the population.\n\nExample: Suppose we want to study the job satisfaction of all teachers in the United States (the population). Instead of surveying millions of teachers, we might select a sample of 5,000 teachers from various states, school districts, and grade levels.\nRandomness in sampling helps ensure that the sample is representative of the population, reducing bias and allowing for more accurate inferences. This is why probability sampling methods, which we’ll discuss next, are often preferred in scientific research.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-methods",
    "href": "chapter3.html#sampling-methods",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.3 Sampling Methods",
    "text": "5.3 Sampling Methods\n\n5.3.1 Probability Sampling\nProbability sampling methods involve random selection, giving each member of the population a known, non-zero chance of being selected.\n\nSimple Random Sampling: Each member of the population has an equal chance of being selected.\nExample: To select 100 students from a university with 10,000 students, you could assign each student a number from 1 to 10,000, then use a random number generator to select 100 numbers.\nStratified Random Sampling: The population is divided into subgroups (strata) based on shared characteristics, then samples are randomly selected from each stratum.\nExample: In a national political survey, you might divide the population into strata based on geographic regions (Northeast, Midwest, South, West) and then randomly sample from each region. This ensures representation from all areas of the country.\nCluster Sampling: The population is divided into clusters (usually geographic), some clusters are randomly selected, and all members within those clusters are studied.\nExample: To study high school students’ study habits, you might randomly select 20 high schools from across the country and then survey all students in those schools.\nSystematic Sampling: Selecting every kth item from a list after a random start.\nExample: At a busy shopping mall, you might survey every 20th person who enters the mall, starting with a randomly chosen number between 1 and 20.\n\n\n\n5.3.2 Non-probability Sampling\nNon-probability sampling doesn’t involve random selection. While it can introduce bias, it may be necessary in certain situations, especially when dealing with hard-to-reach populations or when resources are limited.\n\nConvenience Sampling: Selecting easily accessible subjects.\nExample: A researcher studying college students’ sleep patterns might survey students in their own classes or around campus.\nPurposive Sampling: Selecting subjects based on specific characteristics.\nExample: For a study on the experiences of CEOs in the tech industry, a researcher might intentionally seek out and interview CEOs from various tech companies.\nSnowball Sampling: Participants recruit other participants.\nExample: In a study of undocumented immigrants’ access to healthcare, researchers might ask initial participants to refer other potential participants from their community.\nQuota Sampling: Selecting participants to meet specific quotas for certain characteristics.\nExample: In a market research study, researchers might ensure they interview a specific number of people from different age groups, genders, and income levels to match the demographics of the target market.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#making-inferences-from-samples",
    "href": "chapter3.html#making-inferences-from-samples",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.4 Making Inferences from Samples",
    "text": "5.4 Making Inferences from Samples\nStatistical inference is the process of drawing conclusions about a population based on a sample. This allows researchers to estimate characteristics of the entire population (parameters) using characteristics of the sample (statistics).\n\n\n\n\n\n\nNote\n\n\n\nThe Soup Analogy: A Taste of Statistics\n\n\nWhen you taste a spoonful of soup and decide it isn’t salty enough, that’s exploratory/descriptive analysis.\nIf you generalize and conclude that your entire pot of soup needs salt, that’s an inference.\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population).\nIf the soup is not well stirred (heterogeneous population), it doesn’t matter how large a spoon you have (sample size), it will still not accurately represent the whole. If the soup is well stirred (homogeneous population), even a small spoon will suffice to test the soup.\n\n\n\n\n\n\n\n\ngraph TD\n    DGP[Data Generating Process] --&gt;|Generates| A[Population]\n    A --&gt;|Random Selection| B[Sample]\n    B --&gt;|Statistical Inference| C[Estimates & Conclusions]\n    C --&gt;|Generalize back to| A\n    C -.-&gt;|Attempt to understand| DGP\n\n    style DGP fill:#1E90FF,stroke:#000,stroke-width:4px,color:#FFF\n    style A fill:#DC143C,stroke:#000,stroke-width:4px,color:#FFF\n    style B fill:#228B22,stroke:#000,stroke-width:2px,color:#FFF\n    style C fill:#8B4513,stroke:#000,stroke-width:2px,color:#FFF\n    \n    classDef note fill:#F0F0F0,stroke:#000,stroke-width:1px;\n    D[[\"DGP:\n    Underlying process\n    that generates data\"]]\n    E[[\"Population:\n    Entire group of interest\"]]\n    F[[\"Sample:\n    Subset of population\"]]\n    G[[\"Inference:\n    Drawing conclusions\n    about population\n    and DGP\"]]\n    \n    class D,E,F,G note\n    \n    D --&gt; DGP\n    E --&gt; A\n    F --&gt; B\n    G --&gt; C\n\n\n\n\n\n\nKey Concepts:\n\nPoint Estimates: A single value used to estimate a population parameter.\nExample: The mean income of a sample of 1000 workers might be used to estimate the mean income of all workers in a country.\nConfidence Intervals: A range of values likely to contain the true population parameter.\nExample: We might say, “We are 95% confident that the true population mean income falls between $45,000 and $55,000.”\nMargin of Error: The range of values above and below the sample statistic in a confidence interval.\nExample: In political polling, you might see a statement like “Candidate A is preferred by 52% of voters, with a margin of error of ±3%.”\nHypothesis Testing: A method for making decisions about population parameters based on sample data.\nExample: A researcher might test whether there’s a significant difference in test scores between students who study with music and those who study in silence.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-and-non-sampling-errors",
    "href": "chapter3.html#sampling-and-non-sampling-errors",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.5 Sampling and Non-sampling Errors",
    "text": "5.5 Sampling and Non-sampling Errors\nUnderstanding potential errors in research is crucial for interpreting results accurately.\nSampling Error: The difference between a sample statistic and the true population parameter, occurring due to chance variations in the selection of sample members.\nExample: If we estimate the average height of all adult males in a country based on a sample, our estimate will likely differ somewhat from the true average due to sampling error.\nNon-sampling Errors: Errors not due to chance, which can occur in both sample surveys and censuses.\n\nCoverage Error: When the sampling frame doesn’t accurately represent the population.\nExample: A telephone survey that only calls landlines would miss people who only have cell phones, potentially biasing the results.\nNon-response Error: When selected participants fail to respond, potentially introducing bias.\nExample: In a survey about job satisfaction, highly satisfied or highly dissatisfied employees might be more likely to respond, skewing the results.\nMeasurement Error: Inaccuracies in the data collected.\nExample: A poorly worded survey question might be interpreted differently by different respondents, leading to inconsistent data.\nProcessing Error: Mistakes made during data entry, coding, or analysis.\nExample: Accidentally entering “99” instead of “9” for a participant’s response could significantly skew the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sample-size-and-power",
    "href": "chapter3.html#sample-size-and-power",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.6 Sample Size and Power",
    "text": "5.6 Sample Size and Power\nDetermining the appropriate sample size involves balancing the need for precision with available resources.\nSample Size Considerations: - Larger samples generally provide more precise estimates but are more costly and time-consuming to obtain. - The required sample size depends on factors such as the desired level of precision, the variability in the population, and the type of analysis planned.\nExample: To estimate the proportion of voters who support a particular policy with a margin of error of ±3% at a 95% confidence level, you would need a sample size of about 1067 voters (assuming maximum variability).\nStatistical Power: The probability that a study will detect an effect when there is an effect to be detected.\nFactors affecting power: 1. Sample size 2. Effect size (the magnitude of the difference or relationship you’re trying to detect) 3. Chosen significance level (usually 0.05)\nExample: In a study comparing two teaching methods, having a larger sample size would increase the likelihood of detecting a significant difference between the methods, if such a difference exists.\n\n\n\n\n\n\nNote\n\n\n\nWhat is Study Power?\nStudy power is about how likely we are to find something if it really exists. It’s like having a good flashlight when you’re looking for something in the dark - the better your flashlight, the more likely you are to find what you’re looking for.\n\nEffect Size: How big the thing (effect, difference) we’re looking for is.\nSample Size: How many people or things we look at in our study.\nStudy Power: How likely we are to find the effect if it’s really there.\n\nThe Relationship Between Effect Size and Sample Size:\nImagine you’re trying to find coins hidden in sand:\n\nBig Effects (Big Coins):\n\nIf you’re looking for big coins (like quarters), you don’t need to search through as much sand to find them.\nIn research, if the effect is big, you can use a smaller sample.\n\nExample: Testing if a new study method improves test scores by 20 points out of 100.\n\nYou might only need to test 30 students to see this big difference.\n\nSmall Effects (Small Coins):\n\nIf you’re looking for tiny coins (like pennies), you’ll need to search through more sand.\nIn research, if the effect is small, you need a larger sample.\n\nExample: Seeing if using social media affects happiness by a tiny amount.\n\nYou might need to study 500 or more people to detect this small effect.\n\n\nWhy Study Power Matters:\n\nNot Missing Real Effects:\n\nWith low power, you might miss real effects, like using a weak flashlight and missing something that’s actually there.\n\nConfidence in Results:\n\nHigher power gives you more confidence that what you found is real and not just luck.\n\n\nExample:\nLet’s say we want to study if a new teaching method helps students learn better:\n\nSmall Study (Low Power):\n\nWe try the method with just 10 students.\nEven if the method works, with such a small group, it’s hard to tell if improvements are due to the new method or just chance.\n\nLarger Study (Higher Power):\n\nWe use the method with 100 students.\nNow we’re more likely to see if the method really helps because we have more data to look at.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-in-the-digital-age",
    "href": "chapter3.html#sampling-in-the-digital-age",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.7 Sampling in the Digital Age",
    "text": "5.7 Sampling in the Digital Age\nThe advent of big data and digital technologies has transformed sampling practices in many fields.\nBig Data Opportunities and Challenges: - Unprecedented volumes of information available - Potential lack of representativeness - Data quality concerns - Privacy and ethical issues\nExample: Social media data can provide real-time insights into public opinion, but users of a particular platform may not be representative of the general population.\nWeb-based Surveys: - Offer new opportunities for data collection - Face challenges such as coverage bias (not everyone has internet access) and self-selection bias\nExample: An online survey about internet usage habits would inherently exclude people without internet access, potentially biasing the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#ethical-considerations-in-sampling",
    "href": "chapter3.html#ethical-considerations-in-sampling",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.8 Ethical Considerations in Sampling",
    "text": "5.8 Ethical Considerations in Sampling\nEthical sampling practices are crucial in social science research:\n\nInformed Consent: Participants should understand the study’s purpose and agree to participate.\nExample: Before conducting interviews about sensitive topics like mental health, researchers must clearly explain the study’s aims and potential risks to participants.\nPrivacy and Confidentiality: Researchers must protect participants’ personal information.\nExample: In a study on workplace harassment, researchers might use code numbers instead of names to protect participants’ identities.\nRepresentativeness and Inclusivity: Samples should fairly represent diverse populations, including marginalized groups.\nExample: A study on urban housing should make efforts to include participants from various socioeconomic backgrounds, ethnicities, and housing situations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#conclusion",
    "href": "chapter3.html#conclusion",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.9 Conclusion",
    "text": "5.9 Conclusion\nSampling remains a cornerstone of social science research, even in the era of big data. Understanding sampling principles helps researchers design studies, interpret results, and make valid inferences about populations. As we’ve seen, the journey from sample to population involves careful consideration of sampling methods, potential errors, ethical issues, and the ever-evolving landscape of data collection in the digital age.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#statistical-errors---summary",
    "href": "chapter3.html#statistical-errors---summary",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.10 Statistical Errors - Summary",
    "text": "5.10 Statistical Errors - Summary\n\n5.10.1 Systematic Error vs. Random Error\nSystematic errors and random errors are two fundamental types of errors in statistical measurements and experiments.\n\nSystematic Error:\n\nDefinition: Consistent, predictable deviations from the true value\nCharacteristics:\n\nBiases the results in a specific direction\nRepeatable and often constant across measurements\nCan be corrected if identified\n\nExamples:\n\nMiscalibrated measuring instrument\nConsistent rounding error in data entry\nBiased sampling method\n\n\nRandom Error:\n\nDefinition: Unpredictable fluctuations in measurements due to chance\nCharacteristics:\n\nVaries in magnitude and direction\nFollows a probability distribution (often normal)\nCan be reduced by increasing sample size or repeated measurements\n\nExamples:\n\nNatural variations in the phenomenon being measured\nSmall fluctuations in measuring instruments\nHuman errors in reading or recording data\n\n\n\n\n\n5.10.2 Sampling Errors vs. Non-Sampling Errors\nSampling and non-sampling errors are categories of errors that can occur in statistical studies, particularly in survey research.\n\nSampling Errors:\n\nDefinition: Errors that occur due to the sample not perfectly representing the population\nCharacteristics:\n\nInherent in any sample-based study\nCan be estimated and quantified using statistical methods\nDecreases as sample size increases\n\nExamples:\n\nRandom fluctuations in sample statistics\nOver- or under-representation of certain groups in the sample\n\n\nNon-Sampling Errors:\n\nDefinition: All errors in a study that are not related to sampling\nCharacteristics:\n\nCan occur in both sample and census studies\nOften more difficult to quantify and control than sampling errors\nCan introduce bias into results\nCan be either systematic or random\n\nExamples:\n\nResponse errors (e.g., misunderstanding questions, deliberate misreporting)\nNonresponse bias (when certain groups are less likely to respond)\nData processing errors (e.g., coding mistakes, data entry errors)\nCoverage errors (when the sampling frame doesn’t accurately represent the population)\n\n\n\nImportant clarification: Non-sampling errors can indeed be either systematic or random. This is a crucial distinction that should have been included in the original description. Non-sampling errors encompass a wide range of potential errors that are not directly related to the sampling process. Some of these can be systematic (e.g., a miscalibrated measuring instrument), while others can be random (e.g., occasional mistakes in data entry).\nThe distinction between sampling and non-sampling errors is independent of the division between systematic and random errors. In practice, non-sampling errors can fall into both of these categories, which makes their identification and control a particularly important aspect of statistical research.\nUnderstanding these types of errors is crucial for designing robust statistical studies, interpreting results accurately, and making valid inferences about populations based on sample data.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#review-questions-and-exercises",
    "href": "chapter3.html#review-questions-and-exercises",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.11 Review Questions and Exercises",
    "text": "5.11 Review Questions and Exercises\n(…)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html",
    "href": "rozdzial3.html",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "",
    "text": "6.1 Wprowadzenie do Losowości\nLosowość jest fundamentalnym pojęciem w statystyce i badaniach naukowych. Odnosi się do nieprzewidywalności indywidualnych wyników, nawet gdy ogólny wzorzec może być przewidywalny. W naukach społecznych zrozumienie losowości jest kluczowe dla projektowania badań, zbierania danych i interpretacji wyników.\nRozważmy rzut uczciwą monetą. Chociaż wiemy, że prawdopodobieństwo wypadnięcia orła wynosi 50%, nie możemy z pewnością przewidzieć wyniku pojedynczego rzutu. Ta nieprzewidywalność jest istotą losowości.\nPrzykłady losowych zjawisk w naukach społecznych obejmują:\nZrozumienie losowości pomaga badaczom odróżnić rzeczywiste efekty od przypadkowych zdarzeń. Na przykład, jeśli zaobserwujemy niewielką różnicę w wynikach testów między dwiema grupami, losowość pomaga nam określić, czy ta różnica jest prawdopodobnie spowodowana rzeczywistym efektem, czy tylko przypadkową zmiennością.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wprowadzenie-do-losowości",
    "href": "rozdzial3.html#wprowadzenie-do-losowości",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "",
    "text": "Wybór uczestników: W eksperymencie psychologicznym badającym czasy reakcji, kolejność, w jakiej uczestnicy przybywają do laboratorium, może być losowa.\nZachowania ekonomiczne: Codzienne wahania cen akcji często wykazują losowe wzorce, na które wpływa niezliczona ilość nieprzewidywalnych czynników.\nInterakcje społeczne: Występowanie przypadkowych spotkań między osobami w społeczności można uznać za zdarzenia losowe.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#próbkowanie-łączenie-próby-i-populacji",
    "href": "rozdzial3.html#próbkowanie-łączenie-próby-i-populacji",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.2 Próbkowanie: Łączenie Próby i Populacji",
    "text": "6.2 Próbkowanie: Łączenie Próby i Populacji\nPróbkowanie to proces wybierania podzbioru (próby) z większej grupy (populacji) w celu wyciągnięcia wniosków o populacji. Jest to kluczowa umiejętność w badaniach nauk społecznych, ponieważ badanie całych populacji jest często niepraktyczne, zbyt kosztowne lub czasami niemożliwe.\nKluczowe pojęcia:\n\nPopulacja: Cała grupa, o której chcemy wyciągnąć wnioski.\nPróba: Podzbiór populacji, który faktycznie badamy.\nOperat losowania: Lista lub procedura używana do identyfikacji wszystkich członków populacji.\n\nPrzykład: Załóżmy, że chcemy zbadać satysfakcję z pracy wszystkich nauczycieli w Polsce (populacja). Zamiast ankietować setki tysięcy nauczycieli, możemy wybrać próbę 5000 nauczycieli z różnych województw, powiatów i poziomów nauczania.\nLosowość w próbkowaniu pomaga zapewnić, że próba jest reprezentatywna dla populacji, zmniejszając błędy systematyczne i umożliwiając dokładniejsze wnioskowanie. Dlatego metody próbkowania probabilistycznego, które omówimy dalej, są często preferowane w badaniach naukowych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#metody-próbkowania",
    "href": "rozdzial3.html#metody-próbkowania",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.3 Metody Próbkowania",
    "text": "6.3 Metody Próbkowania\n\n6.3.1 Próbkowanie Probabilistyczne\nMetody próbkowania probabilistycznego obejmują losowy wybór, dając każdemu członkowi populacji znaną, niezerową szansę na wybór.\n\nProsty Dobór Losowy: Każdy członek populacji ma równą szansę na wybór.\nPrzykład: Aby wybrać 100 studentów z uniwersytetu liczącego 10 000 studentów, można przypisać każdemu studentowi numer od 1 do 10 000, a następnie użyć generatora liczb losowych do wybrania 100 numerów.\nDobór Losowy Warstwowy: Populacja jest podzielona na podgrupy (warstwy) na podstawie wspólnych cech, a następnie próbki są losowo wybierane z każdej warstwy.\nPrzykład: W ogólnopolskim badaniu politycznym można podzielić populację na warstwy na podstawie regionów geograficznych (np. Polska Zachodnia, Centralna, Wschodnia) i losowo pobierać próbki z każdego regionu. Zapewnia to reprezentację ze wszystkich obszarów kraju.\nDobór Losowy Grupowy: Populacja jest podzielona na skupiska (zwykle geograficzne), niektóre skupiska są losowo wybierane, a wszyscy członkowie w tych skupiskach są badani.\nPrzykład: Aby zbadać nawyki uczenia się uczniów szkół średnich, można losowo wybrać 20 szkół z całego kraju, a następnie przeprowadzić ankietę wśród wszystkich uczniów w tych szkołach.\nDobór Systematyczny: Wybieranie co k-tego elementu z listy po losowym starcie.\nPrzykład: W ruchliwym centrum handlowym można ankietować co 20. osobę wchodzącą do centrum, zaczynając od losowo wybranej liczby między 1 a 20.\n\n\n\n6.3.2 Próbkowanie Nieprobabilistyczne\nPróbkowanie nieprobabilistyczne nie obejmuje losowego wyboru. Chociaż może wprowadzać błędy systematyczne, może być konieczne w niektórych sytuacjach, zwłaszcza w przypadku trudno dostępnych populacji lub gdy zasoby są ograniczone.\n\nDobór Wygodny: Wybieranie łatwo dostępnych podmiotów.\nPrzykład: Badacz studiujący wzorce snu studentów może przeprowadzić ankietę wśród studentów na własnych zajęciach lub na terenie kampusu.\nDobór Celowy: Wybieranie podmiotów na podstawie określonych cech.\nPrzykład: W badaniu doświadczeń prezesów w branży technologicznej badacz może celowo szukać i przeprowadzać wywiady z prezesami różnych firm technologicznych.\nDobór Metodą Kuli Śnieżnej: Uczestnicy rekrutują innych uczestników.\nPrzykład: W badaniu dostępu imigrantów bez dokumentów do opieki zdrowotnej, badacze mogą poprosić początkowych uczestników o polecenie innych potencjalnych uczestników z ich społeczności.\nDobór Kwotowy: Wybieranie uczestników w celu spełnienia określonych kwot dla pewnych cech.\nPrzykład: W badaniu rynku badacze mogą zapewnić, że przeprowadzają wywiady z określoną liczbą osób z różnych grup wiekowych, płci i poziomów dochodów, aby dopasować się do demografii rynku docelowego.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wnioskowanie-z-prób",
    "href": "rozdzial3.html#wnioskowanie-z-prób",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.4 Wnioskowanie z Prób",
    "text": "6.4 Wnioskowanie z Prób\nWnioskowanie statystyczne to proces wyciągania wniosków o populacji na podstawie próby. Pozwala to badaczom oszacować charakterystyki całej populacji (parametry) przy użyciu charakterystyk próby (statystyk).\n\n\n\n\n\n\nNote\n\n\n\nThe Soup Analogy: A Taste of Statistics\n\n\nWhen you taste a spoonful of soup and decide it isn’t salty enough, that’s exploratory/descriptive analysis.\nIf you generalize and conclude that your entire pot of soup needs salt, that’s an inference.\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population).\nIf the soup is not well stirred (heterogeneous population), it doesn’t matter how large a spoon you have (sample size), it will still not accurately represent the whole. If the soup is well stirred (homogeneous population), even a small spoon will suffice to test the soup.\n\n\n\nKluczowe pojęcia:\n\nEstymatory punktowe: Pojedyncza wartość używana do oszacowania parametru populacji.\nPrzykład: Średni dochód z próby 1000 pracowników może być użyty do oszacowania średniego dochodu wszystkich pracowników w kraju.\nPrzedziały ufności: Zakres wartości, który prawdopodobnie zawiera prawdziwy parametr populacji.\nPrzykład: Możemy powiedzieć: “Jesteśmy w 95% pewni, że prawdziwy średni dochód populacji mieści się między 4500 a 5500 złotych”.\nMargines błędu: Zakres wartości powyżej i poniżej statystyki z próby w przedziale ufności.\nPrzykład: W sondażach politycznych można zobaczyć stwierdzenie: “Kandydat A jest preferowany przez 52% wyborców, z marginesem błędu ±3%”.\nTestowanie hipotez: Metoda podejmowania decyzji o parametrach populacji na podstawie danych z próby.\nPrzykład: Badacz może testować, czy istnieje istotna różnica w wynikach testów między uczniami, którzy uczą się przy muzyce, a tymi, którzy uczą się w ciszy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#błędy-próbkowania-i-błędy-niepróbkowe",
    "href": "rozdzial3.html#błędy-próbkowania-i-błędy-niepróbkowe",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.5 Błędy Próbkowania i Błędy Niepróbkowe",
    "text": "6.5 Błędy Próbkowania i Błędy Niepróbkowe\nZrozumienie potencjalnych błędów w badaniach jest kluczowe dla dokładnej interpretacji wyników.\nBłąd próbkowania: Różnica między statystyką z próby a prawdziwym parametrem populacji, występująca z powodu przypadkowych wahań w wyborze członków próby.\nPrzykład: Jeśli oszacujemy średni wzrost wszystkich dorosłych mężczyzn w kraju na podstawie próby, nasze oszacowanie prawdopodobnie będzie się nieco różnić od prawdziwej średniej z powodu błędu próbkowania.\nBłędy niepróbkowe: Błędy nie wynikające z przypadku, które mogą wystąpić zarówno w badaniach próbkowych, jak i spisach.\n\nBłąd pokrycia: Gdy operat losowania nie reprezentuje dokładnie populacji.\nPrzykład: Badanie telefoniczne, które dzwoni tylko na telefony stacjonarne, pominęłoby osoby posiadające tylko telefony komórkowe, potencjalnie wypaczając wyniki.\nBłąd braku odpowiedzi: Gdy wybrani uczestnicy nie odpowiadają, potencjalnie wprowadzając błąd systematyczny.\nPrzykład: W badaniu satysfakcji z pracy, bardzo zadowoleni lub bardzo niezadowoleni pracownicy mogą być bardziej skłonni do odpowiedzi, wypaczając wyniki.\nBłąd pomiaru: Niedokładności w zebranych danych.\nPrzykład: Źle sformułowane pytanie ankietowe może być różnie interpretowane przez różnych respondentów, prowadząc do niespójnych danych.\nBłąd przetwarzania: Błędy popełnione podczas wprowadzania danych, kodowania lub analizy.\nPrzykład: Przypadkowe wprowadzenie “99” zamiast “9” dla odpowiedzi uczestnika mogłoby znacząco wypaczyć wyniki.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wielkość-próby-i-moc-statystyczna",
    "href": "rozdzial3.html#wielkość-próby-i-moc-statystyczna",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.6 Wielkość Próby i Moc Statystyczna",
    "text": "6.6 Wielkość Próby i Moc Statystyczna\nOkreślenie odpowiedniej wielkości próby wymaga zrównoważenia potrzeby precyzji z dostępnymi zasobami.\nRozważania dotyczące wielkości próby: - Większe próby generalnie zapewniają bardziej precyzyjne oszacowania, ale są bardziej kosztowne i czasochłonne do uzyskania. - Wymagana wielkość próby zależy od czynników takich jak pożądany poziom precyzji, zmienność w populacji i rodzaj planowanej analizy.\nPrzykład: Aby oszacować proporcję wyborców popierających konkretną politykę z marginesem błędu ±3% na poziomie ufności 95%, potrzebna byłaby próba około 1067 wyborców (zakładając maksymalną zmienność).\nMoc statystyczna: Prawdopodobieństwo, że badanie wykryje efekt, gdy taki efekt istnieje.\nCzynniki wpływające na moc: 1. Wielkość próby 2. Wielkość efektu (wielkość różnicy lub związku, który próbujemy wykryć) 3. Wybrany poziom istotności (zwykle 0,05)\nPrzykład: W badaniu porównującym dwie metody nauczania, większa wielkość próby zwiększyłaby prawdopodobieństwo wykrycia istotnej różnicy między metodami, jeśli taka różnica istnieje.\n\n\n\n\n\n\nNote\n\n\n\nCzym jest Moc Badania?\nMoc badania dotyczy tego, jak prawdopodobne jest, że znajdziemy coś, jeśli to naprawdę istnieje. To jak posiadanie dobrej latarki, gdy szukasz czegoś w ciemności - im lepsza latarka, tym bardziej prawdopodobne, że znajdziesz to, czego szukasz.\n\nWielkość Efektu: Jak duża jest rzecz (efekt, różnica, itp.), której szukamy.\nWielkość Próby: Ile osób lub rzeczy badamy w naszym studium.\nMoc Badania: Jak prawdopodobne jest, że znajdziemy efekt, jeśli naprawdę istnieje.\n\nZwiązek Między Wielkością Efektu a Wielkością Próby:\nWyobraź sobie, że próbujesz znaleźć monety ukryte w piasku:\n\nDuże Efekty (Duże Monety):\n\nJeśli szukasz dużych monet (jak 5 złotych), nie musisz przeszukiwać tak dużo piasku, aby je znaleźć.\nW badaniach, jeśli efekt jest duży, możesz użyć mniejszej próby.\n\nPrzykład: Testowanie, czy nowa metoda nauki poprawia wyniki testów o 20 punktów na 100.\n\nMoże wystarczyć przetestować 30 uczniów, aby zobaczyć tę dużą różnicę.\n\nMałe Efekty (Małe Monety):\n\nJeśli szukasz maleńkich monet (jak 1 grosz), będziesz musiał przeszukać więcej piasku.\nW badaniach, jeśli efekt jest mały, potrzebujesz większej próby.\n\nPrzykład: Sprawdzanie, czy korzystanie z mediów społecznościowych wpływa na szczęście o niewielką ilość.\n\nMoże być potrzeba zbadania 500 lub więcej osób, aby wykryć ten mały efekt.\n\n\nDlaczego Moc Badania Jest Ważna:\n\nNie Przegapienie Rzeczywistych Efektów:\n\nPrzy niskiej mocy możesz przeoczyć rzeczywiste efekty, jak używanie słabej latarki i przeoczenie czegoś, co faktycznie tam jest.\n\nPewność Wyników:\n\nWyższa moc daje większą pewność, że to, co znalazłeś, jest prawdziwe, a nie tylko przypadkiem.\n\n\nPrzykład:\nZałóżmy, że chcemy zbadać, czy nowa metoda nauczania pomaga uczniom lepiej się uczyć:\n\nMałe Badanie (Niska Moc):\n\nPróbujemy metody z zaledwie 10 uczniami.\nNawet jeśli metoda działa, przy tak małej grupie trudno stwierdzić, czy poprawa wynika z nowej metody, czy to tylko przypadek.\n\nWiększe Badanie (Wyższa Moc):\n\nStosujemy metodę ze 100 uczniami.\nTeraz jest bardziej prawdopodobne, że zobaczymy, czy metoda naprawdę pomaga, ponieważ mamy więcej danych do analizy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#próbkowanie-w-erze-cyfrowej",
    "href": "rozdzial3.html#próbkowanie-w-erze-cyfrowej",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.7 Próbkowanie w Erze Cyfrowej",
    "text": "6.7 Próbkowanie w Erze Cyfrowej\nPojawienie się big data i technologii cyfrowych zmieniło praktyki próbkowania w wielu dziedzinach.\nMożliwości i wyzwania Big Data: - Bezprecedensowe ilości dostępnych informacji - Potencjalny brak reprezentatywności - Problemy z jakością danych - Kwestie prywatności i etyki\nPrzykład: Dane z mediów społecznościowych mogą dostarczyć wglądu w opinię publiczną w czasie rzeczywistym, ale użytkownicy konkretnej platformy mogą nie być reprezentatywni dla ogólnej populacji.\nBadania internetowe: - Oferują nowe możliwości zbierania danych - Stają przed wyzwaniami takimi jak błąd pokrycia (nie każdy ma dostęp do internetu) i błąd samoselekcji\nPrzykład: Ankieta online na temat nawyków korzystania z internetu z natury wykluczałaby osoby bez dostępu do internetu, potencjalnie wypaczając wyniki.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#etyczne-aspekty-próbkowania",
    "href": "rozdzial3.html#etyczne-aspekty-próbkowania",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.8 Etyczne Aspekty Próbkowania",
    "text": "6.8 Etyczne Aspekty Próbkowania\nEtyczne praktyki próbkowania są kluczowe w badaniach nauk społecznych:\n\nŚwiadoma zgoda: Uczestnicy powinni rozumieć cel badania i zgodzić się na udział.\nPrzykład: Przed przeprowadzeniem wywiadów na temat wrażliwych tematów, takich jak zdrowie psychiczne, badacze muszą jasno wyjaśnić cele badania i potencjalne ryzyko uczestnikom.\nPrywatność i poufność: Badacze muszą chronić dane osobowe uczestników.\nPrzykład: W badaniu dotyczącym mobbingu w miejscu pracy, badacze mogą używać kodów numerycznych zamiast nazwisk, aby chronić tożsamość uczestników.\nReprezentatywność i inkluzywność: Próby powinny sprawiedliwie reprezentować zróżnicowane populacje, w tym grupy marginalizowane.\n\nPrzykład: Badanie dotyczące mieszkalnictwa miejskiego powinno dołożyć starań, aby uwzględnić uczestników z różnych środowisk społeczno-ekonomicznych, grup etnicznych i sytuacji mieszkaniowych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#podsumowanie",
    "href": "rozdzial3.html#podsumowanie",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.9 Podsumowanie",
    "text": "6.9 Podsumowanie\nPróbkowanie pozostaje fundamentem badań w naukach społecznych, nawet w erze big data. Zrozumienie zasad próbkowania pomaga badaczom projektować badania, interpretować wyniki i wyciągać trafne wnioski o populacjach. Jak widzieliśmy, droga od próby do populacji wymaga starannego rozważenia metod próbkowania, potencjalnych błędów, kwestii etycznych i stale ewoluującego krajobrazu gromadzenia danych w erze cyfrowej.\nDziękuję za to bardzo trafne pytanie. Ma Pan/Pani rację, i doceniam tę uwagę. Rzeczywiście, tłumaczenie “błąd samplowy vs. niesamplowy” jest bardziej precyzyjne i lepiej oddaje istotę tych pojęć w polskiej terminologii statystycznej. Pozwolę sobie wprowadzić odpowiednie korekty i wyjaśnienia:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#błędy-statystyczne---podsumowanie",
    "href": "rozdzial3.html#błędy-statystyczne---podsumowanie",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.10 Błędy Statystyczne - podsumowanie",
    "text": "6.10 Błędy Statystyczne - podsumowanie\n\n6.10.1 Błąd Systematyczny vs. Błąd Losowy\nBłędy systematyczne i losowe to dwa podstawowe rodzaje błędów w pomiarach i eksperymentach statystycznych.\n\nBłąd Systematyczny:\n\nDefinicja: Konsekwentne, przewidywalne odchylenia od prawdziwej wartości\nCharakterystyka:\n\nZniekształca wyniki w określonym kierunku\nPowtarzalny i często stały w różnych pomiarach\nMoże być skorygowany, jeśli zostanie zidentyfikowany\n\nPrzykłady:\n\nŹle skalibrowany przyrząd pomiarowy\nKonsekwentny błąd zaokrąglania przy wprowadzaniu danych\nStronnicza metoda pobierania próbek\n\n\nBłąd Losowy:\n\nDefinicja: Nieprzewidywalne wahania w pomiarach wynikające z przypadku\nCharakterystyka:\n\nZmienia się co do wielkości i kierunku\nPodąża za rozkładem prawdopodobieństwa (często normalnym)\nMożna go zmniejszyć zwiększając wielkość próby lub powtarzając pomiary\n\nPrzykłady:\n\nNaturalne wahania w badanym zjawisku\nMałe fluktuacje w przyrządach pomiarowych\nBłędy ludzkie przy odczytywaniu lub zapisywaniu danych\n\n\n\n\n\n6.10.2 Błędy Samplowe vs. Błędy Niesamplowe\n\nBłędy Samplowe (lub Błędy Próbkowania):\n\nDefinicja: Błędy wynikające z tego, że próba (sample) nie reprezentuje idealnie populacji\nCharakterystyka:\n\nNieodłączne w każdym badaniu opartym na próbie\nMożna je oszacować i skwantyfikować za pomocą metod statystycznych\nZmniejszają się wraz ze wzrostem wielkości próby\n\nPrzykłady:\n\nLosowe wahania w statystykach (z) próby\nNadreprezentacja lub niedoreprezentowanie niektórych grup w próbie\n\n\nBłędy Niesamplowe:\n\nDefinicja: Wszystkie błędy w badaniu, które nie są związane z próbkowaniem (samplingiem)\nCharakterystyka:\n\nMogą wystąpić zarówno w badaniach próbkowych, jak i pełnych (spisach)\nCzęsto trudniejsze do skwantyfikowania i kontrolowania niż błędy samplowe\nMogą wprowadzać stronniczość do wyników\nMogą być systematyczne lub losowe\n\nPrzykłady:\n\nBłędy odpowiedzi (np. niezrozumienie pytań, celowe błędne raportowanie)\nBłąd braku odpowiedzi (gdy niektóre grupy są mniej skłonne do odpowiedzi)\nBłędy przetwarzania danych (np. błędy kodowania, błędy wprowadzania danych)\nBłędy pokrycia (gdy operat losowania nie reprezentuje dokładnie populacji)\n\n\n\nWażne wyjaśnienie: Błędy niesamplowe mogą być zarówno systematyczne, jak i losowe. To kluczowe rozróżnienie, które powinienem był uwzględnić w pierwotnym opisie. Błędy niesamplowe obejmują szeroki zakres możliwych błędów, które nie są bezpośrednio związane z procesem próbkowania. Niektóre z nich mogą być systematyczne (np. błędnie skalibrowany instrument pomiarowy), podczas gdy inne mogą być losowe (np. przypadkowe błędy przy wprowadzaniu danych).\nRozróżnienie na błędy samplowe i niesamplowe jest niezależne od podziału na błędy systematyczne i losowe. W praktyce, błędy niesamplowe mogą należeć do obu tych kategorii, co czyni ich identyfikację i kontrolę szczególnie ważnym aspektem badań statystycznych.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff\n\n\nKluczowe punkty do zapamiętania:\n\nLosowość jest podstawą wielu metod próbkowania i pomaga zapewnić reprezentatywność próby.\nIstnieją różne metody próbkowania, zarówno probabilistyczne, jak i nieprobabilistyczne, każda z własnymi zaletami i ograniczeniami.\nWnioskowanie statystyczne pozwala nam wyciągać wnioski o populacji na podstawie danych z próby.\nBłędy próbkowania i niepróbkowe mogą wpływać na jakość naszych wniosków, dlatego ważne jest ich zrozumienie i minimalizowanie.\nWielkość próby i moc statystyczna są kluczowe dla zapewnienia wiarygodności wyników badań.\nEra cyfrowa przynosi nowe możliwości i wyzwania w zakresie próbkowania i gromadzenia danych.\nEtyczne aspekty próbkowania, w tym świadoma zgoda, prywatność i reprezentatywność, są nieodłączną częścią procesu badawczego.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "chapter3b.html",
    "href": "chapter3b.html",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "",
    "text": "7.1 Defining Reliability and Validity\nReliability refers to the consistency of a measure. A reliable measurement or study produces similar results under consistent conditions.\nValidity refers to the accuracy of a measure. A valid measurement or study accurately represents what it claims to measure.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#the-four-combinations-of-reliability-and-validity",
    "href": "chapter3b.html#the-four-combinations-of-reliability-and-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.2 The Four Combinations of Reliability and Validity",
    "text": "7.2 The Four Combinations of Reliability and Validity\nThere are four possible combinations of reliability and validity:\n\nHigh Reliability, High Validity\nHigh Reliability, Low Validity\nLow Reliability, High Validity\nLow Reliability, Low Validity\n\nLet’s explore each of these combinations with examples and visualizations.\n\n7.2.1 1. High Reliability, High Validity\nThis is the ideal scenario in research. Measurements are both consistent and accurate.\nExample: A well-calibrated digital scale used to measure weight. It consistently gives the same reading for the same object and accurately represents the true weight.\n\n\n7.2.2 2. High Reliability, Low Validity\nIn this case, measurements are consistent but not accurate.\nExample: A miscalibrated scale that always measures 5 kg too heavy. It gives consistent results (high reliability) but doesn’t represent the true weight (low validity).\n\n\n7.2.3 3. Low Reliability, High Validity\nHere, measurements are accurate on average but inconsistent.\nExample: A scale that fluctuates around the true weight. Sometimes it’s a bit over, sometimes a bit under, but on average, it’s correct.\n\n\n7.2.4 4. Low Reliability, Low Validity\nThis is the worst-case scenario, where measurements are neither consistent nor accurate.\nExample: A broken scale that gives random readings unrelated to the true weight.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#visualizing-reliability-and-validity",
    "href": "chapter3b.html#visualizing-reliability-and-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.3 Visualizing Reliability and Validity",
    "text": "7.3 Visualizing Reliability and Validity\nTo better understand these concepts, let’s create visualizations using ggplot2 in R. We’ll simulate measurement data for each scenario and plot them.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generate data for each scenario\nn &lt;- 100\ntrue_value &lt;- 50\n\ndata &lt;- tibble(\n  high_rel_high_val = rnorm(n, mean = true_value, sd = 1),\n  high_rel_low_val = rnorm(n, mean = true_value + 5, sd = 1),\n  low_rel_high_val = rnorm(n, mean = true_value, sd = 5),\n  low_rel_low_val = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenario\", values_to = \"measurement\")\n\n# Create the scatterplot\nscatter_plot &lt;- ggplot(data, aes(x = id, y = measurement, color = scenario)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = true_value, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Scatterplots of Measurements\",\n       subtitle = \"Dashed line represents the true value\",\n       x = \"Measurement ID\",\n       y = \"Measured Value\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Create the histogram\nhist_plot &lt;- ggplot(data, aes(x = measurement, fill = scenario)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = true_value, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Histograms of Measurements\",\n       subtitle = \"Red dashed line represents the true value\",\n       x = \"Measured Value\",\n       y = \"Count\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Combine the plots\ncombined_plot &lt;- scatter_plot / hist_plot +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Reliability and Validity in Measurements\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Display the combined plot\ncombined_plot\n\n\n\n\n\n\n\n\n\n7.3.1 Interpreting the Visualizations\n\nHigh Reliability, High Validity: Points cluster tightly around the true value (dashed line).\nHigh Reliability, Low Validity: Points cluster tightly, but consistently above the true value.\nLow Reliability, High Validity: Points scatter widely but center around the true value.\nLow Reliability, Low Validity: Points scatter randomly with no clear pattern or relation to the true value.\n\nUnderstanding reliability and validity is crucial in data science and research. High reliability ensures consistent measurements, while high validity ensures accurate representations of what we intend to measure. By considering both aspects, researchers can design more robust studies and draw more meaningful conclusions from their data.\nWhen conducting your own research or analyzing others’ work, always consider: - How reliable are the measurements? - How valid is the approach for measuring the intended concept? - Do the methods used support both reliability and validity?\nBy keeping these questions in mind, you’ll be better equipped to produce and interpret high-quality research in data science.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#types-of-reliability",
    "href": "chapter3b.html#types-of-reliability",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.4 Types of Reliability",
    "text": "7.4 Types of Reliability\nReliability can be assessed in several ways, each focusing on a different aspect of consistency:\n\nTest-Retest Reliability: This measures the consistency of a test over time. It involves administering the same test to the same group of participants at different times and comparing the results.\nInter-Rater Reliability: This assesses the degree of agreement among different raters or observers. It’s crucial when subjective judgments are involved in data collection.\nInternal Consistency: This evaluates how well different items on a test or scale measure the same construct. Cronbach’s alpha is a common measure of internal consistency.\nParallel Forms Reliability: This involves creating two equivalent forms of a test and administering them to the same group. The correlation between the two sets of scores indicates reliability.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#types-of-validity",
    "href": "chapter3b.html#types-of-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.5 Types of Validity",
    "text": "7.5 Types of Validity\nValidity is a multifaceted concept, with several types that researchers need to consider:\n\nContent Validity: This ensures that a measure covers all aspects of the construct it aims to measure. It’s often assessed by expert judgment.\nConstruct Validity: This evaluates whether a test measures the intended theoretical construct. It includes:\n\nConvergent Validity: The degree to which the measure correlates with other measures of the same construct.\nDiscriminant Validity: The extent to which the measure does not correlate with measures of different constructs.\n\nCriterion Validity: This assesses how well a measure predicts an outcome. It includes:\n\nConcurrent Validity: How well the measure correlates with other measures of the same construct at the same time.\nPredictive Validity: How well the measure predicts future outcomes.\n\nFace Validity: Face validity describes how test subjects perceive the test and whether - from their point of view - it is adequate for the purpose it is supposed to serve. A lack of face validity, even though the test may be valid from the perspective of a specific purpose, can contribute to a decrease in motivation among test subjects, which directly affects the results achieved or may lead to rejection of the test. While not a scientific measure, it can be important for participant buy-in.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#internal-vs.-external-validity",
    "href": "chapter3b.html#internal-vs.-external-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.6 Internal vs. External Validity",
    "text": "7.6 Internal vs. External Validity\nThese concepts are crucial in experimental design and the generalizability of research findings:\n\n7.6.1 Internal Validity\nInternal validity refers to the extent to which a study establishes a causal relationship between the independent and dependent variables. It answers the question: “Did the experimental treatment actually cause the observed effects?”\nFactors that can threaten internal validity include: - History: External events occurring between pre-test and post-test - Maturation: Natural changes in participants over time - Testing effects: Changes due to taking a pre-test - Instrumentation: Changes in the measurement tool or observers - Selection bias: Non-random assignment to groups - Attrition: Loss of participants during the study\n\n\n7.6.2 External Validity\nExternal validity refers to the extent to which the results of a study can be generalized to other situations, populations, or settings. It addresses the question: “To what extent can the findings be applied beyond the specific context of the study?”\nFactors that can affect external validity include: - Population validity: How well the sample represents the larger population - Ecological validity: How well the study setting represents real-world conditions - Temporal validity: Whether the results hold true across time",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#consistency-in-research",
    "href": "chapter3b.html#consistency-in-research",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.7 Consistency in Research",
    "text": "7.7 Consistency in Research\nConsistency is closely related to reliability but extends beyond just measurement. In research, consistency refers to the overall coherence and stability of results across different contexts, methods, or studies.\nKey aspects of consistency in research include:\n\nReplicability: The ability to reproduce study results using the same methods and data.\nRobustness: The stability of findings across different analytical approaches or slight variations in methodology.\nConvergence: The alignment of results from different studies or methods investigating the same phenomenon.\nLongitudinal Consistency: The stability of findings over time, especially important in longitudinal studies.\n\nEnsuring consistency in research involves: - Using standardized procedures and measures - Thoroughly documenting methods and analytical decisions - Conducting replication studies - Meta-analyses to synthesize findings across multiple studies",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#balancing-reliability-validity-and-consistency",
    "href": "chapter3b.html#balancing-reliability-validity-and-consistency",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.8 Balancing Reliability, Validity, and Consistency",
    "text": "7.8 Balancing Reliability, Validity, and Consistency\nWhile reliability, validity, and consistency are all crucial for high-quality research, they sometimes involve trade-offs:\n\nA highly reliable measure might lack validity if it consistently measures the wrong thing.\nStriving for perfect internal validity (e.g., in tightly controlled lab experiments) might reduce external validity.\nEnsuring high consistency across diverse contexts might require sacrificing some degree of precision or depth in specific situations.\n\nResearchers must carefully balance these aspects based on their research questions and the nature of their study. A comprehensive understanding of reliability, validity, and consistency helps in designing robust studies, interpreting results accurately, and contributing meaningfully to the body of scientific knowledge.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#bias-variance-tradeoff",
    "href": "chapter3b.html#bias-variance-tradeoff",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.9 Bias-Variance Tradeoff",
    "text": "7.9 Bias-Variance Tradeoff\nThe concepts of reliability and validity are closely related to the statistical notion of the bias-variance tradeoff. This tradeoff is fundamental in machine learning and statistical modeling.\n\nBias refers to the error introduced by approximating a real-world problem with a simplified model. High bias can lead to underfitting.\nVariance refers to the error introduced by the model’s sensitivity to small fluctuations in the training set. High variance can lead to overfitting.\n\nLet’s visualize this concept with a simplified plot:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_true &lt;- sin(x)\ny_low_bias_high_var &lt;- y_true + rnorm(100, 0, 0.3)\ny_high_bias_low_var &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_true, y_low_bias_high_var, y_high_bias_low_var),\n                 type = rep(c(\"True Function\", \"Low Bias, High Variance\", \"High Bias, Low Variance\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = type)) +\n  geom_line() +\n  geom_point(data = subset(df, type != \"True Function\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Bias-Variance Tradeoff\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Model Type\") +\n  theme_minimal()\n\n\n\n\nVisualization of Bias-Variance Tradeoff\n\n\n\n\nIn this plot: - The black line represents the true underlying function. - The blue points represent a model with low bias but high variance. It follows the true function closely on average but has a lot of noise. - The red line represents a model with high bias but low variance. It consistently underestimates the true function but has less noise.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#accuracy-and-precision",
    "href": "chapter3b.html#accuracy-and-precision",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.10 Accuracy and Precision",
    "text": "7.10 Accuracy and Precision\nThe concepts of accuracy and precision are closely related to validity and reliability:\n\nAccuracy refers to how close a measurement is to the true value (similar to validity).\nPrecision refers to how consistent or reproducible the measurements are (similar to reliability).\n\nWe can visualize these concepts using a simplified target analogy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"High Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Low Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"High Accuracy\\nLow Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Low Accuracy\\nLow Precision\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Accuracy vs Precision\")\n\n\n\n\nVisualization of Accuracy vs Precision\n\n\n\n\nIn this visualization: - High accuracy means the points are close to the center (bullseye). - High precision means the points are tightly clustered. - Each panel represents a different combination of accuracy and precision.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#conclusion",
    "href": "chapter3b.html#conclusion",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.11 Conclusion",
    "text": "7.11 Conclusion\nUnderstanding reliability and validity is crucial for conducting robust research. These concepts help us ensure that our measurements are both consistent and accurate. By relating them to ideas like the bias-variance tradeoff and accuracy-precision, we gain a deeper appreciation of the challenges involved in measurement and modeling in scientific research. As researchers, we must strive to develop measures and models that are both reliable and valid, balancing the tradeoffs between bias and variance, and between accuracy and precision. This requires careful design of research methodologies, rigorous testing of our measurement instruments, and thoughtful interpretation of our results.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#understanding-bias-vs.-variance-in-statistical-measurement",
    "href": "chapter3b.html#understanding-bias-vs.-variance-in-statistical-measurement",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.12 Understanding Bias vs. Variance in Statistical Measurement",
    "text": "7.12 Understanding Bias vs. Variance in Statistical Measurement\n\n7.12.1 Introduction\nIn statistics and machine learning, two important concepts that affect the performance of our models are bias and variance. Understanding these concepts is crucial for building effective predictive models and avoiding common pitfalls like overfitting and underfitting.\n\nBias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can lead to underfitting.\n\nThink of bias as how far off our predictions are from the true values on average.\nIn terms of validity, high bias means our model isn’t capturing the true relationship in the data.\n\nVariance refers to the amount by which our model would change if we estimated it using a different training dataset. High variance can lead to overfitting.\n\nThink of variance as how much our predictions would fluctuate if we used different datasets.\nIn terms of reliability, high variance means our model is too sensitive to the specific data it was trained on.\n\n\nWe’ll explore four scenarios to illustrate different combinations of bias and variance using synthetic data and regression models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#data-generation-and-model-fitting-function",
    "href": "chapter3b.html#data-generation-and-model-fitting-function",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.13 Data Generation and Model Fitting Function",
    "text": "7.13 Data Generation and Model Fitting Function\nFirst, let’s create a function that will help us generate data and fit models for each scenario:\n\ngenerate_and_fit &lt;- function(n, intercept, slope, noise_sd, model_degree) {\n  # Generate synthetic data\n  x &lt;- runif(n, 0, 10)\n  y &lt;- intercept + slope * x + rnorm(n, 0, noise_sd)\n  \n  # Fit model\n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", model_degree, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  # Generate predictions\n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  # Plot\n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    geom_abline(intercept = intercept, slope = slope, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Model Degree:\", model_degree),\n         subtitle = paste(\"Noise SD:\", noise_sd),\n         x = \"X (Input Variable)\",\n         y = \"Y (Target Variable)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nThis function does the following: 1. Generates synthetic data based on our parameters 2. Fits a polynomial regression model 3. Creates a plot showing the true relationship (blue dashed line), our model’s predictions (red solid line), and the data points\nNow, let’s explore our four scenarios!\n\n7.13.1 Scenario 1: Low Bias, Low Variance\nIn this ideal scenario, we use a linear model to fit linear data with low noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 1, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The model (red line) closely follows the true relationship (blue dashed line). - Data points are clustered tightly around the line, indicating low noise. - This scenario represents a good fit: the model captures the underlying trend without being overly complex.\n\n\n7.13.2 Scenario 2: Low Bias, High Variance\nHere, we use a linear model to fit linear data, but with high noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 5, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The model still captures the general trend, but data points are more scattered. - This high variance means our model’s predictions would be less reliable. - In real-world terms, this might represent a situation where our measurements are correct on average but have a lot of random error.\n\n\n7.13.3 Scenario 3: High Bias, Low Variance\nIn this case, we use a linear model to fit quadratic (curved) data with low noise.\n\nquadratic_data &lt;- function(n, intercept, slope, noise_sd, model_degree) {\n  x &lt;- runif(n, 0, 10)\n  y &lt;- intercept + slope * x^2 + rnorm(n, 0, noise_sd)\n  \n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", model_degree, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    stat_function(fun = function(x) intercept + slope * x^2, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Model Degree:\", model_degree),\n         subtitle = paste(\"Noise SD:\", noise_sd),\n         x = \"X (Input Variable)\",\n         y = \"Y (Target Variable)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nquadratic_data(n = 100, intercept = 1, slope = 0.2, noise_sd = 1, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The linear model (red line) fails to capture the curvature of the true relationship (blue dashed line). - This high bias means our model is consistently off in its predictions. - In real-world terms, this might represent using an overly simplistic model for a complex phenomenon.\n\n\n7.13.4 Scenario 4: High Bias, High Variance\nFinally, we use a high-degree polynomial to fit linear data with high noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 5, model_degree = 5)\n\n\n\n\n\n\n\n\nExplanation: - The model (red line) is overly complex, trying to fit the noise rather than the underlying trend. - This combination of high bias and high variance leads to poor generalization. - In real-world terms, this might represent overcomplicating our analysis and drawing false conclusions from random fluctuations in our data.\n\n\n7.13.5 Conclusion\nUnderstanding the bias-variance trade-off is crucial in statistical modeling:\n\nLow Bias, Low Variance: The ideal scenario, where our model accurately captures the underlying relationship without being overly sensitive to noise.\nLow Bias, High Variance: Our model is correct on average but unreliable due to high sensitivity to individual data points.\nHigh Bias, Low Variance: Our model is consistently wrong due to oversimplification but gives stable predictions.\nHigh Bias, High Variance: The worst-case scenario, where our model is both inaccurate and unreliable.\n\nIn practice, we often need to balance bias and variance. Techniques like cross-validation, regularization, and ensemble methods can help find this balance.\nRemember: - A model with high bias is too simple and misses important patterns in the data. - A model with high variance is too complex and fits noise in the training data. - The goal is to find a sweet spot that captures true patterns without overfitting to noise.\nBy understanding these concepts, you’ll be better equipped to choose appropriate models, avoid overfitting and underfitting, and build more effective predictive models in your future statistical analyses!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html",
    "href": "rozdzial3b.html",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "",
    "text": "8.1 Definiowanie Rzetelności i Trafności\nRzetelność odnosi się do spójności pomiaru. Rzetelny pomiar lub badanie daje podobne wyniki w spójnych warunkach.\nTrafność odnosi się do dokładności pomiaru. Trafny pomiar lub badanie dokładnie reprezentuje to, co twierdzi, że mierzy.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#cztery-kombinacje-rzetelności-i-trafności",
    "href": "rozdzial3b.html#cztery-kombinacje-rzetelności-i-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.2 Cztery Kombinacje Rzetelności i Trafności",
    "text": "8.2 Cztery Kombinacje Rzetelności i Trafności\nIstnieją cztery możliwe kombinacje rzetelności i trafności:\n\nWysoka Rzetelność, Wysoka Trafność\nWysoka Rzetelność, Niska Trafność\nNiska Rzetelność, Wysoka Trafność\nNiska Rzetelność, Niska Trafność\n\nPrzyjrzyjmy się każdej z tych kombinacji z przykładami i wizualizacjami.\n\n8.2.1 1. Wysoka Rzetelność, Wysoka Trafność\nTo idealny scenariusz w badaniach. Pomiary są zarówno spójne, jak i dokładne.\nPrzykład: Dobrze skalibrowana waga cyfrowa używana do pomiaru wagi. Konsekwentnie daje ten sam odczyt dla tego samego obiektu i dokładnie reprezentuje prawdziwą wagę.\n\n\n8.2.2 2. Wysoka Rzetelność, Niska Trafność\nW tym przypadku pomiary są spójne, ale niedokładne.\nPrzykład: Źle skalibrowana waga, która zawsze mierzy 5 kg za ciężko. Daje spójne wyniki (wysoka rzetelność), ale nie reprezentuje prawdziwej wagi (niska trafność).\n\n\n8.2.3 3. Niska Rzetelność, Wysoka Trafność\nTutaj pomiary są dokładne średnio, ale niespójne.\nPrzykład: Waga, która waha się wokół prawdziwej wagi. Czasami pokazuje trochę więcej, czasami trochę mniej, ale średnio jest poprawna.\n\n\n8.2.4 4. Niska Rzetelność, Niska Trafność\nTo najgorszy scenariusz, gdzie pomiary nie są ani spójne, ani dokładne.\nPrzykład: Zepsuta waga, która daje losowe odczyty niezwiązane z prawdziwą wagą.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#wizualizacja-rzetelności-i-trafności",
    "href": "rozdzial3b.html#wizualizacja-rzetelności-i-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.3 Wizualizacja Rzetelności i Trafności",
    "text": "8.3 Wizualizacja Rzetelności i Trafności\nAby lepiej zrozumieć te pojęcia, stwórzmy wizualizacje przy użyciu ggplot2 w R. Zasymulujemy dane pomiarowe dla każdego scenariusza i narysujemy je.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generowanie danych dla każdego scenariusza\nn &lt;- 100\nprawdziwa_wartosc &lt;- 50\n\ndane &lt;- tibble(\n  wysoka_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 1),\n  wysoka_rz_niska_tr = rnorm(n, mean = prawdziwa_wartosc + 5, sd = 1),\n  niska_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 5),\n  niska_rz_niska_tr = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenariusz\", values_to = \"pomiar\")\n\n# Tworzenie wykresu punktowego\nwykres_punktowy &lt;- ggplot(dane, aes(x = id, y = pomiar, color = scenariusz)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = prawdziwa_wartosc, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelność, Wysoka Trafność\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelność, Niska Trafność\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelność, Wysoka Trafność\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelność, Niska Trafność\"\n             ))) +\n  labs(title = \"Wykresy punktowe pomiarów\",\n       subtitle = \"Przerywana linia reprezentuje prawdziwą wartość\",\n       x = \"ID pomiaru\",\n       y = \"Zmierzona wartość\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Tworzenie histogramu\nwykres_hist &lt;- ggplot(dane, aes(x = pomiar, fill = scenariusz)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = prawdziwa_wartosc, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelność, Wysoka Trafność\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelność, Niska Trafność\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelność, Wysoka Trafność\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelność, Niska Trafność\"\n             ))) +\n  labs(title = \"Histogramy pomiarów\",\n       subtitle = \"Czerwona przerywana linia reprezentuje prawdziwą wartość\",\n       x = \"Zmierzona wartość\",\n       y = \"Liczba\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Łączenie wykresów\nwykres_polaczony &lt;- wykres_punktowy / wykres_hist +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Rzetelność i Trafność w Pomiarach\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Wyświetlanie połączonego wykresu\nwykres_polaczony\n\n\n\n\n\n\n\n\n\n8.3.1 Interpretacja Wizualizacji\n\nWysoka Rzetelność, Wysoka Trafność: Punkty grupują się ciasno wokół prawdziwej wartości (przerywana linia).\nWysoka Rzetelność, Niska Trafność: Punkty grupują się ciasno, ale konsekwentnie powyżej prawdziwej wartości.\nNiska Rzetelność, Wysoka Trafność: Punkty rozpraszają się szeroko, ale centrują się wokół prawdziwej wartości.\nNiska Rzetelność, Niska Trafność: Punkty rozpraszają się losowo bez wyraźnego wzoru lub relacji do prawdziwej wartości.\n\nZrozumienie rzetelności i trafności jest kluczowe w naukach o danych i badaniach. Wysoka rzetelność zapewnia spójne pomiary, podczas gdy wysoka trafność zapewnia dokładne reprezentacje tego, co zamierzamy zmierzyć. Biorąc pod uwagę oba aspekty, badacze mogą projektować bardziej solidne badania i wyciągać bardziej znaczące wnioski ze swoich danych.\nProwadząc własne badania lub analizując pracę innych, zawsze należy rozważyć: - Jak rzetelne są pomiary? - Jak trafne jest podejście do pomiaru zamierzonego pojęcia? - Czy stosowane metody wspierają zarówno rzetelność, jak i trafność?\nMając na uwadze te pytania, będziesz lepiej przygotowany do prowadzenia i interpretowania wysokiej jakości badań w naukach o danych.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#rodzaje-rzetelności",
    "href": "rozdzial3b.html#rodzaje-rzetelności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.4 Rodzaje Rzetelności",
    "text": "8.4 Rodzaje Rzetelności\nRzetelność można oceniać na kilka sposobów, każdy skupiający się na innym aspekcie spójności:\n\nRzetelność test-retest: Mierzy spójność testu w czasie. Polega na przeprowadzeniu tego samego testu na tej samej grupie uczestników w różnych momentach i porównaniu wyników.\nRzetelność między oceniającymi: Ocenia stopień zgodności między różnymi oceniającymi lub obserwatorami. Jest kluczowa, gdy w zbieraniu danych biorą udział subiektywne osądy.\nSpójność wewnętrzna: Ocenia, jak dobrze różne elementy testu lub skali mierzą ten sam konstrukt. Alfa Cronbacha jest powszechną miarą spójności wewnętrznej.\nRzetelność form równoległych: Polega na stworzeniu dwóch równoważnych form testu i przeprowadzeniu ich na tej samej grupie. Korelacja między dwoma zestawami wyników wskazuje na rzetelność.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#rodzaje-trafności",
    "href": "rozdzial3b.html#rodzaje-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.5 Rodzaje Trafności",
    "text": "8.5 Rodzaje Trafności\nTrafność jest pojęciem wieloaspektowym, z kilkoma rodzajami, które badacze muszą wziąć pod uwagę:\n\nTrafność treściowa: Zapewnia, że pomiar obejmuje wszystkie aspekty konstruktu, który ma mierzyć. Często jest oceniana przez osąd ekspertów.\nTrafność konstrukcyjna: Ocenia, czy test mierzy zamierzony konstrukt teoretyczny. Obejmuje:\n\nTrafność zbieżną: Stopień, w jakim pomiar koreluje z innymi pomiarami tego samego konstruktu.\nTrafność różnicową: Zakres, w jakim pomiar nie koreluje z pomiarami różnych konstruktów.\n\nTrafność kryterialną: Ocenia, jak dobrze pomiar przewiduje wynik. Obejmuje:\n\nTrafność współbieżną: Jak dobrze pomiar koreluje z innymi pomiarami tego samego konstruktu w tym samym czasie.\nTrafność predykcyjną: Jak dobrze pomiar przewiduje przyszłe wyniki.\n\nTrafność fasadowa: Trafność fasadowa odnosi się do tego, jak osoby badane postrzegają test i czy uważają go za odpowiedni do celu, któremu ma służyć. Brak trafności fasadowej może mieć negatywne konsekwencje, nawet jeśli test jest faktycznie trafny (czyli mierzy to, co powinien mierzyć) z punktu widzenia jego zamierzonego celu. Choć nie jest to naukowa miara, może być ważna dla zaangażowania uczestników.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#trafność-wewnętrzna-vs-zewnętrzna",
    "href": "rozdzial3b.html#trafność-wewnętrzna-vs-zewnętrzna",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.6 Trafność Wewnętrzna vs Zewnętrzna",
    "text": "8.6 Trafność Wewnętrzna vs Zewnętrzna\nTe pojęcia są kluczowe w projektowaniu eksperymentów i możliwości uogólniania wyników badań:\n\n8.6.1 Trafność Wewnętrzna\nTrafność wewnętrzna odnosi się do zakresu, w jakim badanie ustanawia związek przyczynowy między zmiennymi niezależnymi a zależnymi. Odpowiada na pytanie: “Czy eksperymentalne traktowanie rzeczywiście spowodowało zaobserwowane efekty?”\nCzynniki, które mogą zagrażać trafności wewnętrznej, obejmują: - Historia: Zewnętrzne wydarzenia występujące między pre-testem a post-testem - Dojrzewanie: Naturalne zmiany u uczestników w czasie - Efekty testowania: Zmiany wynikające z przeprowadzenia pre-testu - Instrumentacja: Zmiany w narzędziu pomiarowym lub obserwatorach - Błąd selekcji: Nielosowy przydział do grup - Utrata: Utrata uczestników podczas badania\n\n\n8.6.2 Trafność Zewnętrzna\nTrafność zewnętrzna odnosi się do zakresu, w jakim wyniki badania mogą być uogólnione na inne sytuacje, populacje lub ustawienia. Odpowiada na pytanie: “W jakim stopniu wyniki mogą być zastosowane poza konkretnym kontekstem badania?”\nCzynniki, które mogą wpływać na trafność zewnętrzną, obejmują: - Trafność populacyjna: Jak dobrze próba reprezentuje szerszą populację - Trafność ekologiczna: Jak dobrze ustawienie badania reprezentuje warunki świata rzeczywistego - Trafność czasowa: Czy wyniki pozostają prawdziwe w czasie",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#spójność-w-badaniach",
    "href": "rozdzial3b.html#spójność-w-badaniach",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.7 Spójność w Badaniach",
    "text": "8.7 Spójność w Badaniach\nSpójność jest ściśle związana z rzetelnością, ale wykracza poza sam pomiar. W badaniach spójność odnosi się do ogólnej koherencji i stabilności wyników w różnych kontekstach, metodach lub badaniach.\nKluczowe aspekty spójności w badaniach obejmują:\n\nReplikowalność: Zdolność do odtworzenia wyników badania przy użyciu tych samych metod i danych.\nOdporność: Stabilność wyników w różnych podejściach analitycznych lub niewielkich zmianach w metodologii.\nKonwergencja: Zbieżność wyników z różnych badań lub metod badających to samo zjawisko.\nSpójność długoterminowa: Stabilność wyników w czasie, szczególnie ważna w badaniach długoterminowych.\n\nZapewnienie spójności w badaniach obejmuje: - Stosowanie standaryzowanych procedur i miar - Dokładne dokumentowanie metod i decyzji analitycznych - Przeprowadzanie badań replikacyjnych - Meta-analizy w celu syntezy wyników z wielu badań",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#równoważenie-rzetelności-trafności-i-spójności",
    "href": "rozdzial3b.html#równoważenie-rzetelności-trafności-i-spójności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.8 Równoważenie Rzetelności, Trafności i Spójności",
    "text": "8.8 Równoważenie Rzetelności, Trafności i Spójności\nChociaż rzetelność, trafność i spójność są kluczowe dla wysokiej jakości badań, czasami wiążą się z kompromisami:\n\nWysoce rzetelna miara może nie mieć trafności, jeśli konsekwentnie mierzy niewłaściwą rzecz.\nDążenie do idealnej trafności wewnętrznej (np. w ściśle kontrolowanych eksperymentach laboratoryjnych) może zmniejszyć trafność zewnętrzną.\nZapewnienie wysokiej spójności w różnych kontekstach może wymagać poświęcenia pewnego stopnia precyzji lub głębi w konkretnych sytuacjach.\n\nBadacze muszą starannie równoważyć te aspekty w oparciu o swoje pytania badawcze i charakter badania. Kompleksowe zrozumienie rzetelności, trafności i spójności pomaga w projektowaniu solidnych badań, dokładnej interpretacji wyników i znaczącym wkładzie do korpusu wiedzy naukowej.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#kompromis-między-obciążeniem-a-wariancją",
    "href": "rozdzial3b.html#kompromis-między-obciążeniem-a-wariancją",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.9 Kompromis między Obciążeniem a Wariancją",
    "text": "8.9 Kompromis między Obciążeniem a Wariancją\nPojęcia rzetelności i trafności są ściśle związane ze statystycznym pojęciem kompromisu między obciążeniem a wariancją. Ten kompromis jest fundamentalny w uczeniu maszynowym i modelowaniu statystycznym.\n\nObciążenie odnosi się do błędu wprowadzonego przez przybliżenie problemu ze świata rzeczywistego uproszczonym modelem. Wysokie obciążenie może prowadzić do niedopasowania.\nWariancja odnosi się do błędu wprowadzonego przez wrażliwość modelu na małe fluktuacje w zbiorze treningowym. Wysoka wariancja może prowadzić do przeuczenia.\n\nZobrazujmy to pojęcie za pomocą uproszczonego wykresu:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_prawdziwa &lt;- sin(x)\ny_niskie_obciazenie_wysoka_wariancja &lt;- y_prawdziwa + rnorm(100, 0, 0.3)\ny_wysokie_obciazenie_niska_wariancja &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_prawdziwa, y_niskie_obciazenie_wysoka_wariancja, y_wysokie_obciazenie_niska_wariancja),\n                 typ = rep(c(\"Prawdziwa Funkcja\", \"Niskie Obciążenie, Wysoka Wariancja\", \"Wysokie Obciążenie, Niska Wariancja\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = typ)) +\n  geom_line() +\n  geom_point(data = subset(df, typ != \"Prawdziwa Funkcja\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Kompromis między Obciążeniem a Wariancją\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Typ Modelu\") +\n  theme_minimal()\n\n\n\n\nWizualizacja kompromisu między obciążeniem a wariancją\n\n\n\n\nNa tym wykresie: - Czarna linia reprezentuje prawdziwą funkcję bazową. - Niebieskie punkty reprezentują model z niskim obciążeniem, ale wysoką wariancją. Średnio podąża blisko prawdziwej funkcji, ale ma dużo szumu. - Czerwona linia reprezentuje model z wysokim obciążeniem, ale niską wariancją. Konsekwentnie niedoszacowuje prawdziwej funkcji, ale ma mniej szumu.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#dokładność-i-precyzja",
    "href": "rozdzial3b.html#dokładność-i-precyzja",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.10 Dokładność i Precyzja",
    "text": "8.10 Dokładność i Precyzja\nPojęcia dokładności i precyzji są ściśle związane z trafnością i rzetelnością:\n\nDokładność odnosi się do tego, jak blisko pomiar jest prawdziwej wartości (podobnie do trafności).\nPrecyzja odnosi się do tego, jak spójne lub powtarzalne są pomiary (podobnie do rzetelności).\n\nMożemy zobrazować te pojęcia za pomocą uproszczonej analogii do tarczy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"Wysoka Dokładność\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Niska Dokładność\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"Wysoka Dokładność\\nNiska Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Niska Dokładność\\nNiska Precyzja\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Dokładność vs Precyzja\")\n\n\n\n\nWizualizacja Dokładności vs Precyzji\n\n\n\n\nW tej wizualizacji: - Wysoka dokładność oznacza, że punkty są blisko środka (dziesiątki). - Wysoka precyzja oznacza, że punkty są ściśle zgrupowane. - Każdy panel reprezentuje inną kombinację dokładności i precyzji.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#podsumowanie",
    "href": "rozdzial3b.html#podsumowanie",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.11 Podsumowanie",
    "text": "8.11 Podsumowanie\nZrozumienie rzetelności i trafności jest kluczowe dla prowadzenia solidnych badań. Pojęcia te pomagają nam zapewnić, że nasze pomiary są zarówno spójne, jak i dokładne. Łącząc je z ideami takimi jak kompromis między obciążeniem a wariancją oraz dokładnością i precyzją, zyskujemy głębsze zrozumienie wyzwań związanych z pomiarem i modelowaniem w badaniach naukowych. Jako badacze musimy dążyć do opracowania miar i modeli, które są zarówno rzetelne, jak i trafne, równoważąc kompromisy między obciążeniem a wariancją oraz między dokładnością a precyzją. Wymaga to starannego projektowania metodologii badań, rygorystycznego testowania naszych instrumentów pomiarowych i przemyślanej interpretacji naszych wyników.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#zrozumienie-obciążenia-vs.-wariancji-w-pomiarach-statystycznych",
    "href": "rozdzial3b.html#zrozumienie-obciążenia-vs.-wariancji-w-pomiarach-statystycznych",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.12 Zrozumienie Obciążenia vs. Wariancji w Pomiarach Statystycznych",
    "text": "8.12 Zrozumienie Obciążenia vs. Wariancji w Pomiarach Statystycznych\n\n8.12.1 Wprowadzenie\nW statystyce i uczeniu maszynowym dwa ważne pojęcia, które wpływają na wydajność naszych modeli, to obciążenie (bias) i wariancja (variance). Zrozumienie tych pojęć jest kluczowe dla budowania efektywnych modeli predykcyjnych i unikania typowych pułapek, takich jak przeuczenie i niedouczenie.\n\nObciążenie odnosi się do błędu wprowadzonego przez przybliżenie rzeczywistego problemu, który może być złożony, za pomocą uproszczonego modelu. Wysokie obciążenie może prowadzić do niedouczenia.\n\nWyobraź sobie obciążenie jako średnią odległość naszych przewidywań od prawdziwych wartości.\nW kontekście trafności, wysokie obciążenie oznacza, że nasz model nie uchwycił prawdziwej zależności w danych.\n\nWariancja odnosi się do tego, jak bardzo nasz model zmieniłby się, gdybyśmy oszacowali go przy użyciu innego zbioru treningowego. Wysoka wariancja może prowadzić do przeuczenia.\n\nWyobraź sobie wariancję jako to, jak bardzo nasze przewidywania wahałyby się, gdybyśmy użyli różnych zbiorów danych.\nW kontekście rzetelności, wysoka wariancja oznacza, że nasz model jest zbyt wrażliwy na konkretne dane, na których został wytrenowany.\n\n\nZbadamy cztery scenariusze, aby zilustrować różne kombinacje obciążenia i wariancji przy użyciu syntetycznych danych i modeli regresji.\n\n\n8.12.2 Funkcja Generowania Danych i Dopasowywania Modelu\nNajpierw stwórzmy funkcję, która pomoże nam generować dane i dopasowywać modele dla każdego scenariusza:\n\ngeneruj_i_dopasuj &lt;- function(n, wyraz_wolny, nachylenie, odch_szumu, stopien_modelu) {\n  # Generowanie syntetycznych danych\n  x &lt;- runif(n, 0, 10)\n  y &lt;- wyraz_wolny + nachylenie * x + rnorm(n, 0, odch_szumu)\n  \n  # Dopasowanie modelu\n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", stopien_modelu, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  # Generowanie przewidywań\n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  # Wykres\n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    geom_abline(intercept = wyraz_wolny, slope = nachylenie, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Stopień Modelu:\", stopien_modelu),\n         subtitle = paste(\"Odchylenie Standardowe Szumu:\", odch_szumu),\n         x = \"X (Zmienna Wejściowa)\",\n         y = \"Y (Zmienna Docelowa)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nTa funkcja wykonuje następujące czynności: 1. Generuje syntetyczne dane na podstawie naszych parametrów 2. Dopasowuje model regresji wielomianowej 3. Tworzy wykres pokazujący prawdziwą zależność (niebieska przerywana linia), przewidywania naszego modelu (czerwona ciągła linia) i punkty danych\nTeraz zbadajmy nasze cztery scenariusze!\n\n\n8.12.3 Scenariusz 1: Niskie Obciążenie, Niska Wariancja\nW tym idealnym scenariuszu używamy modelu liniowego do dopasowania danych liniowych z niskim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 1, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model (czerwona linia) ściśle podąża za prawdziwą zależnością (niebieska przerywana linia). - Punkty danych są skupione blisko linii, co wskazuje na niski szum. - Ten scenariusz reprezentuje dobre dopasowanie: model uchwycił podstawowy trend bez nadmiernej złożoności.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#scenariusz-2-niskie-obciążenie-wysoka-wariancja",
    "href": "rozdzial3b.html#scenariusz-2-niskie-obciążenie-wysoka-wariancja",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.13 Scenariusz 2: Niskie Obciążenie, Wysoka Wariancja",
    "text": "8.13 Scenariusz 2: Niskie Obciążenie, Wysoka Wariancja\nTutaj używamy modelu liniowego do dopasowania danych liniowych, ale z wysokim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 5, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model nadal uchwycił ogólny trend, ale punkty danych są bardziej rozproszone. - Ta wysoka wariancja oznacza, że przewidywania naszego modelu byłyby mniej wiarygodne. - W rzeczywistych warunkach mogłoby to reprezentować sytuację, w której nasze pomiary są średnio poprawne, ale mają dużo losowego błędu.\n\n8.13.1 Scenariusz 3: Wysokie Obciążenie, Niska Wariancja\nW tym przypadku używamy modelu liniowego do dopasowania danych kwadratowych (zakrzywionych) z niskim szumem.\n\ndane_kwadratowe &lt;- function(n, wyraz_wolny, nachylenie, odch_szumu, stopien_modelu) {\n  x &lt;- runif(n, 0, 10)\n  y &lt;- wyraz_wolny + nachylenie * x^2 + rnorm(n, 0, odch_szumu)\n  \n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", stopien_modelu, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    stat_function(fun = function(x) wyraz_wolny + nachylenie * x^2, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Stopień Modelu:\", stopien_modelu),\n         subtitle = paste(\"Odchylenie Standardowe Szumu:\", odch_szumu),\n         x = \"X (Zmienna Wejściowa)\",\n         y = \"Y (Zmienna Docelowa)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\ndane_kwadratowe(n = 100, wyraz_wolny = 1, nachylenie = 0.2, odch_szumu = 1, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model liniowy (czerwona linia) nie uchwycił krzywizny prawdziwej zależności (niebieska przerywana linia). - To wysokie obciążenie oznacza, że nasz model konsekwentnie myli się w swoich przewidywaniach. - W rzeczywistych warunkach mogłoby to reprezentować użycie zbyt uproszczonego modelu dla złożonego zjawiska.\n\n\n8.13.2 Scenariusz 4: Wysokie Obciążenie, Wysoka Wariancja\nNa koniec używamy wielomianu wysokiego stopnia do dopasowania danych liniowych z wysokim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 5, stopien_modelu = 5)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model (czerwona linia) jest zbyt złożony, próbując dopasować się do szumu zamiast do podstawowego trendu. - Ta kombinacja wysokiego obciążenia i wysokiej wariancji prowadzi do słabej generalizacji. - W rzeczywistych warunkach mogłoby to reprezentować nadmierne skomplikowanie naszej analizy i wyciąganie fałszywych wniosków z losowych fluktuacji w naszych danych.\n\n\n8.13.3 Podsumowanie\nZrozumienie kompromisu między obciążeniem a wariancją jest kluczowe w modelowaniu statystycznym:\n\nNiskie Obciążenie, Niska Wariancja: Idealny scenariusz, w którym nasz model dokładnie uchwycił podstawową zależność bez nadmiernej wrażliwości na szum.\nNiskie Obciążenie, Wysoka Wariancja: Nasz model jest średnio poprawny, ale niewiarygodny ze względu na wysoką wrażliwość na pojedyncze punkty danych.\nWysokie Obciążenie, Niska Wariancja: Nasz model jest konsekwentnie błędny z powodu nadmiernego uproszczenia, ale daje stabilne przewidywania.\nWysokie Obciążenie, Wysoka Wariancja: Najgorszy scenariusz, w którym nasz model jest zarówno niedokładny, jak i niewiarygodny.\n\nW praktyce często musimy zrównoważyć obciążenie i wariancję. Techniki takie jak walidacja krzyżowa, regularyzacja i metody zespołowe mogą pomóc w znalezieniu tej równowagi.\nPamiętaj: - Model z wysokim obciążeniem jest zbyt prosty i pomija ważne wzorce w danych. - Model z wysoką wariancją jest zbyt złożony i dopasowuje się do szumu w danych treningowych. - Celem jest znalezienie złotego środka, który uchwyci prawdziwe wzorce bez nadmiernego dopasowania do szumu.\nZrozumienie tych pojęć pomoże ci lepiej wybierać odpowiednie modele, unikać przeuczenia i niedouczenia oraz budować bardziej efektywne modele predykcyjne w przyszłych analizach statystycznych!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "",
    "text": "9.1 Introduction\nResearch designs are fundamental to the scientific process, providing structured approaches to investigate hypotheses and answer research questions. This chapter explores two main categories of research designs: experimental and non-experimental, with a focus on the Neyman-Rubin potential outcome framework. We’ll delve into various design types, their characteristics, and provide practical examples using R for data analysis and visualization.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#experimental-designs",
    "href": "chapter4.html#experimental-designs",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.2 Experimental Designs",
    "text": "9.2 Experimental Designs\nExperimental designs are characterized by the researcher’s control over the independent variable(s) and random assignment of subjects to different conditions. These designs are considered the gold standard for establishing causal relationships.\n\n9.2.1 Randomized Controlled Trials (RCTs)\nRCTs are the most rigorous form of experimental design. They involve:\n\nRandom assignment of subjects to treatment and control groups\nManipulation of the independent variable\nMeasurement of the dependent variable\n\nLet’s visualize a simple RCT design:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Create sample data\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  group = factor(rep(c(\"Control\", \"Treatment\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Simulate treatment effect\ndata$post_test &lt;- ifelse(data$group == \"Treatment\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Reshape data for plotting\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"time\", values_to = \"score\")\n\n# Create plot\nggplot(data_long, aes(x = time, y = score, color = group, group = interaction(id, group))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = group), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Pre-test and Post-test Scores in RCT\",\n       x = \"Time\", y = \"Score\", color = \"Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nRandomized Controlled Trial Design\n\n\n\n\nThis plot shows individual trajectories and group means for pre-test and post-test scores in a hypothetical RCT. The treatment group shows a clear increase in scores compared to the control group.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "href": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.3 A/B Testing: An Example and Comparison with RCTs",
    "text": "9.3 A/B Testing: An Example and Comparison with RCTs\nA/B testing is a widely used experimental method in digital marketing, user experience design, and product development. This chapter will present an example of A/B testing, explain its methodology, and discuss how it differs from Randomized Controlled Trials (RCTs).\n\n9.3.1 Example: Website Landing Page Conversion Rate\nLet’s consider an example where an e-commerce company wants to improve the conversion rate of their landing page. They decide to test two different layouts: the current layout (A) and a new layout (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Simulate data\nn_visitors &lt;- 10000\ndata &lt;- data.frame(\n  Version = sample(c(\"A\", \"B\"), n_visitors, replace = TRUE),\n  Converted = rbinom(n_visitors, 1, ifelse(sample(c(\"A\", \"B\"), n_visitors, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Calculate conversion rates\nconversion_rates &lt;- data %&gt;%\n  group_by(Version) %&gt;%\n  summarise(\n    Visitors = n(),\n    Conversions = sum(Converted),\n    ConversionRate = mean(Converted)\n  )\n\n# Visualize results\nggplot(conversion_rates, aes(x = Version, y = ConversionRate, fill = Version)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", ConversionRate * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"A/B Test: Landing Page Conversion Rates\",\n       x = \"Page Version\", y = \"Conversion Rate\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure 9.1: A/B Test Results: Landing Page Conversion Rates\n\n\n\n\n\nIn this example, we simulated data for 10,000 visitors randomly assigned to either version A or B of the landing page. The results show that version B has a slightly higher conversion rate (11.44%) compared to version A (10.94%).\n\n\n9.3.2 A/B Testing Methodology\nA/B testing typically follows these steps:\n\nIdentify the element to be tested (e.g., landing page layout).\nCreate two versions: the control (A) and the variant (B).\nRandomly assign visitors to either version.\nCollect data on the metric of interest (e.g., conversion rate).\nAnalyze the results using statistical methods.\nMake a decision based on the results.\n\n\n\n9.3.3 Differences between A/B Testing and RCTs\nWhile A/B testing and Randomized Controlled Trials (RCTs) share some similarities, they have several key differences:\n\nScope and Context:\n\nA/B Testing: Typically used in digital environments for quick, iterative improvements.\nRCTs: Used in various fields, including medicine, psychology, and social sciences, often for more complex interventions.\n\nDuration:\n\nA/B Testing: Usually shorter, often running for days or weeks.\nRCTs: Can last months or years, especially in medical research.\n\nSample Size:\n\nA/B Testing: Can involve very large sample sizes due to ease of implementation in digital platforms.\nRCTs: Sample sizes are often smaller due to practical and cost constraints.\n\nBlinding:\n\nA/B Testing: Participants are usually unaware they’re part of a test.\nRCTs: May involve single, double, or triple blinding to reduce bias.\n\nEthical Considerations:\n\nA/B Testing: Generally involves low-risk changes with minimal ethical concerns.\nRCTs: Often require extensive ethical review, especially in medical contexts.\n\nOutcome Measures:\n\nA/B Testing: Typically focuses on a single, easily measurable outcome (e.g., click-through rate).\nRCTs: Often measure multiple outcomes, including potential side effects or long-term impacts.\n\nGeneralizability:\n\nA/B Testing: Results are often specific to the platform or context tested.\nRCTs: Aim for broader generalizability, though this can vary.\n\nAnalysis Complexity:\n\nA/B Testing: Often uses simpler statistical analyses.\nRCTs: May involve more complex statistical methods to account for various factors.\n\n\nA/B testing is a powerful tool for making data-driven decisions in digital environments. While it shares the fundamental principle of randomization with RCTs, it is typically simpler, faster, and more focused on specific, measurable outcomes in digital contexts. Understanding these differences helps researchers and practitioners choose the most appropriate method for their specific needs and constraints.\n\n\n9.3.4 Example 1: Effect of Sleep Duration on Cognitive Performance\nResearch Question: Does increasing sleep duration improve cognitive performance in college students?\n\n# Generating sample data\nset.seed(456)\nn &lt;- 100\npre_experimental &lt;- rnorm(n, mean = 70, sd = 10)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = 8, sd = 5)\npre_control &lt;- rnorm(n, mean = 70, sd = 10)\npost_control &lt;- pre_control + rnorm(n, mean = 1, sd = 5)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Experimental\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  Score = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = Score, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Effect of Increased Sleep Duration on Cognitive Performance\") +\n  xlab(\"Time\") +\n  ylab(\"Cognitive Performance Score\")\n\n\n\n\n\n\n\nFigure 9.2: Effect of Sleep Duration on Cognitive Performance\n\n\n\n\n\n\n9.3.4.1 Interpretation\nThis plot demonstrates the effect of increased sleep duration on cognitive performance. The experimental group, which increased their sleep duration, shows a more substantial improvement in cognitive performance compared to the control group. This suggests that increasing sleep duration may positively impact cognitive abilities in college students.\n\n\n\n9.3.5 Example 2: Impact of Mindfulness Training on Stress Levels\nResearch Question: Can a short-term mindfulness training program reduce stress levels in healthcare workers?\n\n# Generating sample data\nset.seed(789)\nn &lt;- 120\npre_experimental &lt;- rnorm(n, mean = 60, sd = 15)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = -12, sd = 8)\npre_control &lt;- rnorm(n, mean = 60, sd = 15)\npost_control &lt;- pre_control + rnorm(n, mean = -2, sd = 6)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Mindfulness\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  StressScore = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = StressScore, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Impact of Mindfulness Training on Stress Levels\") +\n  xlab(\"Time\") +\n  ylab(\"Stress Score\")\n\n\n\n\n\n\n\nFigure 9.3: Impact of Mindfulness Training on Stress Levels\n\n\n\n\n\n\n9.3.5.1 Interpretation\nThis visualization illustrates the impact of a mindfulness training program on stress levels in healthcare workers. The mindfulness group shows a more significant decrease in stress scores compared to the control group. This suggests that the mindfulness training program may be effective in reducing stress levels among healthcare workers.\nWhen interpreting such results, it’s important to consider:\n\nThe magnitude of the change in each group\nThe difference in change between the experimental and control groups\nThe variability within each group\nAny potential confounding factors not accounted for in the experimental design\n\nThese examples provide a template for visualizing and interpreting similar experimental designs across different research contexts.\n\n\n\n9.3.6 Factorial Designs\nFactorial designs allow researchers to study the effects of multiple independent variables simultaneously. They are efficient and can reveal interaction effects between variables.\nExample of a 2x2 factorial design:\n\n# Create sample data for 2x2 factorial design\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  factor_a = rep(rep(c(\"Low\", \"High\"), each = n_per_group), 2),\n  factor_b = rep(c(\"Control\", \"Treatment\"), each = n_per_group * 2),\n  outcome = NA\n)\n\n# Generate outcomes\nfactorial_data$outcome &lt;- ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Control\",\n                                 rnorm(n_per_group, 40, 5),\n                                 ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Treatment\",\n                                        rnorm(n_per_group, 45, 5),\n                                        ifelse(factorial_data$factor_a == \"High\" & factorial_data$factor_b == \"Control\",\n                                               rnorm(n_per_group, 50, 5),\n                                               rnorm(n_per_group, 60, 5))))\n\n# Create plot\nggplot(factorial_data, aes(x = factor_b, y = outcome, fill = factor_a)) +\n  geom_boxplot() +\n  facet_wrap(~factor_a, scales = \"free_x\") +\n  labs(title = \"2x2 Factorial Design\",\n       x = \"Factor B\", y = \"Outcome\", fill = \"Factor A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n2x2 Factorial Design\n\n\n\n\nThis plot illustrates a 2x2 factorial design, showing the effects of two factors (A and B) on the outcome variable. We can observe main effects for both factors and a potential interaction effect.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#non-experimental-designs",
    "href": "chapter4.html#non-experimental-designs",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.4 Non-Experimental Designs",
    "text": "9.4 Non-Experimental Designs\nNon-experimental designs are used when randomization or manipulation of variables is not possible or ethical. They include observational/descriptive studies and quasi-experimental designs.\n\n9.4.1 Observational Studies\nObservational studies involve collecting data without manipulating variables. They are useful for exploring relationships and generating hypotheses.\nExample: Correlation study\n\nset.seed(789)\nn &lt;- 100\nstudy_time &lt;- runif(n, 0, 10)\nexam_score &lt;- 50 + 5 * study_time + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(study_time, exam_score)\n\nggplot(correlation_data, aes(x = study_time, y = exam_score)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Correlation between Study Time and Exam Score\",\n       x = \"Study Time (hours)\", y = \"Exam Score\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCorrelation between Study Time and Exam Score\n\n\n\n\nThis scatter plot shows the relationship between study time and exam scores, illustrating a positive correlation typical in observational studies.\n\n\n9.4.2 Quasi-Experimental Designs\nQuasi-experimental designs lack random assignment but attempt to establish causal relationships. Common types include:\n\nDifference-in-Differences (DiD)\nRegression Discontinuity Design (RDD)\n\n\n9.4.2.1 Difference-in-Differences (DiD)\nDiD is used to estimate treatment effects by comparing the average change over time in the outcome variable for the treatment group to the average change over time for the control group.\nLet’s simulate a DiD analysis using the plm package:\n\nlibrary(plm)\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\nDifference-in-Differences Analysis\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nThe plot shows the average outcomes for treatment and control groups over time. The vertical dashed line indicates the intervention point. The DiD estimate is the difference between the two groups’ changes from pre- to post-intervention periods.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt’s important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\n9.4.2.2 Regression Discontinuity Design (RDD)\nRDD is used when treatment assignment is determined by a cutoff value on a continuous variable. It compares observations just above and below the cutoff to estimate the treatment effect.\nLet’s implement an RDD analysis using the rdrobust package:\n\nlibrary(rdrobust)\n\n# Generate synthetic RDD data\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# RDD analysis\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     4.092     0.231    17.723     0.000     [3.640 , 4.545]     \n        Robust         -         -    15.013     0.000     [3.600 , 4.680]     \n=============================================================================\n\n# Visualize RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regression Discontinuity Design\",\n       x = \"Running Variable\", y = \"Outcome\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nRegression Discontinuity Design Analysis\n\n\n\n\nThe plot shows the discontinuity at the cutoff point (x = 0), with separate regression lines fitted on either side. The treatment effect is estimated by the gap between these lines at the cutoff.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "href": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.5 The Neyman-Rubin Potential Outcome Framework",
    "text": "9.5 The Neyman-Rubin Potential Outcome Framework\nThe Neyman-Rubin potential outcome framework provides a formal approach to causal inference. It introduces the concept of potential outcomes: for each unit, we consider the outcome under treatment and the outcome under control, even though we can only observe one in reality.\nKey concepts:\n\nPotential Outcomes: Y_i(1) and Y_i(0) for treatment and control, respectively.\nObserved Outcome: Y_i = Y_i(1)T_i + Y_i(0)(1-T_i), where T_i is the treatment indicator.\nIndividual Treatment Effect: \\tau_i = Y_i(1) - Y_i(0)\nAverage Treatment Effect (ATE): E[\\tau_i] = E[Y_i(1) - Y_i(0)]\n\nThe framework emphasizes the “fundamental problem of causal inference”: we can never observe both potential outcomes for a single unit simultaneously.\n\n9.5.1 Example: Estimating ATE in an RCT\nIn an RCT, random assignment ensures that treatment is independent of potential outcomes, allowing unbiased estimation of the ATE:\n\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\nWhere n_1 and n_0 are the numbers of treated and control units, respectively.\n\n# Using the RCT data from earlier\nate_estimate &lt;- mean(data$post_test[data$group == \"Treatment\"]) - \n                mean(data$post_test[data$group == \"Control\"])\n\nWarning in mean.default(data$post_test[data$group == \"Treatment\"]): argument is\nnot numeric or logical: returning NA\n\n\nWarning in mean.default(data$post_test[data$group == \"Control\"]): argument is\nnot numeric or logical: returning NA\n\ncat(\"Estimated Average Treatment Effect:\", round(ate_estimate, 2))\n\nEstimated Average Treatment Effect: NA\n\n\nThis estimate represents the causal effect of the treatment under the assumptions of the potential outcome framework.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#conclusion",
    "href": "chapter4.html#conclusion",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.6 Conclusion",
    "text": "9.6 Conclusion\nThis chapter has explored various research designs, from experimental approaches like RCTs and factorial designs to non-experimental methods such as observational studies and quasi-experimental designs. We’ve demonstrated how to implement and visualize these designs using R, and introduced the Neyman-Rubin potential outcome framework for causal inference.\nUnderstanding these designs and their appropriate use is crucial for conducting rigorous research and drawing valid causal conclusions. Each design has its strengths and limitations, and the choice of design should be guided by the research question, ethical considerations, and practical constraints.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#references",
    "href": "chapter4.html#references",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.7 References",
    "text": "9.7 References\n\nImbens, G. W., & Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press.\nAngrist, J. D., & Pischke, J. S. (2008). Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton University Press.\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and Quasi-Experimental Designs for Generalized Causal Inference. Houghton Mifflin.\nCunningham, S. (2021). Causal Inference: The Mixtape. Yale University Press.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html",
    "href": "rozdzial4.html",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "",
    "text": "10.1 Wstęp\nProjekty badawcze stanowią fundament procesu naukowego, zapewniając ustrukturyzowane podejście do badania hipotez i odpowiadania na pytania badawcze. Ten rozdział analizuje dwie główne kategorie projektów badawczych: eksperymentalne i nieeksperymentalne, ze szczególnym uwzględnieniem modelu potencjalnych wyników Neymana-Rubina. Zagłębimy się w różne typy projektów, ich charakterystykę i przedstawimy praktyczne przykłady wykorzystania R do analizy danych i wizualizacji.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-eksperymentalne",
    "href": "rozdzial4.html#projekty-eksperymentalne",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.2 Projekty Eksperymentalne",
    "text": "10.2 Projekty Eksperymentalne\nProjekty eksperymentalne charakteryzują się kontrolą badacza nad zmienną(ymi) niezależną(ymi) oraz losowym przydziałem uczestników do różnych warunków. Te projekty są uważane za złoty standard w ustalaniu związków przyczynowych.\n\n10.2.1 Randomizowane Badania Kontrolowane (RCT)\nRCT są najbardziej rygorystyczną formą projektu eksperymentalnego. Obejmują one:\n\nLosowy przydział uczestników do grup eksperymentalnej i kontrolnej\nManipulację zmienną niezależną\nPomiar zmiennej zależnej\n\nZobaczmy wizualizację prostego projektu RCT:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Tworzenie przykładowych danych\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  grupa = factor(rep(c(\"Kontrolna\", \"Eksperymentalna\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Symulacja efektu leczenia\ndata$post_test &lt;- ifelse(data$grupa == \"Eksperymentalna\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Przekształcenie danych do formatu długiego\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"czas\", values_to = \"wynik\")\n\n# Tworzenie wykresu\nggplot(data_long, aes(x = czas, y = wynik, color = grupa, group = interaction(id, grupa))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = grupa), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Wyniki Pre-test i Post-test w RCT\",\n       x = \"Czas\", y = \"Wynik\", color = \"Grupa\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_x_discrete(labels = c(\"pre_test\" = \"Pre-test\", \"post_test\" = \"Post-test\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nProjekt Randomizowanego Badania Kontrolowanego\n\n\n\n\nTen wykres pokazuje indywidualne trajektorie i średnie grupowe dla wyników pre-test i post-test w hipotetycznym RCT. Grupa eksperymentalna wykazuje wyraźny wzrost wyników w porównaniu do grupy kontrolnej.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#testy-ab-przykład-i-porównanie-z-rct",
    "href": "rozdzial4.html#testy-ab-przykład-i-porównanie-z-rct",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.3 Testy A/B: Przykład i Porównanie z RCT",
    "text": "10.3 Testy A/B: Przykład i Porównanie z RCT\nTesty A/B to szeroko stosowana metoda eksperymentalna w marketingu cyfrowym, projektowaniu doświadczeń użytkownika i rozwoju produktów. Ten rozdział przedstawi przykład testu A/B, wyjaśni jego metodologię i omówi, czym różni się od Randomizowanych Badań Kontrolowanych (RCT).\n\n10.3.1 Przykład: Współczynnik Konwersji Strony Docelowej\nRozważmy przykład, w którym firma e-commerce chce poprawić współczynnik konwersji swojej strony docelowej. Decydują się przetestować dwa różne układy: obecny układ (A) i nowy układ (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Symulacja danych\nn_odwiedzajacych &lt;- 10000\ndane &lt;- data.frame(\n  Wersja = sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE),\n  Konwersja = rbinom(n_odwiedzajacych, 1, ifelse(sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Obliczenie współczynników konwersji\nwspolczynniki_konwersji &lt;- dane %&gt;%\n  group_by(Wersja) %&gt;%\n  summarise(\n    Odwiedzajacy = n(),\n    Konwersje = sum(Konwersja),\n    WspolczynnikKonwersji = mean(Konwersja)\n  )\n\n# Wizualizacja wyników\nggplot(wspolczynniki_konwersji, aes(x = Wersja, y = WspolczynnikKonwersji, fill = Wersja)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", WspolczynnikKonwersji * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"Test A/B: Współczynniki Konwersji Strony Docelowej\",\n       x = \"Wersja Strony\", y = \"Współczynnik Konwersji\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure 10.1: Wyniki Testu A/B: Współczynniki Konwersji Strony Docelowej\n\n\n\n\n\nW tym przykładzie zasymulowaliśmy dane dla 10 000 odwiedzających losowo przypisanych do wersji A lub B strony docelowej. Wyniki pokazują, że wersja B ma nieco wyższy współczynnik konwersji (11,44%) w porównaniu do wersji A (10,94%).\n\n\n10.3.2 Metodologia Testów A/B\nTesty A/B zazwyczaj przebiegają według następujących kroków:\n\nZidentyfikowanie elementu do przetestowania (np. układ strony docelowej).\nStworzenie dwóch wersji: kontrolnej (A) i wariantu (B).\nLosowe przypisanie odwiedzających do jednej z wersji.\nZbieranie danych o interesującej nas metryce (np. współczynniku konwersji).\nAnaliza wyników przy użyciu metod statystycznych.\nPodjęcie decyzji na podstawie wyników.\n\n\n\n10.3.3 Różnice między Testami A/B a RCT\nChoć testy A/B i Randomizowane Badania Kontrolowane (RCT) mają pewne podobieństwa, istnieje kilka kluczowych różnic:\n\nZakres i Kontekst:\n\nTesty A/B: Zazwyczaj stosowane w środowiskach cyfrowych do szybkich, iteracyjnych ulepszeń.\nRCT: Stosowane w różnych dziedzinach, w tym medycynie, psychologii i naukach społecznych, często dla bardziej złożonych interwencji.\n\nCzas Trwania:\n\nTesty A/B: Zwykle krótsze, często trwające dni lub tygodnie.\nRCT: Mogą trwać miesiące lub lata, szczególnie w badaniach medycznych.\n\nWielkość Próby:\n\nTesty A/B: Mogą obejmować bardzo duże próby ze względu na łatwość implementacji na platformach cyfrowych.\nRCT: Wielkości prób są często mniejsze ze względu na praktyczne i kosztowe ograniczenia.\n\nZaślepienie:\n\nTesty A/B: Uczestnicy zazwyczaj nie są świadomi, że biorą udział w teście.\nRCT: Mogą obejmować pojedyncze, podwójne lub potrójne zaślepienie w celu zmniejszenia błędu systematycznego.\n\nWzględy Etyczne:\n\nTesty A/B: Generalnie obejmują zmiany niskiego ryzyka z minimalnymi obawami etycznymi.\nRCT: Często wymagają obszernej oceny etycznej, szczególnie w kontekście medycznym.\n\nMiary Wyników:\n\nTesty A/B: Zazwyczaj skupiają się na pojedynczym, łatwo mierzalnym wyniku (np. współczynnik klikalności).\nRCT: Często mierzą wiele wyników, w tym potencjalne skutki uboczne lub długoterminowe efekty.\n\nMożliwość Uogólnienia:\n\nTesty A/B: Wyniki są często specyficzne dla testowanej platformy lub kontekstu.\nRCT: Dążą do szerszej możliwości uogólnienia, choć może to się różnić.\n\nZłożoność Analizy:\n\nTesty A/B: Często wykorzystują prostsze analizy statystyczne.\nRCT: Mogą obejmować bardziej złożone metody statystyczne, aby uwzględnić różne czynniki.\n\n\nTesty A/B są potężnym narzędziem do podejmowania decyzji opartych na danych w środowiskach cyfrowych. Choć dzielą podstawową zasadę randomizacji z RCT, są zazwyczaj prostsze, szybsze i bardziej skoncentrowane na konkretnych, mierzalnych wynikach w kontekstach cyfrowych. Zrozumienie tych różnic pomaga badaczom i praktykom wybrać najbardziej odpowiednią metodę do ich konkretnych potrzeb i ograniczeń.\nTesty A/B są szczególnie przydatne w optymalizacji stron internetowych, aplikacji mobilnych i kampanii marketingowych, gdzie szybkie iteracje i ciągłe ulepszenia są kluczowe. Z kolei RCT pozostają złotym standardem w badaniach naukowych, szczególnie w dziedzinach takich jak medycyna, gdzie rygorystyczna kontrola i długoterminowa obserwacja są niezbędne.\nNiezależnie od wybranej metody, kluczowe jest staranne planowanie, precyzyjne wykonanie i ostrożna interpretacja wyników. Zarówno testy A/B, jak i RCT, gdy są odpowiednio stosowane, mogą dostarczyć cennych informacji i przyczynić się do podejmowania lepszych decyzji opartych na danych.\n\n\n10.3.4 Przykład 1: Wpływ Długości Snu na Wydajność Poznawczą\nPytanie Badawcze: Czy zwiększenie długości snu poprawia wydajność poznawczą u studentów?\n\n# Generowanie przykładowych danych\nset.seed(456)\nn &lt;- 100\npre_eksperymentalna &lt;- rnorm(n, mean = 70, sd = 10)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = 8, sd = 5)\npre_kontrolna &lt;- rnorm(n, mean = 70, sd = 10)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = 1, sd = 5)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Eksperymentalna\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  Wynik = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = Wynik, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wpływ Zwiększonej Długości Snu na Wydajność Poznawczą\") +\n  xlab(\"Czas\") +\n  ylab(\"Wynik Wydajności Poznawczej\")\n\n\n\n\n\n\n\nFigure 10.2: Wpływ Długości Snu na Wydajność Poznawczą\n\n\n\n\n\n\n10.3.4.1 Interpretacja\nTen wykres pokazuje wpływ zwiększonej długości snu na wydajność poznawczą. Grupa eksperymentalna, która zwiększyła długość snu, wykazuje znacznie większą poprawę w wydajności poznawczej w porównaniu do grupy kontrolnej. Sugeruje to, że zwiększenie długości snu może pozytywnie wpływać na zdolności poznawcze studentów.\n\n\n\n10.3.5 Przykład 2: Wpływ Treningu Uważności na Poziom Stresu\nPytanie Badawcze: Czy krótkoterminowy program treningu uważności może obniżyć poziom stresu u pracowników służby zdrowia?\n\n# Generowanie przykładowych danych\nset.seed(789)\nn &lt;- 120\npre_eksperymentalna &lt;- rnorm(n, mean = 60, sd = 15)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = -12, sd = 8)\npre_kontrolna &lt;- rnorm(n, mean = 60, sd = 15)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = -2, sd = 6)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Uważność\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  PoziomStresu = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = PoziomStresu, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wpływ Treningu Uważności na Poziom Stresu\") +\n  xlab(\"Czas\") +\n  ylab(\"Poziom Stresu\")\n\n\n\n\n\n\n\nFigure 10.3: Wpływ Treningu Uważności na Poziom Stresu\n\n\n\n\n\n\n10.3.5.1 Interpretacja\nTa wizualizacja ilustruje wpływ programu treningu uważności na poziom stresu u pracowników służby zdrowia. Grupa uważności wykazuje znacznie większy spadek poziomu stresu w porównaniu do grupy kontrolnej. Sugeruje to, że program treningu uważności może być skuteczny w redukcji poziomu stresu wśród pracowników służby zdrowia.\n\n\n\n10.3.6 Projekty Czynnikowe\nProjekty czynnikowe pozwalają badaczom na jednoczesne badanie efektów wielu zmiennych niezależnych. Są one efektywne i mogą ujawniać efekty interakcji między zmiennymi.\nPrzykład projektu czynnikowego 2x2:\n\n# Tworzenie przykładowych danych dla projektu czynnikowego 2x2\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  czynnik_a = rep(rep(c(\"Niski\", \"Wysoki\"), each = n_per_group), 2),\n  czynnik_b = rep(c(\"Kontrola\", \"Interwencja\"), each = n_per_group * 2),\n  wynik = NA\n)\n\n# Generowanie wyników\nfactorial_data$wynik &lt;- ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Kontrola\",\n                               rnorm(n_per_group, 40, 5),\n                               ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Interwencja\",\n                                      rnorm(n_per_group, 45, 5),\n                                      ifelse(factorial_data$czynnik_a == \"Wysoki\" & factorial_data$czynnik_b == \"Kontrola\",\n                                             rnorm(n_per_group, 50, 5),\n                                             rnorm(n_per_group, 60, 5))))\n\n# Tworzenie wykresu\nggplot(factorial_data, aes(x = czynnik_b, y = wynik, fill = czynnik_a)) +\n  geom_boxplot() +\n  facet_wrap(~czynnik_a, scales = \"free_x\") +\n  labs(title = \"Projekt Czynnikowy 2x2\",\n       x = \"Czynnik B\", y = \"Wynik\", fill = \"Czynnik A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nProjekt Czynnikowy 2x2\n\n\n\n\nTen wykres ilustruje projekt czynnikowy 2x2, pokazując efekty dwóch czynników (A i B) na zmienną wynikową. Możemy zaobserwować główne efekty dla obu czynników oraz potencjalny efekt interakcji.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-nieeksperymentalne",
    "href": "rozdzial4.html#projekty-nieeksperymentalne",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.4 Projekty Nieeksperymentalne",
    "text": "10.4 Projekty Nieeksperymentalne\nProjekty nieeksperymentalne są stosowane, gdy randomizacja lub manipulacja zmiennymi nie jest możliwa lub etyczna. Obejmują one badania obserwacyjne/opisowe i quasi-eksperymentalne.\n\n10.4.1 Badania Obserwacyjne\nBadania obserwacyjne polegają na zbieraniu danych bez manipulowania zmiennymi. Są one przydatne do eksploracji relacji i generowania hipotez.\nPrzykład: Badanie korelacyjne\n\nset.seed(789)\nn &lt;- 100\nczas_nauki &lt;- runif(n, 0, 10)\nwynik_egzaminu &lt;- 50 + 5 * czas_nauki + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(czas_nauki, wynik_egzaminu)\n\nggplot(correlation_data, aes(x = czas_nauki, y = wynik_egzaminu)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Korelacja między Czasem Nauki a Wynikiem Egzaminu\",\n       x = \"Czas Nauki (godziny)\", y = \"Wynik Egzaminu\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nKorelacja między Czasem Nauki a Wynikiem Egzaminu\n\n\n\n\nTen wykres punktowy pokazuje relację między czasem nauki a wynikami egzaminu, ilustrując pozytywną korelację typową dla badań obserwacyjnych.\n\n\n10.4.2 Projekty Quasi-Eksperymentalne\nProjekty quasi-eksperymentalne nie mają losowego przydziału, ale próbują ustalić związki przyczynowe. Popularne typy to:\n\nRóżnica w Różnicach (DiD)\nRegresja Nieciągła (RDD)\n\n\n10.4.2.1 Różnica w Różnicach (DiD)\nDiD jest używana do oszacowania efektów interwencji poprzez porównanie średniej zmiany w czasie w zmiennej wynikowej dla grupy eksperymentalnej ze średnią zmianą w czasie dla grupy kontrolnej.\nPrzeprowadźmy symulację analizy DiD przy użyciu pakietu plm:\n\nlibrary(plm)\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nWykres pokazuje średnie wyniki dla grup interwencji i kontrolnej w czasie. Pionowa przerywana linia wskazuje punkt interwencji. Oszacowanie DiD to różnica między zmianami obu grup od okresu przed do po interwencji.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt’s important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\n10.4.2.2 Regresja Nieciągła (RDD)\nRDD jest stosowana, gdy przydział do interwencji jest określony przez wartość graniczną na ciągłej zmiennej. Porównuje obserwacje tuż powyżej i poniżej punktu granicznego, aby oszacować efekt interwencji.\nPrzeprowadźmy analizę RDD przy użyciu pakietu rdrobust:\n\nlibrary(rdrobust)\n\n# Generowanie syntetycznych danych RDD\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# Analiza RDD\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     4.092     0.231    17.723     0.000     [3.640 , 4.545]     \n        Robust         -         -    15.013     0.000     [3.600 , 4.680]     \n=============================================================================\n\n# Wizualizacja RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regresja Nieciągła\",\n       x = \"Zmienna Bieżąca\", y = \"Wynik\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAnaliza Regresji Nieciągłej\n\n\n\n\nWykres pokazuje nieciągłość w punkcie granicznym (x = 0), z oddzielnymi liniami regresji dopasowanymi po obu stronach. Efekt interwencji jest szacowany przez różnicę między tymi liniami w punkcie granicznym.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#model-potencjalnych-wyników-neymana-rubina",
    "href": "rozdzial4.html#model-potencjalnych-wyników-neymana-rubina",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.5 Model Potencjalnych Wyników Neymana-Rubina",
    "text": "10.5 Model Potencjalnych Wyników Neymana-Rubina\nModel potencjalnych wyników Neymana-Rubina zapewnia formalne podejście do wnioskowania przyczynowego. Wprowadza on koncepcję potencjalnych wyników: dla każdej jednostki rozważamy wynik w warunkach interwencji i w warunkach kontrolnych, mimo że w rzeczywistości możemy zaobserwować tylko jeden z nich.\nKluczowe pojęcia:\n\nPotencjalne Wyniki: Y_i(1) i Y_i(0) odpowiednio dla interwencji i kontroli.\nObserwowany Wynik: Y_i = Y_i(1)T_i + Y_i(0)(1-T_i), gdzie T_i to wskaźnik interwencji.\nIndywidualny Efekt Interwencji: \\tau_i = Y_i(1) - Y_i(0)\nPrzeciętny Efekt Interwencji (ATE): E[\\tau_i] = E[Y_i(1) - Y_i(0)]\n\nModel podkreśla “fundamentalny problem wnioskowania przyczynowego”: nigdy nie możemy zaobserwować obu potencjalnych wyników dla pojedynczej jednostki jednocześnie.\n\n10.5.1 Przykład: Szacowanie ATE w RCT\nW RCT, losowy przydział zapewnia, że interwencja jest niezależna od potencjalnych wyników, umożliwiając nieobciążone oszacowanie ATE:\n\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\nGdzie n_1 i n_0 to odpowiednio liczby jednostek w grupie interwencji i kontrolnej.\n\n# Używając danych RCT z wcześniejszego przykładu\nate_estimate &lt;- mean(data$post_test[data$grupa == \"Eksperymentalna\"]) - \n                mean(data$post_test[data$grupa == \"Kontrolna\"])\n\ncat(\"Oszacowany Przeciętny Efekt Interwencji:\", round(ate_estimate, 2))\n\nOszacowany Przeciętny Efekt Interwencji: 9.66",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "11.1 Introduction to Sigma Notation (Σ) | Wprowadzenie do Notacji Sigma (Σ)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-sigma-notation-σ-wprowadzenie-do-notacji-sigma-σ",
    "href": "chapter5.html#introduction-to-sigma-notation-σ-wprowadzenie-do-notacji-sigma-σ",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "What is Sigma? | Co to jest notacja sumacyjna Sigma? Sigma (Σ) is a mathematical operator that tells us to sum (add up) a sequence of terms - it functions as an instruction to perform addition of all elements in a specified range. | Sigma (Σ) to operator matematyczny, który nakazuje nam zsumować (dodać) sekwencję wyrazów - działa jak instrukcja wykonania dodawania wszystkich elementów w określonym zakresie.\nPurpose: | Cel: Provides a compact way to write sums of many similar terms using a single symbol, avoiding lengthy addition expressions. | Zapewnia zwięzły sposób zapisu sum wielu podobnych wyrazów za pomocą jednego symbolu, unikając długich wyrażeń dodawania.\n\n\n11.1.1 Basic Formula | Podstawowa formuła\n\nThe general form of a sigma notation is: | Ogólna forma notacji sigma to:\n\n\\sum_{i=a}^{b} f(i)\n\nIndex of Summation: | Indeks sumowania: i\nLower Limit: | Dolna granica: a\nUpper Limit: | Górna granica: b\nFunction: | Funkcja: f(i)\n\n\n\n11.1.2 Simple Example | Prosty przykład\n\nConsider you want to add the first five positive integers: | Załóżmy, że chcesz dodać pierwsze pięć dodatnich liczb całkowitych:\n\n\\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\n\nAdds the first five positive integers. | Dodaje pierwsze pięć dodatnich liczb całkowitych.\n\n\n\n11.1.3 Example with a Function | Przykład z funkcją\n\nSuppose you want to sum the squares of the first four positive integers: | Załóżmy, że chcesz zsumować kwadraty pierwszych czterech dodatnich liczb całkowitych:\n\n\\sum_{i=1}^{4} i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 30\n\nSum of the squares of the first four positive integers. | Suma kwadratów pierwszych czterech dodatnich liczb całkowitych.\n\n\n\n11.1.4 Practical Application in Statistics | Praktyczne zastosowanie w statystyce\n\nCalculating the Mean: | Obliczanie średniej:\n\nData Points: | Punkty danych: x_1, x_2, ..., x_n\nMean \\bar{x}: | Średnia \\bar{x}:\n\n\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\nExample: | Przykład: x_1, x_2, x_3, x_4 are 4, 8, 15, 16 | x_1, x_2, x_3, x_4 to 4, 8, 15, 16\n\n\\bar{x} = \\frac{43}{4} = 10.75\n\n\n11.1.5 Benefits of Using Sigma Notation | Korzyści z używania notacji Sigma\n\nClarity: | Jasność: Provides a clear and concise representation of various statistical formulas. | Zapewnia jasne i zwięzłe przedstawienie statystycznych formuł.\n\n\n\n\n\n\n\nSummation (Σ) and Product (Π) Operators\n\n\n\n\n11.1.5.1 Sigma (Σ) Operator\n\\sum is a summation operator that instructs us to add terms:\n\\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n\nwhere: - i is the index variable - The lower value under Σ (here i=1) is the starting point - The upper value (here n) is the ending point\n\n\n11.1.5.2 Pi (Π) Operator\n\\prod is a product operator that instructs us to multiply terms:\n\\prod_{i=1}^{n} x_i = x_1 \\times x_2 \\times ... \\times x_n\nwhere: - i is the index variable - The lower value under Π (here i=1) is the starting point - The upper value (here n) is the ending point\n\n\n\n\n\n\n\n\n\nExample of Σ\n\n\n\n\\sum_{i=1}^{4} i = 1 + 2 + 3 + 4 = 10\n\n\n\n\n\n\n\n\nExample of Π\n\n\n\n\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\n\n\n\n\n\n\n\n\nKey Differences\n\n\n\n\nΣ represents repeated addition\nΠ represents repeated multiplication",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#types-of-data-distributions",
    "href": "chapter5.html#types-of-data-distributions",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.2 Types of Data Distributions",
    "text": "11.2 Types of Data Distributions\n\n\n\n\n\n\nImportant\n\n\n\nData distribution informs what values a variable takes and how often.\n\n\nUnderstanding data distributions is crucial for data analysis and visualization. In this document, we’ll explore various types of distributions and how to visualize them using ggplot2 in R.\n\n11.2.1 Normal Distribution\nThe normal distribution, also known as the Gaussian distribution, is symmetric and bell-shaped.\n\n# Generate normal distribution data\nnormal_data &lt;- data.frame(x = rnorm(1000))\n\n# Plot\nggplot(normal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Normal Distribution\", x = \"Value\", y = \"Density\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\n11.2.2 Uniform Distribution\nIn a uniform distribution, all values have an equal probability of occurrence.\n\n# Generate uniform distribution data\nuniform_data &lt;- data.frame(x = runif(1000))\n\n# Plot\nggplot(uniform_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Uniform Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n11.2.3 Skewed Distributions\nSkewed distributions are asymmetric, with one tail longer than the other.\n\n# Generate right-skewed data\nright_skewed &lt;- data.frame(x = rlnorm(1000))\n\n# Plot\nggplot(right_skewed, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Right-Skewed Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n11.2.4 Bimodal Distribution\nA bimodal distribution has two peaks, indicating two distinct subgroups in the data.\n\n# Generate bimodal data\nbimodal_data &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Plot\nggplot(bimodal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Bimodal Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nKey Properties\nSocial Examples\n\n\n\n\nNormal\nSymmetric, bell-shaped, most values near mean\nHeight, IQ scores, standardized test scores\n\n\nUniform\nEqual probability across range\nBirth dates in year, arrival times in hour\n\n\nBimodal\nTwo peaks, suggests subgroups\nAge in college towns, polarized opinions\n\n\nLog-normal\nRight-skewed, cannot be negative\nIncome, house prices, social media followers\n\n\nPower law\nExtreme skew, “rich get richer”\nCity sizes",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#visualizing-real-world-data-distributions",
    "href": "chapter5.html#visualizing-real-world-data-distributions",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.3 Visualizing Real-World Data Distributions",
    "text": "11.3 Visualizing Real-World Data Distributions\nLet’s use the palmerpenguins dataset to explore real-world data distributions.\n\n11.3.1 Histogram and Density Plot\n\n\n\n\n\n\nUnderstanding Histograms and Density\n\n\n\n⭐ A histogram is a special graph for numerical data where:\n\nData is grouped into ranges (called “bins”)\nBars touch each other (unlike bar charts!) because the data is continuous\nEach bar’s height shows how many values fall into that range\n\nThink of density as showing how common or concentrated certain values are in your data:\n\nA higher point on a density curve (or taller bar in a histogram) means those values appear more frequently in your data\nA lower point means those values are less common\n\nJust like a crowded area has more people per space (higher density), a taller part of the graph shows values that appear more often in your dataset!\n\n\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Distribution of Penguin Flipper Lengths\", \n       x = \"Flipper Length (mm)\", \n       y = \"Density\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\n11.3.2 Box Plot\nBox plots are useful for comparing distributions across categories.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n11.3.3 Violin Plot\nViolin plots combine box plot and density plot features.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n11.3.4 Ridgeline Plot\nRidgeline plots are useful for comparing multiple distributions.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Distribution of Flipper Length by Penguin Species\",\n       x = \"Flipper Length (mm)\",\n       y = \"Species\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n\n\n11.3.5 Conclusion\nUnderstanding and visualizing data distributions is crucial in data analysis. ggplot2 provides a flexible and powerful toolkit for creating various types of distribution plots. By exploring different visualization techniques, we can gain insights into the underlying patterns and characteristics of our data.\n\n\n\n\n\n\nUnderstanding Different Types of Data Sets and Their Formats\n\n\n\n\n11.3.6 Cross-sectional Data\nObservations collected at a single point in time across multiple entities/individuals:\n\n\n\nIndividual\nAge\nIncome\nEducation\n\n\n\n\n1\n25\n50000\nBachelor’s\n\n\n2\n35\n75000\nMaster’s\n\n\n3\n45\n90000\nPhD\n\n\n\n\n\n11.3.7 Time Series Data\nObservations of a single entity tracked over multiple time points:\n\n\n\nYear\nGDP (in billions)\nUnemployment Rate\n\n\n\n\n2018\n20,580\n3.9%\n\n\n2019\n21,433\n3.7%\n\n\n2020\n20,933\n8.1%\n\n\n\n\n\n11.3.8 Panel Data (Longitudinal Data)\nObservations of multiple entities tracked over time:\n\n\n\nCountry\nYear\nGDP per capita\nLife Expectancy\n\n\n\n\nUSA\n2018\n62,794\n78.7\n\n\nUSA\n2019\n65,118\n78.8\n\n\nCanada\n2018\n46,194\n81.9\n\n\nCanada\n2019\n46,194\n82.0\n\n\n\n\n\n11.3.9 Time-series Cross-sectional (TSCS) Data\nA special case of panel data where:\n\nNumber of time points &gt; Number of entities\nSimilar structure to panel data but with emphasis on temporal depth\nCommon in political science and economics research\n\n\n\n11.3.10 Data Formats\n\n11.3.10.1 Wide Format\nEach row represents an entity; columns represent variables/time points:\n\n\n\nCountry\nGDP_2018\nGDP_2019\nLE_2018\nLE_2019\n\n\n\n\nUSA\n62,794\n65,118\n78.7\n78.8\n\n\nCanada\n46,194\n46,194\n81.9\n82.0\n\n\n\n\n\n11.3.10.2 Long Format\nEach row represents a unique entity-time-variable combination:\n\n\n\nCountry\nYear\nVariable\nValue\n\n\n\n\nUSA\n2018\nGDP per capita\n62,794\n\n\nUSA\n2019\nGDP per capita\n65,118\n\n\nUSA\n2018\nLife Expectancy\n78.7\n\n\nUSA\n2019\nLife Expectancy\n78.8\n\n\nCanada\n2018\nGDP per capita\n46,194\n\n\nCanada\n2019\nGDP per capita\n46,194\n\n\nCanada\n2018\nLife Expectancy\n81.9\n\n\nCanada\n2019\nLife Expectancy\n82.0\n\n\n\nNote: Long format is generally preferred for:\n\nData manipulation in R and Python\nStatistical analysis\nData visualization\nMixed-effects modeling\nRepeated measures analyses",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-outliers",
    "href": "chapter5.html#understanding-outliers",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.4 Understanding Outliers",
    "text": "11.4 Understanding Outliers\nBefore diving into specific measures, it’s crucial to understand the concept of outliers, as they can significantly impact many descriptive statistics.\nOutliers are data points that differ significantly from other observations in the dataset. They can occur due to:\n\nMeasurement or recording errors\nGenuine extreme values in the population\nSampling from a different population\n\nOutliers can have a substantial effect on many statistical measures, especially those based on means or sums of squared deviations. Therefore, it’s essential to:\n\nIdentify outliers through both statistical methods and domain knowledge\nInvestigate the cause of outliers\nMake informed decisions about whether to include or exclude them in analyses\n\nThroughout this guide, we’ll discuss how different descriptive measures are affected by outliers.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#statistical-symbols-and-notations---summary",
    "href": "chapter5.html#statistical-symbols-and-notations---summary",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.5 Statistical Symbols and Notations - Summary",
    "text": "11.5 Statistical Symbols and Notations - Summary\n\n\n\n\n\n\n\n\n\n\nMeasure\nPopulation Parameter\nSample Statistic\nAlternative Notations\nUsage Notes\n\n\n\n\nSize\nN\nn\n-\nTotal count of observations\n\n\nMean\n\\mu\n\\bar{x}, m\nM, E(X)\nE(X) used in probability theory\n\n\nVariance\n\\sigma^2\ns^2\n\\text{Var}(X), V(X)\nSquared deviations from mean\n\n\nStandard Deviation\n\\sigma\ns\n\\text{SD}, \\text{std}\nSquare root of variance\n\n\nProportion\n\\pi, P\n\\hat{p}\n\\text{prop}\nRelative frequencies\n\n\nCorrelation\n\\rho\nr\n\\text{corr}(x,y)\nRanges from -1 to +1\n\n\nStandard Error\n\\sigma_{\\bar{x}}\ns_{\\bar{x}}\n\\text{SE}\nStandard error of mean\n\n\nSum\n\\sum\n\\sum\n\\sum_{i=1}^n\nWith indexing\n\n\nIndividual Value\nX_i\nx_i\n-\nith observation\n\n\nCovariance\n\\sigma_{xy}\ns_{xy}\n\\text{Cov}(X,Y)\nJoint variation\n\n\nMedian\n\\eta\n\\text{Med}\nM\nCentral value\n\n\nRange\nR\nr\n\\text{max}(X) - \\text{min}(X)\nSpread measure\n\n\nMode\n\\text{Mo}\n\\text{mo}\n\\text{mod}\nMost frequent value\n\n\nSkewness\n\\gamma_1\ng_1\n\\text{SK}\nDistribution asymmetry\n\n\nKurtosis\n\\gamma_2\ng_2\n\\text{KU}\nDistribution peakedness\n\n\n\nAdditional useful notations:\n\nSample moments: m_k = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^k\nPopulation moments: \\mu_k = E[(X - \\mu)^k]\nPopulation standard error: \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\nSample standard error: s_{\\bar{x}} = \\frac{s}{\\sqrt{n}}",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-central-tendency",
    "href": "chapter5.html#measures-of-central-tendency",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.6 Measures of Central Tendency",
    "text": "11.6 Measures of Central Tendency\nMeasures of central tendency aim to identify the “typical” or “central” value in a dataset. The three primary measures are mean, median, and mode.\n\n11.6.1 Arithmetic Mean\nThe arithmetic mean is the sum of all values divided by the number of values.\nFormula: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\nImportant Property: The mean is a balancing point in the data. The sum of deviations from the mean is always zero:\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0\nThis property makes the mean useful in many statistical calculations.\n\n\n\n\n\n\nUnderstanding Mean as a Balance Point 🎯\n\n\n\nLet’s consider a dataset X = \\{1, 2, 6, 7, 9\\} on a number line, imagining it as a seesaw:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\nThe mean (\\mu) acts as the perfect balance point of this seesaw. For our data:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\n11.6.2 What happens at different support points? 🤔\n\nSupport point at 6 (too high):\n\nLeft side: Values (1, 2) are below\nRight side: Values (7, 9) are above\n\\sum distances from left = (6-1) + (6-2) = 9\n\\sum distances from right = (7-6) + (9-6) = 4\nThe seesaw tilts left! ⬅️ because 9 &gt; 4\n\nSupport point at 4 (too low):\n\nLeft side: Values (1, 2) are below\nRight side: Values (6, 7, 9) are above\n\\sum distances from left = (4-1) + (4-2) = 5\n\\sum distances from right = (6-4) + (7-4) + (9-4) = 10\nThe seesaw tilts right! ➡️ because 5 &lt; 10\n\nSupport point at mean (5) (perfect balance):\n\n\\sum distances below = \\sum distances above\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ✨ Perfect balance!\n\n\nThis shows why the mean is the unique balance point, where:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nThe seesaw will always tilt unless the support point is placed exactly at the mean! 🎪\n\n\n\n\n\n\n\n\n\nMean as a Balance Point\n\n\n\nThis visualization shows how the arithmetic mean (5) acts as a balance point between clustered points on the left and dispersed points on the right:\nLeft side of the mean: - Points with values 2 and 3 - Close together (difference of 1 unit) - Distances from mean: 3 and 2 units - Sum of “pull” = 5 units\nRight side of the mean: - Points with values 6 and 9 - More spread out (difference of 3 units) - Distances from mean: 1 and 4 units - Sum of “pull” = 5 units\nKey observations:\n\nThe mean (5) is a balance point, even though:\n\nPoints on the left are clustered (2,3)\nPoints on the right are dispersed (6,9)\nGreen arrows show distances from the mean\n\nBalance is maintained because:\n\nSum of distances balances out: (5-2) + (5-3) = (6-5) + (9-5)\nTotal sum of distances = 5 units on each side\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual Calculation Example:\nLet’s calculate the mean for the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nSum all values\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nCount the number of values\nn = 7\n\n\n3\nDivide the sum by n\n36 / 7 = 5.14\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(data)\n\n[1] 5.142857\n\n\nPros:\n\nEasy to calculate and understand\nUses all data points\nUseful for further statistical calculations\n\nCons:\n\nSensitive to outliers\nNot ideal for skewed distributions\n\nExample with outlier:\n\ndata_with_outlier &lt;- c(2, 4, 4, 5, 5, 7, 100)\nmean(data_with_outlier)\n\n[1] 18.14286\n\n\nAs we can see, the outlier (100) drastically affects the mean.\n\n\n11.6.3 Median\nThe median is the middle value when the data is ordered.\nManual Calculation Example:\nUsing the same dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nResult\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind the middle value\n5\n\n\n\nFor even number of values, take the average of the two middle values.\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmedian(data)\n\n[1] 5\n\nmedian(data_with_outlier)\n\n[1] 5\n\n\nPros:\n\nNot affected by extreme outliers\nBetter for skewed distributions\n\nCons:\n\nDoesn’t use all data points\nLess useful for further statistical calculations\n\n\n\n\n\n\n\nWarning\n\n\n\nTo find the position of the median in a dataset:\n\nFirst sort the data in ascending order\nIf n is odd:\n\nMedian position = \\frac{n + 1}{2}\n\nIf n is even:\n\nFirst median position = \\frac{n}{2}\nSecond median position = \\frac{n}{2} + 1\nMedian = \\frac{\\text{value at }\\frac{n}{2} + \\text{value at }(\\frac{n}{2}+1)}{2}\n\n\nFor example:\n\nOdd n=7: position = \\frac{7+1}{2} = 4th value\nEven n=8: positions = \\frac{8}{2} = 4th and 4+1 = 5th value\n\n\n\n\n\n11.6.4 Mode\nThe mode is the most frequently occurring value.\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nValue\nFrequency\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nThe mode is 4 and 5 (bimodal).\nR calculation:\n\nlibrary(modeest)\nmfv(data)  # Most frequent value\n\n[1] 4 5\n\n\nPros:\n\nOnly measure of central tendency for nominal data\nCan identify multiple peaks in the data\n\nCons:\n\nNot always uniquely defined\nNot useful for continuous data\n\n\n\n11.6.5 Weighted (arithmetic) Mean (*)\nThe weighted mean is used when some data points are more important than others. There are two types of weighted means: with not normalized weights and with normalized weights.\n\n11.6.5.1 Weighted Mean with Not Normalized Weights\nThis is the standard form of the weighted mean, where weights can be any positive numbers representing the importance of each data point.\nFormula: \\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\nManual Calculation Example:\nLet’s calculate the weighted mean for the dataset: 2, 4, 5, 7 with weights 1, 2, 3, 1\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 1) + (4 * 2) + (5 * 3) + (7 * 1) = 2 + 8 + 15 + 7 = 32\n\n\n2\nSum the weights\n1 + 2 + 3 + 1 = 7\n\n\n3\nDivide the result from step 1 by the result from step 2\n32 / 7 = 4.57\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\n11.6.5.2 Weighted Mean with Normalized Weights (Fractions)\nIn this case, the weights are fractions that sum to 1, representing the proportion of importance for each data point.\nFormula: \\bar{x}_w = \\sum_{i=1}^n w_i x_i, where \\sum_{i=1}^n w_i = 1\nManual Calculation Example:\nLet’s calculate the weighted mean for the dataset: 2, 4, 5, 7 with normalized weights 0.1, 0.3, 0.4, 0.2\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 0.1) + (4 * 0.3) + (5 * 0.4) + (7 * 0.2)\n\n\n2\nSum the results\n0.2 + 1.2 + 2.0 + 1.4 = 4.8\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Note: these sum to 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nPros of Weighted Means:\n\nAccount for varying importance of data points\nUseful in survey analysis with different sample sizes or importance levels\nCan adjust for unequal probabilities in sampling designs\n\nCons of Weighted Means:\n\nRequire justification for weights\nCan be misused to manipulate results\nMay be less intuitive to interpret than simple arithmetic mean",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-variability",
    "href": "chapter5.html#measures-of-variability",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.7 Measures of Variability",
    "text": "11.7 Measures of Variability\nThese measures describe how spread out the data is. They are crucial for understanding the dispersion of data points around the central tendency.\n\n\n\n\n\n\nUnderstanding Variance\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.1: Three dot plots showing increasing variance with constant mean\n\n\n\n\n\nThe three dot plots above demonstrate how variance measures the spread of data around a central value:\n\nAll distributions have the same mean (μ = 10), shown by the dashed line\nLow Variance (σ² = 1): Points cluster tightly around the mean\nMedium Variance (σ² = 4): Points show moderate spread\nHigh Variance (σ² = 9): Points spread widely around the mean\n\n\n\n\n\n\n\n\n\nUnderstanding Different Levels of Variability\n\n\n\n\n\n\n\n\n\n\n\n\nThis visualization shows three normal distributions with the same mean (μ = 10) but different levels of variability:\n\nLow Variability (σ = 0.5)\n\nData points cluster tightly around the mean\nThe density curve is tall and narrow\nMost observations fall within ±0.5 units of the mean\n\nMedium Variability (σ = 2.0)\n\nData points spread out more from the mean\nThe density curve is lower and wider\nMost observations fall within ±2 units of the mean\n\nHigh Variability (σ = 4.0)\n\nData points spread widely from the mean\nThe density curve is much flatter and wider\nMost observations fall within ±4 units of the mean\n\n\n\n\n\n11.7.1 Range\nThe range is the difference between the maximum and minimum values.\nFormula: R = x_{max} - x_{min}\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nFind the maximum value\n9\n\n\n2\nFind the minimum value\n2\n\n\n3\nSubtract minimum from maximum\n9 - 2 = 7\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nrange(data)\n\n[1] 2 9\n\nmax(data) - min(data)\n\n[1] 7\n\n\nPros:\n\nSimple to calculate and understand\nGives an immediate sense of data spread\n\nCons:\n\nExtremely sensitive to outliers\nDoesn’t provide information about the distribution between extremes\n\n\n\n11.7.2 Interquartile Range (IQR)\nThe IQR is the difference between the 75th and 25th percentiles.\nFormula: IQR = Q_3 - Q_1\nTo find quartiles manually:\n\nFor odd number of values:\n\nQ2 (median) is the middle value\nQ1 is the median of the lower half (excluding the median of all observations)\nQ3 is the median of the upper half (excluding the median of all observations)\n\nFor even number of values:\n\nQ2 is the average of the two middle values\nQ1 is the median of the lower half (excluding the median of all observations)\nQ3 is the median of the upper half (excluding the median of all observations)\n\n\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind Q2 (median)\n5\n\n\n3\nFind Q1 (median of lower half)\n4\n\n\n4\nFind Q3 (median of upper half)\n7\n\n\n5\nCalculate IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nprint(data)\n\n[1] 2 4 4 5 5 7 9\n\nquantile(data, type = 1)\n\n  0%  25%  50%  75% 100% \n   2    4    5    7    9 \n\nIQR(data, type = 1)\n\n[1] 3\n\n\nPros:\n\nRobust to outliers\nProvides information about the spread of the middle 50% of the data\n\nCons:\n\nIgnores the tails of the distribution\nLess efficient than standard deviation for normal distributions\n\n\n\n11.7.3 Variance\nVariance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\n\n\n\n\n\nVariance: Understanding Average Squared Deviations\n\n\n\nWhat is Variance? Variance measures how “spread out” numbers are from their mean - it’s the average of squared deviations from the mean.\nFormula: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nSimple Example: Consider numbers: 2, 4, 6, 8, 10 Mean (\\bar{x}) = 6\n\n\n\n\n\n\n\n\n\nCalculating Deviations:\n\n\n\n\n\n\n\n\n\n\n\n\nValue\nDeviation from mean\nSquare of deviation\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n+2\n4\n\n\n10\n+4\n16\n\n\n\nVariance = \\frac{16 + 4 + 0 + 4 + 16}{4} = 10\nKey Points:\n\nMean acts as a reference line (blue dashed line)\nDeviations show distance from mean (red dotted lines)\nSquaring makes all deviations positive (blue bars)\nLarger deviations contribute more to variance\n\n\n\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\bar{x} = 5.14\n\n\n2\nSubtract the mean from each value and square the result\n(2 - 5.14)^2 = 9.86\n\n\n\n\n(4 - 5.14)^2 = 1.30\n\n\n\n\n(4 - 5.14)^2 = 1.30\n\n\n\n\n(5 - 5.14)^2 = 0.02\n\n\n\n\n(5 - 5.14)^2 = 0.02\n\n\n\n\n(7 - 5.14)^2 = 3.46\n\n\n\n\n(9 - 5.14)^2 = 14.90\n\n\n3\nSum the squared differences\n30.86\n\n\n4\nDivide by (n-1), i.e. by the number of observations - 1\n30.86 / 6 = 5.14\n\n\n\nR calculation:\n\nvar(data)\n\n[1] 5.142857\n\n\nPros:\n\nUses all data points\nFoundation for many statistical tests\n\nCons:\n\nUnits are squared, making interpretation less intuitive\nSensitive to outliers\n\n\n\n\n\n\n\nBessel’s Correction: Why We Divide by (n-1) And Not by n\n\n\n\nThe Key Insight:\nWhen we calculate deviations from the mean, they must sum to zero. This is a mathematical fact: \\sum(x_i - \\bar{x}) = 0\nThink of it Like This:\nIf you have 5 numbers and their mean:\n\nOnce you calculate 4 deviations from the mean\nThe 5th deviation MUST be whatever makes the sum zero\nYou don’t really have 5 independent deviations\nYou only have 4 truly “free” deviations\n\nSimple Example:\nNumbers: 2, 4, 6, 8, 10\n\nMean = 6\nDeviations: -4, -2, 0, +2, +4\nNotice they sum to zero\nIf you know any 4 deviations, the 5th is predetermined!\n\nThis is Why:\n\nWhen calculating variance: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nWe divide by (n-1) not n\nBecause only (n-1) deviations are truly independent\nThe last one is determined by the others\n\nDegrees of Freedom:\n\nn = number of observations\n1 = constraint (deviations must sum to zero)\nn-1 = degrees of freedom = number of truly independent deviations\n\nWhen to Use It:\n\nWhen calculating sample variance\nWhen calculating sample standard deviation\n\nWhen NOT to Use It:\n\nPopulation calculations (when you have all data)\n\nRemember:\n\nIt’s not just a statistical trick\nDeviations from the mean must sum to zero\nThis constraint costs us one degree of freedom\n\n\n\n\n\n11.7.4 Standard Deviation\nThe standard deviation is the square root of the variance and measures the average dispersion of the data about their arithmetic mean. In contrast to the variance, it has the advantage of being expressed in the same units as the original measurements, making its interpretation more intuitive.\nFormula: s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the variance\ns^2 = 5.14 (from previous calculation)\n\n\n2\nTake the square root\ns = \\sqrt{5.14} = 2.27\n\n\n\nR calculation:\n\nsd(data)\n\n[1] 2.267787\n\n\nPros:\n\nIn same units as original data\nWidely used and understood\n\nCons:\n\nStill sensitive to outliers\nAssumes data is roughly “normally” distributed\n\n\n\n11.7.5 Coefficient of Variation (*)\nThe coefficient of variation is the standard deviation divided by the mean, often expressed as a percentage.\nFormula: CV = \\frac{s}{\\bar{x}} \\times 100\\%\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\bar{x} = 5.14\n\n\n2\nCalculate the standard deviation\ns = 2.27\n\n\n3\nDivide s by the mean and multiply by 100\n(2.27 / 5.14) * 100 = 44.16\\%\n\n\n\nR calculation:\n\n(sd(data) / mean(data)) * 100\n\n[1] 44.09586\n\n\nPros:\n\nAllows comparison of variability between datasets with different units or means\nUseful in fields like finance for risk assessment\n\nCons:\n\nNot meaningful for data with both positive and negative values\nCan be misleading when mean is close to zero\n\n\n\n\n\n\n\nLimitations of Coefficient of Variation (CV)\n\n\n\nThe coefficient of variation, calculated as (σ/μ) × 100\\%, has two important limitations:\n\n11.7.5.1 Not meaningful for data with both positive and negative values\n\nThe mean could be close to zero due to positive and negative values cancelling out\nExample: Dataset {-5, -3, 2, 6} has mean = 0\n\nCV = (std dev / 0) × 100%\nThis leads to division by zero\nEven if mean isn’t exactly zero, the CV doesn’t represent true relative variability when data cross zero\n\nThe CV assumes a natural zero point and meaningful ratios between values\n\n\n\n11.7.5.2 Misleading when mean is close to zero\n\nSince CV = (σ/μ) × 100\\%, as μ approaches zero:\n\nThe denominator becomes very small\nResults in extremely large CV values\nThese large values don’t meaningfully represent relative variability\n\nExample:\n\nDataset A: {0.001, 0.002, 0.003} has mean = 0.002\nEven small standard deviations will produce very large CVs\nThe resulting large CV might suggest extreme variability when the data are actually quite close together\n\n\n\n\n11.7.5.3 Best Use Cases\nCV is most useful for:\n\nStrictly positive data\nData measured on a ratio scale\nData with means well above zero\nComparing variability between datasets with different units or scales",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-relative-position-standing",
    "href": "chapter5.html#measures-of-relative-position-standing",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.8 Measures of Relative Position (Standing)",
    "text": "11.8 Measures of Relative Position (Standing)\nUnderstanding where values sit within a dataset is crucial for data analysis. Let’s explore these concepts step by step.\n\n11.8.1 Quartiles (Q): The Basics\nThink of quartiles as special numbers that split your ordered data into four equal parts.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\n11.8.1.1 What Are Quartiles?\nFirst Quartile (Q1):\n\nSeparates the lowest 25% of data from the rest\nAlso called the 25th percentile\nExample: If Q1 = 50 in a test score dataset, 25% of students scored below 50\n\nSecond Quartile (Q2):\n\nThe median - splits data in half\nAlso called the 50th percentile\nExample: If Q2 = 70, half the students scored below 70\n\nThird Quartile (Q3):\n\nSeparates the highest 25% of data from the rest\nAlso called the 75th percentile\nExample: If Q3 = 85, 75% of students scored below 85\n\n\n\n11.8.1.2 How to Calculate Quartiles (Step by Step) - Two Methods\nLet’s examine student test scores using both common quartile calculation methods:\nExample 1: Odd Number Case (11 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 11 values (odd)\nMedian position = (n + 1)/2 = 6\nQ2 = 78\n\nStep 2: Find Q1\n\nTukey’s Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3rd value)\n\n\nStep 3: Find Q3\n\nTukey’s Method:\n\nLook at upper half: 80, 82, 85, 88, 90\nQ3 = median of upper half = 85\n\nInterpolation Method:\n\nPosition = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9th value)\n\n\nExample 2: Even Number Case (10 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 10 values (even)\nMedian positions = 5 and 6\nQ2 = (75 + 78)/2 = 76.5\n\nStep 2: Find Q1\n\nTukey’s Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nStep 3: Find Q3\n\nTukey’s Method:\n\nLook at upper half: 78, 80, 82, 85, 90\nQ3 = median of upper half = 82\n\nInterpolation Method:\n\nPosition = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nImportant Notes:\n\nTukey’s Method:\n\nFirst find the median (Q2)\nSplit the data into lower and upper halves\nFind Q1 as the median of the lower half\nFind Q3 as the median of the upper half\nWhen n is odd, the median is not included in either half\n\nInterpolation Method:\n\nUses positions (n+1)/4 for Q1 and 3(n+1)/4 for Q3\nWhen position falls between values, uses linear interpolation\nDoesn’t require splitting data into halves\n\n\nBoth methods give the same results for simple positions (Example 1) but can differ when interpolation is needed (Example 2).\n\n\n\n\n\n\nManual Construction of Tukey Boxplot\n\n\n\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\n\n\n\n11.8.2 Percentiles: A More Precise Measure of Relative Standing (*)\n\n11.8.2.1 What Are Percentiles?\nPercentiles give us a more detailed view by dividing data into 100 equal parts. Unlike quartiles, percentiles use linear interpolation for more precise measurements.\nKey Points:\n\nThe 25th percentile equals Q1\nThe 50th percentile equals Q2 (median)\nThe 75th percentile equals Q3\n\n\n\n11.8.2.2 Calculating Percentiles\nThe Formula: P_k = \\frac{k(n+1)}{100}\nWhere:\n\nP_k is the position for the kth percentile\nk is the percentile we want (1-100)\nn is the number of observations\n\nExample 3: Finding the 60th Percentile Let’s use student homework scores: 72, 75, 78, 80, 82, 85, 88, 90, 92, 95\nStep 1: Calculate position\n\nn = 10 scores\nFor 60th percentile: P_{60} = \\frac{60(10+1)}{100} = 6.6\n\nStep 2: Find surrounding values\n\nPosition 6: score of 85\nPosition 7: score of 88\n\nStep 3: Interpolate (important: percentiles use linear interpolation)\n\nWe need to go 0.6 of the way between 85 and 88 P_{60} = 85 + 0.6(88-85) P_{60} = 85 + 0.6(3) P_{60} = 85 + 1.8 = 86.8\n\nWhat this means: 60% of students scored 86.8 or below.\n\n\n\n11.8.3 Percentile Ranks (PR) (*)\n\n11.8.3.1 What is a Percentile Rank?\nWhile percentiles tell us the value at a certain position, percentile rank tells us what percentage of values fall below a specific score. Think of it as answering the question “What percentage of the class did I score higher than?”\nPR = \\frac{\\text{number of values below } + 0.5 \\times \\text{number of equal values}}{\\text{total number of values}} \\times 100\nExample 4: Finding a Percentile Rank Consider these exam scores:\n65, 70, 70, 75, 75, 75, 80, 85, 85, 90\nLet’s find the PR for a score of 75.\nStep 1: Count carefully\n\nValues below 75: 65, 70, 70 (3 values)\nValues equal to 75: 75, 75, 75 (3 values)\nTotal values: 10\n\nStep 2: Apply the formula\nPR = \\frac{3 + 0.5(3)}{10} \\times 100 PR = \\frac{3 + 1.5}{10} \\times 100 PR = \\frac{4.5}{10} \\times 100 = 45\\%\nInterpretation: A score of 75 is higher than 45% of the class scores.\nRemark:\nQ1: “Why do we use 0.5 for equal values in PR?”\nA1: This is because we’re assuming people with the same score are evenly spread across that position. It’s like saying they share the position equally.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-shape",
    "href": "chapter5.html#measures-of-shape",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.9 Measures of Shape",
    "text": "11.9 Measures of Shape\n\n11.9.1 Skewness\n\n11.9.1.1 Definition\nSkewness quantifies the asymmetry of a data distribution. It indicates whether the data clusters more on one side of the mean than the other.\n\n\n11.9.1.2 Mathematical Expression\nSK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3\nwhere:\n\nn is the sample size\nx_i is the i-th observation\n\\bar{x} is the sample mean\ns is the sample standard deviation\n\n\n\n11.9.1.3 Example: Voter Turnout Analysis\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n# Generate example precinct-level voter turnout data\nset.seed(123)\nturnout_data &lt;- c(\n  # Urban precincts\n  rnorm(300, mean = 65, sd = 12),\n  # Suburban precincts\n  rnorm(400, mean = 70, sd = 10),\n  # Rural precincts\n  rnorm(300, mean = 68, sd = 15)\n) |&gt; \n  # Ensure turnout stays between 0-100%\n  pmax(0) |&gt; \n  pmin(100)\n\n# Calculate and visualize\nskew_value &lt;- skewness(turnout_data)\nskew_value\n\n[1] 0.02558143\n\nggplot(data.frame(x = turnout_data), aes(x = x)) +\n  geom_histogram(bins = 50, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = mean(turnout_data), color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = median(turnout_data), color = \"blue\", linetype = \"dashed\") +\n  labs(\n    title = str_glue(\"Precinct-Level Voter Turnout Distribution (Skewness = {round(skew_value, 4)})\"),\n    subtitle = \"Red: Mean, Blue: Median\",\n    x = \"Voter Turnout (%)\",\n    y = \"Number of Precincts\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n11.9.1.4 Interpretation Guide\n\nPositive Skewness (&gt; 0): Distribution has a longer right tail\nNegative Skewness (&lt; 0): Distribution has a longer left tail\nZero Skewness: Approximately symmetric distribution\n\n\n\n\n11.9.2 Kurtosis\n\n\n11.9.3 Definition\nKurtosis measures the “tailedness” of a distribution, indicating the presence of extreme values relative to a normal distribution.\n\n11.9.3.1 Mathematical Expression\nK = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\n\n11.9.3.2 Example: Legislative Voting Analysis\n\n# Generate example legislative voting agreement scores\nset.seed(456)\nvoting_agreement &lt;- c(\n  # Regular voting patterns\n  rnorm(400, mean = 75, sd = 10),\n  # Cross-party cooperation instances\n  rnorm(80, mean = 50, sd = 15),\n  # Party-line votes\n  rnorm(20, mean = 95, sd = 5)\n) |&gt; \n  pmax(0) |&gt; \n  pmin(100)\n\nkurt_value &lt;- kurtosis(voting_agreement)\nkurt_value\n\n[1] 3.849939\n\n# Visualization with normal distribution comparison\nx_range &lt;- seq(min(voting_agreement)-1, max(voting_agreement)+1, length.out = 100)\nnormal_dist &lt;- dnorm(x_range, mean = mean(voting_agreement), sd = sd(voting_agreement))\n\nggplot() +\n  geom_density(\n    data = data.frame(x = voting_agreement), \n    aes(x = x), \n    fill = \"skyblue\", \n    alpha = 0.5\n  ) +\n  geom_line(\n    data = data.frame(x = x_range, y = normal_dist),\n    aes(x = x, y = y),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) +\n  labs(\n    title = str_glue(\"Legislative Voting Agreement Distribution (Kurtosis = {round(kurt_value, 4)})\"),\n    subtitle = \"Observed distribution (blue) vs. Normal distribution (red)\",\n    x = \"Voting Agreement Score (%)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n11.9.3.3 Interpretation Guide\n\nExcess Kurtosis: Difference from normal distribution’s kurtosis\n\n\n\nLeptokurtic (&gt; 3): More extreme values than normal distribution\nPlatykurtic (&lt; 3): Fewer extreme values than normal distribution\nMesokurtic (= 3): Similar to normal distribution",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "href": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.10 Exercise 1. Center and dispersion of data",
    "text": "11.10 Exercise 1. Center and dispersion of data\n\n11.10.1 Data\nWe have salary data (in thousands of euros) from two small European companies:\n\n\n\nIndex\nCompany X\nCompany Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\nThis table presents the data for both Company X and Company Y side by side, with an index column for easy reference.\n\n\n11.10.2 Measures of Central Tendency\n\n11.10.2.1 Mean\nThe mean is the average of all values in a dataset.\nFormula: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\n11.10.2.1.1 Manual Calculation for Company X\n\n\n\nValue (x_i)\nFrequency (f_i)\nx_i \\cdot f_i\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nTotal\nn = 20\nSum = 119\n\n\n\n\\bar{x} = \\frac{119}{20} = 5.95\n\n\n11.10.2.1.2 Manual Calculation for Company Y\n\n\n\nValue (x_i)\nFrequency (f_i)\nx_i \\cdot f_i\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nTotal\nn = 20\nSum = 100\n\n\n\n\\bar{y} = \\frac{100}{20} = 5\n\n\n11.10.2.1.3 R Verification\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\n11.10.2.2 Median\nThe median is the middle value when the data is ordered.\n\n11.10.2.2.1 Manual Calculation for Company X\nOrdered data: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\frac{4 + 4}{2} = 4\n\n\n11.10.2.2.2 Manual Calculation for Company Y\nOrdered data: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\frac{5 + 5}{2} = 5\n\n\n11.10.2.2.3 R Verification\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\n11.10.2.3 Mode\nThe mode is the most frequent value in the dataset.\nFor Company X, the mode is 3 (appears 6 times). For Company Y, there are two modes: 4 and 5 (both appear 6 times).\n\n# Function to calculate mode\nget_mode &lt;- function(x) {\n  unique_x &lt;- unique(x)\n  unique_x[which.max(tabulate(match(x, unique_x)))]\n}\n\nget_mode(X)\n\n[1] 3\n\nget_mode(Y)\n\n[1] 4\n\n\n\n\n\n11.10.3 Measures of Dispersion\n\n11.10.3.1 Variance\nThe variance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\n11.10.3.1.1 Manual Calculation for Company X\n\n\n\n\n\n\n\n\n\n\nx_i\nf_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\nf_i(x_i - \\bar{x})^2\n\n\n\n\n2\n3\n-3.95\n15.6025\n46.8075\n\n\n3\n6\n-2.95\n8.7025\n52.215\n\n\n4\n5\n-1.95\n3.8025\n19.0125\n\n\n5\n4\n-0.95\n0.9025\n3.61\n\n\n20\n1\n14.05\n197.4025\n197.4025\n\n\n35\n1\n29.05\n843.9025\n843.9025\n\n\nTotal\n20\n\n\n1162.95\n\n\n\ns^2 = \\frac{1162.95}{19} = 61.21\n\n\n11.10.3.1.2 Manual Calculation for Company Y\n\n\n\n\n\n\n\n\n\n\ny_i\nf_i\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\nf_i(y_i - \\bar{y})^2\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nTotal\n20\n\n\n34\n\n\n\ns^2 = \\frac{34}{19} = 1.79\n\n\n11.10.3.1.3 R Verification\n\nvar(X)\n\n[1] 61.20789\n\nvar(Y)\n\n[1] 1.789474\n\n\n\n\n\n11.10.3.2 Standard Deviation\nThe standard deviation is the square root of the variance.\nFormula: s = \\sqrt{s^2}\n\nFor Company X: s = \\sqrt{61.21} = 7.82\nFor Company Y: s = \\sqrt{1.79} = 1.34\n\n\n11.10.3.2.1 R Verification\n\nsd(X)\n\n[1] 7.823547\n\nsd(Y)\n\n[1] 1.337712\n\n\n\n\n\n\n11.10.4 Quartiles\nQuartiles divide the dataset into four equal parts.\n\n11.10.4.1 Manual Calculation for Company X\nOrdered data: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\n\nQ1 (25th percentile): median of first 10 numbers = 3\nQ2 (50th percentile, median): 4\nQ3 (75th percentile): median of last 10 numbers = 5\n\n\n\n11.10.4.2 Manual Calculation for Company Y\nOrdered data: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\n\nQ1 (25th percentile): median of first 10 numbers = 4\nQ2 (50th percentile, median): 5\nQ3 (75th percentile): median of last 10 numbers = 6\n\n\n\n11.10.4.3 R Verification\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\n11.10.4.4 IQR\n\nIQR_x = 5 - 3 = 2\nIQR_y = 6 - 4 = 2\n\n\n\n\n11.10.5 Tukey Box Plot\nA Tukey box plot visually represents the distribution of data based on quartiles. We’ll use ggplot2 to create the plot.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Prepare the data\ndata &lt;- data.frame(\n  Company = rep(c(\"X\", \"Y\"), each = 20),\n  Salary = c(X, Y)\n)\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot() +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot(outliers = F) +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\n11.10.5.1 Interpreting the Box Plot\n\nThe box represents the interquartile range (IQR) from Q1 to Q3.\nThe line inside the box is the median (Q2).\nWhiskers extend to the smallest and largest values within 1.5 * IQR.\nPoints beyond the whiskers are considered outliers.\n\n\n\n\n11.10.6 Comparison of Results\n\n\n\nMeasure\nCompany X\nCompany Y\n\n\n\n\nMean\n5.95\n5.00\n\n\nMedian\n4\n5\n\n\nMode\n3\n4 and 5\n\n\nVariance\n61.21\n1.79\n\n\nStandard Deviation\n7.82\n1.34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\n11.10.6.1 Key Observations:\n\nCentral Tendency: Company X has a higher mean but lower median than Company Y, indicating a right-skewed distribution for Company X.\nDispersion: Company X shows much higher variance and standard deviation, suggesting greater salary disparities.\nDistribution Shape: Company Y’s salaries are more tightly clustered, while Company X has extreme values (potential outliers) that significantly affect its mean and variance.\nQuartiles: Company Y’s interquartile range (Q3 - Q1) is slightly larger, but its overall range is much smaller than Company X’s.\n\n\n\n\n11.10.7 Conclusion\nThis comparative analysis reveals significant differences in salary structures between the two companies. Company X shows greater variability and potential inequality in its pay scale, while Company Y demonstrates a more consistent and narrowly distributed salary range.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-2.-comparing-electoral-district-size-variation-between-countries",
    "href": "chapter5.html#exercise-2.-comparing-electoral-district-size-variation-between-countries",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.11 Exercise 2. Comparing Electoral District Size Variation Between Countries",
    "text": "11.11 Exercise 2. Comparing Electoral District Size Variation Between Countries\n\n11.11.1 Data\nWe have electoral district size data from two countries:\n\nx &lt;- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Country high variance\ny &lt;- c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13)  # Country low variance\n\nkable(data.frame(\n  \"Country X (High var.)\" = x,\n  \"Country Y (Low var.)\" = y\n))\n\n\n\n\nCountry.X..High.var..\nCountry.Y..Low.var..\n\n\n\n\n1\n8\n\n\n3\n9\n\n\n5\n9\n\n\n7\n10\n\n\n9\n10\n\n\n11\n11\n\n\n13\n11\n\n\n15\n12\n\n\n17\n12\n\n\n19\n13\n\n\n\n\n\n\n\n11.11.2 Measures of Central Tendency\n\n11.11.2.1 Arithmetic Mean\nFormula: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\n11.11.2.1.1 Calculations for Country X\n\n\n\nElement\nValue\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n5\n\n\n4\n7\n\n\n5\n9\n\n\n6\n11\n\n\n7\n13\n\n\n8\n15\n\n\n9\n17\n\n\n10\n19\n\n\nSum\n100\n\n\n\n\\bar{x} = \\frac{100}{10} = 10\n\nmean_x &lt;- mean(x)\nc(\"Manual\" = 10, \"R\" = mean_x)\n\nManual      R \n    10     10 \n\n\n\n\n11.11.2.1.2 Calculations for Country Y\n\n\n\nElement\nValue\n\n\n\n\n1\n8\n\n\n2\n9\n\n\n3\n9\n\n\n4\n10\n\n\n5\n10\n\n\n6\n11\n\n\n7\n11\n\n\n8\n12\n\n\n9\n12\n\n\n10\n13\n\n\nSum\n105\n\n\n\n\\bar{y} = \\frac{105}{10} = 10.5\n\nmean_y &lt;- mean(y)\nc(\"Manual\" = 10.5, \"R\" = mean_y)\n\nManual      R \n  10.5   10.5 \n\n\n\n\n\n11.11.2.2 Median\nThe median is the middle value in an ordered dataset.\n\n11.11.2.2.1 Calculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nFor n = 10 (even number of observations): Middle positions: 5 and 6 Middle values: 9 and 11\nMedian = \\frac{9 + 11}{2} = 10\n\nmedian_x &lt;- median(x)\nc(\"Manual\" = 10, \"R\" = median_x)\n\nManual      R \n    10     10 \n\n\n\n\n11.11.2.2.2 Calculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nFor n = 10 (even number of observations): Middle positions: 5 and 6 Middle values: 10 and 11\nMedian = \\frac{10 + 11}{2} = 10.5\n\nmedian_y &lt;- median(y)\nc(\"Manual\" = 10.5, \"R\" = median_y)\n\nManual      R \n  10.5   10.5 \n\n\n\n\n\n11.11.2.3 Mode\n\n11.11.2.3.1 Calculations for Country X\n\n\n\nValue\nFrequency\n\n\n\n\n1\n1\n\n\n3\n1\n\n\n5\n1\n\n\n7\n1\n\n\n9\n1\n\n\n11\n1\n\n\n13\n1\n\n\n15\n1\n\n\n17\n1\n\n\n19\n1\n\n\n\nConclusion: No mode (all values occur once)\n\n\n11.11.2.3.2 Calculations for Country Y\n\n\n\nValue\nFrequency\n\n\n\n\n8\n1\n\n\n9\n2\n\n\n10\n2\n\n\n11\n2\n\n\n12\n2\n\n\n13\n1\n\n\n\nConclusion: Four modes: 9, 10, 11, 12 (each occurs twice)\n\n# Frequency tables\ntable_x &lt;- table(x)\ntable_y &lt;- table(y)\n\nlist(\n  \"Country X\" = table_x,\n  \"Country Y\" = table_y\n)\n\n$`Country X`\nx\n 1  3  5  7  9 11 13 15 17 19 \n 1  1  1  1  1  1  1  1  1  1 \n\n$`Country Y`\ny\n 8  9 10 11 12 13 \n 1  2  2  2  2  1 \n\n\n\n\n\n11.11.2.4 Variance\nVariance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\n11.11.2.4.1 Calculations for Country X\n\n\n\nx_i\n(x_i - \\bar{x})\n(x_i - \\bar{x})^2\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSum\n\n330\n\n\n\ns^2_X = \\frac{330}{9} = 36.67\n\nvar_x &lt;- var(x)\nc(\"Manual\" = 36.67, \"R\" = var_x)\n\nManual      R \n 36.67  36.67 \n\n\n\n\n11.11.2.4.2 Calculations for Country Y\n\n\n\nx_i\n(y_i - \\bar{y})\n(y_i - \\bar{y})^2\n\n\n\n\n8\n-2.5\n6.25\n\n\n9\n-1.5\n2.25\n\n\n9\n-1.5\n2.25\n\n\n10\n-0.5\n0.25\n\n\n10\n-0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n12\n1.5\n2.25\n\n\n12\n1.5\n2.25\n\n\n13\n2.5\n6.25\n\n\nSum\n\n22.5\n\n\n\ns^2_Y = \\frac{22.5}{9} = 2.5\n\nvar_y &lt;- var(y)\nc(\"Manual\" = 2.5, \"R\" = var_y)\n\nManual      R \n   2.5    2.5 \n\n\n\n\n\n11.11.2.5 Standard Deviation\nStandard deviation is the square root of variance. It measures variability in the same units as the data.\nFormula: s = \\sqrt{s^2}\n\n11.11.2.5.1 Calculations for Country X\nUsing previously calculated variance: s^2_X = 36.67\nCalculate square root: s_X = \\sqrt{36.67} \\approx 6.06\n\n\n\nStep\nCalculation\nResult\n\n\n\n\n1. Variance\ns^2_X\n36.67\n\n\n2. Square root\n\\sqrt{36.67}\n6.06\n\n\n\n\nsd_x &lt;- sd(x)\nc(\"Manual\" = 6.06, \"R\" = sd_x)\n\nManual      R \n 6.060  6.055 \n\n\n\n\n11.11.2.5.2 Calculations for Country Y\nUsing previously calculated variance: s^2_Y = 2.5\nCalculate square root: s_Y = \\sqrt{2.5} \\approx 1.58\n\n\n\nStep\nCalculation\nResult\n\n\n\n\n1. Variance\ns^2_Y\n2.5\n\n\n2. Square root\n\\sqrt{2.5}\n1.58\n\n\n\n\nsd_y &lt;- sd(y)\nc(\"Manual\" = 1.58, \"R\" = sd_y)\n\nManual      R \n 1.580  1.581 \n\n\nInterpretation:\n\nCountry X: Average deviation from the mean is about 6 seats\nCountry Y: Average deviation from the mean is about 1.6 seats\n\n\n\n\n\n11.11.3 Coefficient of Variation (CV)\nThe coefficient of variation is the ratio of standard deviation to mean, expressed as a percentage.\nFormula: CV = \\frac{s}{\\bar{x}} \\times 100\\%\n\n11.11.3.1 Calculations for Country X\nCV_X = \\frac{6.06}{10} \\times 100\\% = 60.6\\%\n\n\n\nComponent\nValue\n\n\n\n\nStandard deviation (s)\n6.06\n\n\nMean (\\bar{x})\n10\n\n\nCV\n60.6%\n\n\n\n\ncv_x &lt;- sd(x) / mean(x) * 100\nc(\"Manual\" = 60.6, \"R\" = cv_x)\n\nManual      R \n 60.60  60.55 \n\n\n\n\n11.11.3.2 Calculations for Country Y\nCV_Y = \\frac{1.58}{10.5} \\times 100\\% = 15.0\\%\n\n\n\nComponent\nValue\n\n\n\n\nStandard deviation (s)\n1.58\n\n\nMean (\\bar{x})\n10.5\n\n\nCV\n15.0%\n\n\n\n\ncv_y &lt;- sd(y) / mean(y) * 100\nc(\"Manual\" = 15.0, \"R\" = cv_y)\n\nManual      R \n 15.00  15.06 \n\n\n\n\n\n11.11.4 Quartiles and Interquartile Range (IQR)\n\n11.11.4.1 Methods for Calculating Quartiles\nThere are different methods for calculating quartiles. In our manual calculations, we’ll use the median-excluding method:\n\nSplit the series at the median\nMedian is not included in quartile calculations\nCalculate median of each part - these will be Q1 and Q3 respectively\n\n\n\n11.11.4.2 Calculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMedian = 10 (not included in quartile calculations)\nLower half: 1, 3, 5, 7, 9 Q1 = median of lower half = 5\nUpper half: 11, 13, 15, 17, 19 Q3 = median of upper half = 15\nIQR = Q3 - Q1 = 15 - 5 = 10\n\n\n11.11.4.3 Calculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMedian = 10.5 (not included in quartile calculations)\nLower half: 8, 9, 9, 10, 10 Q1 = median of lower half = 9\nUpper half: 11, 11, 12, 12, 13 Q3 = median of upper half = 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n# Comparison of different quartile calculation methods in R\nmethods_comparison &lt;- data.frame(\n  Method = c(\"Manual (excl. median)\", \n             \"R type=1\", \"R type=2\", \"R type=7 (default)\"),\n  \"Q1 Country X\" = c(5, \n                    quantile(x, 0.25, type=1),\n                    quantile(x, 0.25, type=2),\n                    quantile(x, 0.25, type=7)),\n  \"Q3 Country X\" = c(15,\n                    quantile(x, 0.75, type=1),\n                    quantile(x, 0.75, type=2),\n                    quantile(x, 0.75, type=7)),\n  \"Q1 Country Y\" = c(9,\n                    quantile(y, 0.25, type=1),\n                    quantile(y, 0.25, type=2),\n                    quantile(y, 0.25, type=7)),\n  \"Q3 Country Y\" = c(12,\n                    quantile(y, 0.75, type=1),\n                    quantile(y, 0.75, type=2),\n                    quantile(y, 0.75, type=7))\n)\n\nkable(methods_comparison, digits = 2,\n      caption = \"Comparison of different quartile calculation methods\")\n\n\nComparison of different quartile calculation methods\n\n\n\n\n\n\n\n\n\nMethod\nQ1.Country.X\nQ3.Country.X\nQ1.Country.Y\nQ3.Country.Y\n\n\n\n\nManual (excl. median)\n5.0\n15.0\n9.00\n12.00\n\n\nR type=1\n5.0\n15.0\n9.00\n12.00\n\n\nR type=2\n5.0\n15.0\n9.00\n12.00\n\n\nR type=7 (default)\n5.5\n14.5\n9.25\n11.75\n\n\n\n\n\n\n\n11.11.4.4 Explanation of Different Quartile Calculation Methods\n\nManual method (excluding median):\n\nSplits data into two parts\nExcludes median\nFinds median of each part\n\nR type=1:\n\nFirst method in R\nUses whole positions\nNo interpolation\n\nR type=2:\n\nSecond method in R\nUses whole positions\nInterpolates when position is not whole\n\nR type=7 (default):\n\nDefault method in R\nUses quantile()[5] from SAS\nInterpolates according to Hyndman and Fan method\n\n\n\n\n\n11.11.5 Results Comparison\n\nsummary_df &lt;- data.frame(\n  Measure = c(\"Mean\", \"Median\", \"Mode\", \"Range\", \"Variance\", \n              \"Std. Dev.\", \"Q1\", \"Q3\", \"IQR\", \"CV (%)\"),\n  \"Country X\" = c(10, 10, \"none\", 18, 36.67, 6.06, 5, 15, 10, 60.6),\n  \"Country Y\" = c(10.5, 10.5, \"9,10,11,12\", 5, 2.5, 1.58, 9, 12, 3, 15.0)\n)\n\nkable(summary_df, \n      caption = \"Summary of all statistical measures\",\n      align = c('l', 'r', 'r'))\n\n\nSummary of all statistical measures\n\n\nMeasure\nCountry.X\nCountry.Y\n\n\n\n\nMean\n10\n10.5\n\n\nMedian\n10\n10.5\n\n\nMode\nnone\n9,10,11,12\n\n\nRange\n18\n5\n\n\nVariance\n36.67\n2.5\n\n\nStd. Dev.\n6.06\n1.58\n\n\nQ1\n5\n9\n\n\nQ3\n15\n12\n\n\nIQR\n10\n3\n\n\nCV (%)\n60.6\n15\n\n\n\n\n\n\n\n11.11.6 Comparison using Box Plot\n\ndf_long &lt;- data.frame(\n  country = rep(c(\"X\", \"Y\"), each = 10),\n  size = c(x, y)\n)\n\n# Basic plot\np &lt;- ggplot(df_long, aes(x = country, y = size, fill = country)) +\n  geom_boxplot(outlier.shape = NA) +  # Disable default outlier points\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Add points with transparency\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\")) +\n  labs(\n    title = \"Comparison of Electoral District Size Variation\",\n    subtitle = paste(\"CV: Country X =\", round(cv_x, 1), \"%, Country Y =\", round(cv_y, 1), \"%\"),\n    x = \"Country\",\n    y = \"District Size\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Add quartile annotations\np + annotate(\n  \"text\", \n  x = c(1, 1, 1, 2, 2, 2), \n  y = c(max(x)+1, mean(x), min(x)-1, max(y)+1, mean(y), min(y)-1),\n  label = c(\n    paste(\"Q3 =\", quantile(x, 0.75, type=1)),\n    paste(\"M =\", median(x)),\n    paste(\"Q1 =\", quantile(x, 0.25, type=1)),\n    paste(\"Q3 =\", quantile(y, 0.75, type=1)),\n    paste(\"M =\", median(y)),\n    paste(\"Q1 =\", quantile(y, 0.25, type=1))\n  ),\n  size = 3\n)\n\n\n\n\n\n\n\n\n\n\n11.11.7 Methodological Notes\n\nQuartile Calculations:\n\nThe median-excluding method used may give different results than R’s default functions\nDifferences in calculation methods don’t affect overall conclusions\nAlways important to specify the method used in reports\n\nVisualization:\n\nBox plot effectively shows differences in distributions\nAdditional points show actual values\nAnnotations facilitate interpretation\n\n\n\n\n11.11.8 Application Notes\n\nUsing the Analysis:\n\nAll calculations can be reproduced using the provided R code\nCode chunks are self-contained and documented\nData format requirements are clearly specified\n\nCustomization:\n\nAnalysis can be adapted for different district size datasets\nVisualization parameters can be adjusted for different presentation needs\nStatistical methods can be modified based on specific requirements\n\n\n\n\n11.11.9 Conclusion\n\n11.11.9.1 Summary Statistics Comparison\n\n\n\nMeasure\nCountry X\nCountry Y\nRelative Difference\n\n\n\n\nMean\n10.0\n10.5\nSimilar\n\n\nMedian\n10.0\n10.5\nSimilar\n\n\nMode\nNone\nMultiple (9,10,11,12)\n-\n\n\nRange\n18\n5\n3.6× larger in X\n\n\nVariance\n36.67\n2.5\n14.7× larger in X\n\n\nIQR\n10\n3\n3.3× larger in X\n\n\nCV\n60.6%\n15.0%\n4.0× larger in X\n\n\n\n\n\n11.11.9.2 Distribution Characteristics\nCountry X:\n\nUniform distribution pattern\nNo dominant district size (no mode)\nWide range: 1 to 19 seats\nHigh variability (CV = 60.6%) - Even spread of values across range\n\nCountry Y:\n\nClustered distribution pattern\nMultiple common sizes (four modes)\nNarrow range: 8 to 13 seats\nLow variability (CV = 15.0%) - Values concentrated around mean\n\n\n\n11.11.9.3 Box Plot Interpretation\nThe box plot visualization reveals:\nStructure Elements:\n\nBox: Shows interquartile range (IQR)\nLower edge: First quartile (Q1)\nUpper edge: Third quartile (Q3)\nInternal line: Median (Q2)\nWhiskers: Extend to ±1.5 IQR - Points: Individual district sizes\n\nKey Visual Findings:\n\nBox Size:\n\n\nCountry X: Large box indicates wide spread of middle 50%\nCountry Y: Small box shows tight clustering of middle values\n\n\nWhisker Length:\n\nCountry X: Long whiskers indicate broad overall distribution\nCountry Y: Short whiskers show limited total spread\n\nPoint Distribution:\n\nCountry X: Points widely dispersed\nCountry Y: Points densely clustered\n\n\n\n\n11.11.9.4 Key Observations\n\nCentral Tendency:\n\nSimilar average district sizes\nDifferent distribution patterns\nDistinct approaches to standardization\n\nVariability Measures:\n\nAll metrics show Country X with 3-15 times more variation\nConsistent pattern across different statistical measures\nSystematic difference in district design\n\nSystem Design:\n\nCountry X: Flexible, varied approach\nCountry Y: Standardized, uniform approach\nDifferent philosophical approaches to representation\n\nRepresentative Implications:\n\nCountry X: Variable voter-to-representative ratios\nCountry Y: More consistent representation levels\nDifferent approaches to democratic representation\n\n\nThis analysis demonstrates fundamental differences in electoral system design between the two countries, with Country X adopting a more varied approach and Country Y maintaining greater uniformity in district sizes.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise.-understanding-boxplots-through-life-expectancy-data",
    "href": "chapter5.html#exercise.-understanding-boxplots-through-life-expectancy-data",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.12 Exercise. Understanding Boxplots Through Life Expectancy Data",
    "text": "11.12 Exercise. Understanding Boxplots Through Life Expectancy Data\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Prepare data\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-boxplots",
    "href": "chapter5.html#introduction-to-boxplots",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.13 Introduction to Boxplots",
    "text": "11.13 Introduction to Boxplots\nA boxplot (also known as a box-and-whisker plot) reveals key statistics about your data:\n\nMedian: The middle line in the box (50th percentile)\nFirst quartile (Q1): Bottom of the box (25th percentile)\nThird quartile (Q3): Top of the box (75th percentile)\nInterquartile Range (IQR): The height of the box (Q3 - Q1)\nWhiskers: Extend to the most extreme non-outlier values (Tukey’s method: 1.5 × IQR)\nOutliers: Individual points beyond the whiskers\n\n\n11.13.1 Visualizing Life Expectancy\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 24, \n               outlier.fill = \"red\", outlier.alpha = 0.6, outlier.size = 4) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"darkblue\") +\n  labs(title = \"Life Expectancy by Continent (2007)\",\n       subtitle = \"Individual points show raw data; red points indicate outliers\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-the-data",
    "href": "chapter5.html#understanding-the-data",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.14 Understanding the Data",
    "text": "11.14 Understanding the Data\n\n11.14.1 Median and Distribution\nAnswer True or False:\n\n50% of African countries have life expectancy below 54 years\nThe median life expectancy in Europe is approximately 78 years\nMore than 75% of countries in Oceania have life expectancy above 74 years\n25% of Asian countries have life expectancy below 65 years\nThe middle 50% of life expectancies in Europe fall between 74 and 80 years\n\n\n\n11.14.2 Spread and Variation\nAnswer True or False:\n\nAsia shows the largest spread (IQR) in life expectancy\nEurope has the smallest IQR among all continents\nThe variation in Africa’s life expectancy is greater than in the Americas\nOceania shows the least variation in life expectancy\nThe range (excluding outliers) in Asia is approximately 20 years\n\n\n\n11.14.3 Outliers and Extremes\nAnswer True or False:\n\nAfrica has two countries with unusually low life expectancy\nThere are no outliers in Oceania’s distribution\nAsia has both high and low outliers",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#changes-over-time",
    "href": "chapter5.html#changes-over-time",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.15 Changes Over Time",
    "text": "11.15 Changes Over Time\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = \"dodge\", outlier.shape = 21,\n               outlier.alpha = 0.6) +\n  labs(title = \"Life Expectancy: 1957 vs 2007\",\n       subtitle = \"Comparing distribution changes over 50 years\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\",\n       fill = \"Year\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\n11.15.1 Time Comparison Questions\nAnswer True or False:\n\nThe median life expectancy increased in all continents between 1957 and 2007\nThe variation in life expectancy (IQR) decreased in most continents over time\nAfrica showed the smallest improvement in median life expectancy\nThe spread of life expectancies in Asia decreased substantially from 1957 to 2007\nOceania maintained the highest median life expectancy in both time periods\n\n\n\n11.15.2 Statistical Summary\n\n# Calculate summary statistics\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    n_outliers = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics by Continent and Year\")\n\n\nSummary Statistics by Continent and Year\n\n\ncontinent\nyear\nmedian\nq1\nq3\niqr\nn_outliers\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n0",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#key-learning-points",
    "href": "chapter5.html#key-learning-points",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.16 Key Learning Points",
    "text": "11.16 Key Learning Points\n\nDistribution Center:\n\nMedian shows the typical life expectancy\nChanges in median reflect overall improvements\n\nSpread and Variation:\n\nIQR (box height) indicates data dispersion\nWider boxes suggest more inequality in life expectancy\n\nOutliers and Extremes:\n\nOutliers often represent countries with unique circumstances\n\nTime Comparison:\n\nShows both absolute improvements and changes in variation\nHighlights persistent regional disparities\nReveals different rates of progress across continents",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "href": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.17 Appendix: Summary Tables for Data Types and Applicable Statistical Measures",
    "text": "11.17 Appendix: Summary Tables for Data Types and Applicable Statistical Measures\n\n11.17.1 Table 1: Pros and Cons of Various Statistical Measures\n\n11.17.1.1 Measures of Center\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nMean\n- Uses all data points- Allows for further statistical calculations- Ideal for normally distributed data\n- Sensitive to outliers- Not ideal for skewed distributions- Not meaningful for nominal data\nInterval, Ratio, some Discrete, Continuous\n\n\nMedian\n- Not affected by outliers- Good for skewed distributions- Can be used with ordinal data\n- Ignores the actual values of most data points- Less useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nMode\n- Can be used with any data type- Good for finding most common category\n- May not be unique (multimodal)- Not useful for many types of analyses- Ignores magnitude of differences between values\nAll types\n\n\n\n\n\n11.17.1.2 Measures of Variability\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nRange\n- Simple to calculate and understand- Gives quick idea of data spread\n- Very sensitive to outliers- Ignores all data between extremes- Not useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nInterquartile Range (IQR)\n- Not affected by outliers- Good for skewed distributions\n- Ignores 50% of the data- Less intuitive than range\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nVariance\n- Uses all data points- Basis for many statistical procedures\n- Sensitive to outliers- Units are squared (less intuitive)\nInterval, Ratio, some Discrete, Continuous\n\n\nStandard Deviation\n- Uses all data points- Same units as original data- Widely used and understood\n- Sensitive to outliers- Assumes roughly normal distribution for interpretation\nInterval, Ratio, some Discrete, Continuous\n\n\nCoefficient of Variation\n- Allows comparison between datasets with different units or means\n- Can be misleading when means are close to zero- Not meaningful for data with negative values\nRatio, some Interval\n\n\n\n\n\n11.17.1.3 Measures of Correlation/Association\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nPearson’s r\n- Measures linear relationship- Widely used and understood\n- Assumes normal distribution- Sensitive to outliers- Only captures linear relationships\nInterval, Ratio, Continuous\n\n\nSpearman’s rho\n- Can be used with ordinal data- Captures monotonic relationships- Less sensitive to outliers\n- Loses information by converting to ranks- May miss some types of relationships\nOrdinal, Interval, Ratio\n\n\nKendall’s tau\n- Can be used with ordinal data- More robust than Spearman’s for small samples- Has nice interpretation (probability of concordance)\n- Loses information by only considering order- Computationally more intensive\nOrdinal, Interval, Ratio\n\n\nChi-square\n- Can be used with nominal data- Tests independence of categorical variables\n- Requires large sample sizes- Sensitive to sample size- Doesn’t measure strength of association\nNominal, Ordinal\n\n\nCramér’s V\n- Can be used with nominal data- Provides measure of strength of association- Normalized to [0,1] range\n- Interpretation can be subjective- May overestimate association in small samples\nNominal, Ordinal\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓*\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\n11.17.2 Notes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html",
    "href": "rozdzial5.html",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "12.1 Introduction to Sigma Notation (Σ) | Wprowadzenie do Notacji Sigma (Σ)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#introduction-to-sigma-notation-σ-wprowadzenie-do-notacji-sigma-σ",
    "href": "rozdzial5.html#introduction-to-sigma-notation-σ-wprowadzenie-do-notacji-sigma-σ",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "What is Sigma? | Co to jest notacja sumacyjna Sigma? Sigma (Σ) is a mathematical operator that tells us to sum (add up) a sequence of terms - it functions as an instruction to perform addition of all elements in a specified range. | Sigma (Σ) to operator matematyczny, który nakazuje nam zsumować (dodać) sekwencję wyrazów - działa jak instrukcja wykonania dodawania wszystkich elementów w określonym zakresie.\nPurpose: | Cel: Provides a compact way to write sums of many similar terms using a single symbol, avoiding lengthy addition expressions. | Zapewnia zwięzły sposób zapisu sum wielu podobnych wyrazów za pomocą jednego symbolu, unikając długich wyrażeń dodawania.\n\n\n12.1.1 Basic Formula | Podstawowa formuła\n\nThe general form of a sigma notation is: | Ogólna forma notacji sigma to:\n\n\\sum_{i=a}^{b} f(i)\n\nIndex of Summation: | Indeks sumowania: i\nLower Limit: | Dolna granica: a\nUpper Limit: | Górna granica: b\nFunction: | Funkcja: f(i)\n\n\n\n12.1.2 Simple Example | Prosty przykład\n\nConsider you want to add the first five positive integers: | Załóżmy, że chcesz dodać pierwsze pięć dodatnich liczb całkowitych:\n\n\\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\n\nAdds the first five positive integers. | Dodaje pierwsze pięć dodatnich liczb całkowitych.\n\n\n\n12.1.3 Example with a Function | Przykład z funkcją\n\nSuppose you want to sum the squares of the first four positive integers: | Załóżmy, że chcesz zsumować kwadraty pierwszych czterech dodatnich liczb całkowitych:\n\n\\sum_{i=1}^{4} i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 30\n\nSum of the squares of the first four positive integers. | Suma kwadratów pierwszych czterech dodatnich liczb całkowitych.\n\n\n\n12.1.4 Practical Application in Statistics | Praktyczne zastosowanie w statystyce\n\nCalculating the Mean: | Obliczanie średniej:\n\nData Points: | Punkty danych: x_1, x_2, ..., x_n\nMean \\bar{x}: | Średnia \\bar{x}:\n\n\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\nExample: | Przykład: x_1, x_2, x_3, x_4 are 4, 8, 15, 16 | x_1, x_2, x_3, x_4 to 4, 8, 15, 16\n\n\\bar{x} = \\frac{43}{4} = 10.75\n\n\n12.1.5 Benefits of Using Sigma Notation | Korzyści z używania notacji Sigma\n\nClarity: | Jasność: Provides a clear and concise representation of various statistical formulas. | Zapewnia jasne i zwięzłe przedstawienie statystycznych formuł.\n\n\n\n\n\n\n\nOperatory Sumy (Σ) i Iloczynu (Π)\n\n\n\n\n12.1.5.1 Operator Sigma (Σ)\n\\sum to operator sumowania, który nakazuje nam dodać wyrazy:\n\\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n\ngdzie: - i to zmienna indeksowa - Dolna wartość pod Σ (tutaj i=1) to punkt początkowy - Górna wartość (tutaj n) to punkt końcowy\n\n\n12.1.5.2 Operator Pi (Π)\n\\prod to operator iloczynu, który nakazuje nam pomnożyć wyrazy:\n\\prod_{i=1}^{n} x_i = x_1 \\times x_2 \\times ... \\times x_n\ngdzie: - i to zmienna indeksowa - Dolna wartość pod Π (tutaj i=1) to punkt początkowy - Górna wartość (tutaj n) to punkt końcowy\n\n\n\n\n\n\n\n\n\nPrzykład Σ\n\n\n\n\\sum_{i=1}^{4} i = 1 + 2 + 3 + 4 = 10\n\n\n\n\n\n\n\n\nPrzykład Π\n\n\n\n\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\n\n\n\n\n\n\n\n\nKluczowe Różnice\n\n\n\n\nΣ oznacza wielokrotne dodawanie\nΠ oznacza wielokrotne mnożenie",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#typy-rozkładów-danych",
    "href": "rozdzial5.html#typy-rozkładów-danych",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.2 Typy rozkładów danych",
    "text": "12.2 Typy rozkładów danych\n\n\n\n\n\n\nImportant\n\n\n\nRozkład danych informuje o tym, jakie wartości przyjmuje zmienna i jak często.\n\n\nZrozumienie rozkładów danych jest kluczowe dla analizy i wizualizacji danych. W tym dokumencie przyjrzymy się różnym typom rozkładów i sposobom ich wizualizacji przy użyciu ggplot2 w R.\n\n12.2.1 Rozkład normalny\nRozkład normalny, znany również jako rozkład Gaussa, jest symetryczny i ma kształt dzwonu.\n\n# Generowanie danych o rozkładzie normalnym\ndane_normalne &lt;- data.frame(x = rnorm(1000))\n\n# Wykres\nggplot(dane_normalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład normalny\", x = \"Wartość\", y = \"Gęstość\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\n12.2.2 Rozkład jednostajny\nW rozkładzie jednostajnym wszystkie wartości mają równe prawdopodobieństwo wystąpienia.\n\n# Generowanie danych o rozkładzie jednostajnym\ndane_jednostajne &lt;- data.frame(x = runif(1000))\n\n# Wykres\nggplot(dane_jednostajne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład jednostajny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\n12.2.3 Rozkłady skośne\nRozkłady skośne są asymetryczne, z jednym ogonem dłuższym niż drugi.\n\n# Generowanie danych o rozkładzie prawoskośnym\ndane_prawoskosne &lt;- data.frame(x = rlnorm(1000))\n\n# Wykres\nggplot(dane_prawoskosne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład prawoskośny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\n12.2.4 Rozkład bimodalny\nRozkład bimodalny ma dwa szczyty (dwie dominanty), wskazujące na dwie odrębne podgrupy w danych.\n\n# Generowanie danych bimodalnych\ndane_bimodalne &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Wykres\nggplot(dane_bimodalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład bimodalny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nKey Properties\nSocial Examples\n\n\n\n\nNormal\nSymmetric, bell-shaped, most values near mean\nHeight, IQ scores, standardized test scores\n\n\nUniform\nEqual probability across range\nBirth dates in year, arrival times in hour\n\n\nBimodal\nTwo peaks, suggests subgroups\nAge in college towns, polarized opinions\n\n\nLog-normal\nRight-skewed, cannot be negative\nIncome, house prices, social media followers\n\n\nPower law\nExtreme skew, “rich get richer”\nCity sizes",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wizualizacja-rozkładów-danych-rzeczywistych",
    "href": "rozdzial5.html#wizualizacja-rozkładów-danych-rzeczywistych",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.3 Wizualizacja rozkładów danych rzeczywistych",
    "text": "12.3 Wizualizacja rozkładów danych rzeczywistych\nUżyjemy zbioru danych palmerpenguins do zbadania rozkładów danych rzeczywistych.\n\n12.3.1 Histogram i wykres gęstości\n\n\n\n\n\n\nUnderstanding Histograms and Density\n\n\n\n⭐ A histogram is a special graph for numerical data where:\n\nData is grouped into ranges (called “bins”)\nBars touch each other (unlike bar charts!) because the data is continuous\nEach bar’s height shows how many values fall into that range\n\nThink of density as showing how common or concentrated certain values are in your data:\n\nA higher point on a density curve (or taller bar in a histogram) means those values appear more frequently in your data\nA lower point means those values are less common\n\nJust like a crowded area has more people per space (higher density), a taller part of the graph shows values that appear more often in your dataset!\n\n\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład długości płetw pingwinów\", \n       x = \"Długość płetwy (mm)\", \n       y = \"Gęstość\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\n12.3.2 Wykres pudełkowy\nWykresy pudełkowe są przydatne do porównywania rozkładów między kategoriami.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Rozkład masy ciała pingwinów według gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa ciała (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n12.3.3 Wykres skrzypcowy\nWykresy skrzypcowe łączą cechy wykresu pudełkowego i wykresu gęstości.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Rozkład masy ciała pingwinów według gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa ciała (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n12.3.4 Wykres grzbietowy\nWykresy grzbietowe są przydatne do porównywania wielu rozkładów.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Rozkład długości płetw według gatunku pingwina\",\n       x = \"Długość płetwy (mm)\",\n       y = \"Gatunek\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n\n\n12.3.5 Podsumowanie\nZrozumienie i wizualizacja rozkładów danych są kluczowe w analizie danych. ggplot2 zapewnia elastyczny i potężny zestaw narzędzi do tworzenia różnych typów wykresów rozkładów. Badając różne techniki wizualizacji, możemy uzyskać wgląd w podstawowe wzorce i charakterystyki naszych danych.\n\n\n\n\n\n\nRodzaje i Formaty Zbiorów Danych\n\n\n\n\n12.3.6 Dane Przekrojowe\nObserwacje zebrane w jednym punkcie czasowym dla wielu podmiotów:\n\n\n\nOsoba\nWiek\nDochód\nWykształcenie\n\n\n\n\n1\n25\n5000\nLicencjat\n\n\n2\n35\n7500\nMagister\n\n\n3\n45\n9000\nDoktorat\n\n\n\n\n\n12.3.7 Szeregi Czasowe\nObserwacje jednego podmiotu w kolejnych punktach czasowych:\n\n\n\nRok\nPKB (w mld)\nStopa Bezrobocia\n\n\n\n\n2018\n20.580\n3,9%\n\n\n2019\n21.433\n3,7%\n\n\n2020\n20.933\n8,1%\n\n\n\n\n\n12.3.8 Dane Panelowe (Longitudinalne)\nObserwacje wielu podmiotów w czasie:\n\n\n\nKraj\nRok\nPKB per capita\nDługość życia\n\n\n\n\nPolska\n2018\n32.794\n76,7\n\n\nPolska\n2019\n35.118\n76,8\n\n\nNiemcy\n2018\n46.194\n81,9\n\n\nNiemcy\n2019\n46.194\n82,0\n\n\n\n\n\n12.3.9 Dane Przekrojowo-Czasowe (TSCS)\nSzczególny przypadek danych panelowych gdzie:\n\nLiczba punktów czasowych &gt; liczba podmiotów\nStruktura podobna do danych panelowych\nCzęsto stosowane w ekonomii i naukach politycznych\n\n\n\n12.3.10 Formaty Danych\n\n12.3.10.1 Format Szeroki\nKażdy wiersz to podmiot; kolumny to zmienne/punkty czasowe:\n\n\n\nKraj\nPKB_2018\nPKB_2019\nDŻ_2018\nDŻ_2019\n\n\n\n\nPolska\n32.794\n35.118\n76,7\n76,8\n\n\nNiemcy\n46.194\n46.194\n81,9\n82,0\n\n\n\n\n\n12.3.10.2 Format Długi\nKażdy wiersz to unikalna kombinacja podmiot-czas-zmienna:\n\n\n\nKraj\nRok\nZmienna\nWartość\n\n\n\n\nPolska\n2018\nPKB per capita\n32.794\n\n\nPolska\n2019\nPKB per capita\n35.118\n\n\nPolska\n2018\nDługość życia\n76,7\n\n\nPolska\n2019\nDługość życia\n76,8\n\n\nNiemcy\n2018\nPKB per capita\n46.194\n\n\nNiemcy\n2019\nPKB per capita\n46.194\n\n\nNiemcy\n2018\nDługość życia\n81,9\n\n\nNiemcy\n2019\nDługość życia\n82,0\n\n\n\nUwaga: Format długi jest zazwyczaj preferowany do:\n\nManipulacji danymi w R i Pythonie\nAnaliz statystycznych\nWizualizacji danych\nModelowania efektów mieszanych\nAnaliz powtarzanych pomiarów",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wartości-odstające-outliers",
    "href": "rozdzial5.html#wartości-odstające-outliers",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.4 Wartości Odstające (Outliers)",
    "text": "12.4 Wartości Odstające (Outliers)\nPrzed zagłębieniem się w konkretne miary, kluczowe jest zrozumienie pojęcia wartości odstających, ponieważ mogą one znacząco wpływać na wiele statystyk opisowych.\nWartości odstające to punkty danych, które znacznie różnią się od innych obserwacji w zbiorze danych. Mogą wystąpić z powodu:\n\nBłędów pomiaru lub zapisu\nPrawdziwych ekstremalnych wartości w populacji\nPróbkowania z innej populacji\n\nWartości odstające mogą mieć istotny wpływ na wiele miar statystycznych, szczególnie tych opartych na średnich lub sumach kwadratów odchyleń. Dlatego ważne jest, aby:\n\nIdentyfikować wartości odstające zarówno poprzez metody statystyczne, jak i wiedzę dziedzinową\nBadać przyczyny wartości odstających\nPodejmować świadome decyzje o tym, czy włączać je do analiz, czy nie\n\nW tym przewodniku omówimy, jak różne miary opisowe są dotknięte przez wartości odstające.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#symbole-stosowane-w-statystyce---podsumowanie",
    "href": "rozdzial5.html#symbole-stosowane-w-statystyce---podsumowanie",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.5 Symbole Stosowane w Statystyce - podsumowanie",
    "text": "12.5 Symbole Stosowane w Statystyce - podsumowanie\n\n\n\n\n\n\n\n\n\n\nMiara\nParametr Populacji\nStatystyka z Próby\nAlternatywne Oznaczenia\nUwagi\n\n\n\n\nLiczebność\nN\nn\n-\nCałkowita liczba obserwacji\n\n\nŚrednia\n\\mu\n\\bar{x}\nE(X), M\nE(X) stosowane w rachunku prawdopodobieństwa\n\n\nWariancja\n\\sigma^2\ns^2\n\\text{Var}(X), V(X)\nKwadrat odchyleń od średniej\n\n\nOdchylenie standardowe\n\\sigma\ns\n\\text{OS}, \\text{std}\nPierwiastek z wariancji\n\n\nFrakcja/Proporcja\n\\pi, P\n\\hat{p}\n\\text{fr}\nCzęstości względne\n\n\nWspółczynnik korelacji\n\\rho\nr\n\\text{kor}(x,y)\nWartości od -1 do +1\n\n\nBłąd standardowy\n\\sigma_{\\bar{x}}\ns_{\\bar{x}}\n\\text{BS}\nBłąd standardowy średniej\n\n\nSuma\n\\sum\n\\sum\n\\sum_{i=1}^n\nZ indeksowaniem\n\n\nPojedyncza obserwacja\nX_i\nx_i\n-\ni-ta obserwacja\n\n\nKowariancja\n\\sigma_{xy}\ns_{xy}\n\\text{Cov}(X,Y)\nWspólna zmienność\n\n\nMediana\n\\eta\n\\text{Me}\nM\nWartość środkowa\n\n\nRozstęp\nR\nr\n\\text{max}(X) - \\text{min}(X)\nMiara rozproszenia\n\n\nDominanta\n\\text{Mo}\n\\text{mo}\n\\text{mod}\nWartość najczęstsza\n\n\nSkośność\n\\gamma_1\ng_1\n\\text{SK}\nAsymetria rozkładu\n\n\nKurtoza\n\\gamma_2\ng_2\n\\text{KU}\nSpłaszczenie rozkładu\n\n\n\nDodatkowe ważne wzory:\n\nMomenty z próby: m_k = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^k\nMomenty populacji: \\mu_k = E[(X - \\mu)^k]\nBłąd standardowy dla populacji: \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\nBłąd standardowy z próby: s_{\\bar{x}} = \\frac{s}{\\sqrt{n}}",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-tendencji-centralnej",
    "href": "rozdzial5.html#miary-tendencji-centralnej",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.6 Miary Tendencji Centralnej",
    "text": "12.6 Miary Tendencji Centralnej\nMiary tendencji centralnej mają na celu identyfikację “typowej” lub “centralnej” wartości w zbiorze danych. Trzy podstawowe miary to średnia, mediana i moda.\n\n12.6.1 Średnia Arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez liczbę wartości.\nWzór: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\nWażna Właściwość: Średnia jest punktem równowagi w danych. Suma odchyleń od średniej zawsze wynosi zero:\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0\nTa właściwość sprawia, że średnia jest użyteczna w wielu obliczeniach statystycznych.\n\n\n\n\n\n\nZrozumienie średniej jako punktu równowagi 🎯\n\n\n\nRozważmy zbiór danych X = \\{1, 2, 6, 7, 9\\} na osi liczbowej, wyobrażając go sobie jako huśtawkę:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\nŚrednia (\\mu) działa jak idealny punkt równowagi tej huśtawki. Dla naszych danych:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\n12.6.2 Co się dzieje przy różnych punktach podparcia? 🤔\n\nPunkt podparcia w 6 (za wysoko):\n\nLewa strona: Wartości (1, 2) są poniżej\nPrawa strona: Wartości (7, 9) są powyżej\n\\sum odległości z lewej = (6-1) + (6-2) = 9\n\\sum odległości z prawej = (7-6) + (9-6) = 4\nHuśtawka przechyla się w lewo! ⬅️ bo 9 &gt; 4\n\nPunkt podparcia w 4 (za nisko):\n\nLewa strona: Wartości (1, 2) są poniżej\nPrawa strona: Wartości (6, 7, 9) są powyżej\n\\sum odległości z lewej = (4-1) + (4-2) = 5\n\\sum odległości z prawej = (6-4) + (7-4) + (9-4) = 10\nHuśtawka przechyla się w prawo! ➡️ bo 5 &lt; 10\n\nPunkt podparcia w średniej (5) (idealna równowaga):\n\n\\sum odległości poniżej = \\sum odległości powyżej\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ✨ Idealna równowaga!\n\n\nTo pokazuje, dlaczego średnia jest unikalnym punktem równowagi, gdzie:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nHuśtawka zawsze będzie się przechylać, chyba że punkt podparcia zostanie umieszczony dokładnie w średniej! 🎪\n\n\n\n\n\n\n\n\n\nŚrednia jako punkt równowagi\n\n\n\nTa wizualizacja pokazuje, jak średnia arytmetyczna (5) działa jako punkt równowagi pomiędzy skupionymi punktami z lewej strony a rozproszonymi punktami z prawej strony:\nLewa strona średniej:\n\nPunkty o wartościach 2 i 3\nBlisko siebie (różnica 1 jednostka)\nOdległości od średniej: 3 i 2 jednostki\nSuma “ciążenia” = 5 jednostek\n\nPrawa strona średniej:\n\nPunkty o wartościach 6 i 9\nBardziej oddalone (różnica 3 jednostki)\nOdległości od średniej: 1 i 4 jednostki\nSuma “ciążenia” = 5 jednostek\n\nKluczowe obserwacje:\n\nŚrednia (5) jest punktem równowagi, mimo że:\n\nPunkty po lewej są skupione (2,3)\nPunkty po prawej są rozproszone (6,9)\nZielone strzałki pokazują odległości od średniej\n\nRównowaga jest zachowana ponieważ:\n\nSuma odległości się równoważy: (5-2) + (5-3) = (6-5) + (9-5)\nCałkowita suma odległości = 5 jednostek po każdej stronie\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrzykład Ręcznego Obliczenia:\nObliczmy średnią dla zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nSumuj wszystkie wartości\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nPolicz liczbę wartości\nn = 7\n\n\n3\nPodziel sumę przez n\n36 / 7 = 5,14\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nŁatwa do obliczenia i zrozumienia\nWykorzystuje wszystkie punkty danych\nPrzydatna do dalszych obliczeń statystycznych\n\nWady:\n\nWrażliwa na wartości odstające\nNie idealna dla rozkładów skośnych\n\n\n\n12.6.3 Mediana\nMediana to środkowa wartość, gdy dane są uporządkowane.\nPrzykład Ręcznego Obliczenia:\nUżywając tego samego zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nWynik\n\n\n\n\n1\nUporządkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajdź środkową wartość\n5\n\n\n\nDla parzystej liczby wartości, weź średnią z dwóch środkowych wartości.\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmedian(dane)\n\n[1] 5\n\n\nZalety:\n\nNie jest zniekształcona przez skrajne wartości odstające (outliers)\nLepsza dla rozkładów skośnych\n\nWady:\n\nNie wykorzystuje wszystkich punktów danych\nMniej przydatna do dalszych obliczeń statystycznych\n\n\n\n\n\n\n\nWarning\n\n\n\nJak znaleźć pozycję mediany w zbiorze danych:\n\nNajpierw posortuj dane rosnąco\nGdy n jest nieparzyste:\n\nPozycja mediany = \\frac{n + 1}{2}\n\nGdy n jest parzyste:\n\nPierwsza pozycja mediany = \\frac{n}{2}\nDruga pozycja mediany = \\frac{n}{2} + 1\nMediana = \\frac{\\text{wartość na pozycji }\\frac{n}{2} + \\text{wartość na pozycji }(\\frac{n}{2}+1)}{2}\n\n\nPrzykłady:\n\nNieparzyste n=7: pozycja = \\frac{7+1}{2} = 4-ta wartość\nParzyste n=8: pozycje = \\frac{8}{2} = 4-ta i 4+1 = 5-ta wartość\n\n\n\n\n\n12.6.4 Moda (Dominanta)\nModa to najczęściej występująca wartość.\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nWartość\nCzęstość\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nModa to 4 i 5 (rozkład bimodalny).\nObliczenie w R:\n\nlibrary(modeest)\nmfv(dane)  # Najczęściej występująca wartość\n\n[1] 4 5\n\n\nZalety:\n\nJedyna miara tendencji centralnej dla danych nominalnych\nMoże identyfikować wiele punktów szczytowych (dominujących) w danych\n\nWady:\n\nNie zawsze jednoznacznie zdefiniowana\nNie przydatna dla danych ciągłych\n\n\n\n12.6.5 Średnia (arytmetyczna) Ważona (*)\nŚrednia ważona jest używana, gdy niektóre punkty danych są ważniejsze niż inne. Występują dwa typy średnich ważonych: z wagami nienormalizowanymi i z wagami znormalizowanymi.\n\n12.6.5.1 Średnia Ważona z Wagami Nienormalizowanymi\nJest to standardowa forma średniej ważonej, gdzie wagi mogą być dowolnymi liczbami dodatnimi reprezentującymi ważność każdego punktu danych.\nWzór: \\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\nPrzykład Obliczeń Ręcznych: Obliczmy średnią ważoną dla zbioru danych: 2, 4, 5, 7 z wagami 1, 2, 3, 1\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPomnóż każdą wartość przez jej wagę\n(2 * 1) + (4 * 2) + (5 * 3) + (7 * 1) = 2 + 8 + 15 + 7 = 32\n\n\n2\nZsumuj wagi\n1 + 2 + 3 + 1 = 7\n\n\n3\nPodziel wynik z kroku 1 przez wynik z kroku 2\n32 / 7 = 4.57\n\n\n\nObliczenia w R:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\n12.6.5.2 Średnia Ważona z Wagami Znormalizowanymi (Ułamki)\nW tym przypadku wagi są ułamkami sumującymi się do 1, reprezentującymi proporcję ważności dla każdego punktu danych.\nWzór: \\bar{x}_w = \\sum_{i=1}^n w_i x_i, gdzie \\sum_{i=1}^n w_i = 1\nPrzykład Obliczeń Ręcznych:\nObliczmy średnią ważoną dla zbioru danych: 2, 4, 5, 7 z wagami znormalizowanymi 0.1, 0.3, 0.4, 0.2\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPomnóż każdą wartość przez jej wagę\n(2 * 0.1) + (4 * 0.3) + (5 * 0.4) + (7 * 0.2)\n\n\n2\nZsumuj wyniki\n0.2 + 1.2 + 2.0 + 1.4 = 4.8\n\n\n\nObliczenia w R:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Uwaga: sumują się do 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nZalety Średnich Ważonych:\n\nUwzględniają różną ważność punktów danych\nPrzydatne w analizie ankiet o różnych wielkościach próby lub poziomach ważności\nMogą korygować nierówne prawdopodobieństwa w projektach próbkowania\n\nWady Średnich Ważonych:\n\nWymagają uzasadnienia dla wag\nMogą być niewłaściwie wykorzystane do manipulacji wynikami\nMogą być mniej intuicyjne w interpretacji niż prosta średnia arytmetyczna",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-zmienności-rozproszenia",
    "href": "rozdzial5.html#miary-zmienności-rozproszenia",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.7 Miary Zmienności (Rozproszenia)",
    "text": "12.7 Miary Zmienności (Rozproszenia)\nTe miary opisują, jak bardzo rozproszone są dane.\n\n\n\n\n\n\nZrozumienie Wariancji\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.1: Trzy wykresy punktowe pokazujące rosnącą wariancję przy stałej średniej\n\n\n\n\n\nPowyższe trzy wykresy punktowe pokazują, w jaki sposób wariancja mierzy rozproszenie danych wokół wartości centralnej:\n\nWszystkie rozkłady mają tę samą średnią (μ = 10), oznaczoną linią przerywaną\nMała Wariancja (σ² = 1): Punkty są skupione blisko średniej\nŚrednia Wariancja (σ² = 4): Punkty wykazują umiarkowane rozproszenie\nDuża Wariancja (σ² = 9): Punkty są szeroko rozproszone wokół średniej\n\n\n\n\n\n\n\n\n\nZrozumienie Różnych Poziomów Zmienności\n\n\n\n\n\n\n\n\n\n\n\n\nTa wizualizacja przedstawia trzy rozkłady normalne o tej samej średniej (μ = 10), ale różnych poziomach zmienności:\n\nMała zmienność (σ = 0.5)\n\nPunkty danych grupują się ściśle wokół średniej\nKrzywa gęstości jest wysoka i wąska\nWiększość obserwacji mieści się w przedziale ±0.5 jednostki (odchylenia stand.) od średniej\n\nŚrednia zmienność (σ = 2.0)\n\nPunkty danych są bardziej rozproszone wokół średniej\nKrzywa gęstości jest niższa i szersza\nWiększość obserwacji mieści się w przedziale ±2 jednostki od średniej\n\nDuża zmienność (σ = 4.0)\n\nPunkty danych są szeroko rozproszone wokół średniej\nKrzywa gęstości jest znacznie bardziej płaska i szeroka\nWiększość obserwacji mieści się w przedziale ±4 jednostki od średniej\n\n\nZwróć uwagę, jak odchylenie standardowe (σ) bezpośrednio wpływa na rozproszenie rozkładu - większe wartości σ wskazują na większą zmienność danych, podczas gdy mniejsze wartości oznaczają, że punkty danych mają tendencję do grupowania się bliżej średniej.\n\n\n\n12.7.1 Rozstęp\nRozstęp to różnica między wartością maksymalną a minimalną.\nWzór: R = x_{max} - x_{min}\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nZnajdź wartość maksymalną\n9\n\n\n2\nZnajdź wartość minimalną\n2\n\n\n3\nOdejmij minimum od maksimum\n9 - 2 = 7\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nrange(dane)\n\n[1] 2 9\n\nmax(dane) - min(dane)\n\n[1] 7\n\n\nZalety:\n\nProsty do obliczenia i zrozumienia\nSzybka informacja o ogólnym rozproszeniu danych\n\nWady:\n\nBardzo wrażliwy na wartości odstające\nNie dostarcza informacji o rozkładzie między skrajnościami\n\n\n\n12.7.2 Rozstęp Międzykwartylowy (IQR)\nIQR to różnica między 75. a 25. percentylem (3. a 1. kwartylem).\nWzór: IQR = Q_3 - Q_1\nAby znaleźć kwartyle ręcznie:\n\nDla nieparzystej liczby wartości:\n\nQ2 (mediana) to środkowa wartość\nQ1 to mediana dolnej połowy (wyłączając medianę dla wszystkich obserwacji)\nQ3 to mediana górnej połowy (wyłączając medianę dla wszystkich obserwacji)\n\nDla parzystej liczby wartości:\n\nQ2 to średnia z dwóch środkowych wartości\nQ1 to mediana dolnej połowy (wyłączając medianę dla wszystkich obserwacji)\nQ3 to mediana górnej połowy (wyłączając medianę dla wszystkich obserwacji)\n\n\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nUporządkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajdź Q2 (medianę)\n5\n\n\n3\nZnajdź Q1 (medianę dolnej połowy)\n4\n\n\n4\nZnajdź Q3 (medianę górnej połowy)\n7\n\n\n5\nOblicz IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nprint(dane)\n\n[1] 2 4 4 5 5 7 9\n\nquantile(dane, type = 1)\n\n  0%  25%  50%  75% 100% \n   2    4    5    7    9 \n\nIQR(dane, type = 1)\n\n[1] 3\n\n\nZalety:\n\nOdporny na wartości odstające\nDostarcza informacji o rozproszeniu środkowych 50% danych\n\nWady:\n\nIgnoruje ogony rozkładu\nMniej efektywny niż odchylenie standardowe dla rozkładów normalnych\n\n\n\n12.7.3 Wariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\n\n\n\n\n\nWariancja: Zrozumienie Średniego Odchylenia Kwadratowego\n\n\n\nCzym jest Wariancja? Wariancja mierzy, jak bardzo punkty danych są “rozrzucone” wokół średniej - jest średnią kwadratów odchyleń od średniej.\nWzór: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nProsty Przykład: Rozważmy liczby: 2, 4, 6, 8, 10 Średnia (\\bar{x}) = 6\n\n\n\n\n\n\n\n\n\nObliczanie Odchyleń:\n\n\n\n\n\n\n\n\n\n\n\n\nWartość\nOdchylenie od średniej\nKwadrat odchylenia\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n+2\n4\n\n\n10\n+4\n16\n\n\n\nWariancja = \\frac{16 + 4 + 0 + 4 + 16}{4} = 10\nKluczowe Punkty:\n\nŚrednia służy jako punkt odniesienia (niebieska przerywana linia)\nOdchylenia pokazują odległość od średniej (czerwone kropkowane linie)\nPodniesienie do kwadratu sprawia, że wszystkie odchylenia są dodatnie (niebieskie słupki)\nWiększe odchylenia mają większy wpływ na wariancję\n\n\n\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz średnią\n\\bar{x} = 5,14\n\n\n2\nOdejmij średnią od każdej obserwacji i podnieś wynik do kwadratu\n(2 - 5,14)^2 = 9,86\n\n\n\n\n(4 - 5,14)^2 = 1,30\n\n\n\n\n(4 - 5,14)^2 = 1,30\n\n\n\n\n(5 - 5,14)^2 = 0,02\n\n\n\n\n(5 - 5,14)^2 = 0,02\n\n\n\n\n(7 - 5,14)^2 = 3,46\n\n\n\n\n(9 - 5,14)^2 = 14,90\n\n\n3\nSumuj kwadraty różnic\n30,86\n\n\n4\nPodziel przez (n-1), czyli przez liczbę obserwacji - 1\n30,86 / 6 = 5,14\n\n\n\nObliczenie w R:\n\nvar(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nWykorzystuje wszystkie punkty danych\nPodstawa dla wielu testów statystycznych*\n\nWady:\n\nJednostki są podniesione do kwadratu, co utrudnia interpretację\nWrażliwa na wartości odstające\n\n\n\n\n\n\n\nPoprawka Bessela: Dlaczego Dzielimy przez (n-1), a nie po prostu przez n\n\n\n\nGdy obliczamy odchylenia od średniej, ich suma musi wynosić zero. To matematyczny fakt: \\sum(x_i - \\bar{x}) = 0\nPomyśl o tym Tak:\nJeśli masz 5 liczb i ich średnią:\n\nPo obliczeniu 4 odchyleń od średniej\n5-te odchylenie MUSI być takie, żeby suma była zero\nNie masz tak naprawdę 5 niezależnych odchyleń\nMasz tylko 4 prawdziwie “swobodne” odchylenia\n\nProsty Przykład:\nLiczby: 2, 4, 6, 8, 10\n\nŚrednia = 6\nOdchylenia: -4, -2, 0, +2, +4\nZauważ, że sumują się do zera\nJeśli znasz dowolne 4 odchylenia, 5-te jest z góry określone!\n\nDlatego Właśnie:\n\nPrzy obliczaniu wariancji: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nDzielimy przez (n-1), a nie n\nPonieważ tylko (n-1) odchyleń jest naprawdę niezależnych\nOstatnie jest określone przez pozostałe\n\nStopnie Swobody:\n\nn = liczba obserwacji\n1 = ograniczenie (odchylenia muszą sumować się do zera)\nn-1 = stopnie swobody = liczba prawdziwie niezależnych odchyleń\n\nKiedy Stosować:\n\nPrzy obliczaniu wariancji z próby\nPrzy obliczaniu odchylenia standardowego z próby\n\nKiedy NIE Stosować:\n\nW obliczeniach dla całej populacji (gdy mamy wszystkie dane)\nPrzy obliczaniu odchylenia od ustalonej, znanej wartości (nie obliczonej średniej)\n\nPamiętaj:\n\nTo nie jest tylko statystyczny trik\nOdchylenia od średniej muszą sumować się do zera\nTo ograniczenie kosztuje nas jeden stopień swobody\n\n\n\n\n\n12.7.4 Odchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji i mierzy przeciętne rozproszenie danych względem ich średniej arytmetycznej. W przeciwieństwie do wariancji, jest to miara mianowana i interpretowana w jednostkach bdanej zmiennej.\nWzór: s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz wariancję\ns^2 = 5,14 (z poprzedniego obliczenia)\n\n\n2\nWyciągnij pierwiastek kwadratowy\ns = \\sqrt{5,14} = 2,27\n\n\n\nObliczenie w R:\n\nsd(dane)\n\n[1] 2.267787\n\n\nZalety:\n\nW tych samych jednostkach co oryginalne dane\nSzeroko stosowane i zrozumiałe\n\nWady:\n\nNadal wrażliwe na wartości odstające\nZakłada, że dane są w przybliżeniu “normalnie” rozłożone\n\n\n\n12.7.5 Współczynnik zmienności (*)\nWspółczynnik zmienności to odchylenie standardowe podzielone przez średnią arytmetyczną, często wyrażany jako wartość procentowa.\nWzór: CV = \\frac{s}{\\bar{x}} \\times 100\\%\nPrzykład obliczeń ręcznych:\nDla zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nOblicz średnią arytmetyczną\n\\bar{x} = 5,14\n\n\n2\nOblicz odchylenie standardowe\ns = 2,27\n\n\n3\nPodziel s przez średnią i pomnóż przez 100\n(2,27 / 5,14) * 100 = 44,16\\%\n\n\n\nObliczenia w R:\n\n(sd(dane) / mean(dane)) * 100\n\n[1] 44.09586\n\n\nZalety:\n- Umożliwia porównanie zmienności między zbiorami danych o różnych jednostkach lub średnich\n- Przydatny w dziedzinach takich jak finanse do oceny ryzyka\nWady:\n- Nie ma znaczenia dla danych zawierających zarówno wartości dodatnie, jak i ujemne\n- Może być mylący, gdy średnia jest bliska zeru\n\n\n\n\n\n\nOgraniczenia Współczynnika Zmienności (CV)\n\n\n\nWspółczynnik zmienności, obliczany jako (σ/μ) × 100\\%, ma dwa istotne ograniczenia:\n\n12.7.5.1 Nie ma interpretacji dla danych zawierających wartości dodatnie i ujemne\n\nŚrednia może być bliska zeru ze względu na wzajemne znoszenie się wartości dodatnich i ujemnych\nPrzykład: Zbiór danych {-5, -3, 2, 6} ma średnią = 0\n\nCV = (odch. std. / 0) × 100%\nProwadzi to do dzielenia przez zero\nNawet gdy średnia nie jest dokładnie zero, CV nie reprezentuje prawdziwej względnej zmienności, gdy dane przechodzą przez zero\n\nCV zakłada naturalny punkt zerowy i sensowne proporcje między wartościami\n\n\n\n12.7.5.2 Mylący gdy średnia jest bliska zeru\n\nPonieważ CV = (σ/μ) × 100\\%, gdy μ zbliża się do zera:\n\nMianownik staje się bardzo mały\nSkutkuje to ekstremalnie dużymi wartościami CV\nTe duże wartości nie reprezentują sensownie względnej zmienności\n\nPrzykład:\n\nZbiór danych A: {0.001, 0.002, 0.003} ma średnią = 0.002\nNawet małe odchylenia standardowe dadzą bardzo duże CV\nWynikający z tego duży CV może sugerować ekstremalne zróżnicowanie, gdy w rzeczywistości dane są dość skoncentrowane\n\n\n\n\n12.7.5.3 Najlepsze zastosowania\nCV jest najbardziej użyteczny dla:\n\nDanych ściśle dodatnich\nDanych mierzonych na skali ilorazowej\nDanych ze średnią znacznie powyżej zera\nPorównywania zmienności między zbiorami danych o różnych jednostkach lub skalach",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-położenia-względnego-względnej-pozycji",
    "href": "rozdzial5.html#miary-położenia-względnego-względnej-pozycji",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.8 Miary Położenia Względnego (Względnej Pozycji)",
    "text": "12.8 Miary Położenia Względnego (Względnej Pozycji)\nZrozumienie relatywnej (względnej) pozycji wartości w zbiorze danych.\n\n12.8.1 Kwartyle (Q): Podstawy\nKwartyle to specjalne liczby, które dzielą uporządkowane dane na cztery równe części.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\n12.8.1.1 Czym są Kwartyle?\nPierwszy Kwartyl (Q1):\n\nOddziela najniższe 25% danych od reszty\nNazywany również 25-tym percentylem\nPrzykład: Jeśli Q1 = 50 w zbiorze wyników testu, 25% uczniów uzyskało wynik poniżej 50\n\nDrugi Kwartyl (Q2):\n\nMediana - dzieli dane na pół\nNazywany również 50-tym percentylem\nPrzykład: Jeśli Q2 = 70, połowa uczniów uzyskała wynik poniżej 70\n\nTrzeci Kwartyl (Q3):\n\nOddziela najwyższe 25% danych od reszty\nNazywany również 75-tym percentylem\nPrzykład: Jeśli Q3 = 85, 75% uczniów uzyskało wynik poniżej 85\n\n\n\n12.8.1.2 Jak Obliczać Kwartyle (Krok po Kroku) - Dwie Metody\nPrzeanalizujmy wyniki testów uczniów używając obu popularnych metod wyznaczania kwartyli:\nPrzykład 1: Przypadek Nieparzystej Liczby Wyników (11 wyników)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nKrok 1: Znajdź Q2 (medianę) - Tak samo dla obu metod\n\nPrzy n = 11 wartościach (nieparzyste)\nPozycja mediany = (n + 1)/2 = 6\nQ2 = 78\n\nKrok 2: Znajdź Q1\n\nMetoda Tukeya:\n\nSpójrz na dolną połowę: 60, 65, 70, 72, 75\nQ1 = mediana dolnej połowy = 70\n\nMetoda Interpolacji:\n\nPozycja = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3-cia wartość)\n\n\nKrok 3: Znajdź Q3\n\nMetoda Tukeya:\n\nSpójrz na górną połowę: 80, 82, 85, 88, 90\nQ3 = mediana górnej połowy = 85\n\nMetoda Interpolacji:\n\nPozycja = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9-ta wartość)\n\n\nPrzykład 2: Przypadek Parzystej Liczby (10 wyników)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nKrok 1: Znajdź Q2 (medianę) - Tak samo dla obu metod\n\nPrzy n = 10 wartościach (parzyste)\nPozycje mediany = 5 i 6\nQ2 = (75 + 78)/2 = 76.5\n\nKrok 2: Znajdź Q1\n\nMetoda Tukeya:\n\nSpójrz na dolną połowę: 60, 65, 70, 72, 75\nQ1 = mediana dolnej połowy = 70\n\nMetoda Interpolacji:\n\nPozycja = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nKrok 3: Znajdź Q3\n\nMetoda Tukeya:\n\nSpójrz na górną połowę: 78, 80, 82, 85, 90\nQ3 = mediana górnej połowy = 82\n\nMetoda Interpolacji:\n\nPozycja = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nWażne Uwagi:\n\nMetoda Tukeya:\n\nNajpierw znajdź medianę (Q2)\nPodziel dane na dolną i górną połowę\nZnajdź Q1 jako medianę dolnej połowy\nZnajdź Q3 jako medianę górnej połowy\nGdy n jest nieparzyste, mediana nie jest uwzględniana w żadnej połowie\n\nMetoda Interpolacji:\n\nUżywa pozycji (n+1)/4 dla Q1 i 3(n+1)/4 dla Q3\nGdy pozycja wypada między wartościami, stosuje interpolację liniową\nNie wymaga podziału danych na połowy\n\n\nObie metody dają te same wyniki dla prostych pozycji (Przykład 1), ale mogą się różnić, gdy potrzebna jest interpolacja (Przykład 2).\n\n\n\n\n\n\nManual Construction of Tukey Boxplot\n\n\n\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\n\n\n\n12.8.2 Percentyle: Bardziej Precyzyjna Miara Względnej Pozycji (*)\n\n12.8.2.1 Czym są Percentyle?\nPercentyle dają nam bardziej szczegółowy obraz, dzieląc dane na 100 równych części. W przeciwieństwie do kwartyli, percentyle używają interpolacji liniowej.\nKluczowe Punkty:\n\n25-ty percentyl równa się Q1\n50-ty percentyl równa się Q2 (mediana)\n75-ty percentyl równa się Q3\n\n\n\n12.8.2.2 Obliczanie Percentyli\nWzór: P_k = \\frac{k(n+1)}{100}\nGdzie:\n\nP_k to pozycja dla k-tego percentyla\nk to percentyl, który chcemy znaleźć (1-100)\nn to liczba obserwacji\n\nPrzykład 3: Znajdowanie 60-tego Percentyla Użyjmy wyników zadań domowych uczniów: 72, 75, 78, 80, 82, 85, 88, 90, 92, 95\nKrok 1: Oblicz pozycję\n\nn = 10 wyników\nDla 60-tego percentyla: P_{60} = \\frac{60(10+1)}{100} = 6.6\n\nKrok 2: Znajdź otaczające wartości\n\nPozycja 6: wynik 85\nPozycja 7: wynik 88\n\nKrok 3: Interpoluj (ważne: percentyle używają interpolacji liniowej)\n\nMusimy przejść 0.6 drogi między 85 a 88 P_{60} = 85 + 0.6(88-85) P_{60} = 85 + 0.6(3) P_{60} = 85 + 1.8 = 86.8\n\nCo to oznacza: 60% uczniów uzyskało wynik 86.8 lub niższy.\n\n\n\n12.8.3 Rangi Percentylowe (PR) (*)\n\n12.8.3.1 Czym jest Ranga Percentylowa?\nPodczas gdy percentyle mówią nam o wartości na określonej pozycji, ranga percentylowa mówi nam, jaki procent wartości znajduje się poniżej określonego wyniku. Można to traktować jako odpowiedź na pytanie “Jaki procent klasy uzyskał wynik niższy niż ja?”\nPR = \\frac{\\text{liczba wartości poniżej } + 0.5 \\times \\text{liczba równych wartości}}{\\text{całkowita liczba wartości}} \\times 100\nPrzykład 4: Znajdowanie Rangi Percentylowej Rozważmy te wyniki egzaminu:\n65, 70, 70, 75, 75, 75, 80, 85, 85, 90\nZnajdźmy PR dla wyniku 75.\nKrok 1: Dokładnie policz\n\nWartości poniżej 75: 65, 70, 70 (3 wartości)\nWartości równe 75: 75, 75, 75 (3 wartości)\nCałkowita liczba wartości: 10\n\nKrok 2: Zastosuj wzór\nPR = \\frac{3 + 0.5(3)}{10} \\times 100 PR = \\frac{3 + 1.5}{10} \\times 100 PR = \\frac{4.5}{10} \\times 100 = 45\\%\nInterpretacja: Wynik 75 jest wyższy niż 45% wyników w klasie.\nUwaga:\nP1: “Dlaczego używamy 0.5 dla równych wartości w PR?”\nO1: Jest tak, ponieważ zakładamy, że osoby z tym samym wynikiem są równomiernie rozłożone na tej pozycji. To jak powiedzenie, że dzielą pozycję po równo.\n\n\n\n12.8.4 Zadania z Rozwiązaniami\nZadanie 1: Podstawowe Kwartyle\nDane: 10, 12, 15, 15, 18, 20, 22, 25, 25 Znajdź: Q1, Q2, Q3\nRozwiązanie:\n\nQ2 (n = 9, nieparzyste)\n\nPozycja = (9 + 1)/2 = 5\nQ2 = 18\n\nQ1\n\nPozycja = (9 + 1)/4 = 2.5\nMiędzy 12 a 15\nQ1 = (12 + 15)/2 = 13.5\n\nQ3\n\nPozycja = 3(9 + 1)/4 = 7.5\nMiędzy 22 a 25\nQ3 = (22 + 25)/2 = 23.5\n\n\n\n\n\n\n\n\nPodwójna Rola Mediany\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.2: Wizualizacja podwójnej roli mediany\n\n\n\n\n\nMediana pełni dwie odrębne, ale powiązane ze sobą role:\nA. Jako Miara Centrum:\n\nReprezentuje środkowy punkt danych\nRównoważy liczbę obserwacji po obu stronach\nJest odporna na wartości odstające (w przeciwieństwie do średniej arytmetycznej)\n\nB. Jako Miara Pozycji Względnej:\n\nWyznacza 50-ty percentyl\nDzieli dane na dwie równe części\nKażdą wartość można do niej odnieść:\n\nPoniżej mediany: dolne 50%\nPowyżej mediany: górne 50%\n\n\nTa podwójna natura sprawia, że mediana jest szczególnie przydatna do:\n\nOpisywania wartości typowych (tendencja centralna)\nZrozumienia pozycji w rozkładzie (pozycja względna)\nDokonywania porównań między różnymi zbiorami danych",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-kształtu",
    "href": "rozdzial5.html#miary-kształtu",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.9 Miary Kształtu",
    "text": "12.9 Miary Kształtu\n\n12.9.1 Skośność\n\n12.9.1.1 Definicja\nSkośność kwantyfikuje asymetrię rozkładu danych. Wskazuje, czy dane grupują się bardziej po jednej stronie średniej niż po drugiej.\n\n\n12.9.1.2 Wyrażenie Matematyczne\nSK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3 gdzie:\n\nn to wielkość próby\nx_i to i-ta obserwacja\n\\bar{x} to średnia z próby\ns to odchylenie standardowe z próby\n\n\n\n12.9.1.3 Przykład: Analiza Frekwencji Wyborczej\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n# Generowanie przykładowych danych frekwencji w obwodach wyborczych\nset.seed(123)\ndane_frekwencji &lt;- c(\n  # Obwody miejskie\n  rnorm(300, mean = 65, sd = 12),\n  # Obwody podmiejskie\n  rnorm(400, mean = 70, sd = 10),\n  # Obwody wiejskie\n  rnorm(300, mean = 68, sd = 15)\n) |&gt; \n  # Zapewnienie, że frekwencja mieści się w przedziale 0-100%\n  pmax(0) |&gt; \n  pmin(100)\n\n# Obliczenie i wizualizacja\nwartosc_skosnosci &lt;- skewness(dane_frekwencji)\nwartosc_skosnosci\n\n[1] 0.02558143\n\nggplot(data.frame(x = dane_frekwencji), aes(x = x)) +\n  geom_histogram(bins = 50, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = mean(dane_frekwencji), color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = median(dane_frekwencji), color = \"blue\", linetype = \"dashed\") +\n  labs(\n    title = str_glue(\"Rozkład Frekwencji w Obwodach Wyborczych (Skośność = {round(wartosc_skosnosci, 4)})\"),\n    subtitle = \"Czerwona: Średnia, Niebieska: Mediana\",\n    x = \"Frekwencja Wyborcza (%)\",\n    y = \"Liczba Obwodów\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n12.9.1.4 Przewodnik Interpretacji\n\nSkośność Dodatnia (&gt; 0): Rozkład ma dłuższy ogon prawy\nSkośność Ujemna (&lt; 0): Rozkład ma dłuższy ogon lewy\nSkośność Zero: Rozkład w przybliżeniu symetryczny\n\n\n\n\n12.9.2 Kurtoza\n\n12.9.2.1 Definicja\nKurtoza mierzy “ogoniastość” rozkładu, wskazując na obecność wartości ekstremalnych w porównaniu z rozkładem normalnym.\n\n\n12.9.2.2 Wyrażenie Matematyczne\nK = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\n\n12.9.2.3 Przykład: Analiza Głosowań Parlamentarnych\n\n# Generowanie przykładowych wyników zgodności głosowań\nset.seed(456)\nzgodnosc_glosowan &lt;- c(\n  # Standardowe wzorce głosowania\n  rnorm(400, mean = 75, sd = 10),\n  # Przypadki współpracy międzypartyjnej\n  rnorm(80, mean = 50, sd = 15),\n  # Głosowania zgodne z linią partii\n  rnorm(20, mean = 95, sd = 5)\n) |&gt; \n  pmax(0) |&gt; \n  pmin(100)\n\nwartosc_kurtozy &lt;- kurtosis(zgodnosc_glosowan)\nwartosc_kurtozy\n\n[1] 3.849939\n\n# Wizualizacja z porównaniem do rozkładu normalnego\nzakres_x &lt;- seq(min(zgodnosc_glosowan)-1, max(zgodnosc_glosowan)+1, length.out = 100)\nrozklad_normalny &lt;- dnorm(zakres_x, mean = mean(zgodnosc_glosowan), sd = sd(zgodnosc_glosowan))\n\nggplot() +\n  geom_density(\n    data = data.frame(x = zgodnosc_glosowan), \n    aes(x = x), \n    fill = \"skyblue\", \n    alpha = 0.5\n  ) +\n  geom_line(\n    data = data.frame(x = zakres_x, y = rozklad_normalny),\n    aes(x = x, y = y),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) +\n  labs(\n    title = str_glue(\"Rozkład Zgodności Głosowań (Kurtoza = {round(wartosc_kurtozy, 4)})\"),\n    subtitle = \"Rozkład obserwowany (niebieski) vs. Rozkład normalny (czerwony)\",\n    x = \"Wskaźnik Zgodności Głosowań (%)\",\n    y = \"Gęstość\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n12.9.2.4 Przewodnik Interpretacji\n\nNadwyżka Kurtozy: Różnica względem kurtozy rozkładu normalnego\n\n\n\nLeptokurtyczny (&gt; 3): Więcej wartości ekstremalnych niż w rozkładzie normalnym\nPlatykurtyczny (&lt; 3): Mniej wartości ekstremalnych niż w rozkładzie normalnym\nMezokurtyczny (= 3): Podobny do rozkładu normalnego",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ćwiczenie-1.-porównanie-wynagrodzeń",
    "href": "rozdzial5.html#ćwiczenie-1.-porównanie-wynagrodzeń",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.10 Ćwiczenie 1. Porównanie wynagrodzeń",
    "text": "12.10 Ćwiczenie 1. Porównanie wynagrodzeń\n\n12.10.1 Dane\nMamy dane o wynagrodzeniach (w tysiącach euro) z dwóch małych firm europejskich:\n\n\n\nIndex\nFirma X\nFirma Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\n\n\n12.10.2 Miary tendencji centralnej\n\n12.10.2.1 Średnia arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez ich liczbę.\nWzór: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\n12.10.2.1.1 Obliczenia ręczne dla Firmy X\n\n\n\nWartość (x_i)\nCzęstość (f_i)\nx_i \\cdot f_i\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nSuma\nn = 20\nSuma = 119\n\n\n\n\\bar{x} = \\frac{119}{20} = 5,95\n\n\n12.10.2.1.2 Obliczenia ręczne dla Firmy Y\n\n\n\nWartość (x_i)\nCzęstość (f_i)\nx_i \\cdot f_i\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nSuma\nn = 20\nSuma = 100\n\n\n\n\\bar{y} = \\frac{100}{20} = 5\n\n\n12.10.2.1.3 Weryfikacja w R\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\n12.10.2.2 Mediana\nMediana to wartość środkowa w uporządkowanym zbiorze danych.\n\n12.10.2.2.1 Obliczenia ręczne dla Firmy X\nUporządkowane dane: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\nn = 20 (parzyste), więc bierzemy średnią z 10. i 11. wartości:\nMediana = \\frac{4 + 4}{2} = 4\n\n\n12.10.2.2.2 Obliczenia ręczne dla Firmy Y\nUporządkowane dane: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\nn = 20 (parzyste), więc bierzemy średnią z 10. i 11. wartości:\nMediana = \\frac{5 + 5}{2} = 5\n\n\n12.10.2.2.3 Weryfikacja w R\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\n12.10.2.3 Dominanta (moda)\nDominanta to najczęściej występująca wartość w zbiorze danych.\nDla Firmy X dominanta wynosi 3 (występuje 6 razy). Dla Firmy Y są dwie dominanty: 4 i 5 (obie występują 6 razy).\n\n# Funkcja do obliczania dominanty\nznajdz_dominante &lt;- function(x) {\n  unikalne_x &lt;- unique(x)\n  unikalne_x[which.max(tabulate(match(x, unikalne_x)))]\n}\n\nznajdz_dominante(X)\n\n[1] 3\n\nznajdz_dominante(Y)\n\n[1] 4\n\n\n\n\n\n12.10.3 Miary rozproszenia\n\n12.10.3.1 Wariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\n12.10.3.1.1 Obliczenia ręczne dla Firmy X\n\n\n\n\n\n\n\n\n\n\nx_i\nf_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\nf_i(x_i - \\bar{x})^2\n\n\n\n\n2\n3\n-3,95\n15,6025\n46,8075\n\n\n3\n6\n-2,95\n8,7025\n52,215\n\n\n4\n5\n-1,95\n3,8025\n19,0125\n\n\n5\n4\n-0,95\n0,9025\n3,61\n\n\n20\n1\n14,05\n197,4025\n197,4025\n\n\n35\n1\n29,05\n843,9025\n843,9025\n\n\nSuma\n20\n\n\n1162,95\n\n\n\ns^2 = \\frac{1162,95}{19} = 61,21\n\n\n12.10.3.1.2 Obliczenia ręczne dla Firmy Y\n\n\n\n\n\n\n\n\n\n\ny_i\nf_i\ny_i - \\bar{x}\n(y_i - \\bar{y})^2\nf_i(y_i - \\bar{y})^2\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nSuma\n20\n\n\n34\n\n\n\ns^2 = \\frac{34}{19} = 1,79\n\n\n12.10.3.1.3 Weryfikacja w R\n\nvar(X)\n\n[1] 61.20789\n\nvar(Y)\n\n[1] 1.789474\n\n\n\n\n\n12.10.3.2 Odchylenie standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji.\nWzór: s = \\sqrt{s^2}\n\nDla Firmy X: s = \\sqrt{61,21} = 7,82\nDla Firmy Y: s = \\sqrt{1,79} = 1,34\n\n\n12.10.3.2.1 Weryfikacja w R\n\nsd(X)\n\n[1] 7.823547\n\nsd(Y)\n\n[1] 1.337712\n\n\n\n\n\n\n12.10.4 Kwartyle\nKwartyle dzielą zbiór danych na cztery równe części.\n\n12.10.4.1 Obliczenia ręczne dla Firmy X\nUporządkowane dane: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 3\nQ2 (50. percentyl, mediana): 4\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 5\n\n\n\n12.10.4.2 Obliczenia ręczne dla Firmy Y\nUporządkowane dane: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 4\nQ2 (50. percentyl, mediana): 5\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 6\n\n\n\n12.10.4.3 Weryfikacja w R\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\n12.10.4.4 IQR\n\nIQR_x = 5 - 3 = 2\nIQR_y = 6 - 4 = 2\n\n\n\n\n12.10.5 Wykres pudełkowy Tukeya\nWykres pudełkowy Tukeya wizualnie przedstawia rozkład danych na podstawie kwartyli. Użyjemy biblioteki ggplot2 do stworzenia wykresu.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Przygotowanie danych\ndane &lt;- data.frame(\n  Firma = rep(c(\"X\", \"Y\"), each = 20),\n  Wynagrodzenie = c(X, Y)\n)\n\n# Tworzenie wykresu pudełkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot() +\n  labs(title = \"Rozkład wynagrodzeń w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiące euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n# Tworzenie wykresu pudełkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot(outliers = F) +\n  labs(title = \"Rozkład wynagrodzeń w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiące euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\n12.10.5.1 Interpretacja wykresu pudełkowego\n\nPudełko reprezentuje rozstęp międzykwartylowy (IQR) od Q1 do Q3.\nLinia wewnątrz pudełka to mediana (Q2).\nWąsy rozciągają się do najmniejszych i największych wartości w granicach 1,5 * IQR.\nPunkty poza wąsami są uznawane za wartości odstające.\n\n\n\n\n12.10.6 Porównanie wyników\n\n\n\nMiara\nFirma X\nFirma Y\n\n\n\n\nŚrednia\n5,95\n5,00\n\n\nMediana\n4\n5\n\n\nDominanta\n3\n4 i 5\n\n\nWariancja\n61,21\n1,79\n\n\nOdchylenie standard.\n7,82\n1,34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\n12.10.6.1 Kluczowe obserwacje:\n\nTendencja centralna: Firma X ma wyższą średnią, ale niższą medianę niż Firma Y, co wskazuje na prawostronnie skośny rozkład dla Firmy X.\n\nRozproszenie: Firma X wykazuje znacznie wyższą wariancję i odchylenie standardowe, sugerując większe dysproporcje w wynagrodzeniach.\nKształt rozkładu: Wynagrodzenia w Firmie Y są bardziej skupione, podczas gdy Firma X ma wartości ekstremalne (potencjalne wartości odstające), które znacząco wpływają na jej średnią i wariancję.\nKwartyle: Rozstęp międzykwartylowy (Q3 - Q1) Firmy Y jest nieznacznie większy, ale jej ogólny zakres jest znacznie mniejszy niż Firmy X.\n\n\n\n\n12.10.7 Wnioski\nTa analiza porównawcza ujawnia znaczące różnice w strukturach wynagrodzeń między dwiema firmami. Firma X wykazuje większą zmienność i potencjalną nierówność w swojej skali płac, podczas gdy Firma Y demonstruje bardziej spójny i wąsko rozłożony zakres wynagrodzeń.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ćwiczenie-2.-porównanie-zmienności-wielkości-okręgów-wyborczych-między-krajami",
    "href": "rozdzial5.html#ćwiczenie-2.-porównanie-zmienności-wielkości-okręgów-wyborczych-między-krajami",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.11 Ćwiczenie 2. Porównanie Zmienności Wielkości Okręgów Wyborczych Między Krajami",
    "text": "12.11 Ćwiczenie 2. Porównanie Zmienności Wielkości Okręgów Wyborczych Między Krajami\n\n12.11.1 Dane\nMamy dane o wielkości okręgów wyborczych z dwóch krajów:\n\nx &lt;- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Kraj wysoka zmienność\ny &lt;- c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13)  # Kraj niska zmienność\n\nkable(data.frame(\n  \"Kraj X (Wysoka zm.)\" = x,\n  \"Kraj Y (Niska zm.)\" = y\n))\n\n\n\n\nKraj.X..Wysoka.zm..\nKraj.Y..Niska.zm..\n\n\n\n\n1\n8\n\n\n3\n9\n\n\n5\n9\n\n\n7\n10\n\n\n9\n10\n\n\n11\n11\n\n\n13\n11\n\n\n15\n12\n\n\n17\n12\n\n\n19\n13\n\n\n\n\n\n\n\n12.11.2 Miary Tendencji Centralnej\n\n12.11.2.1 Średnia Arytmetyczna\nWzór: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\n12.11.2.1.1 Obliczenia dla Kraju X\n\n\n\nElement\nWartość\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n5\n\n\n4\n7\n\n\n5\n9\n\n\n6\n11\n\n\n7\n13\n\n\n8\n15\n\n\n9\n17\n\n\n10\n19\n\n\nSuma\n100\n\n\n\n\\bar{x} = \\frac{100}{10} = 10\n\nmean_x &lt;- mean(x)\nc(\"Ręcznie\" = 10, \"R\" = mean_x)\n\nRęcznie       R \n     10      10 \n\n\n\n\n12.11.2.1.2 Obliczenia dla Kraju Y\n\n\n\nElement\nWartość\n\n\n\n\n1\n8\n\n\n2\n9\n\n\n3\n9\n\n\n4\n10\n\n\n5\n10\n\n\n6\n11\n\n\n7\n11\n\n\n8\n12\n\n\n9\n12\n\n\n10\n13\n\n\nSuma\n105\n\n\n\n\\bar{y} = \\frac{105}{10} = 10,5\n\nmean_y &lt;- mean(y)\nc(\"Ręcznie\" = 10.5, \"R\" = mean_y)\n\nRęcznie       R \n   10.5    10.5 \n\n\n\n\n\n12.11.2.2 Mediana\nMediana to wartość środkowa w uporządkowanym zbiorze danych.\n\n12.11.2.2.1 Obliczenia dla Kraju X\nUporządkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nDla n = 10 (parzysta liczba obserwacji): Pozycje środkowe: 5 i 6 Wartości środkowe: 9 i 11\nMediana = \\frac{9 + 11}{2} = 10\n\nmedian_x &lt;- median(x)\nc(\"Ręcznie\" = 10, \"R\" = median_x)\n\nRęcznie       R \n     10      10 \n\n\n\n\n12.11.2.2.2 Obliczenia dla Kraju Y\nUporządkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nDla n = 10 (parzysta liczba obserwacji): Pozycje środkowe: 5 i 6 Wartości środkowe: 10 i 11\nMediana = \\frac{10 + 11}{2} = 10,5\n\nmedian_y &lt;- median(y)\nc(\"Ręcznie\" = 10.5, \"R\" = median_y)\n\nRęcznie       R \n   10.5    10.5 \n\n\n\n\n\n12.11.2.3 Dominanta\n\n12.11.2.3.1 Obliczenia dla Kraju X\n\n\n\nWartość\nCzęstość\n\n\n\n\n1\n1\n\n\n3\n1\n\n\n5\n1\n\n\n7\n1\n\n\n9\n1\n\n\n11\n1\n\n\n13\n1\n\n\n15\n1\n\n\n17\n1\n\n\n19\n1\n\n\n\nWniosek: Brak dominanty (wszystkie wartości występują jednokrotnie)\n\n\n12.11.2.3.2 Obliczenia dla Kraju Y\n\n\n\nWartość\nCzęstość\n\n\n\n\n8\n1\n\n\n9\n2\n\n\n10\n2\n\n\n11\n2\n\n\n12\n2\n\n\n13\n1\n\n\n\nWniosek: Cztery dominanty: 9, 10, 11, 12 (każda występuje dwukrotnie)\n\n# Tabele częstości\ntable_x &lt;- table(x)\ntable_y &lt;- table(y)\n\nlist(\n  \"Kraj X\" = table_x,\n  \"Kraj Y\" = table_y\n)\n\n$`Kraj X`\nx\n 1  3  5  7  9 11 13 15 17 19 \n 1  1  1  1  1  1  1  1  1  1 \n\n$`Kraj Y`\ny\n 8  9 10 11 12 13 \n 1  2  2  2  2  1 \n\n\n\n\n\n12.11.2.4 Wariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\n12.11.2.4.1 Obliczenia dla Kraju X\n\n\n\nx_i\n(x_i - \\bar{x})\n(x_i - \\bar{x})^2\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSuma\n\n330\n\n\n\ns^2_X = \\frac{330}{9} = 36,67\n\nvar_x &lt;- var(x)\nc(\"Ręcznie\" = 36.67, \"R\" = var_x)\n\nRęcznie       R \n  36.67   36.67 \n\n\n\n\n12.11.2.4.2 Obliczenia dla Kraju Y\n\n\n\nx_i\n(y_i - \\bar{y})\n(y_i - \\bar{y})^2\n\n\n\n\n8\n-2,5\n6,25\n\n\n9\n-1,5\n2,25\n\n\n9\n-1,5\n2,25\n\n\n10\n-0,5\n0,25\n\n\n10\n-0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n12\n1,5\n2,25\n\n\n12\n1,5\n2,25\n\n\n13\n2,5\n6,25\n\n\nSuma\n\n22,5\n\n\n\ns^2_Y = \\frac{22,5}{9} = 2,5\n\nvar_y &lt;- var(y)\nc(\"Ręcznie\" = 2.5, \"R\" = var_y)\n\nRęcznie       R \n    2.5     2.5 \n\n\n\n\n\n12.11.2.5 Odchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji. Jest miarą zmienności wyrażoną w tych samych jednostkach co dane.\nWzór: s = \\sqrt{s^2}\n\n12.11.2.5.1 Obliczenia dla Kraju X\nWykorzystujemy wcześniej obliczoną wariancję: s^2_X = 36,67\nObliczamy pierwiastek: s_X = \\sqrt{36,67} \\approx 6,06\n\n\n\nKrok\nObliczenie\nWynik\n\n\n\n\n1. Wariancja\ns^2_X\n36,67\n\n\n2. Pierwiastek\n\\sqrt{36,67}\n6,06\n\n\n\n\nsd_x &lt;- sd(x)\nc(\"Ręcznie\" = 6.06, \"R\" = sd_x)\n\nRęcznie       R \n  6.060   6.055 \n\n\n\n\n12.11.2.5.2 Obliczenia dla Kraju Y\nWykorzystujemy wcześniej obliczoną wariancję: s^2_Y = 2,5\nObliczamy pierwiastek: s_Y = \\sqrt{2,5} \\approx 1,58\n\n\n\nKrok\nObliczenie\nWynik\n\n\n\n\n1. Wariancja\ns^2_Y\n2,5\n\n\n2. Pierwiastek\n\\sqrt{2,5}\n1,58\n\n\n\n\nsd_y &lt;- sd(y)\nc(\"Ręcznie\" = 1.58, \"R\" = sd_y)\n\nRęcznie       R \n  1.580   1.581 \n\n\nInterpretacja:\n\nKraj X: Przeciętne odchylenie wielkości okręgu od średniej wynosi około 6 mandatów\nKraj Y: Przeciętne odchylenie wielkości okręgu od średniej wynosi około 1,6 mandatu\n\n\n\n\n\n12.11.3 Współczynnik Zmienności (CV)\nWspółczynnik zmienności to stosunek odchylenia standardowego do średniej, wyrażony w procentach.\nWzór: CV = \\frac{s}{\\bar{x}} \\times 100\\%\n\n12.11.3.1 Obliczenia dla Kraju X\nCV_X = \\frac{6,06}{10} \\times 100\\% = 60,6\\%\n\n\n\nSkładowa\nWartość\n\n\n\n\nOdchylenie standardowe (s)\n6,06\n\n\nŚrednia (\\bar{x})\n10\n\n\nCV\n60,6%\n\n\n\n\ncv_x &lt;- sd(x) / mean(x) * 100\nc(\"Ręcznie\" = 60.6, \"R\" = cv_x)\n\nRęcznie       R \n  60.60   60.55 \n\n\n\n\n12.11.3.2 Obliczenia dla Kraju Y\nCV_Y = \\frac{1,58}{10,5} \\times 100\\% = 15,0\\%\n\n\n\nSkładowa\nWartość\n\n\n\n\nOdchylenie standardowe (s)\n1,58\n\n\nŚrednia (\\bar{x})\n10,5\n\n\nCV\n15,0%\n\n\n\n\ncv_y &lt;- sd(y) / mean(y) * 100\nc(\"Ręcznie\" = 15.0, \"R\" = cv_y)\n\nRęcznie       R \n  15.00   15.06 \n\n\n\n\n\n12.11.4 Kwartyle i Rozstęp Międzykwartylowy (IQR)\n\n12.11.4.1 Metody obliczania kwartyli\nIstnieją różne metody obliczania kwartyli. W naszych obliczeniach ręcznych zastosujemy metodę wyłączającą medianę:\n\nDzielimy szereg na dwie części względem mediany\nMediana nie jest uwzględniana w obliczeniach kwartyli\nDla każdej części obliczamy jej medianę - będzie to odpowiednio Q1 i Q3\n\n\n\n12.11.4.2 Obliczenia dla Kraju X\nUporządkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMediana = 10 (nie uwzględniamy w obliczeniach kwartyli)\nDolna połowa: 1, 3, 5, 7, 9 Q1 = mediana dolnej połowy = 5\nGórna połowa: 11, 13, 15, 17, 19 Q3 = mediana górnej połowy = 15\nIQR = Q3 - Q1 = 15 - 5 = 10\n\n\n12.11.4.3 Obliczenia dla Kraju Y\nUporządkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMediana = 10.5 (nie uwzględniamy w obliczeniach kwartyli)\nDolna połowa: 8, 9, 9, 10, 10 Q1 = mediana dolnej połowy = 9\nGórna połowa: 11, 11, 12, 12, 13 Q3 = mediana górnej połowy = 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n# Porównanie różnych metod obliczania kwartyli w R\nmethods_comparison &lt;- data.frame(\n  Metoda = c(\"Ręcznie (bez mediany)\", \n             \"R type=1\", \"R type=2\", \"R type=7 (domyślna)\"),\n  \"Q1 Kraj X\" = c(5, \n                  quantile(x, 0.25, type=1),\n                  quantile(x, 0.25, type=2),\n                  quantile(x, 0.25, type=7)),\n  \"Q3 Kraj X\" = c(15,\n                  quantile(x, 0.75, type=1),\n                  quantile(x, 0.75, type=2),\n                  quantile(x, 0.75, type=7)),\n  \"Q1 Kraj Y\" = c(9,\n                  quantile(y, 0.25, type=1),\n                  quantile(y, 0.25, type=2),\n                  quantile(y, 0.25, type=7)),\n  \"Q3 Kraj Y\" = c(12,\n                  quantile(y, 0.75, type=1),\n                  quantile(y, 0.75, type=2),\n                  quantile(y, 0.75, type=7))\n)\n\nkable(methods_comparison, digits = 2,\n      caption = \"Porównanie różnych metod obliczania kwartyli\")\n\n\nPorównanie różnych metod obliczania kwartyli\n\n\nMetoda\nQ1.Kraj.X\nQ3.Kraj.X\nQ1.Kraj.Y\nQ3.Kraj.Y\n\n\n\n\nRęcznie (bez mediany)\n5.0\n15.0\n9.00\n12.00\n\n\nR type=1\n5.0\n15.0\n9.00\n12.00\n\n\nR type=2\n5.0\n15.0\n9.00\n12.00\n\n\nR type=7 (domyślna)\n5.5\n14.5\n9.25\n11.75\n\n\n\n\n\n\n\n12.11.4.4 Wyjaśnienie różnic w metodach obliczania kwartyli\n\nMetoda ręczna (bez mediany):\n\nDzieli dane na dwie części\nNie uwzględnia mediany\nZnajduje medianę każdej części\n\nR type=1:\n\nMetoda pierwsza w R\nUżywa pozycji całkowitych\nNie interpoluje\n\nR type=2:\n\nMetoda druga w R\nUżywa pozycji całkowitych\nInterpoluje gdy pozycja nie jest całkowita\n\nR type=7 (domyślna):\n\nMetoda domyślna w R\nUżywa quantile()[5] z SAS\nInterpoluje według metody opisanej przez Hyndmana i Fana\n\n\n\n\n\n12.11.5 Porównanie Wyników\n\nsummary_df &lt;- data.frame(\n  Miara = c(\"Średnia\", \"Mediana\", \"Dominanta\", \"Rozstęp\", \"Wariancja\", \n            \"Odch. Stand.\", \"Q1\", \"Q3\", \"IQR\", \"CV (%)\"),\n  \"Kraj X\" = c(10, 10, \"brak\", 18, 36.67, 6.06, 5, 15, 10, 60.6),\n  \"Kraj Y\" = c(10.5, 10.5, \"9,10,11,12\", 5, 2.5, 1.58, 9, 12, 3, 15.0)\n)\n\nkable(summary_df, \n      caption = \"Zestawienie wszystkich miar statystycznych\",\n      align = c('l', 'r', 'r'))\n\n\nZestawienie wszystkich miar statystycznych\n\n\nMiara\nKraj.X\nKraj.Y\n\n\n\n\nŚrednia\n10\n10.5\n\n\nMediana\n10\n10.5\n\n\nDominanta\nbrak\n9,10,11,12\n\n\nRozstęp\n18\n5\n\n\nWariancja\n36.67\n2.5\n\n\nOdch. Stand.\n6.06\n1.58\n\n\nQ1\n5\n9\n\n\nQ3\n15\n12\n\n\nIQR\n10\n3\n\n\nCV (%)\n60.6\n15\n\n\n\n\n\n\n\n12.11.6 Porównanie za pomocą Wykresu Pudełkowego\n\ndf_long &lt;- data.frame(\n  kraj = rep(c(\"X\", \"Y\"), each = 10),\n  wielkosc = c(x, y)\n)\n\n# Wykres podstawowy\np &lt;- ggplot(df_long, aes(x = kraj, y = wielkosc, fill = kraj)) +\n  geom_boxplot(outlier.shape = NA) +  # Wyłączamy domyślne punkty odstające\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Dodajemy punkty z przezroczystością\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\")) +\n  labs(\n    title = \"Porównanie Zmienności Wielkości Okręgów Wyborczych\",\n    subtitle = paste(\"CV: Kraj X =\", round(cv_x, 1), \"%, Kraj Y =\", round(cv_y, 1), \"%\"),\n    x = \"Kraj\",\n    y = \"Wielkość Okręgu\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Dodajemy adnotacje z kwartylami\np + annotate(\n  \"text\", \n  x = c(1, 1, 1, 2, 2, 2), \n  y = c(max(x)+1, mean(x), min(x)-1, max(y)+1, mean(y), min(y)-1),\n  label = c(\n    paste(\"Q3 =\", quantile(x, 0.75, type=1)),\n    paste(\"M =\", median(x)),\n    paste(\"Q1 =\", quantile(x, 0.25, type=1)),\n    paste(\"Q3 =\", quantile(y, 0.75, type=1)),\n    paste(\"M =\", median(y)),\n    paste(\"Q1 =\", quantile(y, 0.25, type=1))\n  ),\n  size = 3\n)\n\n\n\n\n\n\n\n\n\n\n12.11.7 Uwagi Metodologiczne\n\nObliczenia kwartyli:\n\nZastosowana metoda wyłączająca medianę może dawać inne wyniki niż domyślne funkcje R\nRóżnice w metodach obliczeniowych nie wpływają na ogólne wnioski\nWarto zawsze zaznaczyć stosowaną metodę w raportach\n\nWizualizacja:\n\nWykres pudełkowy skutecznie pokazuje różnice w rozkładach\nDodatkowe punkty pokazują rzeczywiste wartości\nAdnotacje ułatwiają interpretację\n\n\n\n\n12.11.8 Podsumowanie\n\n12.11.8.1 Porównanie Miar Statystycznych\n\n\n\nMiara\nKraj X\nKraj Y\nRóżnica względna\n\n\n\n\nŚrednia\n10,0\n10,5\nPodobna\n\n\nMediana\n10,0\n10,5\nPodobna\n\n\nDominanta\nBrak\nWielokrotna (9,10,11,12)\n-\n\n\nRozstęp\n18\n5\n3,6× większy w X\n\n\nWariancja\n36,67\n2,5\n14,7× większa w X\n\n\nIQR\n10\n3\n3,3× większy w X\n\n\nCV\n60,6%\n15,0%\n4,0× większy w X\n\n\n\n\n\n12.11.8.2 Charakterystyka Rozkładów\nKraj X:\n\nRozkład równomierny\nBrak dominującej wielkości okręgu (brak dominanty)\nSzeroki zakres: od 1 do 19 mandatów\nWysoka zmienność (CV = 60,6%)\nRównomierne rozłożenie wartości w zakresie\n\nKraj Y:\n\nRozkład skupiony\nWiele typowych wielkości (cztery dominanty)\nWąski zakres: od 8 do 13 mandatów\nNiska zmienność (CV = 15,0%)\nWartości skoncentrowane wokół średniej\n\n\n\n12.11.8.3 Interpretacja Wykresu Pudełkowego\nWizualizacja w formie wykresu pudełkowego pokazuje:\nElementy Struktury:\n\nPudełko: Pokazuje rozstęp międzykwartylowy (IQR)\nDolna krawędź: Pierwszy kwartyl (Q1)\nGórna krawędź: Trzeci kwartyl (Q3)\nLinia wewnętrzna: Mediana (Q2)\nWąsy: Rozciągają się do ±1,5 IQR - Punkty: Pojedyncze wielkości okręgów\n\nGłówne Wnioski Wizualne:\n\nRozmiar Pudełka:\n\n\nKraj X: Duże pudełko wskazuje na szeroki rozrzut środkowych 50%\nKraj Y: Małe pudełko pokazuje skupienie wartości środkowych\n\n\nDługość Wąsów:\n\nKraj X: Długie wąsy wskazują na szeroki rozkład całkowity\nKraj Y: Krótkie wąsy pokazują ograniczony rozrzut\n\nRozkład Punktów:\n\nKraj X: Punkty szeroko rozproszone\nKraj Y: Punkty gęsto skupione\n\n\n\n\n12.11.8.4 Kluczowe Obserwacje\n\nTendencja Centralna:\n\nPodobne średnie wielkości okręgów\nRóżne wzorce rozkładu\nOdmienne podejścia do standaryzacji\n\nMiary Zmienności:\n\nWszystkie miary pokazują 3-15 razy większą zmienność w Kraju X\nSpójny wzorzec w różnych miarach statystycznych\nSystematyczna różnica w projekcie okręgów\n\nProjekt Systemu:\n\nKraj X: Elastyczne, zróżnicowane podejście\nKraj Y: Ustandaryzowane, jednolite podejście\nRóżne filozoficzne podejścia do reprezentacji\n\nImplikacje Reprezentatywności:\n\nKraj X: Zmienna proporcja wyborców do przedstawicieli\nKraj Y: Bardziej spójne poziomy reprezentacji\nRóżne podejścia do reprezentacji demokratycznej\n\n\nAnaliza ta pokazuje fundamentalne różnice w projektowaniu systemów wyborczych między dwoma krajami, gdzie Kraj X przyjmuje bardziej zróżnicowane podejście, a Kraj Y utrzymuje większą jednolitość w wielkości okręgów wyborczych.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#zrozumienie-wykresów-pudełkowych-na-przykładzie-danych-o-długości-życia",
    "href": "rozdzial5.html#zrozumienie-wykresów-pudełkowych-na-przykładzie-danych-o-długości-życia",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.12 Zrozumienie Wykresów Pudełkowych na Przykładzie Danych o Długości Życia",
    "text": "12.12 Zrozumienie Wykresów Pudełkowych na Przykładzie Danych o Długości Życia\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Przygotowanie danych\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wprowadzenie-do-wykresów-pudełkowych",
    "href": "rozdzial5.html#wprowadzenie-do-wykresów-pudełkowych",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.13 Wprowadzenie do Wykresów Pudełkowych",
    "text": "12.13 Wprowadzenie do Wykresów Pudełkowych\nWykres pudełkowy (ang. box-and-whisker plot) przedstawia pięć kluczowych statystyk opisowych danych:\n\nMediana: Środkowa linia w pudełku (50. percentyl)\nPierwszy kwartyl (Q1): Dolna krawędź pudełka (25. percentyl)\nTrzeci kwartyl (Q3): Górna krawędź pudełka (75. percentyl)\nRozstęp międzykwartylowy (IQR): Wysokość pudełka (Q3 - Q1)\nWąsy: Rozciągają się do najbardziej skrajnych wartości niebędących obserwacjami odstającymi (metoda Tukeya: 1.5 × IQR)\nObserwacje odstające: Pojedyncze punkty poza wąsami\n\n\n12.13.1 Wizualizacja Długości Życia\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 24, \n               outlier.fill = \"red\", outlier.alpha = 0.6, outlier.size = 4) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"darkblue\") +\n  labs(title = \"Długość Życia według Kontynentów (2007)\",\n       subtitle = \"Pojedyncze punkty pokazują surowe dane; czerwone punkty oznaczają wartości odstające\",\n       x = \"Kontynent\",\n       y = \"Długość życia (w latach)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#analiza-danych",
    "href": "rozdzial5.html#analiza-danych",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.14 Analiza Danych",
    "text": "12.14 Analiza Danych\n\n12.14.1 Mediana i Rozkład\nOdpowiedz Prawda lub Fałsz:\n\n50% krajów afrykańskich ma długość życia poniżej 52 lat\nMediana długości życia w Europie wynosi około 78 lat\nPonad 75% krajów Oceanii ma długość życia powyżej 75 lat\n25% krajów azjatyckich ma długość życia poniżej 68 lat\nŚrodkowe 50% długości życia w Europie mieści się między 76 a 80 lat\n\n\n\n12.14.2 Rozrzut i Zmienność\nOdpowiedz Prawda lub Fałsz:\n\nAzja wykazuje największy rozrzut (IQR) w długości życia\nEuropa ma najmniejszy IQR wśród wszystkich kontynentów\nZmienność długości życia w Afryce jest większa niż w obu Amerykach\nOceania wykazuje najmniejszą zmienność w długości życia\nWąsy dla Azji rozciągają się w przybliżeniu od 58 do 82 lat (z wyłączeniem wartości odstających)\n\n\n\n12.14.3 Wartości Odstające i Ekstrema\nOdpowiedz Prawda lub Fałsz:\n\nAfryka ma dwa kraje z wyjątkowo niską długością życia\nW rozkładzie dla Oceanii nie ma wartości odstających\nAzja ma kilka niskich wartości odstających (poniżej 55 lat)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#zmiany-w-czasie",
    "href": "rozdzial5.html#zmiany-w-czasie",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.15 Zmiany w Czasie",
    "text": "12.15 Zmiany w Czasie\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = \"dodge\", outlier.shape = 21,\n               outlier.alpha = 0.6) +\n  labs(title = \"Długość Życia: 1957 vs 2007\",\n       subtitle = \"Porównanie zmian rozkładu na przestrzeni 50 lat\",\n       x = \"Kontynent\",\n       y = \"Długość życia (w latach)\",\n       fill = \"Rok\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\n12.15.1 Pytania dotyczące Zmian w Czasie\nOdpowiedz Prawda lub Fałsz:\n\nMediana długości życia wzrosła na wszystkich kontynentach między 1957 a 2007 rokiem\nZmienność długości życia (IQR) zmniejszyła się na większości kontynentów w czasie\nAfryka wykazała najmniejszą poprawę mediany długości życia\nRozrzut długości życia w Azji znacząco się zmniejszył od 1957 do 2007 roku\nOceania utrzymała najwyższą medianę długości życia w obu okresach\n\n\n\n12.15.2 Podsumowanie Statystyczne\n\n# Obliczenie statystyk opisowych\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    mediana = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    liczba_odstających = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Statystyki Opisowe według Kontynentu i Roku\")\n\n\nStatystyki Opisowe według Kontynentu i Roku\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontinent\nyear\nmediana\nq1\nq3\niqr\nmin\nmax\nliczba_odstających\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n31.6\n58.1\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n40.7\n70.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n30.3\n67.8\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n48.1\n73.5\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n70.3\n70.3\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n80.2\n81.2\n0",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#najważniejsze-wnioski",
    "href": "rozdzial5.html#najważniejsze-wnioski",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.16 Najważniejsze Wnioski",
    "text": "12.16 Najważniejsze Wnioski\n\nCentrum Rozkładu:\n\nMediana pokazuje typową długość życia\nZmiany mediany odzwierciedlają ogólną poprawę\n\nRozrzut i Zmienność:\n\nIQR (wysokość pudełka) wskazuje na rozproszenie danych\nSzersze pudełka sugerują większe nierówności w długości życia\n\nWartości Odstające i Ekstrema:\n\nWartości odstające często reprezentują kraje o wyjątkowej sytuacji\n\nPorównanie w Czasie:\n\nPokazuje zarówno bezwzględną poprawę, jak i zmiany w wariancji\nUwydatnia utrzymujące się różnice regionalne\nUjawnia różne tempo postępu na poszczególnych kontynentach",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#statistical-summary",
    "href": "rozdzial5.html#statistical-summary",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.17 Statistical Summary",
    "text": "12.17 Statistical Summary\n\n# Calculate summary statistics\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    n_outliers = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics by Continent and Year\")\n\n\nSummary Statistics by Continent and Year\n\n\ncontinent\nyear\nmedian\nq1\nq3\niqr\nmin\nmax\nn_outliers\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n31.6\n58.1\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n40.7\n70.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n30.3\n67.8\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n48.1\n73.5\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n70.3\n70.3\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n80.2\n81.2\n0",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#appendix-tabele-podsumowujące-typy-danych-i-odpowiednie-miary-statystyczne",
    "href": "rozdzial5.html#appendix-tabele-podsumowujące-typy-danych-i-odpowiednie-miary-statystyczne",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.18 Appendix: Tabele Podsumowujące Typy Danych i Odpowiednie Miary Statystyczne",
    "text": "12.18 Appendix: Tabele Podsumowujące Typy Danych i Odpowiednie Miary Statystyczne\n\n12.18.1 Zalety i Wady Różnych Miar Statystycznych\n\n12.18.1.1 Miary Tendencji Centralnej\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nŚrednia\n- Wykorzystuje wszystkie punkty danych- Pozwala na dalsze obliczenia statystyczne- Idealna dla danych o rozkładzie normalnym\n- Wrażliwa na wartości odstające- Nieodpowiednia dla rozkładów skośnych- Bez znaczenia dla danych nominalnych\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nMediana\n- Niewrażliwa na wartości odstające- Dobra dla rozkładów skośnych- Może być stosowana do danych porządkowych\n- Ignoruje rzeczywiste wartości większości punktów danych- Mniej użyteczna do dalszych analiz statystycznych\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nModa\n- Może być stosowana do każdego typu danych- Dobra do znajdowania najczęstszej kategorii\n- Może nie być unikalna (rozkłady multimodalne)- Nieprzydatna do wielu typów analiz- Ignoruje wielkość różnic między wartościami\nWszystkie typy\n\n\n\n\n\n12.18.1.2 Miary Zmienności\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nZakres\n- Prosty do obliczenia i zrozumienia- Daje szybki obraz rozproszenia danych\n- Bardzo wrażliwy na wartości odstające- Ignoruje wszystkie dane między ekstremami- Nieprzydatny do dalszych analiz statystycznych\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nRozstęp międzykwartylowy (IQR)\n- Niewrażliwy na wartości odstające- Dobry dla rozkładów skośnych\n- Ignoruje 50% danych- Mniej intuicyjny niż zakres\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nWariancja\n- Wykorzystuje wszystkie punkty danych- Podstawa wielu procedur statystycznych\n- Wrażliwa na wartości odstające- Jednostki są podniesione do kwadratu (mniej intuicyjne)\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nOdchylenie standardowe\n- Wykorzystuje wszystkie punkty danych- Te same jednostki co oryginalne dane- Szeroko stosowane i zrozumiałe\n- Wrażliwe na wartości odstające- Zakłada w przybliżeniu rozkład normalny dla interpretacji\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nWspółczynnik zmienności\n- Pozwala na porównanie między zbiorami danych o różnych jednostkach lub średnich\n- Może być mylący, gdy średnie są bliskie zeru- Bez znaczenia dla danych z wartościami ujemnymi\nIlorazowe, niektóre Interwałowe\n\n\n\n\n\n12.18.1.3 Miary Korelacji/Asocjacji\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nr Pearsona\n- Mierzy zależność liniową- Szeroko stosowany i zrozumiały\n- Zakłada rozkład normalny- Wrażliwy na wartości odstające- Uchwytuje tylko zależności liniowe\nInterwałowe, Ilorazowe, Ciągłe\n\n\nRho Spearmana\n- Może być stosowany do danych porządkowych- Uchwytuje zależności monotoniczne- Mniej wrażliwy na wartości odstające\n- Traci informacje przez konwersję na rangi- Może pominąć niektóre typy zależności\nPorządkowe, Interwałowe, Ilorazowe\n\n\nTau Kendalla\n- Może być stosowany do danych porządkowych- Bardziej odporny niż Spearman dla małych próbek- Ma ładną interpretację (prawdopodobieństwo zgodności)\n- Traci informacje, biorąc pod uwagę tylko porządek- Bardziej intensywny obliczeniowo\nPorządkowe, Interwałowe, Ilorazowe\n\n\nChi-kwadrat\n- Może być stosowany do danych nominalnych- Testuje niezależność zmiennych kategorycznych\n- Wymaga dużych rozmiarów próbek- Wrażliwy na rozmiar próbki- Nie mierzy siły asocjacji\nNominalne, Porządkowe\n\n\nV Craméra\n- Może być stosowany do danych nominalnych- Dostarcza miarę siły asocjacji- Znormalizowany do zakresu [0,1]\n- Interpretacja może być subiektywna- Może przeszacować asocjację w małych próbkach\nNominalne, Porządkowe\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓*\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\n12.18.2 Notes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "13  Data Visualization: with examples in R",
    "section": "",
    "text": "13.1 Introduction to Data Types and Visualization\nBefore diving into specific visualization techniques, it’s crucial to understand the different types of data you might encounter and how they influence your choice of visualization method. We’ll explore these concepts with practical examples using the ggplot2 library in R.\nFirst, let’s load the necessary libraries:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#bar-plots",
    "href": "chapter6.html#bar-plots",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.2 Bar Plots",
    "text": "13.2 Bar Plots\nBar plots are excellent for displaying categorical data or summarizing continuous data by groups.\n\n13.2.1 Understanding Bar Plots\nA bar plot represents data using rectangular bars with heights proportional to the values they represent. They are used to compare different categories or groups.\nKey components of a bar plot: 1. X-axis: Represents categories 2. Y-axis: Represents values (can be counts, percentages, or any numerical value) 3. Bars: Rectangle for each category, height corresponds to its value\n\n13.2.1.1 Example Data\nLet’s use a simple dataset of fruit sales:\n\nfruits &lt;- c(\"Apple\", \"Banana\", \"Orange\", \"Grape\")\nsales &lt;- c(120, 85, 70, 100)\n\n# Create a data frame\ndf &lt;- data.frame(fruit = fruits, sales = sales)\n\n\n\n\n13.2.2 Hand-Drawn Bar Plot\nTo create a bar plot by hand:\n\nDraw a horizontal line (x-axis) and a vertical line (y-axis) perpendicular to each other.\nLabel the x-axis with your categories (fruits), evenly spaced.\nLabel the y-axis with a suitable scale for your values (sales, 0 to 120 in increments of 20).\nFor each category, draw a rectangle (bar) whose height corresponds to its value on the y-axis scale.\nColor or shade each bar if desired.\nAdd a title and labels for both axes.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen drawing by hand, use graph paper for more precise measurements and straighter lines. Choose a scale that allows all your data to fit while maximizing the use of space.\n\n\n\n\n13.2.3 Bar Plot in Base R\n\n# Create bar plot\nbarplot(sales, names.arg = fruits, \n        main = \"Fruit Sales\",\n        xlab = \"Fruit Types\", ylab = \"Sales\")\n\n\n\n\n\n\n\n\n\n\n13.2.4 Bar Plot with ggplot2\n\n# Create bar plot with ggplot2\nggplot(df, aes(x = fruit, y = sales)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Fruit Sales\",\n       x = \"Fruit Types\", y = \"Sales\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n13.2.5 Interpreting Bar Plots\nWhen interpreting a bar plot, consider the following:\n\nRelative Heights: Compare the heights of the bars to understand which categories have higher or lower values.\nOrdering: Sometimes, bars are ordered by height to make comparisons easier.\nPatterns: Look for any patterns or trends across categories.\nOutliers: Identify any bars that are much taller or shorter than the others.\n\n\n13.2.5.1 Example Interpretation\nFor our fruit sales data:\n\nApples have the highest sales (120), followed by Grapes (100).\nOranges have the lowest sales (70).\nThere’s a considerable difference between the highest (Apples) and lowest (Oranges) sales.\nBananas and Grapes have similar sales figures, in the middle range.\n\nThis information could be useful for inventory management or marketing strategies in a fruit shop.\n\n\n\n\n\n\nNote\n\n\n\nBar plots are great for comparing categories, but they don’t show the distribution within each category. For that, you might need other plot types like box plots.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#histograms",
    "href": "chapter6.html#histograms",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.3 Histograms",
    "text": "13.3 Histograms\nHistograms visualize the distribution of a continuous variable by dividing it into intervals (bins) and showing the frequency or density of data points in each bin.\n\n13.3.1 Understanding Histograms\nKey components of a histogram: 1. X-axis: Represents the variable’s values, divided into bins 2. Y-axis: Represents frequency, relative frequency, or density 3. Bars: Rectangle for each bin, height corresponds to the y-axis measure\nThere are three main types of histograms:\n\nFrequency Histogram: The y-axis shows the count of data points in each bin.\nRelative Frequency Histogram: The y-axis shows the proportion of data points in each bin (frequency divided by total number of data points).\nDensity Histogram: The y-axis shows the density, which is the relative frequency divided by the bin width. The total area of all bars sums to 1.\n\n\n13.3.1.1 Example Data\nLet’s use a dataset of 50 student exam scores (out of 100):\n\nset.seed(123)  # for reproducibility\nscores &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\n13.3.2 Hand-Drawn Histogram\nTo create a frequency histogram by hand:\n\nFind the range of your data.\nChoose a number of bins (let’s use 7 bins).\nCreate a frequency table.\nDraw x and y axes.\nLabel x-axis with bin ranges and y-axis with frequency.\nDraw a rectangle for each bin, with height corresponding to its frequency.\nAdd a title and labels for both axes.\n\nFor a relative frequency histogram, divide each frequency by the total number of data points before drawing the bars.\nFor a density histogram, divide the relative frequency by the bin width before drawing the bars.\n\n\n\n\n\n\nTip\n\n\n\nThe number of bins can affect the interpretation. Too few bins may obscure important features, while too many may introduce noise. A common rule of thumb is to use the square root of the number of data points as the number of bins.\n\n\n\n\n13.3.3 Histograms in Base R\n\n# Frequency Histogram\nhist(scores, breaks = 7, \n     main = \"Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Relative Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Relative Frequency\")\n\n\n\n\n\n\n\n# Density Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Density Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Density\")\nlines(density(scores), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n13.3.4 Histograms with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(score = scores)\n\n# Frequency Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nggplot(df, aes(x = score, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Relative Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Relative Frequency\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Density Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Density Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Density\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n13.3.5 Interpreting Histograms\nWhen interpreting a histogram, consider:\n\nCentral Tendency: Where is the peak of the distribution?\nSpread: How wide is the distribution?\nShape: Is it symmetric, skewed, or multi-modal?\nOutliers: Are there any unusual values far from the main distribution?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#box-plots-and-tukey-box-plots",
    "href": "chapter6.html#box-plots-and-tukey-box-plots",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.4 Box Plots and Tukey Box Plots",
    "text": "13.4 Box Plots and Tukey Box Plots\nBox plots, also known as box-and-whisker plots, provide a concise summary of a distribution. We’ll focus on the Tukey-style box plot, named after the statistician John Tukey who popularized this type of plot.\n\n13.4.1 Understanding Box Plots\nA box plot represents five key statistics:\n\nMinimum value (excluding outliers)\nFirst quartile (Q1)\nMedian\nThird quartile (Q3)\nMaximum value (excluding outliers)\n\nAdditionally, box plots show:\n\nWhiskers: Lines extending from the box to the minimum and maximum values (excluding outliers)\nOutliers: Individual points beyond the whiskers\n\n\n13.4.1.1 Calculating Quartiles and Outliers\nTo create a box plot, follow these steps:\n\nOrder your data from smallest to largest.\nFind the median (middle value if odd number of data points, average of two middle values if even).\nFind Q1 (median of lower half of data) and Q3 (median of upper half of data).\nCalculate the Interquartile Range (IQR) = Q3 - Q1\nDetermine outliers using Tukey’s rule:\n\nLower outliers: &lt; Q1 - 1.5 * IQR\nUpper outliers: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe factor 1.5 in Tukey’s outlier rule is based on the properties of the normal distribution. For normally distributed data, this rule identifies about 0.7% of the data as potential outliers.\n\n\n\n\n13.4.1.2 Example Data\nLet’s use a small dataset to illustrate:\n\ndata &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\n13.4.2 Hand-Drawn Tukey Box Plot\nTo create a Tukey box plot by hand:\n\nDraw a vertical line representing the range from minimum to maximum (2 to 15 in our example, excluding the outlier).\nDraw a box from Q1 to Q3.\nDraw a horizontal line through the box at the median.\nDraw whiskers from the box to the minimum and maximum values (excluding outliers).\nRepresent the outlier (50) as an individual point beyond the whisker.\nAdd a scale to the vertical axis and label it.\n\n\n\n13.4.3 Box Plot in Base R\n\n# Create box plot\nboxplot(data, main = \"Box Plot of Sample Data\",\n        ylab = \"Values\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\n13.4.4 Tukey Box Plot with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(value = data)\n\n# Create Tukey box plot with ggplot2\nggplot(df, aes(x = \"\", y = value)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Tukey Box Plot of Sample Data\",\n       x = \"\", y = \"Values\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n13.4.5 Interpreting Box Plots\nWhen interpreting a box plot, consider the following:\n\nCentral Tendency: The median shows the center of the distribution.\nSpread: The box (IQR) represents the middle 50% of the data.\nSkewness: If the median line is closer to one end of the box, the distribution is skewed.\nOutliers: Points beyond the whiskers are potential outliers.\nComparisons: When comparing multiple box plots, look at relative positions of medians, box sizes, and presence of outliers.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#conclusion",
    "href": "chapter6.html#conclusion",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.5 Conclusion",
    "text": "13.5 Conclusion\nIn this chapter, we explored three fundamental types of data visualizations: bar plots, histograms, and box plots. We demonstrated how to create these plots by hand, using R’s base plotting system, and using the ggplot2 library.\nEach type of plot serves a different purpose: - Bar plots are excellent for comparing categories. - Histograms show the distribution of a continuous variable. - Box plots provide a concise summary of a distribution, highlighting central tendency, spread, and outliers.\nRemember, the choice of visualization depends on your data type and the insights you want to convey. Always consider your audience and the story you want to tell with your data when selecting and designing your visualizations.\nPractice creating these plots by hand to deepen your understanding of their construction and interpretation. Then, leverage the power of R and ggplot2 to quickly create and customize these visualizations for larger datasets and more complex analyses.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html",
    "href": "rozdzial6.html",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "",
    "text": "14.1 Wprowadzenie do Typów Danych i Wizualizacji\nPrzed zagłębieniem się w konkretne techniki wizualizacji, ważne jest zrozumienie różnych typów danych i ich wpływu na wybór metody wizualizacji. Przeanalizujemy te koncepcje na praktycznych przykładach z użyciem biblioteki ggplot2 w R.\nNajpierw załadujmy niezbędne biblioteki:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-słupkowe",
    "href": "rozdzial6.html#wykresy-słupkowe",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.2 Wykresy Słupkowe",
    "text": "14.2 Wykresy Słupkowe\nWykresy słupkowe doskonale nadają się do prezentacji danych kategorycznych lub podsumowania danych ciągłych w grupach.\n\n14.2.1 Zrozumienie Wykresów Słupkowych\nWykres słupkowy przedstawia dane za pomocą prostokątnych słupków, których wysokość jest proporcjonalna do reprezentowanych przez nie wartości. Służą do porównywania różnych kategorii lub grup.\nGłówne elementy wykresu słupkowego: 1. Oś X: Reprezentuje kategorie 2. Oś Y: Reprezentuje wartości (mogą to być liczebności, procenty lub dowolne wartości numeryczne) 3. Słupki: Prostokąt dla każdej kategorii, wysokość odpowiada jej wartości\n\n14.2.1.1 Przykładowe Dane\nUżyjmy prostego zestawu danych dotyczącego sprzedaży owoców:\n\nowoce &lt;- c(\"Jabłko\", \"Banan\", \"Pomarańcza\", \"Winogrono\")\nsprzedaz &lt;- c(120, 85, 70, 100)\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(owoc = owoce, sprzedaz = sprzedaz)\n\n\n\n\n14.2.2 Ręcznie Rysowany Wykres Słupkowy\nAby stworzyć wykres słupkowy ręcznie:\n\nNarysuj linię poziomą (oś X) i pionową (oś Y) prostopadłe do siebie.\nOznacz oś X swoimi kategoriami (owocami), równomiernie rozmieszczonymi.\nOznacz oś Y odpowiednią skalą dla Twoich wartości (sprzedaż, od 0 do 120 z przyrostami co 20).\nDla każdej kategorii narysuj prostokąt (słupek), którego wysokość odpowiada jej wartości na skali osi Y.\nJeśli chcesz, pokoloruj lub zacienuj każdy słupek.\nDodaj tytuł i etykiety dla obu osi.\n\n\n\n\n\n\n\nTip\n\n\n\nPrzy rysowaniu ręcznym użyj papieru milimetrowego dla dokładniejszych pomiarów i prostszych linii. Wybierz skalę, która pozwoli zmieścić wszystkie dane, maksymalnie wykorzystując dostępną przestrzeń.\n\n\n\n\n14.2.3 Wykres Słupkowy w Podstawowym R\n\n# Tworzenie wykresu słupkowego\nbarplot(sprzedaz, names.arg = owoce, \n        main = \"Sprzedaż Owoców\",\n        xlab = \"Rodzaje Owoców\", ylab = \"Sprzedaż\")\n\n\n\n\n\n\n\n\n\n\n14.2.4 Wykres Słupkowy z ggplot2\n\n# Tworzenie wykresu słupkowego z ggplot2\nggplot(df, aes(x = owoc, y = sprzedaz)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Sprzedaż Owoców\",\n       x = \"Rodzaje Owoców\", y = \"Sprzedaż\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n14.2.5 Interpretacja Wykresów Słupkowych\nPodczas interpretacji wykresu słupkowego zwróć uwagę na:\n\nWzględne Wysokości: Porównaj wysokości słupków, aby zrozumieć, które kategorie mają wyższe lub niższe wartości.\nKolejność: Czasami słupki są uporządkowane według wysokości, aby ułatwić porównania.\nWzorce: Poszukaj wzorców lub trendów między kategoriami.\nWartości Odstające: Zidentyfikuj słupki, które są znacznie wyższe lub niższe od pozostałych.\n\n\n14.2.5.1 Przykładowa Interpretacja\nDla naszych danych o sprzedaży owoców:\n\nJabłka mają najwyższą sprzedaż (120), następnie Winogrona (100).\nPomarańcze mają najniższą sprzedaż (70).\nIstnieje znaczna różnica między najwyższą (Jabłka) a najniższą (Pomarańcze) sprzedażą.\nBanany i Winogrona mają podobne wartości sprzedaży, w średnim zakresie.\n\nTa informacja może być przydatna dla zarządzania zapasami lub strategii marketingowych w sklepie owocowym.\n\n\n\n\n\n\nNote\n\n\n\nWykresy słupkowe są świetne do porównywania kategorii, ale nie pokazują rozkładu wewnątrz każdej kategorii. Do tego mogą być potrzebne inne typy wykresów, jak wykresy pudełkowe.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#histogramy",
    "href": "rozdzial6.html#histogramy",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.3 Histogramy",
    "text": "14.3 Histogramy\nHistogramy wizualizują rozkład zmiennej ciągłej poprzez podzielenie jej na przedziały (bins) i pokazanie częstości lub gęstości punktów danych w każdym przedziale.\n\n14.3.1 Zrozumienie Histogramów\nGłówne elementy histogramu: 1. Oś X: Reprezentuje wartości zmiennej, podzielone na przedziały 2. Oś Y: Reprezentuje częstość, względną częstość lub gęstość 3. Słupki: Prostokąt dla każdego przedziału, wysokość odpowiada mierze na osi Y\nIstnieją trzy główne typy histogramów:\n\nHistogram Częstości: Oś Y pokazuje liczbę punktów danych w każdym przedziale.\nHistogram Częstości Względnej: Oś Y pokazuje proporcję punktów danych w każdym przedziale (częstość podzielona przez całkowitą liczbę punktów danych).\nHistogram Gęstości: Oś Y pokazuje gęstość, która jest częstością względną podzieloną przez szerokość przedziału. Całkowita powierzchnia wszystkich słupków sumuje się do 1.\n\n\n14.3.1.1 Przykładowe Dane\nUżyjmy zbioru 50 wyników egzaminów studentów (na 100 punktów):\n\nset.seed(123)  # dla powtarzalności\nwyniki &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\n14.3.2 Ręcznie Rysowany Histogram\nAby stworzyć histogram częstości ręcznie:\n\nZnajdź zakres danych.\nWybierz liczbę przedziałów (użyjmy 7 przedziałów).\nUtwórz tabelę częstości.\nNarysuj osie X i Y.\nOznacz oś X zakresami przedziałów, a oś Y częstością.\nNarysuj prostokąt dla każdego przedziału, z wysokością odpowiadającą jego częstości.\nDodaj tytuł i etykiety dla obu osi.\n\nDla histogramu częstości względnej, podziel każdą częstość przez całkowitą liczbę punktów danych przed narysowaniem słupków.\nDla histogramu gęstości, podziel częstość względną przez szerokość przedziału przed narysowaniem słupków.\n\n\n\n\n\n\nTip\n\n\n\nLiczba przedziałów może wpłynąć na interpretację. Zbyt mało przedziałów może ukryć ważne cechy, podczas gdy zbyt wiele może wprowadzić szum. Powszechną regułą jest użycie pierwiastka kwadratowego z liczby punktów danych jako liczby przedziałów.\n\n\n\n\n14.3.3 Histogramy w Podstawowym R\n\n# Histogram Częstości\nhist(wyniki, breaks = 7, \n     main = \"Histogram Częstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość\")\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Częstości Względnej Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość Względna\")\n\n\n\n\n\n\n\n# Histogram Gęstości\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Gęstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Gęstość\")\nlines(density(wyniki), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n14.3.4 Histogramy z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wynik = wyniki)\n\n# Histogram Częstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nggplot(df, aes(x = wynik, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Względnej Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość Względna\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Histogram Gęstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Histogram Gęstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Gęstość\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n14.3.5 Interpretacja Histogramów\nPodczas interpretacji histogramu zwróć uwagę na:\n\nTendencję Centralną: Gdzie znajduje się szczyt rozkładu?\nRozrzut: Jak szeroki jest rozkład?\nKształt: Czy jest symetryczny, skośny, czy wielomodalny?\nWartości Odstające: Czy są nietypowe wartości daleko od głównego rozkładu?",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "href": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya",
    "text": "14.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya\nWykresy pudełkowe, znane również jako wykresy skrzynkowe, dostarczają zwięzłego podsumowania rozkładu. Skupimy się na wykresie pudełkowym w stylu Tukeya, nazwanym na cześć statystyka Johna Tukeya, który spopularyzował ten typ wykresu.\n\n14.4.1 Zrozumienie Wykresów Pudełkowych\nWykres pudełkowy przedstawia pięć kluczowych statystyk:\n\nWartość minimalna (z wyłączeniem wartości odstających)\nPierwszy kwartyl (Q1)\nMediana\nTrzeci kwartyl (Q3)\nWartość maksymalna (z wyłączeniem wartości odstających)\n\nDodatkowo wykresy pudełkowe pokazują:\n\nWąsy: Linie rozciągające się od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających)\nWartości odstające: Indywidualne punkty poza wąsami\n\n\n14.4.1.1 Obliczanie Kwartyli i Wartości Odstających\nAby stworzyć wykres pudełkowy, postępuj zgodnie z tymi krokami:\n\nUporządkuj dane od najmniejszej do największej wartości.\nZnajdź medianę (środkowa wartość dla nieparzystej liczby punktów danych, średnia z dwóch środkowych wartości dla parzystej).\nZnajdź Q1 (mediana dolnej połowy danych) i Q3 (mediana górnej połowy danych).\nOblicz Rozstęp Międzykwartylowy (IQR) = Q3 - Q1\nOkreśl wartości odstające używając reguły Tukeya:\n\nDolne wartości odstające: &lt; Q1 - 1.5 * IQR\nGórne wartości odstające: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nWspółczynnik 1.5 w regule Tukeya dla wartości odstających opiera się na właściwościach rozkładu normalnego. Dla danych o rozkładzie normalnym, ta reguła identyfikuje około 0.7% danych jako potencjalne wartości odstające.\n\n\n\n\n14.4.1.2 Przykładowe Dane\nUżyjmy małego zbioru danych do ilustracji:\n\ndane &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\n14.4.2 Ręcznie Rysowany Wykres Pudełkowy Tukeya\nAby stworzyć wykres pudełkowy Tukeya ręcznie:\n\nNarysuj linię pionową reprezentującą zakres od minimum do maksimum (2 do 15 w naszym przykładzie, z wyłączeniem wartości odstającej).\nNarysuj pudełko od Q1 do Q3.\nNarysuj poziomą linię przez pudełko na poziomie mediany.\nNarysuj wąsy od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających).\nPrzedstaw wartość odstającą (50) jako indywidualny punkt poza wąsem.\nDodaj skalę do osi pionowej i oznacz ją.\n\n\n\n14.4.3 Wykres Pudełkowy w Podstawowym R\n\n# Tworzenie wykresu pudełkowego\nboxplot(dane, main = \"Wykres Pudełkowy Przykładowych Danych\",\n        ylab = \"Wartości\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\n14.4.4 Wykres Pudełkowy Tukeya z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wartosc = dane)\n\n# Tworzenie wykresu pudełkowego Tukeya z ggplot2\nggplot(df, aes(x = \"\", y = wartosc)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Wykres Pudełkowy Tukeya Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n14.4.5 Interpretacja Wykresów Pudełkowych\nPodczas interpretacji wykresu pudełkowego zwróć uwagę na następujące elementy:\n\nTendencja Centralna: Mediana pokazuje środek rozkładu.\nRozrzut: Pudełko (IQR) reprezentuje środkowe 50% danych.\nSkośność: Jeśli linia mediany jest bliżej jednego końca pudełka, rozkład jest skośny.\nWartości Odstające: Punkty poza wąsami są potencjalnymi wartościami odstającymi.\nPorównania: Przy porównywaniu wielu wykresów pudełkowych, zwróć uwagę na względne położenie median, rozmiary pudełek i obecność wartości odstających.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "href": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.5 Zaawansowane Techniki Wizualizacji",
    "text": "14.5 Zaawansowane Techniki Wizualizacji\nOprócz podstawowych typów wykresów, warto poznać kilka bardziej zaawansowanych technik wizualizacji, które mogą być przydatne w analizie danych.\n\n14.5.1 Wykresy Skrzypcowe\nWykresy skrzypcowe łączą cechy wykresów pudełkowych i wykresów gęstości, dając bardziej kompletny obraz rozkładu danych.\n\n# Tworzenie wykresu skrzypcowego\nggplot(df, aes(x = \"\", y = wartosc)) +\n  geom_violin(fill = \"lightblue\") +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Wykres Skrzypcowy Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n14.5.2 Wykresy Rozrzutu z Marginesami\nŁączenie wykresów rozrzutu z histogramami na marginesach może dostarczyć więcej informacji o rozkładzie danych w dwóch wymiarach.\n\n# Generowanie danych do wykresu rozrzutu\nset.seed(123)\ndf_scatter &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100)\n)\n\n# Tworzenie wykresu rozrzutu z marginesami\nlibrary(ggExtra)\np &lt;- ggplot(df_scatter, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal()\nggMarginal(p, type = \"histogram\", fill = \"lightblue\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wnioski",
    "href": "rozdzial6.html#wnioski",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.6 Wnioski",
    "text": "14.6 Wnioski\nW tym rozdziale poznaliśmy trzy podstawowe typy wizualizacji danych: wykresy słupkowe, histogramy i wykresy pudełkowe. Pokazaliśmy, jak tworzyć te wykresy ręcznie, używając podstawowego systemu wykresów R oraz biblioteki ggplot2.\nKażdy typ wykresu służy innemu celowi: - Wykresy słupkowe doskonale nadają się do porównywania kategorii. - Histogramy pokazują rozkład zmiennej ciągłej. - Wykresy pudełkowe dostarczają zwięzłego podsumowania rozkładu, podkreślając tendencję centralną, rozrzut i wartości odstające.\nPamiętaj, że wybór wizualizacji zależy od typu danych i wniosków, które chcesz przekazać. Zawsze bierz pod uwagę swoją docelową grupę odbiorców i historię, którą chcesz opowiedzieć za pomocą swoich danych, wybierając i projektując wizualizacje.\nĆwicz tworzenie tych wykresów ręcznie, aby pogłębić zrozumienie ich konstrukcji i interpretacji. Następnie wykorzystaj moc R i ggplot2, aby szybko tworzyć i dostosowywać te wizualizacje dla większych zbiorów danych i bardziej złożonych analiz.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#ćwiczenia-praktyczne",
    "href": "rozdzial6.html#ćwiczenia-praktyczne",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.7 Ćwiczenia Praktyczne",
    "text": "14.7 Ćwiczenia Praktyczne\n\nZbierz dane o popularności różnych gatunków muzycznych wśród Twoich znajomych. Stwórz wykres słupkowy przedstawiający te dane.\nZmierz czas reakcji 30 osób na bodziec dźwiękowy (w milisekundach). Utwórz histogram tych danych.\nZbierz dane o wzroście 50 osób w Twojej społeczności. Stwórz wykres pudełkowy dla tych danych, osobno dla mężczyzn i kobiet.\nZnajdź zestaw danych online (np. na Kaggle) i stwórz trzy różne wizualizacje dla tych danych. Opisz, jakie wnioski można wyciągnąć z każdej wizualizacji.\nStwórz wykres skrzypcowy dla danych o cenach domów w różnych dzielnicach miasta. Porównaj go z wykresem pudełkowym tych samych danych. Jakie dodatkowe informacje dostarcza wykres skrzypcowy?\n\nPamiętaj, że praktyka jest kluczem do opanowania sztuki wizualizacji danych. Eksperymentuj z różnymi typami wykresów i parametrami, aby znaleźć najlepszy sposób przedstawienia swoich danych.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "correg_en.html",
    "href": "correg_en.html",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "",
    "text": "15.1 Bivariate Statistics - introduction\nBivariate statistics describe the relationship between two variables. We’ll explore several measures, starting with covariance.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#bivariate-statistics---introduction",
    "href": "correg_en.html#bivariate-statistics---introduction",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "",
    "text": "Understanding Variable Relationships in Social Research\n\n\n\nThis section examines how different social variables interact and correlate with each other. We will analyze four distinct types of relationships commonly observed in social science research, using empirical examples to illustrate key patterns and their implications for data analysis.\n\n\n\n\n\n\n\n\n\nAnalysis of Variable Relationships:\n\nPositive Linear Correlation (Healthcare Access and Life Expectancy)\n\nThe data demonstrates a positive linear relationship between healthcare access and life expectancy. Statistical analysis indicates that a 10-percentage-point increase in healthcare access corresponds to approximately 2.5 years of additional life expectancy. This relationship maintains statistical significance across the observed range.\n\nNegative Linear Correlation (Digital Device Usage and Sleep Quality)\n\nThe analysis reveals an inverse relationship between daily device usage and sleep quality metrics. The data indicates that each additional hour of device usage correlates with a measurable decrease in sleep quality scores, demonstrating a consistent negative linear relationship.\n\nAbsence of Correlation (Infrastructure and Economic Indicators (e.g. GDP per capita in PLN))\n\nThe relationship between infrastructure density and economic indicators like GDP per capita in PLN shows no statistically significant correlation. This absence of correlation suggests that these variables operate independently within the observed parameters, indicating the presence of other determining factors not captured in this analysis.\n\nNon-linear Relationship (Team Size and Productivity)\n\nThe relationship between team size and productivity follows a curvilinear pattern. The data shows an optimal range for team size, with productivity declining both below and above this range. This demonstrates the importance of considering non-linear patterns in organizational research.\nMethodological Considerations:\n\nStatistical Significance: Observed relationships must be evaluated for statistical significance before drawing conclusions.\nVariable Independence: The assumption of variable independence requires verification through appropriate statistical tests.\nConfounding Variables: Analyses must account for potential confounding variables that may influence observed relationships.\nCausality: Correlation patterns do not necessarily imply causal relationships; additional research methods are required to establish causation.\n\nResearch Applications:\nThe understanding of these relationship patterns has significant implications for:\n\nResearch design and methodology\nStatistical analysis procedures\nPolicy implementation and evaluation\nTheory development and testing\n\nCritical evaluation of these relationships enables more robust research design and more reliable conclusions in social science research.\n\n\n\n\n\n\n\n\nThe Critical Distinction Between Correlation and Causation [See e.g. https://www.tylervigen.com/spurious-correlations]\n\n\n\n\n\n\nhttps://x.com/EUFIC/status/1324667630238814209?prefetchTimestamp=1732463940216\n\n\n\n\n\nhttps://sitn.hms.harvard.edu/flash/2021/when-correlation-does-not-imply-causation-why-your-gut-microbes-may-not-yet-be-a-silver-bullet-to-all-your-problems/\n\n\nStatistical relationships between variables represent one of the most frequently misinterpreted aspects of data analysis. While correlations can reveal patterns in data, they require careful interpretation to avoid drawing incorrect causal conclusions. Let us examine this concept through real-world examples.\n\n15.1.1 Seasonal Patterns and Spurious Correlations: A Case Study\n\n\n\n\n\n\n\n\n\nThis visualization demonstrates a classic example of confounding in statistical analysis. The apparent correlation between ice cream sales and crime rates (r = 0.85, p &lt; 0.001) exemplifies how seasonal variation can create misleading statistical relationships. The correlation emerges from a common causal factor: seasonal temperature variations that independently influence both variables through distinct mechanisms.\n\n\n15.1.2 Temporal Trends and Spurious Associations\n\n\n\n\n\n\n\n\n\nThis second analysis illustrates temporal correlation bias, where two independently declining trends create an artificial statistical association. Despite the strong correlation coefficient (r = 0.91, p &lt; 0.001), there is no plausible causal mechanism linking these variables.\n\n\n15.1.3 Understanding Mechanisms of Spurious Correlation\nStatistical analysis can be compromised by several systematic biases that create apparent but meaningless correlations. Here are the primary mechanisms:\n1. Confounding Variables\nA confounding variable creates an apparent relationship between independent variables by independently affecting each one. This statistical phenomenon requires careful experimental design and multivariate analysis to detect and control for potential confounders.\n2. Temporal Autocorrelation\nWhen variables exhibit strong temporal trends, they may show correlation simply because they change over time, rather than due to any meaningful relationship. This effect can be controlled for through methods such as detrending or differencing the time series.\n3. Simultaneous Causality Bias\nThis occurs when the direction of causality is ambiguous or bidirectional. For example, economic growth and investment rates may exhibit simultaneous causality, as each variable potentially influences the other through complex feedback mechanisms.\n\n\n15.1.4 Statistical Methods for Causal Inference\nModern statistical approaches offer several techniques for moving beyond simple correlation toward causal inference, e.g.:\n1. Experimental Design\nRandomized controlled trials represent the gold standard for causal inference, allowing researchers to isolate the effect of individual variables while controlling for confounders.\n2. Instrumental Variables\nThis statistical technique uses a variable that affects the outcome only through its effect on the variable of interest, helping to establish causal relationships in observational data.\n3. Regression Discontinuity\nThis quasi-experimental design exploits naturally occurring thresholds to approximate randomized experiments, providing evidence for causal relationships.\n\n\n15.1.5 Critical Framework for Correlation Analysis\nWhen evaluating correlational findings, consider the following analytical framework:\n\nTheoretical Plausibility: Examine whether there exists a logical mechanism through which one variable could influence the other.\nTemporal Precedence: Verify that the proposed cause precedes the effect in time.\nDose-Response Relationship: Assess whether changes in the magnitude of the proposed cause correspond to proportional changes in the effect.\nConsistency: Evaluate whether the relationship holds across different contexts and populations.\nAlternative Explanations: Systematically consider and test alternative explanations for the observed correlation.\n\nRemember: The path from correlation to causation requires careful experimental design, robust statistical methodology, and systematic consideration of alternative explanations. Statistical correlation represents a necessary but insufficient condition for establishing causality.\n\n\n\n\n15.1.6 Covariance\nCovariance measures how two variables vary together.\nFormula: cov(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n\n\n\n\n\n\nFrom Covariance to Different Correlation Measures\n\n\n\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Generate different types of relationships\nset.seed(123)\nn &lt;- 100\n\n# Linear relationship\nx1 &lt;- rnorm(n)\ny1 &lt;- 0.8*x1 + rnorm(n, sd=0.5)\ndata1 &lt;- data.frame(x=x1, y=y1, type=\"Linear Relationship\")\n\n# Monotonic but nonlinear\nx2 &lt;- rnorm(n)\ny2 &lt;- sign(x2)*(x2^2) + rnorm(n, sd=0.5)\ndata2 &lt;- data.frame(x=x2, y=y2, type=\"Monotonic Nonlinear\")\n\n# Non-monotonic relationship\nx3 &lt;- seq(-3, 3, length.out=n)\ny3 &lt;- x3^2 + rnorm(n, sd=0.5)\ndata3 &lt;- data.frame(x=x3, y=y3, type=\"Non-monotonic\")\n\n# Combine data\nall_data &lt;- rbind(data1, data2, data3)\n\n# Create plot\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6) +\n  facet_wrap(~type, scales = \"free\") +\n  labs(title = \"Different Types of Relationships Between Variables\",\n       x = \"Variable X\",\n       y = \"Variable Y\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    strip.text = element_text(size = 12),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n15.1.7 The Concept of Correlation\nCorrelation is a broad concept that describes how variables are related to each other. This relationship can take many forms, as shown in our plots.\n\n\n15.1.8 Starting with Covariance\nCovariance is the fundamental measure of how variables move together:\nCov(X,Y) = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\nIt tells us:\n\nIf variables tend to move in the same direction (positive covariance)\nIf they move in opposite directions (negative covariance)\nIf they don’t have a clear linear pattern (covariance near zero)\n\nHowever, covariance has a limitation: its value depends on the units of measurement. For example:\n\nHeight in meters vs. weight in kg gives one covariance value\nHeight in centimeters vs. weight in kg gives a different value\nSame relationship, different scales!\n\n\n\n15.1.9 Standardizing to Get Correlation Measures\n\nPearson’s correlation coefficient standardizes covariance: r = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\n\n\nRemoves unit dependency\nAlways between -1 and 1\nMeasures linear relationships\n\n\nSpearman’s rank correlation:\n\n\nBased on ranks rather than raw values\nCaptures monotonic relationships (even nonlinear ones)\nAlso ranges from -1 to 1\n\n\n\n15.1.10 Key Points\n\nStart with covariance to understand joint movement\nUse correlation coefficients for standardized measures\nChoose your correlation measure based on:\n\nType of relationship you expect\nNature of your data\nResearch question\n\nAlways visualize your data\n\n\n\n\n\n\n\n\n\n\nRanks: Positions in an Ordered Sequence\n\n\n\nRanks are simply position numbers in an ordered dataset:\n\n15.1.11 How to Determine Ranks?\n\nOrder data from smallest to largest value\nAssign consecutive natural numbers:\n\nSmallest value → rank 1\nSubsequent values → subsequent ranks\nLargest value → rank n (number of observations)\nFor ties → average of ranks\n\n\n\n\n15.1.12 Example\nWe have 5 students with heights:\nHeight:    165, 182, 170, 168, 175\nRanks:      1,   5,   3,   2,   4\nFor data with ties (e.g., grades):\nGrades:     2,   3,   3,   4,   5\nRanks:      1,  2.5, 2.5,  4,   5\n\n\n\nManual Calculation Example:\nLet’s calculate the covariance for two variables:\n\nx: 1, 2, 3, 4, 5\ny: 2, 4, 5, 4, 5\n\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate means\n\\bar{x} = 3, \\bar{y} = 4\n\n\n2\nCalculate (x_i - \\bar{x})(y_i - \\bar{y}) for each pair\n(-2)(-2) = 4\n\n\n\n\n(-1)(0) = 0\n\n\n\n\n(0)(1) = 0\n\n\n\n\n(1)(0) = 0\n\n\n\n\n(2)(1) = 2\n\n\n3\nSum the results\n4 + 0 + 0 + 0 + 2 = 6\n\n\n4\nDivide by (n-1)\n6 / 4 = 1.5\n\n\n\nR calculation:\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 5, 4, 5)\ncov(x, y)\n\n[1] 1.5\n\n\nInterpretation: - The positive covariance (1.5) indicates that x and y tend to increase together.\nPros:\n\nProvides direction of relationship (positive or negative)\nUseful in calculating other measures like correlation\n\nCons:\n\nScale-dependent, making it difficult to compare across different variable pairs\nDoesn’t provide information about the strength of the relationship\n\n\n\n15.1.13 Pearson Correlation\nPearson correlation measures the strength and direction of the linear relationship between two continuous variables.\nFormula: r = \\frac{cov(X,Y)}{s_X s_Y} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\nManual Calculation Example:\nUsing the same data as above:\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate covariance\n(From previous calculation) 1.5\n\n\n2\nCalculate standard deviations\ns_X = \\sqrt{\\frac{10}{4}} = 1.58, s_Y = \\sqrt{\\frac{6}{4}} = 1.22\n\n\n3\nDivide covariance by product of standard deviations\n1.5 / (1.58 * 1.22) = 0.7746\n\n\n\nR calculation:\n\ncor(x, y, method = \"pearson\")\n\n[1] 0.7745967\n\n\nInterpretation: - The correlation coefficient of 0.7746 indicates a strong positive linear relationship between x and y.\nPros:\n\nScale-independent, always between -1 and 1\nWidely understood and used\nTests for linear relationships\n\nCons:\n\nSensitive to outliers\nOnly measures linear relationships\nAssumes normally distributed variables\n\n\n\n15.1.14 Spearman Correlation\nSpearman correlation measures the strength and direction of the monotonic relationship between two variables, which can be continuous or ordinal.\nFormula: \\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}, where d_i is the difference between ranks.\nManual Calculation Example:\nLet’s use slightly different data:\n\nx: 1, 2, 3, 4, 5\ny: 1, 3, 2, 5, 4\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nRank both variables\nx_rank: 1, 2, 3, 4, 5\n\n\n\n\ny_rank: 1, 3, 2, 5, 4\n\n\n2\nCalculate differences in ranks (d)\n0, -1, 1, -1, 1\n\n\n3\nSquare the differences\n0, 1, 1, 1, 1\n\n\n4\nSum the squared differences\n\\sum d_i^2 = 4\n\n\n5\nApply the formula\n\\rho = 1 - \\frac{6(4)}{5(5^2 - 1)} = 0.8\n\n\n\nR calculation:\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(1, 3, 2, 5, 4)\ncor(x, y, method = \"spearman\")\n\n[1] 0.8\n\n\nInterpretation: - The Spearman correlation of 0.8 indicates a strong positive monotonic relationship between x and y.\nPros:\n\nRobust to outliers\nCan detect non-linear monotonic relationships\nSuitable for ordinal data\n\nCons:\n\nLess powerful than Pearson for detecting linear relationships in normally distributed data\nDoesn’t provide information about the shape of the relationship beyond monotonicity\n\n\n\n15.1.15 Cross-tabulation\nCross-tabulation (contingency table) shows the relationship between two categorical variables.\nExample:\nLet’s create a cross-tabulation of two variables: - Education level: High School, College, Graduate - Employment status: Employed, Unemployed\n\neducation &lt;- factor(c(\"High School\", \"College\", \"Graduate\", \"High School\", \"College\", \"Graduate\", \"High School\", \"College\", \"Graduate\"))\nemployment &lt;- factor(c(\"Employed\", \"Employed\", \"Employed\", \"Unemployed\", \"Employed\", \"Employed\", \"Unemployed\", \"Unemployed\", \"Employed\"))\n\ntable(education, employment)\n\n             employment\neducation     Employed Unemployed\n  College            2          1\n  Graduate           3          0\n  High School        1          2\n\n\nInterpretation:\n\nThis table shows the count of individuals in each combination of education level and employment status.\nFor example, we can see how many high school graduates are employed versus unemployed.\n\nPros:\n\nProvides a clear visual representation of the relationship between categorical variables\nEasy to understand and interpret\nBasis for many statistical tests (e.g., chi-square test of independence)\n\nCons:\n\nLimited to categorical data\nCan become unwieldy with many categories\nDoesn’t provide a single summary statistic of association strength\n\n\n\n15.1.16 Choosing the Appropriate Measure\nWhen deciding which bivariate statistic to use, consider:\n\nData type:\n\nContinuous data: Covariance, Pearson correlation\nOrdinal data: Spearman correlation\nCategorical data: Cross-tabulation\n\nRelationship type:\n\nLinear: Pearson correlation\nMonotonic but potentially non-linear: Spearman correlation\n\nPresence of outliers:\n\nIf outliers are a concern, Spearman correlation is more robust\n\nDistribution:\n\nFor normally distributed data, Pearson correlation is most powerful\nFor non-normal distributions, consider Spearman correlation\n\nSample size:\n\nFor very small samples, non-parametric methods like Spearman correlation might be preferred\n\n\nRemember, it’s often valuable to use multiple measures and visualizations (like scatter plots) to get a comprehensive understanding of the relationship between variables.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#introduction-to-elementary-multivariate-statistics",
    "href": "correg_en.html#introduction-to-elementary-multivariate-statistics",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.2 Introduction to Elementary Multivariate Statistics (*)",
    "text": "15.2 Introduction to Elementary Multivariate Statistics (*)\nMultivariate statistics involve the analysis of relationships among three or more variables simultaneously. This section will introduce some basic concepts and techniques in multivariate analysis, with a focus on correlation-based methods.\n\n15.2.1 Correlation Matrix\nA correlation matrix is a table showing the pairwise correlations of several variables. It’s a fundamental tool in multivariate analysis.\nExample: Let’s create a correlation matrix for four variables: height, weight, age, and income.\n\nset.seed(123)  # For reproducibility\nheight &lt;- rnorm(100, 170, 10)\nweight &lt;- height * 0.5 + rnorm(100, 0, 5)\nage &lt;- rnorm(100, 40, 10)\nincome &lt;- age * 1000 + rnorm(100, 0, 10000)\n\ndata &lt;- data.frame(height, weight, age, income)\n\ncor_matrix &lt;- cor(data)\nprint(cor_matrix)\n\n           height      weight         age      income\nheight  1.0000000  0.66712996 -0.12917601 -0.12246786\nweight  0.6671300  1.00000000 -0.06814187 -0.04579492\nage    -0.1291760 -0.06814187  1.00000000  0.65654902\nincome -0.1224679 -0.04579492  0.65654902  1.00000000\n\n\nInterpretation:\n\nEach cell shows the correlation between two variables.\nThe diagonal is always 1 (correlation of a variable with itself).\nLook for strong correlations (close to 1 or -1) to identify potential relationships.\n\n\n\n15.2.2 Visualizing Multivariate Relationships\n\n15.2.2.1 Scatterplot Matrix\nA scatterplot matrix shows pairwise relationships between multiple variables.\n\npairs(data)\n\n\n\n\n\n\n\n\nInterpretation:\n\nEach plot shows the relationship between two variables.\nDiagonal elements show the distribution of each variable.\nLook for patterns, clusters, or trends in the plots.\n\n\n\n15.2.2.2 Correlation Plot\nA correlation plot provides a visual representation of the correlation matrix.\n\nlibrary(corrplot)\n\ncorrplot 0.94 loaded\n\ncorrplot(cor_matrix, method = \"color\")\n\n\n\n\n\n\n\n\nInterpretation:\n\nColor intensity and size of the circles indicate the strength of correlation.\nBlue colors typically indicate positive correlations, red colors indicate negative correlations.\n\n\n\n\n15.2.3 Partial Correlation\nPartial correlation measures the relationship between two variables while controlling for one or more other variables.\nExample: Let’s calculate the partial correlation between height and weight, controlling for age.\n\nlibrary(ppcor)\npcor.test(data$height, data$weight, data$age)\n\n   estimate      p.value statistic   n gp  Method\n1 0.6654367 5.758157e-14  8.779896 100  1 pearson\n\n\nInterpretation:\n\nCompare this to the simple correlation between height and weight.\nA significant change might indicate that age plays a role in the height-weight relationship.\n\n\n\n15.2.4 Multiple Correlation\nMultiple correlation measures the strength of the relationship between a dependent variable and multiple independent variables.\nExample: Let’s predict weight using height and age.\n\nmodel &lt;- lm(weight ~ height + age, data = data)\nR &lt;- sqrt(summary(model)$r.squared)\nprint(paste(\"Multiple correlation coefficient:\", R))\n\n[1] \"Multiple correlation coefficient: 0.667377840470434\"\n\n\nInterpretation:\n\nR ranges from 0 to 1, with higher values indicating stronger relationships.\nR² (R-squared) represents the proportion of variance in the dependent variable explained by the independent variables.\n\n\n\n15.2.5 Factor Analysis\nFactor analysis is a technique used to reduce many variables to a smaller number of underlying factors.\nExample: Let’s perform a simple factor analysis on our dataset.\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nfa_result &lt;- fa(data, nfactors = 2, rotate = \"varimax\")\nprint(fa_result$loadings, cutoff = 0.3)\n\n\nLoadings:\n       MR2    MR1   \nheight  0.798       \nweight  0.836       \nage            0.729\nincome         0.895\n\n                 MR2   MR1\nSS loadings    1.344 1.341\nProportion Var 0.336 0.335\nCumulative Var 0.336 0.671\n\n\nInterpretation:\n\nLook at which variables load highly on each factor.\nTry to interpret what each factor might represent based on the variables that load on it.\n\n\n\n15.2.6 Considerations in Multivariate Analysis\n\nSample Size: Multivariate techniques often require larger sample sizes for stable results.\nMulticollinearity: High correlations among independent variables can cause issues in some analyses.\nOutliers: Multivariate outliers can have a strong influence on results.\nAssumptions: Many techniques assume multivariate normality and linear relationships.\nInterpretation Complexity: As the number of variables increases, interpretation can become more challenging.\n\n\n\n15.2.7 Conclusion\nThis introduction to multivariate statistics builds upon the concept of correlation to explore relationships among multiple variables. These techniques provide powerful tools for understanding complex datasets, but they also require careful consideration of assumptions and limitations. As you progress in your statistical journey, you’ll encounter more advanced multivariate techniques such as MANOVA, discriminant analysis, and structural equation modeling.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n✔ purrr     1.0.2     ✔ tibble    3.2.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ psych::%+%()         masks ggplot2::%+%()\n✖ psych::alpha()       masks ggplot2::alpha()\n✖ gridExtra::combine() masks dplyr::combine()\n✖ dplyr::filter()      masks stats::filter()\n✖ dplyr::lag()         masks stats::lag()\n✖ MASS::select()       masks dplyr::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(gridExtra)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#introduction-to-regression-analysis",
    "href": "correg_en.html#introduction-to-regression-analysis",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.3 Introduction to Regression Analysis",
    "text": "15.3 Introduction to Regression Analysis\nRegression analysis is a fundamental statistical method that examines and models the relationship between variables to understand how changes in one or more independent variables influence a dependent variable.\nThe Core Concept\nAt its heart, regression analysis helps us answer questions about cause and effect, prediction, and forecasting. For example, a business might use regression analysis to understand how advertising spending affects sales, or how employee training hours impact productivity.\nHow Regression Analysis Works\nThe process begins by collecting data about the variables of interest. The analysis then fits a mathematical model—typically a line or curve—that best represents the relationship between these variables. This model allows us to:\n\nIdentify the strength and direction of relationships between variables\nMake predictions about future outcomes\nUnderstand which factors have the most significant impact on our results\n\n\n15.3.1 Regression as a Stochastic Model\nIn mathematical modeling, we encounter two fundamental approaches to describing relationships between variables:\n\ndeterministic models,\nstochastic models.\n\n\n15.3.1.1 Deterministic vs. Stochastic Models\nA deterministic model assumes a precise, fixed relationship between inputs and outputs. In such models, if we know the inputs, we can calculate the exact output with certainty. Consider the classic physics equation for distance:\n\\text{Distance} = \\text{Speed} × \\text{Time}\nGiven specific values for speed and time, this equation will always yield the same distance. There is no room for variation in the outcome.\nIn contrast, regression analysis embraces the presence of natural variation in data. The fundamental structure of a regression model is:\nY = f(X) + \\epsilon\nWhere:\n\nY represents the outcome we wish to predict\nf(X) represents the systematic relationship between our predictors (X) and the outcome\n\\epsilon represents the random variation that naturally occurs in real-world data\n\n\n\n15.3.1.2 The Nature of Stochastic Models in Regression\nThe inclusion of the error term \\epsilon acknowledges that real relationships between variables are rarely perfect. For example, when studying how study time affects test scores, many factors contribute to the final outcome:\nThe systematic part f(X) captures the general trend: more study time tends to lead to higher scores.\nThe error term \\epsilon accounts for all other influences:\n\nDifferent learning styles\nStudy environment\nPhysical and mental state during the test\nQuality of study materials\n\n\n\n15.3.1.3 Mathematical Framework\nIn its simplest form, linear regression can be expressed as:\nY = \\beta_0 + \\beta_1X + \\epsilon\nWhere:\n\n\\beta_0 represents the baseline value (when X = 0)\n\\beta_1 represents the change in Y for each unit increase in X\n\\epsilon represents the natural variation around this relationship\n\n\n\n15.3.1.4 Practical Implications\nUnderstanding regression as a stochastic model has important practical implications:\n\nPredictions: We recognize that our predictions will have some natural variation around them. Rather than claiming an exact outcome, we acknowledge a range of plausible values.\nModel Evaluation: We assess models by how well they capture both the general trend and the typical amount of variation in the data.\nDecision Making: Understanding the natural variation in our predictions helps us make more realistic plans and decisions.\n\nReal-World Applications\nConsider predicting house prices based on square footage. A deterministic model might say: “A 2000 sq ft house will sell for exactly $300,000”\nA regression model instead recognizes that:\n\nThere’s a general relationship between size and price\nBut many other factors affect the final price\nSimilar houses might sell for somewhat different prices\nOur predictions should reflect this natural variation\n\nConclusion\nThe stochastic nature of regression analysis provides a more realistic framework for understanding real-world relationships between variables. By explicitly accounting for natural variation, regression analysis helps us:\n\nMake more realistic predictions\nBetter understand the limitations of our models\nMake more informed decisions\n\n\n\n\n15.3.2 Why Study Regression?\nRegression analysis is a fundamental statistical tool that helps us understand relationships between variables. Before diving into formulas and technical details, let’s understand what questions regression can help us answer:\n\nHow much does each additional year of education affect someone’s salary?\nWhat is the relationship between advertising spending and sales?\nHow does temperature affect energy consumption?\nDo study hours predict exam scores?\n\nThese questions share a common structure: they all explore how changes in one variable relate to changes in another.\n\n\n\n\n\n\nUnderstanding Linear Regression (OLS): Quickstart guide\n\n\n\n\n15.3.3 The Model Concept\nOLS regression is a statistical model that describes the relationship between variables. Two key assumptions define this model:\n\nThe relationship can be described by a straight line (linearity)\nThe errors in our predictions are not systematically related to our x-variable (strict exogeneity)\n\n\n\n15.3.4 Example: Education and Wages\nConsider studying the effect of education (x) on wages (y). Let’s say we estimate:\nwages = \\beta_0 + \\beta_1 \\cdot education + \\epsilon\nThe error term \\epsilon contains all other factors affecting wages. Strict exogeneity is violated if we omit an important variable like “ability” that affects both education and wages. Why? Because more able people tend to get more education AND higher wages, making our estimate of education’s effect biased upward.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Generate sample data\nset.seed(123)\nn &lt;- 20\nx &lt;- seq(1, 10, length.out = n)\ny &lt;- 2 + 1.5 * x + rnorm(n, sd = 1.5)\ndata &lt;- data.frame(x = x, y = y)\n\n# Calculate OLS parameters\nbeta1 &lt;- cov(x, y) / var(x)\nbeta0 &lt;- mean(y) - beta1 * mean(x)\n\n# Create alternative lines\nlines_data &lt;- data.frame(\n  intercept = c(beta0, beta0 + 1, beta0 - 1),\n  slope = c(beta1, beta1 + 0.3, beta1 - 0.3),\n  line_type = c(\"Best fit (OLS)\", \"Suboptimal 1\", \"Suboptimal 2\")\n)\n\n# Create the plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point(size = 3, alpha = 0.6) +\n  geom_abline(data = lines_data,\n              aes(intercept = intercept, \n                  slope = slope,\n                  color = line_type,\n                  linetype = line_type),\n              size = 1) +\n  scale_color_manual(values = c(\"Best fit (OLS)\" = \"#FF4500\",\n                               \"Suboptimal 1\" = \"#4169E1\",\n                               \"Suboptimal 2\" = \"#228B22\")) +\n  labs(title = \"Finding the Best-Fitting Line\",\n       subtitle = \"Orange line minimizes the sum of squared errors\",\n       x = \"X Variable\",\n       y = \"Y Variable\",\n       color = \"Line Type\",\n       linetype = \"Line Type\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12),\n    legend.position = \"bottom\",\n    panel.grid.minor = element_blank()\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n15.3.5 The Optimization Problem: Understanding OLS\nWhen we analyze relationships between variables like education and wages, we need a systematic method to find the line that best represents this relationship in our data. Ordinary Least Squares (OLS) provides this method through a clear mathematical approach.\nConsider our plot of education levels and wages. Each point represents actual data - one person’s years of education and their corresponding wage. Our goal is to find a single line that most accurately captures the underlying relationship between these variables.\nFor any given observation i, we can express this relationship as: y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\nWhere:\n\ny_i is the actual observed wage\n\\hat{y_i} = \\beta_0 + \\beta_1x_i is our predicted wage\n\\epsilon_i = y_i - \\hat{y_i} is the error term (or residual)\n\nOLS finds the optimal values for \\beta_0 and \\beta_1 by minimizing the sum of squared errors:\n\\min_{\\beta_0, \\beta_1} \\sum \\epsilon_i^2 = \\min_{\\beta_0, \\beta_1} \\sum(y_i - \\hat{y_i})^2 = \\min_{\\beta_0, \\beta_1} \\sum(y_i - (\\beta_0 + \\beta_1x_i))^2\nLooking at our visualization:\n\nThe scattered points show our actual observations (x_i, y_i)\nThe red line represents our fitted values \\hat{y_i} that minimize \\sum \\epsilon_i^2\nThe blue and green lines represent alternative fits with larger total squared errors\nThe vertical distances from each point to these lines represent the errors \\epsilon_i\n\nThe OLS solution provides us with parameter estimates \\hat{\\beta_0} and \\hat{\\beta_1} that minimize the total squared error, giving us the most accurate linear representation of the relationship between education and wages based on our available data.\n\n\n15.3.6 Finding the Best Line\nThe solution to this minimization gives us:\n\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = \\frac{cov(X, Y)}{var(X)}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\n\n\n15.3.7 Key Points\n\nOLS finds the line that minimizes squared prediction errors\nThis line is “best” in terms of fit, but might not capture true relationships if important variables are omitted\nIn the education-wages example, omitting ability means we attribute all the wage increase to education alone\n\n\n\n\n\n\n15.3.8 Basic Concepts and Terminology\nLet’s establish our key terms:\n\nDependent Variable (Y):\n\nThe outcome we want to understand or predict\nAlso called: response variable, target variable\nExamples: salary, sales, exam scores\n\nIndependent Variable (X):\n\nThe variable we think influences Y\nAlso called: predictor, explanatory variable, regressor\nExamples: years of education, advertising budget, study hours\n\nPopulation Parameters (\\beta):\n\nThe true underlying relationships we want to understand\nUsually unknown in practice\nExamples: \\beta_0 (true intercept), \\beta_1 (true slope)\n\nParameter Estimates (\\hat{\\beta}):\n\nOur best guesses of the true parameters based on data\nCalculated from sample data\nExamples: \\hat{\\beta}_0 (estimated intercept), \\hat{\\beta}_1 (estimated slope)\n\n\n\n\n15.3.9 The Core Idea\nLet’s visualize what regression does with a simple example:\n\n# Generate some example data\nset.seed(123)\nx &lt;- seq(1, 10, by = 0.5)\ny &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit the model\nmodel &lt;- lm(y ~ x, data = data)\n\n# Create the plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Simple Linear Regression Example\",\n       subtitle = \"Points represent data, red line shows regression fit\",\n       x = \"Independent Variable (X)\",\n       y = \"Dependent Variable (Y)\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 15.1: Basic Idea of Regression: Fitting a Line to Data\n\n\n\n\n\nThis plot shows the essence of regression:\n\nEach point represents an observation (X, Y)\nThe line represents our best guess at the relationship\nThe spread of points around the line shows the uncertainty",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-linear-regression-model",
    "href": "correg_en.html#the-linear-regression-model",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.4 The Linear Regression Model",
    "text": "15.4 The Linear Regression Model\n\n15.4.1 Population Model vs. Sample Estimates\nIn theory, there exists a true population relationship:\nY = \\beta_0 + \\beta_1X + \\varepsilon\nwhere:\n\n\\beta_0 is the true population intercept\n\\beta_1 is the true population slope\n\\varepsilon is the random error term\n\nIn practice, we work with sample data to estimate this relationship:\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X\nLet’s visualize the difference between population and sample relationships:\n\n# Generate population data\nset.seed(456)\nx_pop &lt;- seq(1, 10, by = 0.1)\ntrue_relationship &lt;- 2 + 3*x_pop  # True β₀=2, β₁=3\ny_pop &lt;- true_relationship + rnorm(length(x_pop), 0, 2)\n\n# Create several samples\nsample_size &lt;- 30\nsamples &lt;- data.frame(\n  x = rep(sample(x_pop, sample_size), 3),\n  sample = rep(1:3, each = sample_size)\n)\n\nsamples$y &lt;- 2 + 3*samples$x + rnorm(nrow(samples), 0, 2)\n\n# Fit models to each sample\nmodels &lt;- samples %&gt;%\n  group_by(sample) %&gt;%\n  summarise(\n    intercept = coef(lm(y ~ x))[1],\n    slope = coef(lm(y ~ x))[2]\n  )\n\n# Plot\nggplot() +\n  geom_point(data = samples, aes(x = x, y = y, color = factor(sample)), \n             alpha = 0.5) +\n  geom_abline(data = models, \n              aes(intercept = intercept, slope = slope, \n                  color = factor(sample)),\n              linetype = \"dashed\") +\n  geom_line(aes(x = x_pop, y = true_relationship), \n            color = \"black\", size = 1) +\n  theme_minimal() +\n  labs(title = \"Population vs. Sample Regression Lines\",\n       subtitle = \"Black line: true population relationship\\nDashed lines: sample estimates\",\n       x = \"X\", y = \"Y\",\n       color = \"Sample\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 15.2: Population vs. Sample Regression Lines\n\n\n\n\n\nThis visualization shows:\n\nThe true population line (black) we’re trying to discover\nDifferent sample estimates (dashed lines) based on different samples\nHow sample estimates vary around the true relationship",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#key-assumptions-of-linear-regression",
    "href": "correg_en.html#key-assumptions-of-linear-regression",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.5 Key Assumptions of Linear Regression",
    "text": "15.5 Key Assumptions of Linear Regression\n\n15.5.1 Strict Exogeneity: The Fundamental Assumption\nThe most crucial assumption in regression is strict exogeneity:\nE[\\varepsilon|X] = 0\nThis means:\n\nThe error term has zero mean conditional on X\nX contains no information about the average error\nThere are no systematic patterns in how our predictions are wrong\n\nLet’s visualize when this assumption holds and when it doesn’t:\n\n# Generate data\nset.seed(789)\nx &lt;- seq(1, 10, by = 0.2)\n\n# Case 1: Exogenous errors\ny_exog &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\n\n# Case 2: Non-exogenous errors (error variance increases with x)\ny_nonexog &lt;- 2 + 3*x + 0.5*x*rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_exog &lt;- data.frame(\n  x = x,\n  y = y_exog,\n  type = \"Exogenous Errors\\n(Assumption Satisfied)\"\n)\n\ndata_nonexog &lt;- data.frame(\n  x = x,\n  y = y_nonexog,\n  type = \"Non-Exogenous Errors\\n(Assumption Violated)\"\n)\n\ndata_combined &lt;- rbind(data_exog, data_nonexog)\n\n# Create plots with residuals\nplot_residuals &lt;- function(data, title) {\n  model &lt;- lm(y ~ x, data = data)\n  data$predicted &lt;- predict(model)\n  data$residuals &lt;- residuals(model)\n  \n  p1 &lt;- ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    theme_minimal() +\n    labs(title = title)\n  \n  p2 &lt;- ggplot(data, aes(x = x, y = residuals)) +\n    geom_point() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    labs(y = \"Residuals\")\n  \n  list(p1, p2)\n}\n\n# Generate plots\nplots_exog &lt;- plot_residuals(data_exog, \"Exogenous Errors\")\nplots_nonexog &lt;- plot_residuals(data_nonexog, \"Non-Exogenous Errors\")\n\n# Arrange plots\ngridExtra::grid.arrange(\n  plots_exog[[1]], plots_exog[[2]],\n  plots_nonexog[[1]], plots_nonexog[[2]],\n  ncol = 2\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 15.3: Exogeneity vs. Non-Exogeneity Examples\n\n\n\n\n\n\n\n15.5.2 Linearity: The Form Assumption\nThe relationship between X and Y should be linear in parameters:\nE[Y|X] = \\beta_0 + \\beta_1X\nNote that this doesn’t mean X and Y must have a straight-line relationship - we can transform variables. Let’s see different types of relationships:\n\n# Generate data\nset.seed(101)\nx &lt;- seq(1, 10, by = 0.1)\n\n# Different relationships\ndata_relationships &lt;- data.frame(\n  x = rep(x, 3),\n  y = c(\n    # Linear\n    2 + 3*x + rnorm(length(x), 0, 2),\n    # Quadratic\n    2 + 0.5*x^2 + rnorm(length(x), 0, 2),\n    # Exponential\n    exp(0.3*x) + rnorm(length(x), 0, 2)\n  ),\n  type = rep(c(\"Linear\", \"Quadratic\", \"Exponential\"), each = length(x))\n)\n\n# Plot\nggplot(data_relationships, aes(x = x, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  facet_wrap(~type, scales = \"free_y\") +\n  theme_minimal() +\n  labs(subtitle = \"Red: linear fit, Blue: true relationship\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 15.4: Linear and Nonlinear Relationships\n\n\n\n\n\n\n\n15.5.3 Understanding Violations and Solutions\nWhen linearity is violated:\n\nTransform variables:\n\nLog transformation: for exponential relationships\nSquare root: for moderate nonlinearity\nPower transformations: for more complex relationships\n\n\n\n# Generate exponential data\nset.seed(102)\nx &lt;- seq(1, 10, by = 0.2)\ny &lt;- exp(0.3*x) + rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_trans &lt;- data.frame(\n  x = x,\n  y = y,\n  log_y = log(y)\n)\n\nWarning in log(y): NaNs produced\n\n# Original scale plot\np1 &lt;- ggplot(data_trans, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Original Scale\")\n\n# Log scale plot\np2 &lt;- ggplot(data_trans, aes(x = x, y = log_y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Log-Transformed Y\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 15.5: Effect of Variable Transformations\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Ordinary Least Squares (OLS) Intuitively\n\n\n\n\n15.5.4 The Basic Problem\nLet’s start with a real-world scenario: understanding how study time affects exam performance. We collect data from your class where:\n\nEach student records their study hours (x), and their final exam score (y)\nSo student 1 might have studied x_1 = 3 hours and scored y_1 = 75 points\nStudent 2 might have studied x_2 = 5 hours and scored y_2 = 82 points\nAnd so on for all n students in the class\n\nOur goal is to find a straight line that best describes this relationship. We’re trying to estimate the true relationship (which we never know exactly) using our sample of data. Let’s explore this step by step.\n\nlibrary(tidyverse)\n\n# Create sample data\nset.seed(123)\nstudy_hours &lt;- runif(20, 1, 8)\nexam_scores &lt;- 60 + 5 * study_hours + rnorm(20, 0, 5)\ndata &lt;- data.frame(study_hours, exam_scores)\n\n# Basic scatter plot with multiple lines\nggplot(data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  labs(x = \"Study Hours\", y = \"Exam Scores\",\n       title = \"Your Class Data: Study Hours vs. Exam Scores\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n\n\n\n\n\n\n\n\n\n15.5.5 What Makes a Line “Good”?\nAny straight line can be written in the form:\ny = \\hat{\\beta}_0 + \\hat{\\beta}_1x\nWhere:\n\n\\hat{\\beta}_0 is our estimate of the y-intercept (the predicted score for zero study hours)\n\\hat{\\beta}_1 is our estimate of the slope (how many points you gain per extra hour of study)\nThe hats (^) indicate these are our estimates of the true (unknown) parameters \\beta_0 and \\beta_1\n\nLet’s look at three possible lines through our data:\n\nggplot(data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  geom_abline(intercept = 50, slope = 8, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_abline(intercept = 70, slope = 2, color = \"green\", linetype = \"dashed\", size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"purple\") +\n  labs(x = \"Study Hours\", y = \"Exam Scores\",\n       title = \"Three Different Lines: Which is Best?\") +\n  annotate(\"text\", x = 7.5, y = 120, color = \"red\", label = \"Line A: Too Steep\") +\n  annotate(\"text\", x = 7.5, y = 85, color = \"green\", label = \"Line B: Too Flat\") +\n  annotate(\"text\", x = 7.5, y = 100, color = \"purple\", label = \"Line C: Just Right\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n15.5.6 Understanding Prediction Errors (Residuals)\nHere’s where the magic of OLS begins. For each student in our data:\n\nWe look at their actual exam score (y_i)\nWe calculate their predicted score using our line (\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nThe difference between these is called a residual:\n\n\\text{residual}_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nLet’s visualize these residuals for one line:\n\n# Fit the model and show residuals\nmodel &lt;- lm(exam_scores ~ study_hours, data = data)\n\nggplot(data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"purple\") +\n  geom_segment(aes(xend = study_hours, \n                  yend = predict(model, data)),\n              color = \"orange\", alpha = 0.5) +\n  labs(x = \"Study Hours\", y = \"Exam Scores\",\n       title = \"Understanding Residuals: The Gaps Between Predictions and Reality\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe orange vertical lines show how far off our predictions are for each student. Some predictions are too high (positive residuals), others too low (negative residuals).\n\n\n15.5.7 Why Do We Square the Residuals?\nThis is a crucial concept! Let’s walk through it with a simple example:\nImagine we have just two students:\n\nAlice: Predicted 80, actual score 85 (residual = +5)\nBob: Predicted 90, actual score 85 (residual = -5)\n\nIf we just add these residuals: (+5) + (-5) = 0\nThis would suggest our line is perfect (total error = 0), but we know it’s not! Both predictions were off by 5 points.\nSolution: Square the residuals before adding them:\n\nAlice’s squared residual: (+5)^2 = 25\nBob’s squared residual: (-5)^2 = 25\nTotal squared error: 25 + 25 = 50\n\nThis gives us a much better measure of how wrong our predictions are!\n\n\n15.5.8 Sum of Squared Errors (SSE)\nThe Sum of Squared Errors (SSE) represents a fundamental measure of fit in linear regression modeling. We can express it mathematically as:\nSSE = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nwhere:\n\ny_i represents the actual value of the dependent variable for the i-th observation\n\\hat{\\beta}_0 represents the estimated intercept (Y-axis intersection point)\n\\hat{\\beta}_1 represents the estimated slope coefficient\nx_i represents the value of the independent variable for the i-th observation\n\nThe process of calculating SSE follows these methodical steps:\n\nFor each observation, we calculate the difference between the actual value (y_i) and the value predicted by our model (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i). This difference is termed the residual.\nWe square each residual, which produces several important effects:\n\n\nAll values become positive, eliminating the possibility of negative and positive errors canceling each other out\nLarger errors receive proportionally greater weight than smaller ones\nThe units of measurement become squared\n\n\nWe sum all these squared residuals, producing a single number that represents the model’s total error.\n\nThe interpretation of SSE is straightforward: a smaller SSE indicates better model fit to the empirical data.\nAn SSE value of 0 would indicate perfect fit, where all points lie exactly on the regression line. However, in practice, such perfect fit rarely occurs and might actually indicate a problematic overfit of the model.\nSSE serves as the foundation for calculating other important measures of model fit quality, such as the coefficient of determination (R²) and the standard error of estimate. It provides a quantitative basis for comparing different models and assessing the accuracy of our predictions.\nUnderstanding SSE is crucial for model evaluation and refinement, as it helps identify how well our model captures the underlying patterns in our data while avoiding the pitfalls of both underfitting and overfitting.\n\n# Compare good vs bad fit\nbad_predictions &lt;- 70 + 2 * data$study_hours\ngood_predictions &lt;- predict(model, data)\n\nbad_sse &lt;- sum((data$exam_scores - bad_predictions)^2)\ngood_sse &lt;- sum((data$exam_scores - good_predictions)^2)\n\nggplot(data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  geom_abline(intercept = 70, slope = 2, color = \"red\", \n              linetype = \"dashed\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"purple\") +\n  annotate(\"text\", x = 2, y = 95, \n           label = paste(\"Red Line: Total Error =\", round(bad_sse)), \n           color = \"red\") +\n  annotate(\"text\", x = 2, y = 90, \n           label = paste(\"Purple Line: Total Error =\", round(good_sse)), \n           color = \"purple\") +\n  labs(x = \"Study Hours\", y = \"Exam Scores\",\n       title = \"Comparing Total Prediction Errors\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n15.5.9 Why Is This Method “Ordinary Least Squares”?\nLet’s break down the name:\n\n“Squares”: We square the residuals\n“Least”: We want the smallest possible total\n“Ordinary”: This is the basic version (there are fancier versions!)\n\nThe OLS line has some nice properties:\n\nThe mean of all residuals equals zero\nThe line always passes through the point (\\bar{x}, \\bar{y}) - the average study hours and average score\nSmall changes in the data lead to small changes in the line (it’s “stable”)\nOur estimates \\hat{\\beta}_0 and \\hat{\\beta}_1 are the best possible estimates of the true parameters \\beta_0 and \\beta_1\n\n\n\n15.5.10 Important Notes\n\nThe hat notation (\\hat{\\beta}_0, \\hat{\\beta}_1) reminds us that we’re estimating the true relationship from our sample. We never know the true \\beta_0 and \\beta_1 - we can only estimate them from our data.\nOLS gives us the best possible estimates when certain conditions are met (like having randomly sampled data and a truly linear relationship).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#assessing-model-fit",
    "href": "correg_en.html#assessing-model-fit",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.6 Assessing Model Fit",
    "text": "15.6 Assessing Model Fit\n\n15.6.1 Decomposition of Variance\nThe total variability in Y can be broken down into explained and unexplained components:\n\\underbrace{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}_{SST} = \\underbrace{\\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2}_{SSR} + \\underbrace{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}_{SSE}\nwhere:\n\nSST (Total Sum of Squares): Total variation in Y\nSSR (Regression Sum of Squares): Variation explained by regression\nSSE (Error Sum of Squares): Unexplained variation\n\n\n\n15.6.2 Understanding the Three Types of Variation\n\nTotal Variation (SST)\n\nQuestion: “How much do observations vary from the mean?”\nFormula: SST = \\sum(y_i - \\bar{y})^2\nIntuition: The “spread” of our data around its average\n\nExplained Variation (SSR)\n\nQuestion: “How much of the variation did our model explain?”\nFormula: SSR = \\sum(\\hat{y}_i - \\bar{y})^2\nIntuition: The improvement we gained by using X\n\nUnexplained Variation (SSE)\n\nQuestion: “What variation remains unexplained?”\nFormula: SSE = \\sum(y_i - \\hat{y}_i)^2\nIntuition: The errors that remain after using X\n\n\n\n\n\n\n\n\nUnderstanding Variance Decomposition in Linear Regression\n\n\n\n\n15.6.3 Why It Matters: Understanding Prediction Improvement Through Additional Variables\nIn statistical analysis and modeling, we frequently encounter situations where we must assess the value of incorporating additional predictor variables. Consider the domain of real estate valuation: While one could estimate property values using the market-wide average price, incorporating specific property characteristics, such as square footage, can substantially enhance prediction accuracy. Variance decomposition provides a rigorous framework for quantifying the incremental improvement in predictive accuracy when we incorporate such additional variables.\n\n\n15.6.4 The Analytical Process: From Baseline to Enhanced Predictions\n\n15.6.4.1 Initial Estimation: The Unconditional Mean\nThe analytical process begins with the simplest possible predictor: the mean of all observed values (\\bar{y}). This represents our initial estimation in the absence of any predictor variables. When we employ this method, we assign the same value—the overall mean—to each observation, resulting in what we term baseline deviations.\n\n\n15.6.4.2 Enhanced Prediction: Incorporating Predictor Variables\nBy introducing predictor variables (denoted as X in our figure), we refine our initial estimates. This enhancement enables us to generate distinct predictions for each observation, typically resulting in substantially reduced prediction errors, as illustrated in the figure below.\n\n\n\n15.6.5 Components of Statistical Variation\nOur figure illustrates three fundamental components of variation:\n\nTotal Sum of Squares (SST)\n\nStatistical Inquiry: What is the aggregate deviation of observations from their mean?\nMathematical Expression: SST = \\sum(y_i - \\bar{y})^2\nGraphical Representation: Depicted by the purple vertical lines in the figure\nStatistical Interpretation: Represents the total variability present in the observed data prior to any modeling\n\nRegression Sum of Squares (SSR)\n\nStatistical Inquiry: What proportion of variation does our model successfully capture?\nMathematical Expression: SSR = \\sum(\\hat{y}_i - \\bar{y})^2\nGraphical Representation: Illustrated by the green dashed lines in the figure\nStatistical Interpretation: Quantifies the reduction in uncertainty achieved through the incorporation of predictor variables\n\nError Sum of Squares (SSE)\n\nStatistical Inquiry: What residual variation persists after model implementation?\nMathematical Expression: SSE = \\sum(y_i - \\hat{y}_i)^2\nGraphical Representation: Shown by the orange dashed lines in the figure\nStatistical Interpretation: Represents the remaining unexplained variation after accounting for all predictor variables\n\n\nThis decomposition provides a formal framework for assessing the effectiveness of our predictive model and the value of including additional variables.\n\n15.6.5.1 Visualizing Variance Decomposition\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(patchwork)\n\n\nAttaching package: 'patchwork'\n\n\nThe following object is masked from 'package:MASS':\n\n    area\n\n# Generate data with clearer pattern\nset.seed(123)\nx &lt;- seq(1, 10, length.out = 50)\ny &lt;- 2 + 0.5 * x + rnorm(50, sd = 0.8)\ndata &lt;- data.frame(x = x, y = y)\n\n# Model and calculations\nmodel &lt;- lm(y ~ x, data)\nmean_y &lt;- mean(y)\ndata$predicted &lt;- predict(model)\n\n# Select specific points for demonstration that are well-spaced\ndemonstration_points &lt;- c(8, 25, 42)  # Changed points for better spacing\n\n# Create main plot with improved aesthetics\np1 &lt;- ggplot(data, aes(x = x, y = y)) +\n  # Add background grid for better readability\n  geom_hline(yintercept = seq(0, 8, by = 0.5), color = \"gray90\", linewidth = 0.2) +\n  geom_vline(xintercept = seq(0, 10, by = 0.5), color = \"gray90\", linewidth = 0.2) +\n  \n  # Add regression line and mean line\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E41A1C\", linewidth = 1.2) +\n  geom_hline(yintercept = mean_y, linetype = \"longdash\", color = \"#377EB8\", linewidth = 1) +\n  \n  # Add data points\n  geom_point(size = 3, alpha = 0.6, color = \"#4A4A4A\") +\n  \n  # Add decomposition segments with improved colors and positioning\n  # Total deviation (purple)\n  geom_segment(data = data[demonstration_points,],\n              aes(x = x, xend = x, y = y, yend = mean_y),\n              color = \"#984EA3\", linetype = \"dashed\", linewidth = 1.8) +\n  # Explained component (green)\n  geom_segment(data = data[demonstration_points,],\n              aes(x = x, xend = x, y = mean_y, yend = predicted),\n              color = \"#4DAF4A\", linetype = \"dashed\", linewidth = 1) +\n  # Unexplained component (orange)\n  geom_segment(data = data[demonstration_points,],\n              aes(x = x, xend = x, y = predicted, yend = y),\n              color = \"#FF7F00\", linetype = \"dashed\", linewidth = 1) +\n  \n  # Add annotations for better understanding\n  annotate(\"text\", x = data$x[demonstration_points[2]], y = mean_y - 0.2,\n           label = \"Mean\", color = \"#377EB8\", hjust = -0.2) +\n  annotate(\"text\", x = data$x[demonstration_points[2]], \n           y = data$predicted[demonstration_points[2]] + 0.2,\n           label = \"Regression Line\", color = \"#E41A1C\", hjust = -0.2) +\n  \n  # Improve theme and labels\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 12),\n    panel.grid = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  labs(\n    title = \"Variance Decomposition in Linear Regression\",\n    subtitle = \"Decomposing total variance into explained and unexplained components\",\n    x = \"Predictor (X)\",\n    y = \"Response (Y)\"\n  )\n\n# Create error distribution plot with improved aesthetics\ndata$mean_error &lt;- y - mean_y\ndata$regression_error &lt;- y - data$predicted\n\np2 &lt;- ggplot(data) +\n  geom_density(aes(x = mean_error, fill = \"Deviation from Mean\"), \n               alpha = 0.5) +\n  geom_density(aes(x = regression_error, fill = \"Regression Residuals\"), \n               alpha = 0.5) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  ) +\n  labs(\n    title = \"Error Distribution Comparison\",\n    x = \"Error Magnitude\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(\n    values = c(\"#377EB8\", \"#E41A1C\")\n  )\n\n# Add legend explaining the decomposition components\nlegend_plot &lt;- ggplot() +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    legend.box = \"horizontal\"\n  ) +\n  annotate(\"text\", x = 0, y = 0, label = \"\") +\n  scale_color_manual(\n    name = \"Variance Components\",\n    values = c(\"#984EA3\", \"#4DAF4A\", \"#FF7F00\"),\n    labels = c(\"Total Deviation\", \"Explained Variance\", \"Unexplained Variance\")\n  )\n\n# Combine plots with adjusted heights\ncombined_plot &lt;- (p1 / p2) +\n  plot_layout(heights = c(2, 1))\n\n# Print the combined plot\ncombined_plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n15.6.6 What is R²?\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nThink of R^2 as answering: “What percentage of the original variation in the data can we explain using our model?”\n\n15.6.6.1 Examples of R^2 Values:\n\nR^2 = 0.80: Our model explains 80% of the variation in house prices.\nR^2 = 0.25: Our model explains 25% of the variation (a weaker model).\nR^2 = 0.00: Our model explains none of the variation (not helpful).\n\n\n\n\n15.6.7 Important Things to Remember About R²\n\nHigh R^2 Isn’t Always Good\n\nA very high R^2 could suggest overfitting. Your model might be too complex, capturing noise rather than real patterns.\nAlways interpret R^2 in the context of your data.\n\nLow R^2 Isn’t Always Bad\n\nIn some fields (like social sciences), a low R^2 can still be useful.\nFocus on practical significance, not just the R^2 value.\n\nConsider Sample Size\n\nFor multiple regression models, use adjusted R^2 to account for the number of predictors.\nFormula: R^2_{adj} = 1 - \\frac{SSE/(n-p)}{SST/(n-1)}\n\n\n\n\n15.6.8 Tips for Effective Analysis\n\nVisualize the Data\n\nPlot your data and residuals to spot patterns.\nCheck for influential points that could skew your results.\n\nUnderstand Your Field’s Context\n\nWhat’s considered a good R^2 value in your field?\nWhat’s the practical impact of the errors you’re seeing?\n\nRun Diagnostics\n\nCheck residuals for normality.\nLook for heteroscedasticity (changing variability of errors).\nWatch for influential data points that affect the model’s accuracy.\n\n\n\n\n\n\n\n15.6.9 R² Demystified\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nThink of R² as answering the question: “What percentage of the original variation in Y can we explain using X?”\n\n15.6.9.1 Intuitive Examples:\n\nR² = 0.80: Using X eliminated 80% of our prediction errors\nR² = 0.25: Using X eliminated 25% of our prediction errors\nR² = 0.00: Using X didn’t help at all\n\n\n\n\n15.6.10 When to Be Cautious\n\nHigh R² Isn’t Everything\n\nA high R² might indicate overfitting\nAlways check if your model makes practical sense\nConsider the context of your field\n\nLow R² Isn’t Always Bad\n\nIn some fields, R² = 0.30 might be impressive\nSocial sciences often have lower R² values\nFocus on practical significance\n\n\n\n\n15.6.11 Practical Tips for Analysis\n\nVisual Inspection\n\nAlways plot your data\nLook for patterns in residuals\nCheck for influential points\n\nContext Consideration\n\nWhat’s a “good” R² in your field?\nWhat’s the practical impact of your errors?\nAre your predictors meaningful?\n\nModel Diagnostics\n\nCheck residual normality\nLook for heteroscedasticity\nExamine influential points\n\n\n\n\n15.6.12 Key Takeaways\n\nVariance decomposition helps us understand prediction improvement\nR² quantifies the proportion of variance explained\nVisual understanding is crucial for interpretation\nContext matters more than absolute R² values\nAlways combine R² with other diagnostic tools\n\n\n\n15.6.13 Measures of Fit\n\nR-squared (R^2): R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nRoot Mean Square Error (RMSE): RMSE = \\sqrt{\\frac{SSE}{n}}\nMean Absolute Error (MAE): MAE = \\frac{1}{n}\\sum_{i=1}^n |Y_i - \\hat{Y}_i|\n\n\n\n\n\n\n\nFormal Derivation of OLS Estimators: Step-by-Step Approach\n\n\n\n\n15.6.14 Initial Setup\nWe seek to minimize the sum of squared residuals (errors):\nSSE = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nLet’s break this into manageable pieces:\n\nEach residual is: e_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nWe square each residual: e_i^2 = (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nSum them all: SSE = \\sum_{i=1}^n e_i^2\n\n\n\n15.6.15 Chain Rule Review\nBefore proceeding, let’s recall the chain rule. For a composite function f(g(x)):\n\\frac{d}{dx}[f(g(x))] = f'(g(x)) \\cdot g'(x)\nIn our case, we’re dealing with the square function f(u) = u^2, where:\n\nf'(u) = 2u\nu = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\n\n\n\n15.6.16 Step 1: Finding \\hat{\\beta}_0 Using First Derivative\nLet’s take the partial derivative with respect to \\hat{\\beta}_0 step by step:\n\nStart with one term of the sum:\n\\frac{\\partial}{\\partial \\hat{\\beta}_0}(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nApply chain rule:\n\nOuter function: f(u) = u^2, so f'(u) = 2u\nInner function: g(\\hat{\\beta}_0) = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nInner derivative: g'(\\hat{\\beta}_0) = -1\n\nTherefore, for each term: \\frac{\\partial}{\\partial \\hat{\\beta}_0}(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2 = 2(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))(-1)\nNow sum all terms and set to zero: \\sum_{i=1}^n 2(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))(-1) = 0\nSimplify: -2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\nRemove the -2: \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\nExpand the sum: \\sum_{i=1}^n y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum_{i=1}^n x_i = 0\nSolve for \\hat{\\beta}_0: n\\hat{\\beta}_0 = \\sum_{i=1}^n y_i - \\hat{\\beta}_1\\sum_{i=1}^n x_i\n\\hat{\\beta}_0 = \\frac{\\sum_{i=1}^n y_i}{n} - \\hat{\\beta}_1\\frac{\\sum_{i=1}^n x_i}{n}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\n\n\n\n15.6.17 Step 2: Finding \\hat{\\beta}_1 Using First Derivative\nNow let’s find \\hat{\\beta}_1 with the same careful approach:\n\nFor one term: \\frac{\\partial}{\\partial \\hat{\\beta}_1}(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nApply chain rule:\n\nOuter function: f(u) = u^2, so f'(u) = 2u\nInner function: g(\\hat{\\beta}_1) = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nInner derivative: g'(\\hat{\\beta}_1) = -x_i\n\nTherefore: \\frac{\\partial}{\\partial \\hat{\\beta}_1}(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2 = 2(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))(-x_i)\nSum all terms and set to zero: \\sum_{i=1}^n 2(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))(-x_i) = 0\nSimplify: -2\\sum_{i=1}^n x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\nSubstitute \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}: -2\\sum_{i=1}^n x_i(y_i - (\\bar{y} - \\hat{\\beta}_1\\bar{x}) - \\hat{\\beta}_1x_i) = 0\nExpand: -2\\sum_{i=1}^n x_i(y_i - \\bar{y} + \\hat{\\beta}_1\\bar{x} - \\hat{\\beta}_1x_i) = 0\nDistribute x_i: -2\\sum_{i=1}^n (x_iy_i - x_i\\bar{y} + x_i\\hat{\\beta}_1\\bar{x} - x_i^2\\hat{\\beta}_1) = 0\nCollect terms with \\hat{\\beta}_1: \\sum_{i=1}^n (x_i^2\\hat{\\beta}_1 - x_i\\hat{\\beta}_1\\bar{x}) = \\sum_{i=1}^n (x_iy_i - x_i\\bar{y})\nFactor out \\hat{\\beta}_1: \\hat{\\beta}_1\\sum_{i=1}^n (x_i^2 - x_i\\bar{x}) = \\sum_{i=1}^n (x_iy_i - x_i\\bar{y})\nFinal form: \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n\n\n\n15.6.18 Step 3: Verifying We Have a Minimum\nTo confirm these critical points are minima, we check the second derivatives:\n\nSecond derivative with respect to \\hat{\\beta}_0: \\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_0^2} = \\frac{\\partial}{\\partial \\hat{\\beta}_0}(-2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)) = 2n &gt; 0\nSecond derivative with respect to \\hat{\\beta}_1: \\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_1^2} = \\frac{\\partial}{\\partial \\hat{\\beta}_1}(-2\\sum_{i=1}^n x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)) = 2\\sum_{i=1}^n x_i^2 &gt; 0\nCross partial derivatives: \\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_0\\partial \\hat{\\beta}_1} = \\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_1\\partial \\hat{\\beta}_0} = 2\\sum_{i=1}^n x_i\nThe Hessian matrix is positive definite: \\mathbf{H} = \\begin{bmatrix} 2n & 2\\sum x_i \\\\ 2\\sum x_i & 2\\sum x_i^2 \\end{bmatrix}\n\nThis confirms we have found a minimum.\n\n\n15.6.19 Visualizing the Process\n\nlibrary(tidyverse)\n\n# Create sample data\nset.seed(123)\nx &lt;- runif(20, 1, 8)\ny &lt;- 2 + 3 * x + rnorm(20, 0, 1)\ndata &lt;- data.frame(x = x, y = y)\n\n# Calculate means\nx_mean &lt;- mean(x)\ny_mean &lt;- mean(y)\n\n# Create visualization\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_hline(yintercept = y_mean, linetype = \"dashed\", color = \"gray\") +\n  geom_vline(xintercept = x_mean, linetype = \"dashed\", color = \"gray\") +\n  geom_segment(aes(xend = x, yend = y_mean), color = \"green\", alpha = 0.3) +\n  geom_segment(aes(yend = y, xend = x_mean), color = \"purple\", alpha = 0.3) +\n  labs(title = \"Understanding the OLS Derivation\",\n       subtitle = \"Green: y deviations, Purple: x deviations\\nTheir product forms the numerator of β̂₁\",\n       x = \"x\", y = \"y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n15.6.20 Key Insights\n\nThe chain rule is crucial in deriving both estimators\n\\hat{\\beta}_0 ensures the line goes through (\\bar{x}, \\bar{y})\n\\hat{\\beta}_1 is a ratio of covariance to variance\nThe second derivatives confirm we’ve found a minimum\nThe entire process relies on calculus to find the optimal values that minimize the sum of squared residuals\n\n\n\n\n\n\n\n\n\n\nUnderstanding Endogeneity in Regression\n\n\n\nEndogeneity is a critical concept in statistical analysis that occurs when an explanatory variable in a regression model is correlated with the error term. This creates challenges for accurately understanding cause-and-effect relationships in research. Let’s examine the three main types of endogeneity and how they affect research outcomes.\n\n15.6.21 1. Omitted Variable Bias (OVB)\nOmitted Variable Bias occurs when an important variable that affects both the dependent and independent variables is left out of the analysis. This omission leads to incorrect conclusions about the relationship between the variables we’re studying.\nConsider a study examining the relationship between education and income:\nExample: Education and Income The observed relationship shows that more education correlates with higher income. However, an individual’s inherent abilities affect both their educational attainment and their earning potential. Without accounting for ability, we may overestimate education’s direct effect on income.\nThe statistical representation shows why this matters:\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2z_i + \\epsilon_i (Complete model)\ny_i = \\beta_0 + \\beta_1x_i + u_i (Incomplete model)\nWhen we omit an important variable, our estimates of the remaining relationships become biased and unreliable.\n\n\n15.6.22 2. Simultaneity\nSimultaneity occurs when two variables simultaneously influence each other, making it difficult to determine the direction of causation. This creates a feedback loop that complicates statistical analysis.\nCommon Examples of Simultaneity:\nAcademic Performance and Study Habits represent a clear case of simultaneity. Academic performance influences how much time students dedicate to studying, while study time affects academic performance. This two-way relationship makes it challenging to measure the isolated effect of either variable.\nMarket Dynamics provide another example. Prices influence demand, while demand influences prices. This concurrent relationship requires special analytical approaches to understand the true relationships.\n\n\n15.6.23 3. Measurement Error\nMeasurement error occurs when we cannot accurately measure our variables of interest. This imprecision can significantly impact our analysis and conclusions.\nCommon Sources of Measurement Error:\nSelf-Reported Data presents a significant challenge. When participants report their own behaviors or characteristics, such as study time, the reported values often differ from actual values. This discrepancy affects our ability to measure true relationships.\nTechnical Limitations also contribute to measurement error through imprecise measuring tools, inconsistent measurement conditions, and recording or data entry errors.\n\n\n15.6.24 Addressing Endogeneity in Research\n\n15.6.24.1 Identification Strategies\n\n# Example of controlling for omitted variables\nmodel_simple &lt;- lm(income ~ education, data = df)\nmodel_full &lt;- lm(income ~ education + ability + experience + region, data = df)\n\n# Compare coefficients\nsummary(model_simple)\nsummary(model_full)\n\n\nInclude Additional Variables: Collect data on potentially important omitted variables and include relevant control variables in your analysis. For example, including measures of ability when studying education’s effect on income.\nUse Panel Data: Collect data across multiple time periods to control for unobserved fixed characteristics and analyze changes over time.\nInstrumental Variables: Find variables that affect your independent variable but not your dependent variable to isolate the relationship of interest.\n\n\n\n15.6.24.2 Improving Measurement\n\nMultiple Measurements: Take several measurements of key variables, use averaging to reduce random error, and compare different measurement methods.\nBetter Data Collection: Use validated measurement instruments, implement quality control procedures, and document potential sources of error.\n\n\n\n\n15.6.25 Best Practices for Researchers\nResearch Design fundamentally shapes your ability to address endogeneity. Plan for potential endogeneity issues before collecting data, include measures for potentially important control variables, and consider using multiple measurement approaches.\nAnalysis should include testing for endogeneity when possible, using appropriate statistical methods for your specific situation, and documenting assumptions and limitations.\nReporting must clearly describe potential endogeneity concerns, explain how you addressed these issues, and discuss implications for your conclusions.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#multiple-regression",
    "href": "correg_en.html#multiple-regression",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.7 Multiple Regression (*)",
    "text": "15.7 Multiple Regression (*)\n\n15.7.1 Extending to Multiple Predictors\nThe multiple regression model extends our simple model to include several predictors:\nPopulation Model: Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k + \\varepsilon\nSample Estimation: \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 + ... + \\hat{\\beta}_kX_k\nLet’s create an example with multiple predictors:\n\n# Generate sample data with two predictors\nset.seed(105)\nn &lt;- 100\nX1 &lt;- rnorm(n, mean = 50, sd = 10)\nX2 &lt;- rnorm(n, mean = 20, sd = 5)\nY &lt;- 10 + 0.5*X1 + 0.8*X2 + rnorm(n, 0, 5)\n\ndata_multiple &lt;- data.frame(Y = Y, X1 = X1, X2 = X2)\n\n# Fit multiple regression model\nmodel_multiple &lt;- lm(Y ~ X1 + X2, data = data_multiple)\n\n# Create 3D visualization using scatter plots\np1 &lt;- ggplot(data_multiple, aes(x = X1, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Y vs X1\")\n\np2 &lt;- ggplot(data_multiple, aes(x = X2, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Y vs X2\")\n\ngrid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nMultiple Regression Example\n\n\n\n# Print model summary\nsummary(model_multiple)\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = data_multiple)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8598  -3.6005   0.1166   3.0892  14.6102 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.77567    4.01351   2.934  0.00418 ** \nX1           0.45849    0.05992   7.651 1.47e-11 ***\nX2           0.81639    0.11370   7.180 1.42e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.122 on 97 degrees of freedom\nMultiple R-squared:  0.5062,    Adjusted R-squared:  0.4961 \nF-statistic: 49.72 on 2 and 97 DF,  p-value: 1.367e-15\n\n\n\n\n15.7.2 Interpretation of Coefficients\nIn multiple regression, each \\hat{\\beta}_k represents the expected change in Y for a one-unit increase in X_k, holding all other variables constant.\n\n# Create prediction grid for X1 (holding X2 at its mean)\nX1_grid &lt;- seq(min(X1), max(X1), length.out = 100)\npred_data_X1 &lt;- data.frame(\n  X1 = X1_grid,\n  X2 = mean(X2)\n)\npred_data_X1$Y_pred &lt;- predict(model_multiple, newdata = pred_data_X1)\n\n# Create prediction grid for X2 (holding X1 at its mean)\nX2_grid &lt;- seq(min(X2), max(X2), length.out = 100)\npred_data_X2 &lt;- data.frame(\n  X1 = mean(X1),\n  X2 = X2_grid\n)\npred_data_X2$Y_pred &lt;- predict(model_multiple, newdata = pred_data_X2)\n\n# Plot partial effects\np3 &lt;- ggplot() +\n  geom_point(data = data_multiple, aes(x = X1, y = Y)) +\n  geom_line(data = pred_data_X1, aes(x = X1, y = Y_pred), \n            color = \"red\", size = 1) +\n  theme_minimal() +\n  labs(title = \"Partial Effect of X1\",\n       subtitle = paste(\"(X2 held at mean =\", round(mean(X2), 2), \")\"))\n\np4 &lt;- ggplot() +\n  geom_point(data = data_multiple, aes(x = X2, y = Y)) +\n  geom_line(data = pred_data_X2, aes(x = X2, y = Y_pred), \n            color = \"red\", size = 1) +\n  theme_minimal() +\n  labs(title = \"Partial Effect of X2\",\n       subtitle = paste(\"(X1 held at mean =\", round(mean(X1), 2), \")\"))\n\ngrid.arrange(p3, p4, ncol = 2)\n\n\n\n\nPartial Effects in Multiple Regression\n\n\n\n\n\n\n15.7.3 Multicollinearity\nMulticollinearity occurs when predictors are highly correlated. Let’s demonstrate its effects:\n\n# Generate data with multicollinearity\nset.seed(106)\nX1_new &lt;- rnorm(n, mean = 50, sd = 10)\nX2_new &lt;- 2*X1_new + rnorm(n, 0, 5)  # X2 highly correlated with X1\nY_new &lt;- 10 + 0.5*X1_new + 0.8*X2_new + rnorm(n, 0, 5)\n\ndata_collinear &lt;- data.frame(Y = Y_new, X1 = X1_new, X2 = X2_new)\n\n# Fit model with multicollinearity\nmodel_collinear &lt;- lm(Y ~ X1 + X2, data = data_collinear)\n\n# Calculate VIF\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nThe following object is masked from 'package:psych':\n\n    logit\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nvif_results &lt;- vif(model_collinear)\n\n# Plot correlation\nggplot(data_collinear, aes(x = X1, y = X2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Correlation between Predictors\",\n       subtitle = paste(\"Correlation =\", \n                       round(cor(X1_new, X2_new), 3)))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nEffects of Multicollinearity",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#advanced-topics",
    "href": "correg_en.html#advanced-topics",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.8 Advanced Topics",
    "text": "15.8 Advanced Topics\n\n15.8.1 Interaction Terms\nInteraction terms allow the effect of one predictor to depend on another:\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3(X_1 \\times X_2) + \\varepsilon\n\n# Generate data with interaction\nset.seed(107)\nX1_int &lt;- rnorm(n, mean = 0, sd = 1)\nX2_int &lt;- rnorm(n, mean = 0, sd = 1)\nY_int &lt;- 1 + 2*X1_int + 3*X2_int + 4*X1_int*X2_int + rnorm(n, 0, 1)\n\ndata_int &lt;- data.frame(X1 = X1_int, X2 = X2_int, Y = Y_int)\nmodel_int &lt;- lm(Y ~ X1 * X2, data = data_int)\n\n# Create interaction plot\nX1_levels &lt;- quantile(X1_int, probs = c(0.25, 0.75))\nX2_seq &lt;- seq(min(X2_int), max(X2_int), length.out = 100)\n\npred_data &lt;- expand.grid(\n  X1 = X1_levels,\n  X2 = X2_seq\n)\npred_data$Y_pred &lt;- predict(model_int, newdata = pred_data)\npred_data$X1_level &lt;- factor(pred_data$X1, \n                            labels = c(\"Low X1\", \"High X1\"))\n\nggplot(pred_data, aes(x = X2, y = Y_pred, color = X1_level)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Interaction Effect\",\n       subtitle = \"Effect of X2 depends on level of X1\",\n       color = \"X1 Level\")\n\n\n\n\nVisualization of Interaction Effects\n\n\n\n\n\n\n15.8.2 Polynomial Terms\nWhen relationships are non-linear, we can add polynomial terms:\nY = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\varepsilon\n\n# Generate data with quadratic relationship\nset.seed(108)\nX_poly &lt;- seq(-3, 3, length.out = 100)\nY_poly &lt;- 1 - 2*X_poly + 3*X_poly^2 + rnorm(length(X_poly), 0, 2)\ndata_poly &lt;- data.frame(X = X_poly, Y = Y_poly)\n\n# Fit linear and quadratic models\nmodel_linear &lt;- lm(Y ~ X, data = data_poly)\nmodel_quad &lt;- lm(Y ~ X + I(X^2), data = data_poly)\n\n# Add predictions\ndata_poly$pred_linear &lt;- predict(model_linear)\ndata_poly$pred_quad &lt;- predict(model_quad)\n\n# Plot\nggplot(data_poly, aes(x = X, y = Y)) +\n  geom_point(alpha = 0.5) +\n  geom_line(aes(y = pred_linear, color = \"Linear\"), size = 1) +\n  geom_line(aes(y = pred_quad, color = \"Quadratic\"), size = 1) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme_minimal() +\n  labs(title = \"Linear vs. Quadratic Fit\",\n       color = \"Model Type\")\n\n\n\n\nPolynomial Regression Example",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#practical-guidelines-for-regression-analysis",
    "href": "correg_en.html#practical-guidelines-for-regression-analysis",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.9 Practical Guidelines for Regression Analysis",
    "text": "15.9 Practical Guidelines for Regression Analysis\n\n15.9.1 Model Building Process\n\nData Exploration\n\n\n# Generate example dataset\nset.seed(109)\nn &lt;- 100\ndata_example &lt;- data.frame(\n  x1 = rnorm(n, mean = 50, sd = 10),\n  x2 = rnorm(n, mean = 20, sd = 5),\n  x3 = runif(n, 0, 100)\n)\ndata_example$y &lt;- 10 + 0.5*data_example$x1 + 0.8*data_example$x2 - \n                 0.3*data_example$x3 + rnorm(n, 0, 5)\n\n# Correlation matrix plot\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nggpairs(data_example) +\n  theme_minimal() +\n  labs(title = \"Exploratory Data Analysis\",\n       subtitle = \"Correlation matrix and distributions\")\n\n\n\n\nData Exploration Example\n\n\n\n\n\nVariable Selection\n\n\n# Fit models with different variables\nmodel1 &lt;- lm(y ~ x1, data = data_example)\nmodel2 &lt;- lm(y ~ x1 + x2, data = data_example)\nmodel3 &lt;- lm(y ~ x1 + x2 + x3, data = data_example)\n\n# Compare models\nmodels_comparison &lt;- data.frame(\n  Model = c(\"y ~ x1\", \"y ~ x1 + x2\", \"y ~ x1 + x2 + x3\"),\n  R_squared = c(summary(model1)$r.squared,\n                summary(model2)$r.squared,\n                summary(model3)$r.squared),\n  Adj_R_squared = c(summary(model1)$adj.r.squared,\n                    summary(model2)$adj.r.squared,\n                    summary(model3)$adj.r.squared)\n)\n\nknitr::kable(models_comparison, digits = 3,\n             caption = \"Model Comparison Summary\")\n\n\nModel Comparison Summary\n\n\nModel\nR_squared\nAdj_R_squared\n\n\n\n\ny ~ x1\n0.323\n0.316\n\n\ny ~ x1 + x2\n0.433\n0.421\n\n\ny ~ x1 + x2 + x3\n0.893\n0.890\n\n\n\nVariable Selection Process\n\n\n\n\n15.9.2 Common Pitfalls and Solutions\n\nOutliers and Influential Points\n\n\n# Create data with outlier\nset.seed(110)\nx_clean &lt;- rnorm(50, mean = 0, sd = 1)\ny_clean &lt;- 2 + 3*x_clean + rnorm(50, 0, 0.5)\ndata_clean &lt;- data.frame(x = x_clean, y = y_clean)\n\n# Add outlier\ndata_outlier &lt;- rbind(data_clean,\n                      data.frame(x = 4, y = -10))\n\n# Fit models\nmodel_clean &lt;- lm(y ~ x, data = data_clean)\nmodel_outlier &lt;- lm(y ~ x, data = data_outlier)\n\n# Plot\nggplot() +\n  geom_point(data = data_clean, aes(x = x, y = y), color = \"blue\") +\n  geom_point(data = data_outlier[51,], aes(x = x, y = y), \n             color = \"red\", size = 3) +\n  geom_line(data = data_clean, \n            aes(x = x, y = predict(model_clean), \n                color = \"Without Outlier\")) +\n  geom_line(data = data_outlier, \n            aes(x = x, y = predict(model_outlier), \n                color = \"With Outlier\")) +\n  theme_minimal() +\n  labs(title = \"Effect of Outliers on Regression\",\n       color = \"Model\") +\n  scale_color_manual(values = c(\"blue\", \"red\"))\n\n\n\n\nIdentifying and Handling Outliers\n\n\n\n\n\nMissing Data Patterns\n\n\n# Create data with missing values\nset.seed(111)\ndata_missing &lt;- data_example\ndata_missing$x1[sample(1:n, 10)] &lt;- NA\ndata_missing$x2[sample(1:n, 15)] &lt;- NA\ndata_missing$x3[sample(1:n, 20)] &lt;- NA\n\n# Visualize missing patterns\nlibrary(naniar)\nvis_miss(data_missing) +\n  theme_minimal() +\n  labs(title = \"Missing Data Patterns\")\n\n\n\n\nMissing Data Patterns\n\n\n\n\n\nHeteroscedasticity\n\n\n# Generate heteroscedastic data\nset.seed(112)\nx_hetero &lt;- seq(-3, 3, length.out = 100)\ny_hetero &lt;- 2 + 1.5*x_hetero + rnorm(100, 0, abs(x_hetero)/2)\ndata_hetero &lt;- data.frame(x = x_hetero, y = y_hetero)\n\n# Fit model\nmodel_hetero &lt;- lm(y ~ x, data = data_hetero)\n\n# Plot\np1 &lt;- ggplot(data_hetero, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Heteroscedastic Data\")\n\np2 &lt;- ggplot(data_hetero, aes(x = fitted(model_hetero), \n                             y = residuals(model_hetero))) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Residual Plot\",\n       x = \"Fitted values\",\n       y = \"Residuals\")\n\ngrid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nDetecting and Visualizing Heteroscedasticity\n\n\n\n\n\n\n15.9.3 Best Practices\n\nModel Validation\n\n\n# Simple cross-validation example\nset.seed(113)\n\n# Create training and test sets\ntrain_index &lt;- sample(1:nrow(data_example), 0.7*nrow(data_example))\ntrain_data &lt;- data_example[train_index, ]\ntest_data &lt;- data_example[-train_index, ]\n\n# Fit model on training data\nmodel_train &lt;- lm(y ~ x1 + x2 + x3, data = train_data)\n\n# Predict on test data\npredictions &lt;- predict(model_train, newdata = test_data)\nactual &lt;- test_data$y\n\n# Calculate performance metrics\nrmse &lt;- sqrt(mean((predictions - actual)^2))\nmae &lt;- mean(abs(predictions - actual))\nr2 &lt;- cor(predictions, actual)^2\n\n# Plot predictions vs actual\ndata_validation &lt;- data.frame(\n  Predicted = predictions,\n  Actual = actual\n)\n\nggplot(data_validation, aes(x = Actual, y = Predicted)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Model Validation: Predicted vs Actual\",\n       subtitle = sprintf(\"RMSE = %.2f, MAE = %.2f, R² = %.2f\", \n                         rmse, mae, r2))\n\n\n\n\nCross-Validation Example\n\n\n\n\n\nReporting Results\n\nExample of a professional regression results table:\n\n# Create regression results table\nlibrary(broom)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nmodel_final &lt;- lm(y ~ x1 + x2 + x3, data = data_example)\nresults &lt;- tidy(model_final, conf.int = TRUE)\n\nkable(results, digits = 3,\n      caption = \"Regression Results Summary\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nRegression Results Summary\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n9.116\n2.835\n3.216\n0.002\n3.489\n14.743\n\n\nx1\n0.497\n0.039\n12.756\n0.000\n0.419\n0.574\n\n\nx2\n0.905\n0.086\n10.468\n0.000\n0.734\n1.077\n\n\nx3\n-0.324\n0.016\n-20.322\n0.000\n-0.356\n-0.292",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#conclusion-1",
    "href": "correg_en.html#conclusion-1",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.10 Conclusion",
    "text": "15.10 Conclusion\n\n15.10.1 Key Takeaways\n\nAlways start with exploratory data analysis\nCheck assumptions before interpreting results\nBe aware of common pitfalls:\n\nOutliers\nMissing data\nMulticollinearity\nHeteroscedasticity\n\nValidate your model using:\n\nDiagnostic plots\nCross-validation\nResidual analysis\n\nReport results clearly and completely\n\n\n\nFurther Reading\nFor deeper understanding:\n\nWooldridge, J.M. “Introductory Econometrics: A Modern Approach”\nAngrist, J.D. and Pischke, J.S. “Mostly Harmless Econometrics”\nStock & Watson “Introduction to Econometrics”\n\n\n\n\n\n\n\nUnderstanding Ordinary Least Squares (OLS) Regression: A Student’s Guide\n\n\n\nRegression analysis is a fundamental statistical method that examines and models the relationship between variables to understand how changes in one or more independent variables influence a dependent variable.\nAt its heart, regression analysis helps us answer questions about cause and effect, prediction, and forecasting. For example, a business might use regression analysis to understand how advertising spending affects sales, or how employee training hours impact productivity.\n\n15.10.2 Regression as a Stochastic Model\nIn mathematical modelling, we encounter two fundamental approaches to describing relationships between variables:\n\ndeterministic models,\nstochastic models.\n\n\n15.10.2.1 Deterministic vs. Stochastic Models\nA deterministic model assumes a precise, fixed relationship between inputs and outputs. In such models, if we know the inputs, we can calculate the exact output with certainty. Consider the classic physics equation for distance:\n\\text{Distance} = \\text{Speed} × \\text{Time}\nGiven specific values for speed and time, this equation will always yield the same distance. There is no room for variation in the outcome.\nIn contrast, regression analysis embraces the presence of natural variation in data. The fundamental structure of a regression model is:\nY = f(X) + \\epsilon\nWhere:\n\nY represents the outcome we wish to predict\nf(X) represents the systematic relationship between our predictors (X) and the outcome\n\\epsilon represents the random variation that naturally occurs in real-world data\n\n\n\n\n15.10.3 What is Simple Linear Regression?\nConsider the relationship between study hours (predictor) and exam scores (outcome/response variable). Simple linear regression creates an optimal straight line through data points to model this relationship. This line serves two purposes: making predictions and quantifying the relationship between variables.\nThe mathematical relationship is expressed as:\n Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i \nUsing our study hours example to illustrate each component:\n\nY_i represents the dependent variable (exam scores)\nX_i represents the independent variable (study hours)\n\\beta_0 represents the intercept - the expected exam score at zero study hours\n\\beta_1 represents the slope - the change in exam score per additional study hour\n\\epsilon_i represents the error term, accounting for unobserved factors\n\nOur goal is to find estimates \\hat{\\beta}_0 and \\hat{\\beta}_1 that best approximate the true parameters \\beta_0 and \\beta_1. The fitted regression line is then expressed as:\n \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1X_i \n\n\n15.10.4 The Ordinary Least Squares (OLS) Method\nThe “Ordinary Least Squares” method determines optimal values for \\hat{\\beta}_0 and \\hat{\\beta}_1 through a methodical process. While modern statistical software performs these calculations automatically, understanding the underlying process is crucial:\n\nBegin with an initial estimate for the regression line\n\nTypically, we start with the horizontal line at \\bar{Y} (mean of Y)\nThis serves as our baseline, representing no relationship between X and Y\n\nCalculate the vertical distances (residuals) from each observed point to this line\n\nThese residuals \\hat{\\epsilon}_i = e_i = Y_i - \\hat{Y}_i represent our prediction errors\nA positive residual means our line underestimates the true value\nA negative residual means our line overestimates the true value\n\nSquare these distances to:\n\nEnsure positive and negative deviations don’t offset each other\nAssign greater weight to larger deviations\n\nSum all squared distances to obtain the Sum of Squared Errors (SSE)\n\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\nThis represents the total squared deviation of observed values from our predicted values\n\nSystematically adjust the slope and intercept of the line to minimize SSE:\n\nThe process involves gradually modifying the slope and position of the line while observing how the sum of squared deviations (SSE) changes [formally, this requires concepts from differential calculus]\nWhen SSE stops decreasing with subsequent small changes in line parameters, this indicates the optimal fit has been found\nThis point corresponds to the minimum possible value of SSE, meaning the best fit of the line to the data\n\n\nMore formally, OLS finds the optimal values for \\beta_0 and \\beta_1 by minimizing the sum of squared errors:\n\\min_{\\beta_0, \\beta_1} \\sum \\epsilon_i^2 = \\min_{\\beta_0, \\beta_1} \\sum(y_i - \\hat{y_i})^2 = \\min_{\\beta_0, \\beta_1} \\sum(y_i - (\\beta_0 + \\beta_1x_i))^2 The solution to this minimization gives us:\n\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = \\frac{cov(X, Y)}{var(X)}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.10.5 Parameter Estimation\nThe formulas for estimating the parameters employ fundamental statistical measures:\n \\hat{\\beta}_1 = \\frac{Cov(X,Y)}{Var(X)} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} \nThis formula comprises two essential statistical concepts:\n\nCovariance (Cov) quantifies the joint movement of X and Y\nVariance (Var) measures the dispersion of X values around their mean\n\\bar{X} and \\bar{Y} represent the means of X and Y respectively\n\nAfter calculating \\hat{\\beta}_1, we determine \\hat{\\beta}_0:\n \\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X} \nThis ensures the regression line intersects the point of means (\\bar{X}, \\bar{Y}).\n\n\n15.10.6 Understanding Variation in Regression Analysis\nIn regression analysis, we decompose the variation in our data into distinct components to understand how well our model explains the outcome variable. Let’s examine each component of variation in detail.\n\n15.10.6.1 Overview of Variation Components\nThe total Sum of Squares (SST) represents all variation present in our outcome variable. The Regression Sum of Squares (SSR) captures the variation explained by our model. The Error Sum of Squares (SSE) represents the remaining unexplained variation. Let’s examine each in detail.\n\n\n15.10.6.2 Total Sum of Squares (SST)\nThink of total variation (SST) as capturing the full range of differences in our data before we try to explain anything. It measures how much each observation differs from the overall average, giving us a baseline for how much variation exists to be explained.\nSST measures the total variation in our outcome variable by calculating how much each observation deviates from the overall mean:\n SST = \\sum(y_i - \\bar{y})^2 \nIn this formula, y_i represents each individual observation, and \\bar{y} represents the mean of all observations. The squared differences capture the total spread of our data around its average value.\nTo make this concrete, imagine measuring the heights of everyone in a city. Some people are much taller than average, others shorter, and SST captures these differences. Each squared term (y_i - \\bar{y})^2 represents how much one person’s height differs from the city’s average, and we sum up all these differences.\n\n\n15.10.6.3 Regression Sum of Squares (SSR)\nOnce we introduce explanatory variables (like genetics, nutrition, or age), we can predict heights more accurately. The explained variation (SSR) measures how much better our predictions become using these variables.\nSSR quantifies how much variation our model explains through its predictor variables:\n SSR = \\sum(\\hat{y}_i - \\bar{y})^2 \nHere, \\hat{y}_i represents our model’s predicted values. This formula measures how far our predictions deviate from the mean, representing the variation captured by our model.\n\n\n15.10.6.4 Error Sum of Squares (SSE)\nSSE measures the variation that remains unexplained by our model:\n SSE = \\sum(y_i - \\hat{y}_i)^2 \nThis formula calculates the differences between actual values (y_i) and predicted values (\\hat{y}_i), showing us how much variation our model failed to explain.\nIn our height example, these might be variations due to factors we didn’t or couldn’t measure, like specific environmental conditions or measurement errors.\n\n\n15.10.6.5 The Fundamental Relationship\nThese three components are related through a key equation:\n SST = SSR + SSE \nThis equation shows that the total variation (SST) equals the sum of explained variation (SSR) and unexplained variation (SSE). This relationship forms the basis for measuring model performance.\n\n\n15.10.6.6 Coefficient of Determination (R²)\nThe R² statistic, derived from these components, tells us the proportion of variation our model explains:\n R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} \nWhen R² equals 0.75, it indicates our model explains 75% of the variation in the outcome variable. The remaining 25% represents unexplained variation.\n\n\n15.10.6.7 Important Considerations\nUnderstanding these components helps us:\n\nEvaluate model performance through the proportion of explained variation\nIdentify how much variation remains unexplained\nMake informed decisions about model improvement\nCompare different models’ explanatory power\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.10.7 The Coefficient of Determination (R^2)\nR^2 serves as a measure of model fit, representing the proportion of variance explained by our regression:\n R^2 = \\frac{SSR}{SST} = \\frac{SSR}{SSR + SSE} = 1 - \\frac{SSE}{SST} \nTo interpret R^2:\n\nAn R^2 of 0.75 indicates the model explains 75% of variance (e.g. in exam scores)\nAn R^2 of 0.20 suggests predictors (e.g. study hours) account for 20% of score variation\nAn R^2 of 1.00 represents perfect prediction (rarely observed in practice)\nAn R^2 of 0.00 indicates no explanatory power\n\nNote: A lower R^2 value does not necessarily indicate an inferior model - it may simply reflect the complexity of the relationship being studied.\n\n\n15.10.8 Key Takeaway\nLinear regression establishes relationships between variables by identifying the optimal linear fit through data points. This optimization occurs through the minimization of squared residuals. The coefficient of determination (R^2) then quantifies the model’s explanatory power by comparing explained variation to total variation in our data.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "href": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.11 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)",
    "text": "15.11 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)\n\n15.11.1 Dataset Overview\n\ndata &lt;- data.frame(\n  anxiety_level = c(8, 5, 11, 14, 7, 10),\n  cognitive_performance = c(85, 90, 62, 55, 80, 65)\n)\n\n\n\n15.11.2 1. Covariance Calculation\n\n15.11.2.1 Step 1: Calculate Means\n\n\n\n\n\n\n\n\nVariable\nCalculation\nResult\n\n\n\n\nMean Anxiety (\\bar{x})\n(8 + 5 + 11 + 14 + 7 + 10) ÷ 6\n9.17\n\n\nMean Cognitive (\\bar{y})\n(85 + 90 + 62 + 55 + 80 + 65) ÷ 6\n72.83\n\n\n\n\n\n15.11.2.2 Step 2: Calculate Deviations and Products\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n1\n8\n85\n-1.17\n12.17\n-14.24\n\n\n2\n5\n90\n-4.17\n17.17\n-71.60\n\n\n3\n11\n62\n1.83\n-10.83\n-19.82\n\n\n4\n14\n55\n4.83\n-17.83\n-86.12\n\n\n5\n7\n80\n-2.17\n7.17\n-15.56\n\n\n6\n10\n65\n0.83\n-7.83\n-6.50\n\n\nSum\n55\n437\n0.00\n0.00\n-213.84\n\n\n\n\n\n15.11.2.3 Step 3: Calculate Covariance\n \\text{Cov}(X,Y) = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n-1} = \\frac{-213.84}{5} = -42.77 \n\n\n\n15.11.3 2. Pearson Correlation Coefficient\n\n15.11.3.1 Step 1: Calculate Squared Deviations\n\n\n\n\n\n\n\n\n\n\ni\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n-1.17\n12.17\n1.37\n148.11\n\n\n2\n-4.17\n17.17\n17.39\n294.81\n\n\n3\n1.83\n-10.83\n3.35\n117.29\n\n\n4\n4.83\n-17.83\n23.33\n317.91\n\n\n5\n-2.17\n7.17\n4.71\n51.41\n\n\n6\n0.83\n-7.83\n0.69\n61.31\n\n\nSum\n0.00\n0.00\n50.84\n990.84\n\n\n\n\n\n15.11.3.2 Step 2: Calculate Standard Deviations\n\n\n\n\n\n\n\n\n\nMeasure\nFormula\nCalculation\nResult\n\n\n\n\ns_x\n\\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n-1}}\n\\sqrt{\\frac{50.84}{5}}\n3.19\n\n\ns_y\n\\sqrt{\\frac{\\sum (y_i - \\bar{y})^2}{n-1}}\n\\sqrt{\\frac{990.84}{5}}\n14.08\n\n\n\n\n\n15.11.3.3 Step 3: Calculate Pearson Correlation\n r = \\frac{\\text{Cov}(X,Y)}{s_x s_y} = \\frac{-42.77}{3.19 \\times 14.08} = -0.95 \n\n\n\n15.11.4 3. Spearman Rank Correlation\n\n15.11.4.1 Step 1: Assign Ranks\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n8\n85\n3\n2\n1\n1\n\n\n2\n5\n90\n1\n1\n0\n0\n\n\n3\n11\n62\n5\n5\n0\n0\n\n\n4\n14\n55\n6\n6\n0\n0\n\n\n5\n7\n80\n2\n3\n-1\n1\n\n\n6\n10\n65\n4\n4\n0\n0\n\n\nSum\n\n\n\n\n\n2\n\n\n\n\n\n15.11.4.2 Step 2: Calculate Spearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} = 1 - \\frac{6(2)}{6(36-1)} = 1 - \\frac{12}{210} = 0.94 \n\n\n\n15.11.5 Verification using R\n\n# Calculate correlations using R\ncor(data$anxiety_level, data$cognitive_performance, method = \"pearson\")\n\n[1] -0.9527979\n\ncor(data$anxiety_level, data$cognitive_performance, method = \"spearman\")\n\n[1] -0.9428571\n\n\n\n\n15.11.6 Interpretation\n\nThe strong negative Pearson correlation (r = -0.95) indicates a very strong negative linear relationship between anxiety level and cognitive performance.\nThe strong positive Spearman correlation (ρ = 0.94) shows that the relationship is also strongly monotonic.\nThe difference between Pearson and Spearman correlations suggests that while there is a strong relationship, it might not be perfectly linear.\n\n\n\n15.11.7 Exercise\n\nVerify each calculation step in the tables above.\nTry calculating these measures with a modified dataset:\n\nAdd one outlier and observe how it affects both correlation coefficients\nChange one pair of values and recalculate",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.2-calculating-covariance-pearson-spearman-correlation-and-ols---worked-example",
    "href": "correg_en.html#appendix-a.2-calculating-covariance-pearson-spearman-correlation-and-ols---worked-example",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.12 Appendix A.2: Calculating Covariance, Pearson, Spearman Correlation, and OLS - worked example",
    "text": "15.12 Appendix A.2: Calculating Covariance, Pearson, Spearman Correlation, and OLS - worked example\nA political science student is investigating the relationship between district magnitude (DM) and Gallagher’s disproportionality index (GH) in parliamentary elections across 10 randomly selected democracies.\nData on electoral district magnitudes (\\text{DM}) and Gallagher index:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18.2\n\n\n3\n16.7\n\n\n4\n15.8\n\n\n5\n15.3\n\n\n6\n15.0\n\n\n7\n14.8\n\n\n8\n14.7\n\n\n9\n14.6\n\n\n10\n14.55\n\n\n11\n14.52\n\n\n\n\n15.12.1 Step 1: Calculate Basic Statistics\nCalculation of means:\nFor \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nDetailed calculation:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6.5\nFor Gallagher index (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nDetailed calculation:\n18.2 + 16.7 + 15.8 + 15.3 + 15.0 + 14.8 + 14.7 + 14.6 + 14.55 + 14.52 = 154.17 \\bar{y} = \\frac{154.17}{10} = 15.417\n\n\n15.12.2 Step 2: Detailed Covariance Calculations\nComplete working table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n-4.5\n2.783\n-12.5235\n20.25\n7.7451\n\n\n2\n3\n16.7\n-3.5\n1.283\n-4.4905\n12.25\n1.6461\n\n\n3\n4\n15.8\n-2.5\n0.383\n-0.9575\n6.25\n0.1467\n\n\n4\n5\n15.3\n-1.5\n-0.117\n0.1755\n2.25\n0.0137\n\n\n5\n6\n15.0\n-0.5\n-0.417\n0.2085\n0.25\n0.1739\n\n\n6\n7\n14.8\n0.5\n-0.617\n-0.3085\n0.25\n0.3807\n\n\n7\n8\n14.7\n1.5\n-0.717\n-1.0755\n2.25\n0.5141\n\n\n8\n9\n14.6\n2.5\n-0.817\n-2.0425\n6.25\n0.6675\n\n\n9\n10\n14.55\n3.5\n-0.867\n-3.0345\n12.25\n0.7517\n\n\n10\n11\n14.52\n4.5\n-0.897\n-4.0365\n20.25\n0.8047\n\n\nSum\n65\n154.17\n0\n0\n-28.085\n82.5\n12.8442\n\n\n\nCovariance calculation: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28.085}{9} = -3.120556\n\n\n15.12.3 Step 3: Standard Deviation Calculations\nFor \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82.5}{9}} = \\sqrt{9.1667} = 3.026582\nFor Gallagher (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12.8442}{9}} = \\sqrt{1.4271} = 1.194612\n\n\n15.12.4 Step 4: Pearson Correlation Calculation\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3.120556}{3.026582 \\times 1.194612} = \\frac{-3.120556}{3.615752} = -0.863044\n\n\n15.12.5 Step 5: Spearman Rank Correlation Calculation\nComplete ranking table with all calculations:\n\n\n\ni\nX_i\nY_i\nRank X_i\nRank Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18.2\n1\n10\n-9\n81\n\n\n2\n3\n16.7\n2\n9\n-7\n49\n\n\n3\n4\n15.8\n3\n8\n-5\n25\n\n\n4\n5\n15.3\n4\n7\n-3\n9\n\n\n5\n6\n15.0\n5\n6\n-1\n1\n\n\n6\n7\n14.8\n6\n5\n1\n1\n\n\n7\n8\n14.7\n7\n4\n3\n9\n\n\n8\n9\n14.6\n8\n3\n5\n25\n\n\n9\n10\n14.55\n9\n2\n7\n49\n\n\n10\n11\n14.52\n10\n1\n9\n81\n\n\nSum\n\n\n\n\n\n330\n\n\n\nSpearman correlation calculation: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\n15.12.6 Step 6: R Verification\n\n# Create vectors\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Calculate covariance\ncov(DM, GH)\n\n[1] -3.120556\n\n# Calculate correlations\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\n15.12.7 Step 7: Basic Visualization\n\nlibrary(ggplot2)\n\n# Create data frame\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Create scatter plot\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"District Magnitude vs Gallagher Index\",\n    x = \"District Magnitude (DM)\",\n    y = \"Gallagher Index (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n15.12.8 OLS Estimation and Goodness-of-Fit Measures\n\n\n15.12.9 Step 1: Calculate OLS Estimates\nUsing previously calculated values:\n\n\\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28.085\n\\sum(X_i - \\bar{X})^2 = 82.5\n\\bar{X} = 6.5\n\\bar{Y} = 15.417\n\nCalculate slope (\\hat{\\beta_1}): \\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 ÷ 82,5 = -0,3404\nCalculate intercept (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 × 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nTherefore, the OLS regression equation is: \\hat{Y} = 17.6296 - 0.3404X\n\n\n15.12.10 Step 2: Calculate Fitted Values and Residuals\nComplete table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n16.9488\n1.2512\n1.5655\n7.7451\n2.3404\n\n\n2\n3\n16.7\n16.6084\n0.0916\n0.0084\n1.6461\n1.4241\n\n\n3\n4\n15.8\n16.2680\n-0.4680\n0.2190\n0.1467\n0.7225\n\n\n4\n5\n15.3\n15.9276\n-0.6276\n0.3939\n0.0137\n0.2601\n\n\n5\n6\n15.0\n15.5872\n-0.5872\n0.3448\n0.1739\n0.0289\n\n\n6\n7\n14.8\n15.2468\n-0.4468\n0.1996\n0.3807\n0.0290\n\n\n7\n8\n14.7\n14.9064\n-0.2064\n0.0426\n0.5141\n0.2610\n\n\n8\n9\n14.6\n14.5660\n0.0340\n0.0012\n0.6675\n0.7241\n\n\n9\n10\n14.55\n14.2256\n0.3244\n0.1052\n0.7517\n1.4184\n\n\n10\n11\n14.52\n13.8852\n0.6348\n0.4030\n0.8047\n2.3439\n\n\nSum\n65\n154.17\n154.17\n0\n3.2832\n12.8442\n9.5524\n\n\n\nCalculations for fitted values:\nFor X = 2:\nŶ = 17.6296 + (-0.3404 × 2) = 16.9488\n\nFor X = 3:\nŶ = 17.6296 + (-0.3404 × 3) = 16.6084\n\n[... continue for all values]\n\n\n15.12.11 Step 3: Calculate Goodness-of-Fit Measures\nSum of Squared Errors (SSE): SSE = \\sum e_i^2\nSSE = 3.2832\nSum of Squared Total (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12.8442\nSum of Squared Regression (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9.5524\nVerify decomposition: SST = SSR + SSE\n12.8442 = 9.5524 + 3.2832 (within rounding error)\nR-squared calculation: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR² = 9.5524 ÷ 12.8442\n   = 0.7438\n\n\n15.12.12 Step 4: R Verification\n\n# Fit linear model\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# View summary statistics\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 4.67e-10 ***\nDM          -0.34042    0.07053  -4.827  0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Calculate R-squared manually\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\n15.12.13 Step 5: Residual Analysis\n\n# Create residual plots\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\n15.12.14 Step 6: Predicted vs Actual Values Plot\n\n# Create predicted vs actual plot\nggplot(data.frame(\n  Actual = GH,\n  Predicted = fitted(model)\n), aes(x = Predicted, y = Actual)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Predicted vs Actual Values\",\n    x = \"Predicted Gallagher Index\",\n    y = \"Actual Gallagher Index\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n15.12.15 Log-Transformed Models\n\n\n15.12.16 Step 1: Data Transformation\nFirst, calculate natural logarithms of variables:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18.2\n0.6931\n2.9014\n\n\n2\n3\n16.7\n1.0986\n2.8154\n\n\n3\n4\n15.8\n1.3863\n2.7600\n\n\n4\n5\n15.3\n1.6094\n2.7278\n\n\n5\n6\n15.0\n1.7918\n2.7081\n\n\n6\n7\n14.8\n1.9459\n2.6946\n\n\n7\n8\n14.7\n2.0794\n2.6878\n\n\n8\n9\n14.6\n2.1972\n2.6810\n\n\n9\n10\n14.55\n2.3026\n2.6777\n\n\n10\n11\n14.52\n2.3979\n2.6757\n\n\n\n\n\n15.12.17 Step 2: Compare Different Model Specifications\nWe estimate three alternative specifications:\n\nLog-linear model: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nLinear-log model: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nLog-log model: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Create transformed variables\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Fit models\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Compare R-squared values\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Linear\", \"Log-linear\", \"Linear-log\", \"Log-log\"),\n  R_squared = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Display comparison\nmodels_comparison\n\n       Model R_squared\n1     Linear 0.7443793\n2 Log-linear 0.7670346\n3 Linear-log 0.9141560\n4    Log-log 0.9288088\n\n\n\n\n15.12.18 Step 3: Visual Comparison\n\n# Create plots for each model\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear Model\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-linear Model\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear-log Model\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-log Model\") +\n  theme_minimal()\n\n# Arrange plots in a grid\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n15.12.19 Step 4: Residual Analysis for Best Model\nBased on R-squared values, analyze residuals for the best-fitting model:\n\n# Residual plots for best model\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\n15.12.20 Step 5: Interpretation of Best Model\nThe linear-log model coefficients:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 4.94e-11 ***\nlog_DM       -2.0599     0.2232   -9.23 1.54e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 1.539e-05\n\n\nInterpretation: - \\hat{\\beta_0} represents the expected Gallagher Index when ln(DM) = 0 (i.e., when DM = 1) - \\hat{\\beta_1} represents the change in Gallagher Index associated with a one-unit increase in ln(DM)\n\n\n15.12.21 Step 6: Model Predictions\n\n# Create prediction plot for best model\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Linear-log Model: Gallagher Index vs ln(District Magnitude)\",\n    x = \"ln(District Magnitude)\",\n    y = \"Gallagher Index\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n15.12.22 Step 7: Elasticity Analysis\nFor the log-log model, coefficients represent elasticities directly. Calculate average elasticity for the linear-log model:\n\n# Calculate elasticity at means\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelasticity &lt;- beta1 * (1/mean_GH)\nelasticity\n\n    log_DM \n-0.1336136 \n\n\nThis represents the percentage change in the Gallagher Index for a 1% change in District Magnitude.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "href": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.13 Appendix A.3: Understanding Pearson, Spearman, and Kendall",
    "text": "15.13 Appendix A.3: Understanding Pearson, Spearman, and Kendall\n\n15.13.1 Dataset\n\ndata &lt;- data.frame(\n  x = c(2, 4, 5, 3, 8),\n  y = c(3, 5, 4, 4, 7)\n)\n\n\n\n15.13.2 Pearson Correlation\n r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}} \n\n15.13.2.1 Step-by-Step Calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n2\n3\n-2.4\n-1.6\n3.84\n5.76\n2.56\n\n\n2\n4\n5\n-0.4\n0.4\n-0.16\n0.16\n0.16\n\n\n3\n5\n4\n0.6\n-0.6\n-0.36\n0.36\n0.36\n\n\n4\n3\n4\n-1.4\n-0.6\n0.84\n1.96\n0.36\n\n\n5\n8\n7\n3.6\n2.4\n8.64\n12.96\n5.76\n\n\nSum\n22\n23\n0\n0\n12.8\n21.2\n9.2\n\n\n\n\\bar{x} = 4.4 \\bar{y} = 4.6\n r = \\frac{12.8}{\\sqrt{21.2 \\times 9.2}} = \\frac{12.8}{\\sqrt{195.04}} = \\frac{12.8}{13.97} = 0.92 \n\n\n\n15.13.3 Spearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} \n\n15.13.3.1 Step-by-Step Calculations:\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n3\n1\n1\n0\n0\n\n\n2\n4\n5\n3\n5\n-2\n4\n\n\n3\n5\n4\n4\n2.5\n1.5\n2.25\n\n\n4\n3\n4\n2\n2.5\n-0.5\n0.25\n\n\n5\n8\n7\n5\n4\n1\n1\n\n\nSum\n\n\n\n\n\n7.5\n\n\n\n \\rho = 1 - \\frac{6(7.5)}{5(25-1)} = 1 - \\frac{45}{120} = 0.82 \n\n\n\n15.13.4 Kendall’s Tau\n \\tau = \\frac{\\text{number of concordant pairs} - \\text{number of discordant pairs}}{\\frac{1}{2}n(n-1)} \n\n15.13.4.1 Step-by-Step Calculations:\n\n\n\nPair (i,j)\nx_i,x_j\ny_i,y_j\nx_j-x_i\ny_j-y_i\nResult\n\n\n\n\n(1,2)\n2,4\n3,5\n+2\n+2\nC\n\n\n(1,3)\n2,5\n3,4\n+3\n+1\nC\n\n\n(1,4)\n2,3\n3,4\n+1\n+1\nC\n\n\n(1,5)\n2,8\n3,7\n+6\n+4\nC\n\n\n(2,3)\n4,5\n5,4\n+1\n-1\nD\n\n\n(2,4)\n4,3\n5,4\n-1\n-1\nC\n\n\n(2,5)\n4,8\n5,7\n+4\n+2\nC\n\n\n(3,4)\n5,3\n4,4\n-2\n0\nD\n\n\n(3,5)\n5,8\n4,7\n+3\n+3\nC\n\n\n(4,5)\n3,8\n4,7\n+5\n+3\nC\n\n\n\nNumber of concordant pairs = 8 Number of discordant pairs = 2  \\tau = \\frac{8-2}{10} = 0.74 \n\n\n\n15.13.5 Verification in R\n\ncat(\"Pearson:\", round(cor(data$x, data$y, method=\"pearson\"), 2), \"\\n\")\n\nPearson: 0.92 \n\ncat(\"Spearman:\", round(cor(data$x, data$y, method=\"spearman\"), 2), \"\\n\")\n\nSpearman: 0.82 \n\ncat(\"Kendall:\", round(cor(data$x, data$y, method=\"kendall\"), 2), \"\\n\")\n\nKendall: 0.74 \n\n\n\n\n15.13.6 Interpretation of Results\n\nPearson Correlation (r = 0.92)\n\nStrong positive linear correlation\nIndicates a very strong linear relationship between variables\n\nSpearman Correlation (ρ = 0.82)\n\nAlso strong positive correlation\nSlightly lower than Pearson’s, suggesting some deviations from monotonicity\n\nKendall’s Tau (τ = 0.74)\n\nLowest of the three values, but still indicates strong association\nMore robust to outliers\n\n\n\n\n15.13.7 Comparison of Measures\n\nDifferences in Values:\n\nPearson (0.92) - highest value, strong linearity\nSpearman (0.82) - considers only ranking\nKendall (0.74) - most conservative measure\n\nPractical Application:\n\nAll measures confirm strong positive association\nDifferences between measures indicate slight deviations from perfect linearity\nKendall provides the most conservative estimate of relationship strength\n\n\n\n\n15.13.8 Exercises\n\nChange y[3] from 4 to 6 and recalculate all three correlations\nAdd an outlier (x=10, y=2) and recalculate correlations\nCompare which measure is most sensitive to changes in the data\n\n\n\n15.13.9 Key Points to Remember\n\nPearson Correlation:\n\nMeasures linear relationship\nMost sensitive to outliers\nRequires interval or ratio data\n\nSpearman Correlation:\n\nMeasures monotonic relationship\nLess sensitive to outliers\nWorks with ordinal data\n\nKendall’s Tau:\n\nMeasures ordinal association\nMost robust to outliers\nBest for small samples and tied ranks",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "href": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.14 Appendix B: Bias in OLS Estimation with Endogenous Regressors",
    "text": "15.14 Appendix B: Bias in OLS Estimation with Endogenous Regressors\nIn this tutorial, we will explore the bias in Ordinary Least Squares (OLS) estimation when the error term is correlated with the explanatory variable, a situation known as endogeneity. We will first derive the bias mathematically and then illustrate it using a simulated dataset in R.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#theoretical-derivation",
    "href": "correg_en.html#theoretical-derivation",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.15 Theoretical Derivation",
    "text": "15.15 Theoretical Derivation\nConsider a data generating process (DGP) where the true relationship between x and y is:\n y = 2x + e \nHowever, there is an endogeneity problem because the error term e is correlated with x in the following way:\n e = 1x + u \nwhere u is an independent error term.\nIf we estimate the simple linear model y = \\hat{\\beta_0} + \\hat{\\beta_1}x + \\varepsilon using OLS, the OLS estimator of \\hat{\\beta_1} will be biased due to the endogeneity issue.\nTo understand the bias, let’s derive the expected value of the OLS estimator \\hat{\\beta}_1:\n\\begin{align*}\nE[\\hat{\\beta}_1] &= E[(X'X)^{-1}X'y] \\\\\n                 &= E[(X'X)^{-1}X'(2x + 1x + u)] \\\\\n                 &= E[(X'X)^{-1}X'(3x + u)] \\\\\n                 &= 3 + E[(X'X)^{-1}X'u]\n\\end{align*}\nIf the error term u is uncorrelated with x, then E[(X'X)^{-1}X'u] = 0, and the OLS estimator would be unbiased: E[\\hat{\\beta}_1] = 3. However, in this case, the original error term e is correlated with x, so u is also likely to be correlated with x.\nAssuming E[(X'X)^{-1}X'u] \\neq 0, the OLS estimator will be biased:\n\\begin{align*}\n\\text{Bias}(\\hat{\\beta}_1) &= E[\\hat{\\beta}_1] - \\beta_{1,\\text{true}} \\\\\n                           &= 3 + E[(X'X)^{-1}X'u] - 2 \\\\\n                           &= 1 + E[(X'X)^{-1}X'u]\n\\end{align*}\nThe direction and magnitude of the bias will depend on the correlation between x and u. If x and u are positively correlated, the bias will be positive, and the OLS estimator will overestimate the true coefficient. Conversely, if x and u are negatively correlated, the bias will be negative, and the OLS estimator will underestimate the true coefficient.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simulation-in-r",
    "href": "correg_en.html#simulation-in-r",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.16 Simulation in R",
    "text": "15.16 Simulation in R\nLet’s create a simple dataset with 10 observations where x is in the interval 1:10, and generate y values based on the given DGP: y = 2x + e, where e = 1x + u, and u is a random error term.\n\nset.seed(123)  # for reproducibility\nx &lt;- 1:10\nu &lt;- rnorm(10, mean = 0, sd = 1)\ne &lt;- 1*x + u\n# e &lt;- 1*x\ny &lt;- 2*x + e\n\n# Generate the data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Estimate the OLS model\nmodel &lt;- lm(y ~ x, data = data)\n\n# Print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1348 -0.5624 -0.1393  0.3854  1.6814 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.5255     0.6673   0.787    0.454    \nx             2.9180     0.1075  27.134 3.67e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9768 on 8 degrees of freedom\nMultiple R-squared:  0.9893,    Adjusted R-squared:  0.9879 \nF-statistic: 736.3 on 1 and 8 DF,  p-value: 3.666e-09\n\n\nIn this example, the true relationship is y = 2x + e, where e = 1x + u. However, when we estimate the OLS model, we get:\n \\hat{y} = 0.18376 + 3.05874x \nThe estimated coefficient for x is 3.05874, which is biased upward from the true value of 2. This bias is due to the correlation between the error term e and the explanatory variable x.\nTo visualize the bias using ggplot2, we can plot the true relationship (y = 2x) and the estimated OLS relationship:\n\nlibrary(ggplot2)\n\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 2, color = \"blue\", linewidth = 1, linetype = \"dashed\") +\n  geom_abline(intercept = coef(model)[1], slope = coef(model)[2], color = \"red\", linewidth = 1) +\n  labs(title = \"True vs. Estimated Relationship\", x = \"x\", y = \"y\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  scale_color_manual(name = \"Lines\", values = c(\"blue\", \"red\"), \n                     labels = c(\"True\", \"OLS\"))\n\n\n\n\n\n\n\n\nThe plot will show that the estimated OLS line (red) is steeper than the true relationship line (blue), illustrating the upward bias in the estimated coefficient.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#conclusion-2",
    "href": "correg_en.html#conclusion-2",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.17 Conclusion",
    "text": "15.17 Conclusion\nIn summary, when the error term is correlated with the explanatory variable (endogeneity), the OLS estimator will be biased. The direction and magnitude of the bias depend on the nature of the correlation between the error term and the explanatory variable. This tutorial demonstrated the bias both mathematically and through a simulated example in R, using ggplot2 for visualization.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-c.-worked-examples",
    "href": "correg_en.html#appendix-c.-worked-examples",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.18 Appendix C. Worked Examples",
    "text": "15.18 Appendix C. Worked Examples",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "href": "correg_en.html#descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.19 Descriptive Statistics and OLS Example - Income and Voter Turnout",
    "text": "15.19 Descriptive Statistics and OLS Example - Income and Voter Turnout\nBackground\nIn preparation for the 2024 municipal elections, the Amsterdam Electoral Commission conducted research on voter participation patterns across different city neighborhoods. A key question emerged:\nDoes economic prosperity of a neighborhood correlate with civic engagement, specifically voter turnout?\nData Collection\nSample: 5 representative neighborhoods in Amsterdam\nTime Period: Data from the 2022 municipal elections\nVariables:\n\nIncome: Average annual household income per capita (thousands €)\nTurnout: Percentage of registered voters who voted in the election\n\n\n15.19.1 Initial R Output for Reference\n\n# Data\nincome &lt;- c(50, 45, 56, 40, 60)  # thousands €\nturnout &lt;- c(60, 56, 70, 50, 75) # %\n\n# Full model check\nmodel &lt;- lm(turnout ~ income)\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n\n\n\n15.19.2 Dispersion Measures\nMeans:\n\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} = \\frac{50 + 45 + 56 + 40 + 60}{5} = \\frac{251}{5} = 50.2\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} = \\frac{60 + 56 + 70 + 50 + 75}{5} = \\frac{311}{5} = 62.2\n\n# Verification\nmean(income)  # 50.2\n\n[1] 50.2\n\nmean(turnout) # 62.2\n\n[1] 62.2\n\n\nVariances:\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X: (-0.2, -5.2, 5.8, -10.2, 9.8)\ns^2_X = \\frac{0.04 + 27.04 + 33.64 + 104.04 + 96.04}{4} = \\frac{260.8}{4} = 65.2\nDeviations for Y: (-2.2, -6.2, 7.8, -12.2, 12.8)\ns^2_Y = \\frac{4.84 + 38.44 + 60.84 + 148.84 + 163.84}{4} = \\frac{416.8}{4} = 104.2\n\n# Verification\nvar(income)  # 65.2\n\n[1] 65.2\n\nvar(turnout) # 104.2\n\n[1] 104.2\n\n\n\n\n15.19.3 Covariance and Correlation\nCovariance:\ns_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-0.2 \\times -2.2) = 0.44 (-5.2 \\times -6.2) = 32.24 (5.8 \\times 7.8) = 45.24 (-10.2 \\times -12.2) = 124.44 (9.8 \\times 12.8) = 125.44\ns_{XY} = \\frac{327.8}{4} = 81.95\n\n# Verification\ncov(income, turnout) # 81.95\n\n[1] 81.95\n\n\nCorrelation:\nr_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{81.95}{\\sqrt{65.2}\\sqrt{104.2}} = 0.994\n\n# Verification\ncor(income, turnout) # 0.994\n\n[1] 0.9942402\n\n\n\n\n15.19.4 OLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{81.95}{65.2} = 1.2571429\nIntercept:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\nStep by step:\n\n1.2571429 \\times 50.2 = 63.1085714\n\\hat{\\beta_0} = 62.2 - 63.1085714 = -0.9085714\n\n\n# Verification\ncoef(model)  # Exact coefficients from R\n\n(Intercept)      income \n -0.8964724   1.2569018 \n\n\n\n\n15.19.5 Detailed Decomposition of Variance and R-squared\nStep 1: Calculate predicted values (\\hat{Y}):\n \\hat{Y} = -0.9085714 + 1.2571429X\nThe predicted values \\hat{Y} for each X value:\nFor X = 50:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (50)\n \\hat{Y} = -0.9085714 + 62.857145 \\hat{Y} = 61.9485736\nFor X = 45:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (45)\n \\hat{Y} = -0.9085714 + 56.5714305 \\hat{Y} = 55.6535591\nFor X = 56:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (56)\n \\hat{Y} = -0.9085714 + 70.4200024 \\hat{Y} = 69.5114310\nFor X = 40:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (40)\n \\hat{Y} = -0.9085714 + 50.2657160 \\hat{Y} = 49.3571446\nFor X = 60:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (60)\n \\hat{Y} = -0.9085714 + 75.4285740 \\hat{Y} = 74.5200026\n\n# Verification of predicted values\ny_hat &lt;- -0.9085714 + 1.2571429 * income\ndata.frame(\n  X = income,\n  Y = turnout,\n  Y_hat = y_hat,\n  row.names = 1:5\n)\n\n   X  Y    Y_hat\n1 50 60 61.94857\n2 45 56 55.66286\n3 56 70 69.49143\n4 40 50 49.37714\n5 60 75 74.52000\n\n\nStep 2: Calculate SST (Total Sum of Squares)\nSST = \\sum(Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = 62.2\n(60 - 62.2)^2 = (-2.2)^2 = 4.84 (56 - 62.2)^2 = (-6.2)^2 = 38.44 (70 - 62.2)^2 = (7.8)^2 = 60.84 (50 - 62.2)^2 = (-12.2)^2 = 148.84 (75 - 62.2)^2 = (12.8)^2 = 163.84\nSST = 4.84 + 38.44 + 60.84 + 148.84 + 163.84 = 416.8\nStep 3: Calculate SSR (Regression Sum of Squares)\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\n(61.9485736 - 62.2)^2 = (-0.2514264)^2 = 0.0632151 (55.6535591 - 62.2)^2 = (-6.5464409)^2 = 42.8558689 (69.5114310 - 62.2)^2 = (7.3114310)^2 = 53.4570178 (49.3571446 - 62.2)^2 = (-12.8428554)^2 = 164.9389370 (74.5200026 - 62.2)^2 = (12.3200026)^2 = 151.7824640\nSSR = 413.0975028\nStep 4: Calculate SSE (Error Sum of Squares)\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n(60 - 61.9485736)^2 = (-1.9485736)^2 = 3.7969384 (56 - 55.6535591)^2 = (0.3464409)^2 = 0.1200212 (70 - 69.5114310)^2 = (0.4885690)^2 = 0.2387198 (50 - 49.3571446)^2 = (0.6428554)^2 = 0.4132631 (75 - 74.5200026)^2 = (0.4799974)^2 = 0.2303975\nSSE = 4.7024972\nStep 5: Verify decomposition\nSST = SSR + SSE 416.8 = 413.0975028 + 4.7024972\nStep 6: Calculate R-squared\nR^2 = \\frac{SSR}{SST} = \\frac{413.0975028}{416.8} = 0.9916\n\n# Verification\nsummary(model)$r.squared  # Should match our calculation\n\n[1] 0.9885135\n\n\n\n\n15.19.6 Visualization\n\nlibrary(ggplot2)\ndf &lt;- data.frame(income = income, turnout = turnout)\n\nggplot(df, aes(x = income, y = turnout)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Voter Turnout vs Income per Capita\",\n    x = \"Income per Capita (thousands €)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\n15.19.7 Interpretation\nThe analysis shows:\n\nA very strong positive correlation (r = 0.994) between income and voter turnout\nThe regression equation \\hat{Y} = -0.9085714 + 1.2571429X indicates that:\n\nFor each €1,000 increase in income, turnout increases by about 1.26 percentage points\nThe intercept (-0.9086) has little practical meaning as income is never zero\n\nThe R-squared of 0.9916 indicates that 99.16% of the variance in turnout is explained by income",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "href": "correg_en.html#anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.20 Anxiety Levels and Cognitive Performance: A Laboratory Study",
    "text": "15.20 Anxiety Levels and Cognitive Performance: A Laboratory Study\n\n15.20.1 Data and Context\nIn a psychology experiment, researchers measured the relationship between anxiety levels (measured by galvanic skin response, GSR) and cognitive performance (score on a working memory task).\n\n# Data\nanxiety &lt;- c(2.1, 3.4, 4.2, 5.1, 5.8, 6.4, 7.2, 8.0)  # GSR readings\nperformance &lt;- c(92, 88, 84, 78, 74, 70, 65, 62)      # Working memory scores\n\n# Initial model check\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8993 -0.6660  0.2162  0.6106  1.5262 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 105.3248     1.3189   79.86 2.60e-10 ***\nanxiety      -5.4407     0.2359  -23.06 4.35e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 6 degrees of freedom\nMultiple R-squared:  0.9888,    Adjusted R-squared:  0.987 \nF-statistic: 531.9 on 1 and 6 DF,  p-value: 4.355e-07\n\n\n\n\n15.20.2 Descriptive Statistics\nMeans: \\bar{X} = \\frac{2.1 + 3.4 + 4.2 + 5.1 + 5.8 + 6.4 + 7.2 + 8.0}{8} = \\frac{42.2}{8} = 5.275\n\\bar{Y} = \\frac{92 + 88 + 84 + 78 + 74 + 70 + 65 + 62}{8} = \\frac{613}{8} = 76.625\n\n# Verification\nmean(anxiety)\n\n[1] 5.275\n\nmean(performance)\n\n[1] 76.625\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(2.1 - 5.275) = -3.175\n(3.4 - 5.275) = -1.875\n(4.2 - 5.275) = -1.075\n(5.1 - 5.275) = -0.175\n(5.8 - 5.275) = 0.525\n(6.4 - 5.275) = 1.125\n(7.2 - 5.275) = 1.925\n(8.0 - 5.275) = 2.725\n\nSquared deviations:\n10.08063 + 3.51563 + 1.15563 + 0.03063 + 0.27563 + 1.26563 + 3.70563 +\n7.42563 = 27.45500\ns^2_X = \\frac{27.45500}{7} = 3.922143\nSimilarly for Y: Deviations:\n15.375, 11.375, 7.375, 1.375, -2.625, -6.625, -11.625, -14.625\ns^2_Y = \\frac{236.875 + 129.391 + 54.391 + 1.891 + 6.891 + 43.891 + 135.141 + 213.891}{7} = \\frac{822.362}{7} = 117.4803\n\n# Verification\nvar(anxiety)\n\n[1] 3.922143\n\nvar(performance)\n\n[1] 117.4107\n\n\n\n\n15.20.3 Covariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-3.175 × 15.375) = -48.815625\n(-1.875 × 11.375) = -21.328125\n(-1.075 × 7.375) = -7.928125\n(-0.175 × 1.375) = -0.240625\n(0.525 × -2.625) = -1.378125\n(1.125 × -6.625) = -7.453125\n(1.925 × -11.625) = -22.378125\n(2.725 × -14.625) = -39.853125\nSum = -149.375\ns_{XY} = \\frac{-149.375}{7} = -21.33929\n\n# Verification\ncov(anxiety, performance)\n\n[1] -21.33929\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-21.33929}{\\sqrt{3.922143}\\sqrt{117.4803}} = -0.9932\n\n# Verification\ncor(anxiety, performance)\n\n[1] -0.9944073\n\n\n\n\n15.20.4 OLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-21.33929}{3.922143} = -5.4407\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-5.4407 × 5.275 = -28.6997\n\\hat{\\beta_0} = 76.625 - (-28.6997) = 105.3247\n\n\n# Verification\ncoef(model)\n\n(Intercept)     anxiety \n 105.324804   -5.440721 \n\n\n\n\n15.20.5 4. R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}): \\hat{Y} = 105.3247 - 5.4407X\n\n# Predicted values\ny_hat &lt;- 105.3247 - 5.4407 * anxiety\ndata.frame(\n  Anxiety = anxiety,\n  Performance = performance,\n  Predicted = y_hat,\n  row.names = 1:8\n)\n\n  Anxiety Performance Predicted\n1     2.1          92  93.89923\n2     3.4          88  86.82632\n3     4.2          84  82.47376\n4     5.1          78  77.57713\n5     5.8          74  73.76864\n6     6.4          70  70.50422\n7     7.2          65  66.15166\n8     8.0          62  61.79910\n\n\nStep 2: Sum of Squares\nSST = \\sum(Y_i - \\bar{Y})^2 = 822.362\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 816.3094\nSSE = \\sum(Y_i - \\hat{Y}_i)^2 = 6.0526\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{816.3094}{822.362} = 0.9926\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.9888459\n\n\n\n\n15.20.6 Visualization\n\nlibrary(ggplot2)\n\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Cognitive Performance vs. Anxiety Levels\",\n    x = \"Anxiety (GSR)\",\n    y = \"Performance Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n15.20.7 Interpretation\n\nStrong negative correlation (r = -0.993) between anxiety and cognitive performance\nFor each unit increase in GSR (anxiety), performance decreases by approximately 5.44 points\nThe model explains 99.26% of the variance in performance scores\nThe relationship appears to be strongly linear, suggesting a reliable anxiety-performance relationship\nThe high intercept (105.32) represents the theoretical maximum performance at zero anxiety\n\n\n\n15.20.8 Study Limitations\n\nSmall sample size (n=8)\nPossible other confounding variables\nLimited range of anxiety levels\nCross-sectional rather than longitudinal data",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "href": "correg_en.html#district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.21 District Magnitude and Electoral Disproportionality: A Comparative Analysis",
    "text": "15.21 District Magnitude and Electoral Disproportionality: A Comparative Analysis\n\n15.21.1 Data Generating Process\nLet’s set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\n15.21.2 Descriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\n15.21.3 Covariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 × 3.2833) = -18.6057\n(-3.6667 × 2.0833) = -7.6387\n(-1.6667 × 3.4833) = -5.8056\n(1.3333 × -1.6167) = -2.1556\n(3.3333 × -3.2167) = -10.7223\n(6.3333 × -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\n15.21.4 OLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 × 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\n15.21.5 R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\n15.21.6 Visualization - True vs. Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + ε\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + ε\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\n15.21.7 Observations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (ε)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\n15.21.8 Interpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\n15.21.9 Study Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\n15.21.10 Limitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_pl.html",
    "href": "correg_pl.html",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "",
    "text": "16.1 Statystyki Dwuwymiarowe\nStatystyki dwuwymiarowe opisują związek między dwiema zmiennymi. Omówimy kilka miar, zaczynając od kowariancji.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#statystyki-dwuwymiarowe",
    "href": "correg_pl.html#statystyki-dwuwymiarowe",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "",
    "text": "Analiza Zależności Zmiennych w Badaniach Społecznych\n\n\n\nW tej sekcji przeanalizujemy, w jaki sposób różne zmienne społeczne wchodzą ze sobą w interakcje i korelują. Zbadamy cztery charakterystyczne typy zależności często obserwowane w naukach społecznych, wykorzystując empiryczne przykłady do zilustrowania kluczowych wzorców i ich znaczenia dla analizy danych.\n\n\n\n\n\n\n\n\n\nAnaliza Zależności między Zmiennymi:\n\nDodatnia Korelacja Liniowa (Dostęp do Opieki Zdrowotnej a Długość Życia)\n\nDane wykazują dodatnią zależność liniową między dostępem do opieki zdrowotnej a oczekiwaną długością życia. Analiza statystyczna wskazuje, że wzrost dostępności opieki zdrowotnej o 10 punktów procentowych koreluje z wydłużeniem oczekiwanej długości życia o około 2,5 roku. Zależność ta utrzymuje istotność statystyczną w całym obserwowanym zakresie.\n\nUjemna Korelacja Liniowa (Użytkowanie Urządzeń Cyfrowych a Jakość Snu)\n\nAnaliza ujawnia odwrotną zależność między czasem spędzonym przed ekranami urządzeń cyfrowych a wskaźnikami jakości snu. Dane wskazują, że każda dodatkowa godzina użytkowania urządzeń koreluje z mierzalnym spadkiem wskaźników jakości snu, wykazując spójną negatywną zależność liniową.\n\nBrak Korelacji (Infrastruktura a Wskaźniki Ekonomiczne (np. GDP per capita in PLN))\n\nZależność między gęstością infrastruktury a wskaźnikami ekonomicznymi (np. GDP per capita in PLN) nie wykazuje statystycznie istotnej korelacji. Ten brak korelacji sugeruje, że zmienne te funkcjonują niezależnie w ramach obserwowanych parametrów, wskazując na obecność innych czynników determinujących, nieuwzględnionych w tej analizie.\n\nZależność Nieliniowa (Wielkość Zespołu a Produktywność)\n\nZależność między wielkością zespołu a produktywnością wykazuje charakterystykę krzywoliniową. Dane wskazują na istnienie optymalnego przedziału liczebności zespołu, przy czym produktywność maleje zarówno poniżej, jak i powyżej tego przedziału. Pokazuje to, jak ważne jest uwzględnianie wzorców nieliniowych w badaniach organizacyjnych.\nAspekty Metodologiczne:\n\nIstotność Statystyczna: Obserwowane zależności wymagają weryfikacji pod kątem istotności statystycznej przed wyciągnięciem wniosków.\nNiezależność Zmiennych: Założenie o niezależności zmiennych wymaga weryfikacji poprzez odpowiednie testy statystyczne.\nZmienne Zakłócające: Analizy muszą uwzględniać potencjalne zmienne zakłócające, które mogą wpływać na obserwowane zależności.\nPrzyczynowość: Wzorce korelacji nie implikują koniecznie związków przyczynowych; ustalenie przyczynowości wymaga zastosowania dodatkowych metod badawczych.\n\nZastosowania Badawcze:\nZrozumienie tych wzorców zależności ma istotne znaczenie dla:\n\nProjektowania i metodologii badań\nProcedur analizy statystycznej\nRozwoju i testowania teorii\n\nKrytyczna ocena tych zależności umożliwia bardziej rzetelne projektowanie badań i formułowanie wiarygodnych wniosków w naukach społecznych.\n\n\n\n\n\n\n\n\nRozróżnienie między Korelacją a Przyczynowością [Zob. np. https://www.tylervigen.com/spurious-correlations]\n\n\n\n\n\n\nhttps://x.com/EUFIC/status/1324667630238814209?prefetchTimestamp=1732463940216\n\n\n\n\n\nhttps://sitn.hms.harvard.edu/flash/2021/when-correlation-does-not-imply-causation-why-your-gut-microbes-may-not-yet-be-a-silver-bullet-to-all-your-problems/\n\n\nZależności statystyczne między zmiennymi stanowią jeden z najczęściej błędnie interpretowanych aspektów analizy danych. Mimo że korelacje mogą ujawniać wzorce w danych, wymagają starannej interpretacji, aby uniknąć wyciągania nieprawidłowych wniosków przyczynowych. Przeanalizujmy to zagadnienie na przykładach.\n\n16.1.1 Wzorce Sezonowe i Pozorne Korelacje: Studium Przypadku\n\n\n\n\n\n\n\n\n\nTa wizualizacja przedstawia klasyczny przykład zmiennej zakłócającej w analizie statystycznej. Pozorna korelacja między sprzedażą lodów a wskaźnikiem przestępczości (r = 0,85, p &lt; 0,001) pokazuje, jak wahania sezonowe mogą tworzyć mylące zależności statystyczne. Korelacja wynika ze wspólnego czynnika przyczynowego: sezonowych zmian temperatury, które niezależnie wpływają na obie zmienne poprzez odrębne mechanizmy.\n\n\n16.1.2 Trendy Czasowe i Pozorne Związki\n\n\n\n\n\n\n\n\n\nTa druga analiza ilustruje błąd korelacji czasowej, gdzie dwa niezależnie malejące trendy tworzą sztuczny związek statystyczny. Pomimo silnego współczynnika korelacji (r = 0,91, p &lt; 0,001), nie istnieje żaden wiarygodny mechanizm przyczynowy łączący te zmienne.\n\n\n16.1.3 Mechanizmy Powstawania Pozornych Korelacji\nAnaliza statystyczna może być zakłócona przez kilka systematycznych błędów, które tworzą pozorne, ale pozbawione znaczenia korelacje. Oto główne mechanizmy:\n1. Zmienne Zakłócające\nZmienna zakłócająca tworzy pozorny związek między niezależnymi zmiennymi poprzez niezależny wpływ na każdą z nich. To zjawisko statystyczne wymaga starannego planowania eksperymentów i analizy wielowymiarowej w celu wykrycia i kontroli potencjalnych czynników zakłócających.\n2. Autokorelacja Czasowa\nGdy zmienne wykazują silne trendy czasowe, mogą wykazywać korelację po prostu dlatego, że zmieniają się w czasie, a nie z powodu jakiejkolwiek znaczącej relacji. Efekt ten można kontrolować poprzez metody takie jak usuwanie trendu lub różnicowanie szeregów czasowych.\n3. Błąd Jednoczesnej Przyczynowości\nWystępuje, gdy kierunek przyczynowości jest niejednoznaczny lub dwukierunkowy. Na przykład, wzrost gospodarczy i stopy inwestycji mogą wykazywać jednoczesną przyczynowość, ponieważ każda zmienna potencjalnie wpływa na drugą poprzez złożone mechanizmy sprzężenia zwrotnego.\n\n\n16.1.4 Metody Statystyczne Wnioskowania Przyczynowego\nWspółczesne podejścia statystyczne oferują kilka technik pozwalających wyjść poza prostą korelację w kierunku wnioskowania przyczynowego, np.:\n1. Planowanie Eksperymentów\nRandomizowane badania kontrolowane stanowią złoty standard wnioskowania przyczynowego, pozwalając badaczom na izolację wpływu pojedynczych zmiennych przy jednoczesnej kontroli czynników zakłócających.\n2. Zmienne Instrumentalne\nTa technika statystyczna wykorzystuje zmienną, która wpływa na wynik wyłącznie poprzez jej wpływ na zmienną będącą przedmiotem zainteresowania, pomagając ustalić związki przyczynowe w danych obserwacyjnych.\n3. Regresja Nieciągła\nTen quasi-eksperymentalny projekt wykorzystuje naturalnie występujące progi do przybliżenia eksperymentów randomizowanych, dostarczając dowodów na istnienie związków przyczynowych.\n\n\n16.1.5 Ramy Krytycznej Analizy Korelacji\nPrzy ocenie wyników korelacyjnych należy wziąć pod uwagę następujące ramy analityczne:\n\nWiarygodność Teoretyczna: Zbadanie, czy istnieje logiczny mechanizm, poprzez który jedna zmienna mogłaby wpływać na drugą.\nPierwszeństwo Czasowe: Weryfikacja, czy proponowana przyczyna poprzedza skutek w czasie.\nZależność Dawka-Odpowiedź: Ocena, czy zmiany w wielkości proponowanej przyczyny odpowiadają proporcjonalnym zmianom w skutku.\nSpójność: Ocena, czy zależność utrzymuje się w różnych kontekstach i populacjach.\nAlternatywne Wyjaśnienia: Systematyczne rozważanie i testowanie alternatywnych wyjaśnień zaobserwowanej korelacji.\n\nPamiętaj: Korelacja statystyczna stanowi warunek konieczny, ale niewystarczający do ustalenia przyczynowości.\n\n\n\n\n16.1.6 Kowariancja\nKowariancja mierzy, jak dwie zmienne są powiązane (zmieniają się razem).\nWzór: cov(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n\n\n\n\n\n\nOd Kowariancji do Różnych Miar Korelacji\n\n\n\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Generowanie różnych typów zależności\nset.seed(123)\nn &lt;- 100\n\n# Zależność liniowa\nx1 &lt;- rnorm(n)\ny1 &lt;- 0.8*x1 + rnorm(n, sd=0.5)\ndata1 &lt;- data.frame(x=x1, y=y1, type=\"Zależność Liniowa\")\n\n# Zależność monotoniczna nieliniowa\nx2 &lt;- rnorm(n)\ny2 &lt;- sign(x2)*(x2^2) + rnorm(n, sd=0.5)\ndata2 &lt;- data.frame(x=x2, y=y2, type=\"Monotoniczna Nieliniowa\")\n\n# Zależność niemonotoniczna\nx3 &lt;- seq(-3, 3, length.out=n)\ny3 &lt;- x3^2 + rnorm(n, sd=0.5)\ndata3 &lt;- data.frame(x=x3, y=y3, type=\"Niemonotoniczna\")\n\n# Łączenie danych\nall_data &lt;- rbind(data1, data2, data3)\n\n# Tworzenie wykresu\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6) +\n  facet_wrap(~type, scales = \"free\") +\n  labs(title = \"Różne Typy Zależności Między Zmiennymi\",\n       x = \"Zmienna X\",\n       y = \"Zmienna Y\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    strip.text = element_text(size = 12),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n16.1.7 Pojęcie Korelacji\nKorelacja to szerokie pojęcie opisujące, jak zmienne są ze sobą powiązane. Jak widać na wykresach, zależności te mogą przybierać różne formy.\n\n\n16.1.8 Rozpoczynając od Kowariancji\nKowariancja jest podstawową miarą wspólnej zmienności zmiennych:\nCov(X,Y) = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\nInformuje nas o tym:\n\nCzy zmienne zmieniają się w tym samym kierunku (kowariancja dodatnia)\nCzy zmieniają się w przeciwnych kierunkach (kowariancja ujemna)\nCzy brak wyraźnego wzorca liniowego (kowariancja bliska zeru)\n\nJednak kowariancja ma ograniczenie: jej wartość zależy od jednostek pomiaru. Na przykład:\n\nWzrost w metrach vs waga w kg daje jedną wartość kowariancji\nWzrost w centymetrach vs waga w kg daje inną wartość\nTa sama zależność, różne skale!\n\n\n\n16.1.9 Standaryzacja i Miary Korelacji\n\nWspółczynnik korelacji Pearsona standaryzuje kowariancję: r = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\n\n\nEliminuje zależność od jednostek\nZawsze między -1 a 1\nMierzy zależności liniowe\n\n\nWspółczynnik korelacji rangowej Spearmana:\n\n\nBazuje na rangach zamiast surowych wartości\nWychwytuje zależności monotoniczne (także nieliniowe)\nRównież przyjmuje wartości od -1 do 1\n\n\n\n16.1.10 Kluczowe Wnioski\n\nKowariancja pokazuje wspólną zmienność\nMiary korelacji dają standaryzowane wartości\nWybór miary korelacji zależy od:\n\nSpodziewanego typu zależności\nCharakteru danych\nPytania badawczego\n\nZawsze wizualizuj dane\n\n\n\n\n\n\n\n\n\n\nRangi: Pozycje w Uporządkowanym Ciągu\n\n\n\nRangi to po prostu numery pozycji w uporządkowanym zbiorze danych:\n\n16.1.11 Jak Wyznaczyć Rangi?\n\nPorządkujemy dane od najmniejszej do największej wartości\nPrzypisujemy kolejne liczby naturalne:\n\nNajmniejsza wartość → ranga 1\nKolejne wartości → kolejne rangi\nNajwiększa wartość → ranga n (liczba obserwacji)\nDla remisów → średnia rang\n\n\n\n\n16.1.12 Przykład\nMamy 5 studentów o wzroście:\nWzrost:   165, 182, 170, 168, 175\nRangi:     1,   5,   3,   2,   4\nDla danych z remisami (np. oceny):\nOceny:     2,   3,   3,   4,   5\nRangi:     1,  2.5, 2.5,  4,   5\n\n\n\nPrzykład Ręcznego Obliczenia:\nObliczmy kowariancję dla dwóch zmiennych:\n\nx: 1, 2, 3, 4, 5\ny: 2, 4, 5, 4, 5\n\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz średnie\n\\bar{x} = 3, \\bar{y} = 4\n\n\n2\nOblicz (x_i - \\bar{x})(y_i - \\bar{y}) dla każdej pary\n(-2)(-2) = 4\n\n\n\n\n(-1)(0) = 0\n\n\n\n\n(0)(1) = 0\n\n\n\n\n(1)(0) = 0\n\n\n\n\n(2)(1) = 2\n\n\n3\nZsumuj wyniki\n4 + 0 + 0 + 0 + 2 = 6\n\n\n4\nPodziel przez (n-1)\n6 / 4 = 1,5\n\n\n\nObliczenie w R:\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 5, 4, 5)\ncov(x, y)\n\n[1] 1.5\n\n\nInterpretacja: - Dodatnia kowariancja (1,5) wskazuje, że x i y mają tendencję do wzrostu razem.\nZalety:\n\nDostarcza informacji o kierunku związku (dodatni lub ujemny)\nPrzydatna w obliczaniu innych miar, takich jak korelacja\n\nWady:\n\nZależna od skali, co utrudnia porównywanie między różnymi parami zmiennych\nNie dostarcza informacji o sile związku\n\n\n\n16.1.13 Korelacja Pearsona\nKorelacja Pearsona mierzy siłę i kierunek liniowego związku między dwiema zmiennymi ciągłymi.\nWzór: r = \\frac{cov(X,Y)}{s_X s_Y} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\nPrzykład Ręcznego Obliczenia:\nUżywając tych samych danych co wyżej:\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz kowariancję\n(Z poprzedniego obliczenia) 1,5\n\n\n2\nOblicz odchylenia standardowe\ns_X = \\sqrt{\\frac{10}{4}} = 1,58, s_Y = \\sqrt{\\frac{6}{4}} = 1,22\n\n\n3\nPodziel kowariancję przez iloczyn odchyleń standardowych\n1,5 / (1,58 * 1,22) = 0,7746\n\n\n\nObliczenie w R:\n\ncor(x, y, method = \"pearson\")\n\n[1] 0.7745967\n\n\nInterpretacja: - Współczynnik korelacji 0,7746 wskazuje na silny dodatni związek liniowy między x i y.\nZalety:\n\nNiezależna od skali, zawsze między -1 a 1\nSzeroko rozumiana i stosowana\nTestuje związki liniowe\n\nWady:\n\nWrażliwa na wartości odstające\nMierzy tylko związki liniowe\nZakłada normalnie rozłożone zmienne\n\n\n\n16.1.14 Korelacja Spearmana\nKorelacja Spearmana mierzy siłę i kierunek monotonicznego związku między dwiema zmiennymi, które mogą być ciągłe lub porządkowe.\nWzór: \\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}, gdzie d_i to różnica między rangami.\nPrzykład Ręcznego Obliczenia:\nUżyjmy nieco innych danych:\n\nx: 1, 2, 3, 4, 5\ny: 1, 3, 2, 5, 4\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPrzypisz rangi obu zmiennym\nx_ranga: 1, 2, 3, 4, 5\n\n\n\n\ny_ranga: 1, 3, 2, 5, 4\n\n\n2\nOblicz różnice w rangach (d)\n0, -1, 1, -1, 1\n\n\n3\nPodnieś różnice do kwadratu\n0, 1, 1, 1, 1\n\n\n4\nZsumuj kwadraty różnic\n\\sum d_i^2 = 4\n\n\n5\nZastosuj wzór\n\\rho = 1 - \\frac{6(4)}{5(5^2 - 1)} = 0,8\n\n\n\nObliczenie w R:\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(1, 3, 2, 5, 4)\ncor(x, y, method = \"spearman\")\n\n[1] 0.8\n\n\nInterpretacja: - Korelacja Spearmana 0,8 wskazuje na silny dodatni związek monotoniczny między x i y.\nZalety:\n\nOdporna na wartości odstające\nMoże wykrywać nieliniowe związki monotoniczne\nOdpowiednia dla danych porządkowych\n\nWady:\n\nMniej odporna niż korelacja Pearsona do wykrywania związków liniowych w normalnie rozłożonych danych\nNie dostarcza informacji o kształcie związku poza monotonicznością\n\n\n\n16.1.15 Tabela Krzyżowa\nTabela krzyżowa (tabela kontyngencji) pokazuje związek między dwiema zmiennymi kategorycznymi.\nPrzykład:\nStwórzmy tabelę krzyżową dla dwóch zmiennych: - Poziom wykształcenia: Średnie, Wyższe, Podyplomowe - Status zatrudnienia: Zatrudniony, Bezrobotny\n\nwyksztalcenie &lt;- factor(c(\"Średnie\", \"Wyższe\", \"Podyplomowe\", \"Średnie\", \"Wyższe\", \"Podyplomowe\", \"Średnie\", \"Wyższe\", \"Podyplomowe\"))\nzatrudnienie &lt;- factor(c(\"Zatrudniony\", \"Zatrudniony\", \"Zatrudniony\", \"Bezrobotny\", \"Zatrudniony\", \"Zatrudniony\", \"Bezrobotny\", \"Bezrobotny\", \"Zatrudniony\"))\n\ntable(wyksztalcenie, zatrudnienie)\n\n             zatrudnienie\nwyksztalcenie Bezrobotny Zatrudniony\n  Podyplomowe          0           3\n  Średnie              2           1\n  Wyższe               1           2\n\n\nInterpretacja:\n\nTa tabela pokazuje liczbę osób w każdej kombinacji poziomu wykształcenia i statusu zatrudnienia.\nNa przykład, możemy zobaczyć, ilu absolwentów szkół średnich jest zatrudnionych, a ilu bezrobotnych.\n\nZalety:\n\nZapewnia jasną wizualną reprezentację związku między zmiennymi kategorycznymi\nŁatwa do zrozumienia i interpretacji\nPodstawa dla wielu testów statystycznych (np. test chi-kwadrat niezależności)\n\nWady:\n\nOgraniczona do danych kategorycznych\nMoże stać się nieporęczna przy wielu kategoriach\nNie dostarcza pojedynczej statystyki podsumowującej siłę związku\n\n\n\n16.1.16 Wybór Odpowiedniej Miary\nPrzy wyborze statystyki dwuwymiarowej należy wziąć pod uwagę:\n\nTyp danych:\n\nDane ciągłe: Kowariancja, korelacja Pearsona\nDane porządkowe: Korelacja Spearmana\nDane kategoryczne: Tabela krzyżowa\n\nTyp związku:\n\nLiniowy: Korelacja Pearsona\nMonotoniczny, ale potencjalnie nieliniowy: Korelacja Spearmana\n\nObecność wartości odstających:\n\nJeśli wartości odstające są problemem, korelacja Spearmana jest bardziej odporna\n\nRozkład:\n\nDla normalnie rozłożonych danych korelacja Pearsona jest najbardziej odporna (robust)\nDla rozkładów “nienormalnych” rozważ korelację Spearmana\n\nWielkość próby:\n\nDla bardzo małych prób metody nieparametryczne, takie jak korelacja Spearmana, mogą być preferowane\n\n\nPamiętaj, że często wartościowe jest użycie wielu miar i wizualizacji (takich jak wykresy rozrzutu), aby uzyskać kompleksowe zrozumienie związku między zmiennymi.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#wprowadzenie-do-podstawowej-statystyki-wielowymiarowej",
    "href": "correg_pl.html#wprowadzenie-do-podstawowej-statystyki-wielowymiarowej",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.2 Wprowadzenie do Podstawowej Statystyki Wielowymiarowej (*)",
    "text": "16.2 Wprowadzenie do Podstawowej Statystyki Wielowymiarowej (*)\nStatystyki wielowymiarowe obejmują analizę związków między trzema lub więcej zmiennymi jednocześnie. Ta sekcja wprowadzi niektóre podstawowe koncepcje i techniki analizy wielowymiarowej, koncentrując się na metodach opartych na korelacji.\n\n16.2.1 Macierz Korelacji\nMacierz korelacji to tabela pokazująca korelacje parami dla kilku zmiennych. Jest to podstawowe narzędzie w analizie wielowymiarowej.\nPrzykład: Stwórzmy macierz korelacji dla czterech zmiennych: wzrost, waga, wiek i dochód.\n\nset.seed(123)  # Dla powtarzalności\nwzrost &lt;- rnorm(100, 170, 10)\nwaga &lt;- wzrost * 0.5 + rnorm(100, 0, 5)\nwiek &lt;- rnorm(100, 40, 10)\ndochod &lt;- wiek * 1000 + rnorm(100, 0, 10000)\n\ndane &lt;- data.frame(wzrost, waga, wiek, dochod)\n\nmacierz_kor &lt;- cor(dane)\nprint(macierz_kor)\n\n           wzrost        waga        wiek      dochod\nwzrost  1.0000000  0.66712996 -0.12917601 -0.12246786\nwaga    0.6671300  1.00000000 -0.06814187 -0.04579492\nwiek   -0.1291760 -0.06814187  1.00000000  0.65654902\ndochod -0.1224679 -0.04579492  0.65654902  1.00000000\n\n\nInterpretacja: - Każda komórka pokazuje korelację między dwiema zmiennymi. - Przekątna zawsze wynosi 1 (korelacja zmiennej z samą sobą). - Szukaj silnych korelacji (bliskich 1 lub -1), aby zidentyfikować potencjalne związki.\n\n\n16.2.2 Wizualizacja Związków Wielowymiarowych\n\n16.2.2.1 Macierz Wykresów Rozrzutu\nMacierz wykresów rozrzutu pokazuje parami związki między wieloma zmiennymi.\n\npairs(dane)\n\n\n\n\n\n\n\n\nInterpretacja:\n\nKażdy wykres pokazuje związek między dwiema zmiennymi.\nElementy na przekątnej pokazują rozkład każdej zmiennej.\nSzukaj wzorców, skupisk lub trendów na wykresach.\n\n\n\n16.2.2.2 Wykres Korelacji\nWykres korelacji zapewnia wizualną reprezentację macierzy korelacji.\n\nlibrary(corrplot)\n\ncorrplot 0.94 loaded\n\ncorrplot(macierz_kor, method = \"color\")\n\n\n\n\n\n\n\n\nInterpretacja:\n\nIntensywność koloru i rozmiar kół wskazują na siłę korelacji.\nNiebieskie kolory zazwyczaj wskazują na dodatnie korelacje, czerwone na ujemne.\n\n\n\n\n16.2.3 Korelacja Cząstkowa\nKorelacja cząstkowa mierzy związek między dwiema zmiennymi przy kontrolowaniu jednej lub więcej innych zmiennych.\nPrzykład: Obliczmy korelację cząstkową między wzrostem a wagą, kontrolując wiek.\n\nlibrary(ppcor)\npcor.test(dane$wzrost, dane$waga, dane$wiek)\n\n   estimate      p.value statistic   n gp  Method\n1 0.6654367 5.758157e-14  8.779896 100  1 pearson\n\n\nInterpretacja:\n\nPorównaj to z prostą korelacją między wzrostem a wagą.\nZnacząca zmiana może wskazywać, że wiek odgrywa rolę w związku między wzrostem a wagą.\n\n\n\n16.2.4 Korelacja Wielokrotna\nKorelacja wielokrotna mierzy siłę związku między zmienną zależną a wieloma zmiennymi niezależnymi.\nPrzykład: Przewidźmy wagę na podstawie wzrostu i wieku.\n\nmodel &lt;- lm(waga ~ wzrost + wiek, data = dane)\nR &lt;- sqrt(summary(model)$r.squared)\nprint(paste(\"Współczynnik korelacji wielokrotnej:\", R))\n\n[1] \"Współczynnik korelacji wielokrotnej: 0.667377840470434\"\n\n\nInterpretacja:\n\nR waha się od 0 do 1, przy czym wyższe wartości wskazują na silniejsze związki.\nR² (R-kwadrat) reprezentuje proporcję wariancji w zmiennej zależnej wyjaśnioną przez zmienne niezależne.\n\n\n\n16.2.5 Analiza Czynnikowa\nAnaliza czynnikowa to technika używana do zredukowania wielu zmiennych do mniejszej liczby czynników leżących u podstaw.\nPrzykład: Wykonajmy prostą analizę czynnikową na naszym zbiorze danych.\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nwynik_fa &lt;- fa(dane, nfactors = 2, rotate = \"varimax\")\nprint(wynik_fa$loadings, cutoff = 0.3)\n\n\nLoadings:\n       MR2    MR1   \nwzrost  0.798       \nwaga    0.836       \nwiek           0.729\ndochod         0.895\n\n                 MR2   MR1\nSS loadings    1.344 1.341\nProportion Var 0.336 0.335\nCumulative Var 0.336 0.671\n\n\nInterpretacja:\n\nSpójrz, które zmienne ładują się wysoko na każdy czynnik.\nSpróbuj zinterpretować, co każdy czynnik może reprezentować na podstawie zmiennych, które się na niego ładują.\n\n\n\n16.2.6 Uwagi dotyczące Analizy Wielowymiarowej\n\nWielkość próby: Techniki wielowymiarowe często wymagają większych prób dla stabilnych wyników.\nWspółliniowość: Wysokie korelacje między zmiennymi niezależnymi mogą powodować problemy w niektórych analizach.\nWartości odstające: Wielowymiarowe wartości odstające mogą mieć silny wpływ na wyniki.\nZałożenia: Wiele technik zakłada wielowymiarową normalność i liniowe związki.\nZłożoność interpretacji: Wraz ze wzrostem liczby zmiennych interpretacja może stać się bardziej wyzwająca.\n\n\n\n16.2.7 Podsumowanie\nTo wprowadzenie do statystyki wielowymiarowej opiera się na koncepcji korelacji, aby badać związki między wieloma zmiennymi. Techniki te zapewniają potężne narzędzia do zrozumienia złożonych zbiorów danych, ale wymagają również starannego rozważenia założeń i ograniczeń. W miarę postępu w Twojej podróży statystycznej napotkasz bardziej zaawansowane techniki wielowymiarowe, takie jak MANOVA, analiza dyskryminacyjna i modelowanie równań strukturalnych.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n✔ purrr     1.0.2     ✔ tibble    3.2.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ psych::%+%()         masks ggplot2::%+%()\n✖ psych::alpha()       masks ggplot2::alpha()\n✖ gridExtra::combine() masks dplyr::combine()\n✖ dplyr::filter()      masks stats::filter()\n✖ dplyr::lag()         masks stats::lag()\n✖ MASS::select()       masks dplyr::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(gridExtra)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#wprowadzenie-do-analizy-regresji",
    "href": "correg_pl.html#wprowadzenie-do-analizy-regresji",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.3 Wprowadzenie do Analizy Regresji",
    "text": "16.3 Wprowadzenie do Analizy Regresji\nAnaliza regresji to metoda statystyczna, która bada i modeluje zależności między zmiennymi w celu zrozumienia, jak zmiany w jednej lub kilku zmiennych niezależnych wpływają na zmienną zależną.\nW swojej istocie analiza regresji pomaga odpowiedzieć na pytania dotyczące przyczyny i skutku, przewidywania oraz prognozowania. Na przykład, przedsiębiorstwo może wykorzystać analizę regresji do zrozumienia, jak wydatki na reklamę wpływają na sprzedaż lub jak liczba godzin szkoleń pracowników przekłada się na produktywność.\nProces rozpoczyna się od zbierania danych o interesujących nas zmiennych. Następnie analiza dopasowuje model matematyczny - zazwyczaj linię lub krzywą - który najlepiej reprezentuje zależność między tymi zmiennymi. Model ten pozwala:\n\nOkreślić siłę i kierunek zależności między zmiennymi\nDokonywać przewidywań dotyczących przyszłych wyników\nZrozumieć, które czynniki mają największy wpływ na nasze rezultaty\n\n\n16.3.1 Regresja jako Model Stochastyczny\nW modelowaniu matematycznym spotykamy dwa podstawowe podejścia do opisywania relacji między zmiennymi:\n\nmodele deterministyczne,\nmodele stochastyczne.\n\n\n16.3.1.1 Modele Deterministyczne a Stochastyczne\nModel deterministyczny zakłada precyzyjną, ustaloną relację między danymi wejściowymi a wyjściowymi. W takich modelach, znając dane wejściowe, możemy z całkowitą pewnością obliczyć dokładny wynik. Weźmy pod uwagę klasyczne równanie fizyczne dotyczące drogi:\n\\text{Droga} = \\text{Prędkość} × \\text{Czas}\nPrzy określonych wartościach prędkości i czasu, równanie to zawsze da tę samą drogę. Nie ma tu miejsca na jakąkolwiek zmienność wyniku.\nW przeciwieństwie do tego, analiza regresji uwzględnia naturalną zmienność danych. Podstawowa struktura modelu regresji to:\nY = f(X) + \\epsilon\nGdzie:\n\nY reprezentuje wynik, który chcemy przewidzieć\nf(X) reprezentuje systematyczną relację między naszymi predyktorami (X) a wynikiem\n\\epsilon reprezentuje losową zmienność naturalnie występującą w rzeczywistych danych\n\n\n\n16.3.1.2 Natura Modeli Stochastycznych w Regresji\nWłączenie składnika błędu \\epsilon uznaje, że rzeczywiste relacje między zmiennymi rzadko są idealne. Na przykład, badając wpływ czasu nauki na wyniki testów, wiele czynników przyczynia się do końcowego rezultatu:\nCzęść systematyczna f(X) wychwytuje ogólną tendencję: więcej czasu nauki zazwyczaj prowadzi do wyższych wyników.\nSkładnik błędu \\epsilon uwzględnia wszystkie inne czynniki:\n\nRóżne style uczenia się\nŚrodowisko nauki\nStan fizyczny i psychiczny podczas testu\nJakość materiałów dydaktycznych\n\n\n\n16.3.1.3 Struktura Matematyczna\nW swojej najprostszej formie regresja liniowa może być wyrażona jako:\nY = \\beta_0 + \\beta_1X + \\epsilon\nGdzie:\n\n\\beta_0 reprezentuje wartość bazową (gdy X = 0)\n\\beta_1 reprezentuje zmianę Y dla każdej jednostkowej zmiany X\n\\epsilon reprezentuje naturalną zmienność wokół tej relacji\n\n\n\n16.3.1.4 Implikacje Praktyczne\nZrozumienie regresji jako modelu stochastycznego ma istotne implikacje praktyczne:\n\nPrzewidywania: Uznajemy, że nasze przewidywania będą charakteryzować się pewną naturalną zmiennością. Zamiast twierdzić, że wynik będzie dokładny, uznajemy zakres prawdopodobnych wartości.\nOcena Modelu: Oceniamy modele na podstawie tego, jak dobrze ujmują zarówno ogólną tendencję, jak i typową wielkość zmienności w danych.\nPodejmowanie Decyzji: Zrozumienie naturalnej zmienności w naszych przewidywaniach pomaga nam podejmować bardziej realistyczne plany i decyzje.\n\nZastosowania w Rzeczywistości\nRozważmy przewidywanie cen domów na podstawie metrażu. Model deterministyczny mógłby stwierdzić: “Dom o powierzchni 186 metrów kwadratowych będzie kosztować dokładnie 1 200 000 złotych”\nModel regresji natomiast uznaje, że:\n\nIstnieje ogólna relacja między wielkością a ceną\nAle wiele innych czynników wpływa na końcową cenę\nPodobne domy mogą być sprzedawane po nieco różnych cenach\nNasze przewidywania powinny odzwierciedlać tę naturalną zmienność\n\nPodsumowanie\nStochastyczna natura analizy regresji zapewnia bardziej realistyczne ramy do zrozumienia rzeczywistych relacji między zmiennymi.\n\n\n\n\n\n\nModel Regresji Liniowej (MNK): Szybki Start\n\n\n\n\n16.3.2 Koncepcja Modelu\nRegresja MNK (Metoda Najmniejszych Kwadratów) to model statystyczny opisujący związek między zmiennymi. Dwa kluczowe założenia definiują ten model:\n\nZwiązek można opisać linią prostą (liniowość)\nBłędy w naszych przewidywaniach nie są systematycznie powiązane ze zmienną x (ścisła egzogeniczność)\n\n\n\n16.3.3 Przykład: Edukacja i Wynagrodzenia\nRozważmy badanie wpływu edukacji (x) na wynagrodzenia (y). Powiedzmy, że szacujemy:\nwynagrodzenia = \\beta_0 + \\beta_1 \\cdot edukacja + \\epsilon\nSkładnik błędu \\epsilon zawiera wszystkie inne czynniki wpływające na wynagrodzenia. Ścisła egzogeniczność jest naruszona, jeśli pominiemy ważną zmienną, jak “zdolności”, która wpływa zarówno na edukację, jak i wynagrodzenia. Dlaczego? Ponieważ bardziej zdolni ludzie mają tendencję do zdobywania lepszego wykształcenia I wyższych wynagrodzeń, co powoduje zawyżenie szacowanego efektu edukacji.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Generate sample data\nset.seed(123)\nn &lt;- 20\nx &lt;- seq(1, 10, length.out = n)\ny &lt;- 2 + 1.5 * x + rnorm(n, sd = 1.5)\ndata &lt;- data.frame(x = x, y = y)\n\n# Calculate OLS parameters\nbeta1 &lt;- cov(x, y) / var(x)\nbeta0 &lt;- mean(y) - beta1 * mean(x)\n\n# Create alternative lines\nlines_data &lt;- data.frame(\n  intercept = c(beta0, beta0 + 1, beta0 - 1),\n  slope = c(beta1, beta1 + 0.3, beta1 - 0.3),\n  line_type = c(\"Best fit (OLS)\", \"Suboptimal 1\", \"Suboptimal 2\")\n)\n\n# Create the plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point(size = 3, alpha = 0.6) +\n  geom_abline(data = lines_data,\n              aes(intercept = intercept, \n                  slope = slope,\n                  color = line_type,\n                  linetype = line_type),\n              size = 1) +\n  scale_color_manual(values = c(\"Best fit (OLS)\" = \"#FF4500\",\n                               \"Suboptimal 1\" = \"#4169E1\",\n                               \"Suboptimal 2\" = \"#228B22\")) +\n  labs(title = \"Finding the Best-Fitting Line\",\n       subtitle = \"Orange line minimizes the sum of squared errors\",\n       x = \"X Variable\",\n       y = \"Y Variable\",\n       color = \"Line Type\",\n       linetype = \"Line Type\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12),\n    legend.position = \"bottom\",\n    panel.grid.minor = element_blank()\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n16.3.4 Problem Optymalizacji: Metoda Najmniejszych Kwadratów (OLS)\nKiedy analizujemy zależności między zmiennymi, takimi jak poziom wykształcenia a wynagrodzenie, potrzebujemy systematycznej metody znalezienia linii, która najlepiej odzwierciedla tę relację w naszych danych. Metoda Najmniejszych Kwadratów (OLS - Ordinary Least Squares) dostarcza nam takiego rozwiązania poprzez precyzyjne podejście matematyczne.\nSpójrzmy na nasz wykres poziomów wykształcenia i wynagrodzeń. Każdy punkt reprezentuje rzeczywiste dane - poziom wykształcenia danej osoby i odpowiadające mu wynagrodzenie. Naszym celem jest znalezienie pojedynczej linii, która najdokładniej uchwyci podstawową zależność między tymi zmiennymi.\nDla każdej obserwacji i możemy wyrazić tę relację jako:\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\nGdzie:\n\ny_i to rzeczywiste zaobserwowane wynagrodzenie\n\\hat{y_i} = \\beta_0 + \\beta_1x_i to przewidywane wynagrodzenie\n\\epsilon_i = y_i - \\hat{y_i} to składnik błędu (reszta)\n\nMetoda OLS znajduje optymalne wartości dla \\beta_0 i \\beta_1 poprzez minimalizację sumy kwadratów błędów:\n\\min_{\\beta_0, \\beta_1} \\sum \\epsilon_i^2 = \\min_{\\beta_0, \\beta_1} \\sum(y_i - \\hat{y_i})^2 = \\min_{\\beta_0, \\beta_1} \\sum(y_i - (\\beta_0 + \\beta_1x_i))^2\nAnalizując naszą wizualizację:\n\nRozproszone punkty pokazują rzeczywiste obserwacje (x_i, y_i)\nCzerwona linia reprezentuje dopasowane wartości \\hat{y_i}, które minimalizują \\sum \\epsilon_i^2\nNiebieska i zielona linia przedstawiają alternatywne dopasowania o większych sumach kwadratów błędów\nPionowe odległości od każdego punktu do tych linii reprezentują błędy \\epsilon_i\n\nRozwiązanie OLS dostarcza nam estymatorów \\hat{\\beta_0} i \\hat{\\beta_1} nieznanych parametrów \\beta_0 i \\beta_1, które minimalizują całkowity błąd kwadratowy, dając nam najdokładniejszą liniową reprezentację zależności między poziomem wykształcenia a wynagrodzeniem na podstawie dostępnych danych.\n\n\n16.3.5 Znalezienie Najlepszej Linii\nRozwiązanie tego problemu minimalizacji daje nam:\n\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = \\frac{cov(X, Y)}{var(X)}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\n\n\n16.3.6 Kluczowe Punkty\n\nMNK znajduje linię, która minimalizuje kwadraty błędów predykcji\nTa linia jest “najlepsza” pod względem dopasowania, ale może nie ujmować prawdziwych relacji, jeśli pominięto ważne zmienne\nW przykładzie edukacja-wynagrodzenia, pominięcie zdolności oznacza, że przypisujemy cały wzrost wynagrodzeń samej edukacji\n\n\n\n\n\n\n\n16.3.7 Podstawowe Pojęcia i Terminologia\nUstalmy kluczowe terminy:\n\nZmienna Zależna (Y):\n\nWynik, który chcemy zrozumieć lub przewidzieć\nNazywana również: zmienna odpowiedzi, zmienna docelowa\nPrzykłady: wynagrodzenie, sprzedaż, wyniki egzaminów\n\nZmienna Niezależna (X):\n\nZmienna, która naszym zdaniem wpływa na Y\nNazywana również: predyktor, zmienna objaśniająca, regresor\nPrzykłady: lata edukacji, budżet reklamowy, godziny nauki\n\nParametry Populacji (\\beta):\n\nPrawdziwe podstawowe zależności, które chcemy zrozumieć\nZazwyczaj nieznane w praktyce\nPrzykłady: \\beta_0 (prawdziwy wyraz wolny), \\beta_1 (prawdziwe nachylenie)\n\nOszacowania Parametrów (\\hat{\\beta}):\n\nNasze najlepsze przypuszczenia dotyczące prawdziwych parametrów na podstawie danych\nObliczane na podstawie danych próby\nPrzykłady: \\hat{\\beta}_0 (oszacowany wyraz wolny), \\hat{\\beta}_1 (oszacowane nachylenie)\n\n\n\n\n16.3.8 Główna Idea\nZobaczmy na przykładzie, co robi regresja:\n\n# Generate some example data\nset.seed(123)\nx &lt;- seq(1, 10, by = 0.5)\ny &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit the model\nmodel &lt;- lm(y ~ x, data = data)\n\n# Create the plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Przykład Prostej Regresji Liniowej\",\n       subtitle = \"Punkty reprezentują dane, czerwona linia pokazuje dopasowanie regresji\",\n       x = \"Zmienna Niezależna (X)\",\n       y = \"Zmienna Zależna (Y)\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 16.1: Podstawowa Idea Regresji: Dopasowanie Linii do Danych\n\n\n\n\n\nTen wykres pokazuje istotę regresji:\n\nKażdy punkt reprezentuje obserwację (X, Y)\nLinia reprezentuje nasze najlepsze przypuszczenie dotyczące zależności\nRozproszenie punktów wokół linii pokazuje niepewność",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#model-regresji-liniowej",
    "href": "correg_pl.html#model-regresji-liniowej",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.4 Model Regresji Liniowej",
    "text": "16.4 Model Regresji Liniowej\n\n16.4.1 Model Populacyjny vs. Oszacowania z Próby\nW teorii istnieje prawdziwa zależność populacyjna:\nY = \\beta_0 + \\beta_1X + \\varepsilon\ngdzie:\n\n\\beta_0 to prawdziwy wyraz wolny populacji\n\\beta_1 to prawdziwe nachylenie populacji\n\\varepsilon to składnik losowy błędu\n\nW praktyce pracujemy z danymi próby, aby oszacować tę zależność:\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X\nZobaczmy wizualizację różnicy między zależnościami populacyjnymi a próbkowymi:\n\n# Generate population data\nset.seed(456)\nx_pop &lt;- seq(1, 10, by = 0.1)\ntrue_relationship &lt;- 2 + 3*x_pop  # True β₀=2, β₁=3\ny_pop &lt;- true_relationship + rnorm(length(x_pop), 0, 2)\n\n# Create several samples\nsample_size &lt;- 30\nsamples &lt;- data.frame(\n  x = rep(sample(x_pop, sample_size), 3),\n  sample = rep(1:3, each = sample_size)\n)\n\nsamples$y &lt;- 2 + 3*samples$x + rnorm(nrow(samples), 0, 2)\n\n# Fit models to each sample\nmodels &lt;- samples %&gt;%\n  group_by(sample) %&gt;%\n  summarise(\n    intercept = coef(lm(y ~ x))[1],\n    slope = coef(lm(y ~ x))[2]\n  )\n\n# Plot\nggplot() +\n  geom_point(data = samples, aes(x = x, y = y, color = factor(sample)), \n             alpha = 0.5) +\n  geom_abline(data = models, \n              aes(intercept = intercept, slope = slope, \n                  color = factor(sample)),\n              linetype = \"dashed\") +\n  geom_line(aes(x = x_pop, y = true_relationship), \n            color = \"black\", size = 1) +\n  theme_minimal() +\n  labs(title = \"Linie Regresji: Populacyjna vs. Próbkowe\",\n       subtitle = \"Czarna linia: prawdziwa zależność populacyjna\\nLinie przerywane: oszacowania próbkowe\",\n       x = \"X\", y = \"Y\",\n       color = \"Próba\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 16.2: Linie Regresji: Populacyjna vs. Próbkowe\n\n\n\n\n\nTa wizualizacja pokazuje:\n\nPrawdziwą linię populacyjną (czarną), którą próbujemy odkryć\nRóżne oszacowania próbkowe (linie przerywane) oparte na różnych próbach\nJak oszacowania próbkowe różnią się wokół prawdziwej zależności",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kluczowe-założenia-regresji-liniowej",
    "href": "correg_pl.html#kluczowe-założenia-regresji-liniowej",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.5 Kluczowe Założenia Regresji Liniowej",
    "text": "16.5 Kluczowe Założenia Regresji Liniowej\n\n16.5.1 Ścisła Egzogeniczność: Podstawowe Założenie\nNajważniejszym założeniem w regresji jest ścisła egzogeniczność:\nE[\\varepsilon|X] = 0\nOznacza to:\n\nWartość oczekiwana składnika błędu warunkowego względem X wynosi zero\nX nie zawiera informacji o przeciętnym błędzie\nNie ma systematycznych wzorców w tym, jak nasze przewidywania są błędne\n\nZobaczmy wizualizację sytuacji, gdy to założenie jest spełnione i gdy nie jest:\n\n# Generate data\nset.seed(789)\nx &lt;- seq(1, 10, by = 0.2)\n\n# Case 1: Exogenous errors\ny_exog &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\n\n# Case 2: Non-exogenous errors (error variance increases with x)\ny_nonexog &lt;- 2 + 3*x + 0.5*x*rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_exog &lt;- data.frame(\n  x = x,\n  y = y_exog,\n  type = \"Błędy Egzogeniczne\\n(Założenie Spełnione)\"\n)\n\ndata_nonexog &lt;- data.frame(\n  x = x,\n  y = y_nonexog,\n  type = \"Błędy Nieegzogeniczne\\n(Założenie Niespełnione)\"\n)\n\ndata_combined &lt;- rbind(data_exog, data_nonexog)\n\n# Create plots with residuals\nplot_residuals &lt;- function(data, title) {\n  model &lt;- lm(y ~ x, data = data)\n  data$predicted &lt;- predict(model)\n  data$residuals &lt;- residuals(model)\n  \n  p1 &lt;- ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    theme_minimal() +\n    labs(title = title)\n  \n  p2 &lt;- ggplot(data, aes(x = x, y = residuals)) +\n    geom_point() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    labs(y = \"Reszty\")\n  \n  list(p1, p2)\n}\n\n# Generate plots\nplots_exog &lt;- plot_residuals(data_exog, \"Błędy Egzogeniczne\")\nplots_nonexog &lt;- plot_residuals(data_nonexog, \"Błędy Nieegzogeniczne\")\n\n# Arrange plots\ngridExtra::grid.arrange(\n  plots_exog[[1]], plots_exog[[2]],\n  plots_nonexog[[1]], plots_nonexog[[2]],\n  ncol = 2\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 16.3: Przykłady Egzogeniczności vs. Nieegzogeniczności\n\n\n\n\n\n\n\n16.5.2 Liniowość: Założenie o Formie\nZależność między X a Y powinna być liniowa w parametrach:\nE[Y|X] = \\beta_0 + \\beta_1X\nZauważ, że nie oznacza to, że X i Y muszą mieć zależność w postaci linii prostej - możemy transformować zmienne. Zobaczmy różne rodzaje zależności:\n\n# Generate data\nset.seed(101)\nx &lt;- seq(1, 10, by = 0.1)\n\n# Different relationships\ndata_relationships &lt;- data.frame(\n  x = rep(x, 3),\n  y = c(\n    # Linear\n    2 + 3*x + rnorm(length(x), 0, 2),\n    # Quadratic\n    2 + 0.5*x^2 + rnorm(length(x), 0, 2),\n    # Exponential\n    exp(0.3*x) + rnorm(length(x), 0, 2)\n  ),\n  type = rep(c(\"Liniowa\", \"Kwadratowa\", \"Wykładnicza\"), each = length(x))\n)\n\n# Plot\nggplot(data_relationships, aes(x = x, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  facet_wrap(~type, scales = \"free_y\") +\n  theme_minimal() +\n  labs(subtitle = \"Czerwona: dopasowanie liniowe, Niebieska: prawdziwa zależność\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 16.4: Zależności Liniowe i Nieliniowe\n\n\n\n\n\n\n\n16.5.3 Zrozumienie Naruszeń i Rozwiązania\nGdy założenie liniowości jest naruszone:\n\nTransformacja zmiennych:\n\nTransformacja logarytmiczna: dla zależności wykładniczych\nPierwiastek kwadratowy: dla umiarkowanej nieliniowości\nTransformacje potęgowe: dla bardziej złożonych zależności\n\n\n\n# Generate exponential data\nset.seed(102)\nx &lt;- seq(1, 10, by = 0.2)\ny &lt;- exp(0.3*x) + rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_trans &lt;- data.frame(\n  x = x,\n  y = y,\n  log_y = log(y)\n)\n\nWarning in log(y): NaNs produced\n\n# Original scale plot\np1 &lt;- ggplot(data_trans, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Skala Oryginalna\")\n\n# Log scale plot\np2 &lt;- ggplot(data_trans, aes(x = x, y = log_y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Y po Transformacji Logarytmicznej\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 16.5: Efekt Transformacji Zmiennych\n\n\n\n\n\n\n\n\n\n\n\nIntuicyjne zrozumienie Metody Najmniejszych Kwadratów (MNK)\n\n\n\n\n16.5.4 Podstawowy Problem\nZacznijmy od rzeczywistego scenariusza: chcemy zrozumieć, jak czas nauki wpływa na wyniki egzaminu. Zbieramy dane z Twojej klasy, gdzie:\n\nKażdy student zapisuje swoje godziny nauki (x), oraz swój końcowy wynik egzaminu (y)\nWięc student 1 mógł się uczyć x_1 = 3 godziny i uzyskać y_1 = 75 punktów\nStudent 2 mógł się uczyć x_2 = 5 godzin i uzyskać y_2 = 82 punkty\nI tak dalej dla wszystkich n studentów w klasie\n\nNaszym celem jest znalezienie prostej, która najlepiej opisuje tę zależność. Próbujemy oszacować prawdziwą zależność (której nigdy nie znamy dokładnie) używając naszej próby danych. Przeanalizujmy to krok po kroku.\n\nlibrary(tidyverse)\n\n# Tworzenie przykładowych danych\nset.seed(123)\ngodziny_nauki &lt;- runif(20, 1, 8)\nwyniki_egzaminu &lt;- 60 + 5 * godziny_nauki + rnorm(20, 0, 5)\ndane &lt;- data.frame(godziny_nauki, wyniki_egzaminu)\n\n# Podstawowy wykres punktowy\nggplot(dane, aes(x = godziny_nauki, y = wyniki_egzaminu)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  labs(x = \"Godziny nauki\", y = \"Wyniki egzaminu\",\n       title = \"Dane z Twojej klasy: Godziny nauki vs. Wyniki egzaminu\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n\n\n\n\n\n\n\n\n\n16.5.5 Co sprawia, że prosta jest “dobra”?\nKażdą prostą można zapisać w postaci:\ny = \\hat{\\beta}_0 + \\hat{\\beta}_1x\nGdzie:\n\n\\hat{\\beta}_0 to nasza estymata wyrazu wolnego (przewidywany wynik dla zero godzin nauki)\n\\hat{\\beta}_1 to nasza estymata nachylenia (ile punktów zyskujemy za każdą dodatkową godzinę nauki)\nDaszki (^) wskazują, że są to nasze estymaty prawdziwych (nieznanych) parametrów \\beta_0 i \\beta_1\n\nSpójrzmy na trzy możliwe proste przechodzące przez nasze dane:\n\nggplot(dane, aes(x = godziny_nauki, y = wyniki_egzaminu)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  geom_abline(intercept = 50, slope = 8, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_abline(intercept = 70, slope = 2, color = \"green\", linetype = \"dashed\", size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"purple\") +\n  labs(x = \"Godziny nauki\", y = \"Wyniki egzaminu\",\n       title = \"Trzy różne proste: Która jest najlepsza?\") +\n  annotate(\"text\", x = 7.5, y = 120, color = \"red\", label = \"Prosta A: Za stroma\") +\n  annotate(\"text\", x = 7.5, y = 85, color = \"green\", label = \"Prosta B: Za płaska\") +\n  annotate(\"text\", x = 7.5, y = 100, color = \"purple\", label = \"Prosta C: W sam raz\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n16.5.6 Zrozumienie błędów przewidywania (reszt)\nDla każdego studenta w naszych danych:\n\nPatrzymy na jego rzeczywisty wynik egzaminu (y_i)\nObliczamy przewidywany wynik używając naszej prostej (\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nRóżnica między nimi nazywana jest resztą:\n\n\\text{reszta}_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nZobaczmy wizualizację tych reszt dla jednej prostej:\n\n# Dopasowanie modelu i pokazanie reszt\nmodel &lt;- lm(wyniki_egzaminu ~ godziny_nauki, data = dane)\n\nggplot(dane, aes(x = godziny_nauki, y = wyniki_egzaminu)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"purple\") +\n  geom_segment(aes(xend = godziny_nauki, \n                  yend = predict(model, dane)),\n              color = \"orange\", alpha = 0.5) +\n  labs(x = \"Godziny nauki\", y = \"Wyniki egzaminu\",\n       title = \"Zrozumienie reszt: Różnice między przewidywaniami a rzeczywistością\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPomarańczowe pionowe linie pokazują, jak bardzo nasze przewidywania odbiegają od rzeczywistości dla każdego studenta. Niektóre przewidywania są za wysokie (dodatnie reszty), inne za niskie (ujemne reszty).\n\n\n16.5.7 Dlaczego podnosimy reszty do kwadratu?\nTo kluczowa koncepcja! Przeanalizujmy to na prostym przykładzie:\nWyobraź sobie, że mamy tylko dwoje studentów:\n\nAla: Przewidywane 80, rzeczywisty wynik 85 (reszta = +5)\nBob: Przewidywane 90, rzeczywisty wynik 85 (reszta = -5)\n\nJeśli po prostu dodamy te reszty: (+5) + (-5) = 0\nTo sugerowałoby, że nasza prosta jest idealna (całkowity błąd = 0), ale wiemy, że tak nie jest! Oba przewidywania były nietrafne o 5 punktów.\nRozwiązanie: Podnosimy reszty do kwadratu przed dodaniem:\n\nKwadrat reszty Ali: (+5)^2 = 25\nKwadrat reszty Boba: (-5)^2 = 25\nCałkowity błąd kwadratowy: 25 + 25 = 50\n\nTo daje nam znacznie lepszą miarę tego, jak bardzo nasze przewidywania są błędne!\n\n\n16.5.8 Suma kwadratów reszt (SSE)\nSuma kwadratów reszt (SSE - Sum of Squared Errors) jest fundamentalną miarą dopasowania modelu regresji liniowej. Możemy ją wyrazić matematycznie jako:\nSSE = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\ngdzie:\n\ny_i to rzeczywista wartość zmiennej zależnej dla i-tej obserwacji\n\\hat{\\beta}_0 to oszacowany wyraz wolny (punkt przecięcia z osią Y)\n\\hat{\\beta}_1 to oszacowany współczynnik nachylenia prostej\nx_i to wartość zmiennej niezależnej dla i-tej obserwacji\n\nProces obliczania SSE można przedstawić w następujących krokach:\n\nDla każdej obserwacji obliczamy różnicę między wartością rzeczywistą (y_i) a wartością przewidywaną przez model (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i). Ta różnica nazywana jest resztą.\nKażdą resztę podnosimy do kwadratu, co powoduje, że:\n\nwszystkie wartości stają się dodatnie\nwiększe błędy są silniej “karane” niż małe\njednostki miary są podnoszone do kwadratu\n\nSumujemy wszystkie kwadraty reszt, otrzymując jedną liczbę reprezentującą całkowity błąd modelu.\n\nIm mniejsza wartość SSE, tym lepsze dopasowanie modelu do danych empirycznych.\nWartość SSE = 0 oznaczałaby idealne dopasowanie, gdzie wszystkie punkty leżą dokładnie na prostej regresji. W praktyce tak doskonałe dopasowanie występuje niezwykle rzadko i może sugerować problem przeuczenia modelu.\nSSE stanowi podstawę do obliczania innych ważnych miar jakości dopasowania modelu, takich jak współczynnik determinacji R² czy błąd standardowy estymacji.\n\n# Porównanie dobrego i złego dopasowania\nzle_przewidywania &lt;- 70 + 2 * dane$godziny_nauki\ndobre_przewidywania &lt;- predict(model, dane)\n\nzle_sse &lt;- sum((dane$wyniki_egzaminu - zle_przewidywania)^2)\ndobre_sse &lt;- sum((dane$wyniki_egzaminu - dobre_przewidywania)^2)\n\nggplot(dane, aes(x = godziny_nauki, y = wyniki_egzaminu)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  geom_abline(intercept = 70, slope = 2, color = \"red\", \n              linetype = \"dashed\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"purple\") +\n  annotate(\"text\", x = 2, y = 95, \n           label = paste(\"Czerwona prosta: Błąd =\", round(zle_sse)), \n           color = \"red\") +\n  annotate(\"text\", x = 2, y = 90, \n           label = paste(\"Fioletowa prosta: Błąd =\", round(dobre_sse)), \n           color = \"purple\") +\n  labs(x = \"Godziny nauki\", y = \"Wyniki egzaminu\",\n       title = \"Porównanie całkowitych błędów przewidywania\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n16.5.9 Dlaczego nazywamy to “Metodą Najmniejszych Kwadratów”?\nPrzeanalizujmy nazwę:\n\n“Kwadratów”: Podnosimy reszty do kwadratu\n“Najmniejszych”: Chcemy najmniejszej możliwej sumy\n“Zwykłych” (w angielskim “Ordinary”): To podstawowa wersja (istnieją bardziej zaawansowane warianty!)\n\nProsta MNK ma kilka ciekawych właściwości:\n\nŚrednia wszystkich reszt równa się zero\nProsta zawsze przechodzi przez punkt (\\bar{x}, \\bar{y}) - średnie godziny nauki i średni wynik\nMałe zmiany w danych prowadzą do małych zmian w prostej (jest “stabilna”)\nNasze estymaty \\hat{\\beta}_0 i \\hat{\\beta}_1 są najlepszymi możliwymi estymatorami prawdziwych parametrów \\beta_0 i \\beta_1\n\n\n\n16.5.10 Ważne uwagi\n\nOznaczenie z daszkiem (\\hat{\\beta}_0, \\hat{\\beta}_1) przypomina nam, że estymujemy prawdziwą zależność z naszej próby. Nigdy nie znamy prawdziwych \\beta_0 i \\beta_1 - możemy je tylko oszacować z naszych danych.\nMNK daje nam najlepsze możliwe estymaty, gdy spełnione są pewne warunki (jak losowo pobrana próba i rzeczywiście liniowa zależność).",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#ocena-dopasowania-modelu",
    "href": "correg_pl.html#ocena-dopasowania-modelu",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.6 Ocena Dopasowania Modelu",
    "text": "16.6 Ocena Dopasowania Modelu\n\n16.6.1 Dekompozycja Wariancji\nCałkowita zmienność Y może być rozłożona na komponenty wyjaśnione i niewyjaśnione:\n\\underbrace{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}_{SST} = \\underbrace{\\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2}_{SSR} + \\underbrace{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}_{SSE}\ngdzie:\n\nSST (Całkowita Suma Kwadratów): Całkowita zmienność Y\nSSR (Regresyjna Suma Kwadratów): Zmienność wyjaśniona przez regresję\nSSE (Suma Kwadratów Błędów): Zmienność niewyjaśniona\n\n\n\n16.6.2 Trzy Komponenty Wariancji\n\nWariancja Całkowita (SST - Suma Kwadratów Całkowita)\n\nPytanie: “Jak bardzo obserwacje różnią się od średniej?”\nWzór: SST = \\sum(y_i - \\bar{y})^2\nIntuicja: “Rozrzut” naszych danych wokół ich średniej\n\nWariancja Wyjaśniona (SSR - Suma Kwadratów Regresji)\n\nPytanie: “Ile wariancji wyjaśnił nasz model?”\nWzór: SSR = \\sum(\\hat{y}_i - \\bar{y})^2\nIntuicja: Poprawa, którą uzyskaliśmy dzięki użyciu X\n\nWariancja Niewyjaśniona (SSE - Suma Kwadratów Błędów)\n\nPytanie: “Jaka wariancja pozostaje niewyjaśniona?”\nWzór: SSE = \\sum(y_i - \\hat{y}_i)^2\nIntuicja: Błędy, które pozostają po użyciu X\n\n\n\n\n16.6.3 Współczynnik R² - Wyjaśnienie\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} Współczynnik R² odpowiada na pytanie: “Jaki procent pierwotnej zmienności Y możemy wyjaśnić za pomocą X?”\n\n16.6.3.1 Intuicyjne Przykłady:\n\nR² = 0,80: Wykorzystanie X wyeliminowało 80% błędów predykcji\nR² = 0,25: Wykorzystanie X wyeliminowało 25% błędów predykcji\nR² = 0,00: Wykorzystanie X nie przyniosło żadnej poprawy\n\n\n\n\n16.6.4 Kiedy Zachować Ostrożność\n\nWysoki R² Nie Jest Wszystkim\n\nWysoki R² może wskazywać na przeuczenie modelu\nZawsze sprawdzaj, czy model ma sens praktyczny\nWeź pod uwagę kontekst swojej dziedziny\n\nNiski R² Nie Zawsze Jest Zły\n\nW niektórych dziedzinach R² = 0,30 może być imponujący\nNauki społeczne często mają niższe wartości R²\nSkup się na znaczeniu praktycznym\n\n\n\n\n16.6.5 Praktyczne Wskazówki do Analizy\n\nInspekcja Wizualna\n\nZawsze wizualizuj swoje dane\nSzukaj wzorców w resztach\nSprawdzaj punkty wpływowe\n\nUwzględnienie Kontekstu\n\nCo jest “dobrym” R² w twojej dziedzinie?\nJaki jest praktyczny wpływ twoich błędów?\nCzy twoje predyktory są znaczące?\n\nDiagnostyka Modelu\n\nSprawdź normalność reszt\nSzukaj heteroskedastyczności\nBadaj punkty wpływowe\n\n\n\n\n16.6.6 Kluczowe Wnioski\n\nDekompozycja wariancji pomaga zrozumieć poprawę predykcji\nR² określa proporcję wyjaśnionej wariancji\nZrozumienie wizualne jest kluczowe dla interpretacji\nKontekst jest ważniejszy niż bezwzględne wartości R²\nZawsze łącz R² z innymi narzędziami diagnostycznymi\n\n\n\n16.6.7 Miary Dopasowania\n\nR-kwadrat (R^2): R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nPierwiastek Błędu Średniokwadratowego (RMSE): RMSE = \\sqrt{\\frac{SSE}{n}}\nŚredni Błąd Bezwzględny (MAE): MAE = \\frac{1}{n}\\sum_{i=1}^n |Y_i - \\hat{Y}_i|\n\n\n\n\n\n\n\nDekompozycja Wariancji w Regresji Liniowej\n\n\n\n\n16.6.8 Dlaczego To Jest Ważne: Zrozumienie Poprawy Prognoz Poprzez Dodatkowe Informacje\nRozważmy prbem zwizany z przewidywaniem cen nieruchomości na rynku mieszkaniowym. Na najbardziej podstawowym poziomie można oszacować cenę dowolnego domu, używając średniej ceny rynkowej. Jednakże takie podejście pomija potencjalnie wartościowe informacje. Gdy uwzględnimy dodatkowe zmienne, takie jak powierzchnia użytkowa, lokalizacja czy liczba sypialni, nasze prognozy zazwyczaj stają się bardziej precyzyjne.\nDekompozycja wariancji dostarcza matematycznych ram do dokładnego określenia, o ile poprawia się trafność prognoz, gdy włączamy takie dodatkowe informacje.\n\n\n16.6.9 Udoskonalenie Estymacji\n\n16.6.9.1 Podejście Początkowe: Uniwersalna Średnia\nZaczynamy od najprostszej możliwej prognozy: średniej wszystkich cen domów (\\bar{y}). Stanowi to nasz punkt wyjścia – prognozę dokonaną bez żadnych szczególnych informacji o poszczególnych nieruchomościach. Choć podejście to zapewnia rozsądny punkt odniesienia, traktuje każdy dom identycznie, prowadząc do tzw. błędów bazowych. Błędy te powstają właśnie dlatego, że ignorujemy unikalne cechy każdej nieruchomości.\n\n\n16.6.9.2 Podejście Zaawansowane: Włączenie Informacji Szczegółowych\nGdy wprowadzamy charakterystyki specyficzne dla nieruchomości (oznaczone jako X), takie jak powierzchnia użytkowa, możemy udoskonalić nasze prognozy. To udoskonalenie pozwala nam różnicować między nieruchomościami, tworząc spersonalizowane prognozy odzwierciedlające indywidualne cechy domów. Wynikające z tego błędy prognoz zazwyczaj maleją, demonstrując wartość włączenia dodatkowych informacji.\n\n\n\n16.6.10 Zrozumienie Komponentów Wariancji\n\nWariancja Całkowita (SST) Wariancja całkowita określa ogólną zmienność cen domów wokół średniej rynkowej.\n\nWyrażenie Matematyczne: SST = \\sum(y_i - \\bar{y})^2\nReprezentacja Wizualna: Zobrazowana przez fioletowe pionowe linie na wykresie, pokazujące odległość każdej rzeczywistej ceny domu od średniej ogólnej\nRamy Koncepcyjne: Można to rozumieć jako pomiar tego, jak bardzo ceny domów różnią się od siebie na rynku\nInterpretacja Praktyczna: Większe SST wskazuje na bardziej zróżnicowany rynek mieszkaniowy z większą zmiennością cen\n\nWariancja Wyjaśniona (SSR) Ten komponent reprezentuje część zmienności cen, którą nasz model skutecznie wychwytuje poprzez uwzględnione zmienne.\n\nWyrażenie Matematyczne: SSR = \\sum(\\hat{y}_i - \\bar{y})^2\nReprezentacja Wizualna: Pokazana przez zielone przerywane linie, reprezentujące jak bardzo prognozy naszego modelu odbiegają od średniej\nRamy Koncepcyjne: Mierzy to, o ile lepsze stają się nasze prognozy, gdy uwzględniamy szczególne cechy domów\nInterpretacja Praktyczna: Większe SSR w stosunku do SST wskazuje, że wybrane przez nas zmienne (jak powierzchnia użytkowa) silnie wpływają na ceny domów\n\nWariancja Niewyjaśniona (SSE) Reprezentuje pozostałą zmienność cen, której nasz model nie może wyjaśnić przy użyciu uwzględnionych zmiennych.\n\nWyrażenie Matematyczne: SSE = \\sum(y_i - \\hat{y}_i)^2\nReprezentacja Wizualna: Zobrazowana przez pomarańczowe przerywane linie, pokazujące pozostałe błędy między naszymi prognozami a rzeczywistymi cenami\nRamy Koncepcyjne: Są to błędy prognoz, które utrzymują się nawet po uwzględnieniu wszystkich wybranych zmiennych\nInterpretacja Praktyczna: Mniejsze SSE sugeruje, że nasz model wychwytuje większość głównych czynników wpływających na cenę\n\n\nSST = SSR + SSE. Ta zależność mówi nam, że cała zmienność cen musi być albo wyjaśniona przez nasz model (SSR), albo pozostać niewyjaśniona (SSE), dostarczając kompleksowych ram do zrozumienia poprawy prognoz. Kolorowe linie na wykresie stanowią wizualne potwierdzenie tej zależności, pomagając zrozumieć, jak te komponenty współdziałają w praktyce.\n\n16.6.10.1 Wizualizacja Dekompozycji Wariancji\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(patchwork)\n\n\nAttaching package: 'patchwork'\n\n\nThe following object is masked from 'package:MASS':\n\n    area\n\n# Generate data with clearer pattern\nset.seed(123)\nx &lt;- seq(1, 10, length.out = 50)\ny &lt;- 2 + 0.5 * x + rnorm(50, sd = 0.8)\ndata &lt;- data.frame(x = x, y = y)\n\n# Model and calculations\nmodel &lt;- lm(y ~ x, data)\nmean_y &lt;- mean(y)\ndata$predicted &lt;- predict(model)\n\n# Select specific points for demonstration that are well-spaced\ndemonstration_points &lt;- c(8, 25, 42)  # Changed points for better spacing\n\n# Create main plot with improved aesthetics\np1 &lt;- ggplot(data, aes(x = x, y = y)) +\n  # Add background grid for better readability\n  geom_hline(yintercept = seq(0, 8, by = 0.5), color = \"gray90\", linewidth = 0.2) +\n  geom_vline(xintercept = seq(0, 10, by = 0.5), color = \"gray90\", linewidth = 0.2) +\n  \n  # Add regression line and mean line\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E41A1C\", linewidth = 1.2) +\n  geom_hline(yintercept = mean_y, linetype = \"longdash\", color = \"#377EB8\", linewidth = 1) +\n  \n  # Add data points\n  geom_point(size = 3, alpha = 0.6, color = \"#4A4A4A\") +\n  \n  # Add decomposition segments with improved colors and positioning\n  # Total deviation (purple)\n  geom_segment(data = data[demonstration_points,],\n              aes(x = x, xend = x, y = y, yend = mean_y),\n              color = \"#984EA3\", linetype = \"dashed\", linewidth = 1.8) +\n  # Explained component (green)\n  geom_segment(data = data[demonstration_points,],\n              aes(x = x, xend = x, y = mean_y, yend = predicted),\n              color = \"#4DAF4A\", linetype = \"dashed\", linewidth = 1) +\n  # Unexplained component (orange)\n  geom_segment(data = data[demonstration_points,],\n              aes(x = x, xend = x, y = predicted, yend = y),\n              color = \"#FF7F00\", linetype = \"dashed\", linewidth = 1) +\n  \n  # Add annotations for better understanding\n  annotate(\"text\", x = data$x[demonstration_points[2]], y = mean_y - 0.2,\n           label = \"Mean\", color = \"#377EB8\", hjust = -0.2) +\n  annotate(\"text\", x = data$x[demonstration_points[2]], \n           y = data$predicted[demonstration_points[2]] + 0.2,\n           label = \"Regression Line\", color = \"#E41A1C\", hjust = -0.2) +\n  \n  # Improve theme and labels\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 12),\n    panel.grid = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  labs(\n    title = \"Variance Decomposition in Linear Regression\",\n    subtitle = \"Decomposing total variance into explained and unexplained components\",\n    x = \"Predictor (X)\",\n    y = \"Response (Y)\"\n  )\n\n# Create error distribution plot with improved aesthetics\ndata$mean_error &lt;- y - mean_y\ndata$regression_error &lt;- y - data$predicted\n\np2 &lt;- ggplot(data) +\n  geom_density(aes(x = mean_error, fill = \"Deviation from Mean\"), \n               alpha = 0.5) +\n  geom_density(aes(x = regression_error, fill = \"Regression Residuals\"), \n               alpha = 0.5) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  ) +\n  labs(\n    title = \"Error Distribution Comparison\",\n    x = \"Error Magnitude\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(\n    values = c(\"#377EB8\", \"#E41A1C\")\n  )\n\n# Add legend explaining the decomposition components\nlegend_plot &lt;- ggplot() +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    legend.box = \"horizontal\"\n  ) +\n  annotate(\"text\", x = 0, y = 0, label = \"\") +\n  scale_color_manual(\n    name = \"Variance Components\",\n    values = c(\"#984EA3\", \"#4DAF4A\", \"#FF7F00\"),\n    labels = c(\"Total Deviation\", \"Explained Variance\", \"Unexplained Variance\")\n  )\n\n# Combine plots with adjusted heights\ncombined_plot &lt;- (p1 / p2) +\n  plot_layout(heights = c(2, 1))\n\n# Print the combined plot\ncombined_plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n16.6.11 R² Wyjaśnione\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nMyśl o R² jako o odpowiedzi na pytanie: “Jaki procent pierwotnej wariancji Y możemy wyjaśnić używając X?”\n\n16.6.11.1 Intuicyjne Przykłady:\n\nR² = 0,80: Użycie X wyeliminowało 80% naszych błędów predykcji\nR² = 0,25: Użycie X wyeliminowało 25% naszych błędów predykcji\nR² = 0,00: Użycie X wcale nie pomogło\n\n\n\n\n16.6.12 Kiedy Zachować Ostrożność\n\nWysoki R² To Nie Wszystko\n\nWysoki R² może wskazywać na przeuczenie\nZawsze sprawdzaj, czy twój model ma praktyczny sens\nWeź pod uwagę kontekst swojej dziedziny\n\nNiski R² Nie Zawsze Jest Zły\n\nW niektórych dziedzinach R² = 0,30 może być imponujący\nNauki społeczne często mają niższe wartości R²\nSkup się na znaczeniu praktycznym\n\nWielkość Próby Ma Znaczenie\n\nUżywaj skorygowanego R² dla regresji wielorakiej: R^2_{adj} = 1 - \\frac{SSE/(n-p)}{SST/(n-1)}\nPenalizuje dodawanie niepotrzebnych predyktorów\n\n\n\n\n16.6.13 Praktyczne Wskazówki do Analizy\n\nInspekcja Wizualna\n\nZawsze wizualizuj swoje dane\nSzukaj wzorców w resztach\nSprawdzaj punkty wpływowe\n\nUwzględnienie Kontekstu\n\nCo jest “dobrym” R² w twojej dziedzinie?\nJaki jest praktyczny wpływ twoich błędów?\nCzy twoje predyktory są znaczące?\n\nDiagnostyka Modelu\n\nSprawdź normalność reszt\nSzukaj heteroskedastyczności\nBadaj punkty wpływowe\n\n\n\n\n16.6.14 Kluczowe Wnioski\n\nDekompozycja wariancji pomaga zrozumieć poprawę predykcji\nR² określa ilościowo proporcję wyjaśnionej wariancji\nZrozumienie wizualne jest kluczowe dla interpretacji\nKontekst jest ważniejszy niż bezwzględne wartości R²\nZawsze łącz R² z innymi narzędziami diagnostycznymi\n\n\n\n\n\n\n\n\n\n\nFormalne wyprowadzenie estymatorów MNK\n\n\n\n\n16.6.15 Założenia wstępne\nChcemy znaleźć prostą y = \\hat{\\beta}_0 + \\hat{\\beta}_1x, która minimalizuje sumę kwadratów reszt. Wyprowadźmy to krok po kroku:\n\nNajpierw zapisujemy funkcję, którą chcemy zminimalizować:\nSSE = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nAby znaleźć minimum, musimy obliczyć pochodne cząstkowe względem \\hat{\\beta}_0 i \\hat{\\beta}_1 oraz przyrównać je do zera:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_0} = 0 oraz \\frac{\\partial SSE}{\\partial \\hat{\\beta}_1} = 0\n\n\n\n16.6.16 Krok 1: Znalezienie \\hat{\\beta}_0\nObliczmy pochodną cząstkową względem \\hat{\\beta}_0:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_0} = \\frac{\\partial}{\\partial \\hat{\\beta}_0} \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2\nKorzystając z reguły łańcuchowej:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_0} = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)(-1) = 0\nUpraszczając:\n-2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\n\\sum_{i=1}^n y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum_{i=1}^n x_i = 0\nRozwiązując względem \\hat{\\beta}_0:\n\\hat{\\beta}_0 = \\frac{\\sum_{i=1}^n y_i}{n} - \\hat{\\beta}_1\\frac{\\sum_{i=1}^n x_i}{n} = \\bar{y} - \\hat{\\beta}_1\\bar{x}\nGdzie \\bar{y} i \\bar{x} to średnie z próby.\n\n\n16.6.17 Krok 2: Znalezienie \\hat{\\beta}_1\nTeraz obliczamy pochodną cząstkową względem \\hat{\\beta}_1:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_1} = \\frac{\\partial}{\\partial \\hat{\\beta}_1} \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2\nKorzystając z reguły łańcuchowej:\n\\frac{\\partial SSE}{\\partial \\hat{\\beta}_1} = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)(-x_i) = 0\nUpraszczając:\n-2\\sum_{i=1}^n x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\nPodstawiając nasze wyrażenie na \\hat{\\beta}_0:\n-2\\sum_{i=1}^n x_i(y_i - (\\bar{y} - \\hat{\\beta}_1\\bar{x}) - \\hat{\\beta}_1x_i) = 0\n\\sum_{i=1}^n x_i(y_i - \\bar{y} + \\hat{\\beta}_1\\bar{x} - \\hat{\\beta}_1x_i) = 0\nPo przekształceniach algebraicznych (rozwinięciu i zgrupowaniu wyrazów):\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n\n\n16.6.18 Wyniki końcowe\nWyprowadziliśmy estymatory MNK:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\nZrozummy, co oznaczają te wzory:\n\nEstymator nachylenia \\hat{\\beta}_1:\n\nLicznik: Mierzy, jak x i y zmieniają się razem (kowariancja)\nMianownik: Mierzy, jak bardzo zmienia się x (wariancja)\nWięc \\hat{\\beta}_1 jest zasadniczo stosunkiem kowariancji do wariancji\n\nEstymator wyrazu wolnego \\hat{\\beta}_0:\n\nZapewnia, że prosta przechodzi przez punkt (\\bar{x}, \\bar{y})\nDostosowuje wysokość prostej na podstawie nachylenia\n\n\n\n\n16.6.19 Weryfikacja: Drugie pochodne\nAby potwierdzić, że znaleźliśmy minimum (a nie maksimum), sprawdzamy drugie pochodne:\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_0^2} = 2n &gt; 0\n\\frac{\\partial^2 SSE}{\\partial \\hat{\\beta}_1^2} = 2\\sum_{i=1}^n x_i^2 &gt; 0\nPonieważ obie drugie pochodne są dodatnie, rzeczywiście znaleźliśmy minimum.\n\n\n16.6.20 Postać macierzowa (Temat zaawansowany, opcjonalny)\nDla osób znających algebrę liniową, możemy zapisać to zwięźlej:\n\\mathbf{X} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}\n\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\nWtedy estymator MNK w postaci macierzowej to:\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\nTo daje nam zarówno \\hat{\\beta}_0 jak i \\hat{\\beta}_1 w jednym eleganckim wyrażeniu.\n\n\n16.6.21 Wizualizacja wyprowadzenia\n\nlibrary(tidyverse)\n\n# Tworzenie przykładowych danych\nset.seed(123)\nx &lt;- runif(20, 1, 8)\ny &lt;- 2 + 3 * x + rnorm(20, 0, 1)\ndane &lt;- data.frame(x = x, y = y)\n\n# Obliczanie średnich\nx_srednia &lt;- mean(x)\ny_srednia &lt;- mean(y)\n\n# Tworzenie wizualizacji odchyleń\nggplot(dane, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_hline(yintercept = y_srednia, linetype = \"dashed\", color = \"gray\") +\n  geom_vline(xintercept = x_srednia, linetype = \"dashed\", color = \"gray\") +\n  geom_segment(aes(xend = x, yend = y_srednia), color = \"green\", alpha = 0.3) +\n  geom_segment(aes(yend = y, xend = x_srednia), color = \"purple\", alpha = 0.3) +\n  labs(title = \"Wizualizacja odchyleń od średnich\",\n       subtitle = \"Zielone: Odchylenia w y, Fioletowe: Odchylenia w x\",\n       x = \"x\", y = \"y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nPowyższy wykres pokazuje, jak działają estymatory MNK z odchyleniami od średnich. Iloczyn tych odchyleń (zielone × fioletowe) dla każdego punktu, zsumowany i znormalizowany, daje nam nasz estymator nachylenia \\hat{\\beta}_1.\n\n\n16.6.22 Ważne uwagi\n\nWyprowadzone estymatory są BLUE (Best Linear Unbiased Estimators - Najlepsze Liniowe Nieobciążone Estymatory) przy spełnieniu założeń Gaussa-Markowa.\nZałożenia te obejmują:\n\nLiniowość zależności\nLosowość próby\nBrak współliniowości idealnej\nHomoskedastyczność (stała wariancja reszt)\nNiezależność obserwacji\n\nMetoda ta minimalizuje sumę kwadratów reszt w kierunku pionowym (odchylenia w y), a nie prostopadłym do prostej.\n\n\n\n\n\n\n\n\n\n\nEndogeniczność w Regresji\n\n\n\nEndogeniczność to kluczowe pojęcie w analizie statystycznej, występujące gdy zmienna objaśniająca w modelu regresji jest skorelowana ze składnikiem resztowym. Stwarza to wyzwania w dokładnym zrozumieniu związków przyczynowo-skutkowych w badaniach. Przyjrzyjmy się trzem głównym typom endogeniczności i ich wpływowi na wyniki badań.\n\n16.6.23 1. Obciążenie Zmiennej Pominiętej (OVB)\nObciążenie zmiennej pominiętej występuje, gdy ważna zmienna wpływająca zarówno na zmienną zależną, jak i niezależną, zostaje pominięta w analizie. To pominięcie prowadzi do błędnych wniosków o relacji między badanymi zmiennymi.\nRozważmy badanie związku między wykształceniem a dochodami:\nPrzykład: Wykształcenie i Dochody Obserwowana relacja pokazuje, że wyższe wykształcenie koreluje z wyższymi dochodami. Jednak naturalne zdolności jednostki wpływają zarówno na poziom wykształcenia, jak i na potencjał zarobkowy. Bez uwzględnienia zdolności możemy przeszacować bezpośredni wpływ edukacji na dochody.\nReprezentacja statystyczna pokazuje, dlaczego to jest istotne:\ny_i = \\beta_0 + \\beta_1x_i + \\beta_2z_i + \\epsilon_i (Model pełny)\ny_i = \\beta_0 + \\beta_1x_i + u_i (Model niepełny)\nGdy pomijamy istotną zmienną, nasze oszacowania pozostałych relacji stają się obciążone i niewiarygodne.\n\n\n16.6.24 2. Jednoczesność\nJednoczesność występuje, gdy dwie zmienne wzajemnie na siebie wpływają, co utrudnia określenie kierunku przyczynowości. Tworzy to pętlę sprzężenia zwrotnego komplikującą analizę statystyczną.\nTypowe Przykłady Jednoczesności:\nWyniki w nauce i nawyki uczenia się stanowią wyraźny przypadek jednoczesności. Wyniki akademickie wpływają na ilość czasu, który studenci poświęcają na naukę, podczas gdy czas nauki wpływa na wyniki akademickie. Ta dwukierunkowa relacja utrudnia pomiar izolowanego wpływu każdej zmiennej.\nDynamika rynkowa dostarcza kolejnego przykładu. Ceny wpływają na popyt, podczas gdy popyt wpływa na ceny. Ta równoczesna relacja wymaga specjalnych podejść analitycznych.\n\n\n16.6.25 3. Błąd Pomiaru\nBłąd pomiaru występuje, gdy nie możemy dokładnie zmierzyć naszych zmiennych. Ta niedokładność może znacząco wpłynąć na naszą analizę i wnioski.\nTypowe Źródła Błędu Pomiaru:\nDane samodzielnie raportowane stanowią znaczące wyzwanie. Gdy uczestnicy raportują własne zachowania lub cechy, na przykład czas nauki, zgłaszane wartości często różnią się od rzeczywistych. Ta rozbieżność wpływa na naszą zdolność do pomiaru prawdziwych relacji.\nOgraniczenia techniczne również przyczyniają się do błędu pomiaru poprzez niedokładne narzędzia pomiarowe, niespójne warunki pomiaru oraz błędy zapisu lub wprowadzania danych.\n\n\n16.6.26 Rozwiązywanie Problemu Endogeniczności w Badaniach\n\n16.6.26.1 Strategie Identyfikacji\n\n# Przykład kontrolowania zmiennych pominiętych\nmodel_prosty &lt;- lm(dochod ~ wyksztalcenie, data = df)\nmodel_pelny &lt;- lm(dochod ~ wyksztalcenie + zdolnosci + doswiadczenie + region, data = df)\n\n# Porównanie współczynników\nsummary(model_prosty)\nsummary(model_pelny)\n\n\nWłączenie Dodatkowych Zmiennych: Zbieranie danych o potencjalnie ważnych pominiętych zmiennych i uwzględnianie odpowiednich zmiennych kontrolnych w analizie. Na przykład, uwzględnienie miar zdolności przy badaniu wpływu edukacji na dochody.\nWykorzystanie Danych Panelowych: Zbieranie danych w wielu okresach czasu w celu kontrolowania nieobserwowanych stałych charakterystyk i analizy zmian w czasie.\nZmienne Instrumentalne: Znalezienie zmiennych, które wpływają na zmienną niezależną, ale nie na zależną, aby wyizolować badaną relację.\n\n\n\n16.6.26.2 Poprawa Pomiaru\n\nWielokrotne Pomiary: Wykonywanie kilku pomiarów kluczowych zmiennych, używanie uśredniania do redukcji błędu losowego i porównywanie różnych metod pomiaru.\nLepsza Metoda Zbierania Danych: Używanie zwalidowanych instrumentów pomiarowych, wdrażanie procedur kontroli jakości i dokumentowanie potencjalnych źródeł błędu.\n\n\n\n\n16.6.27 Najlepsze Praktyki dla Badaczy\nProjekt badania fundamentalnie kształtuje możliwość rozwiązania problemu endogeniczności. Należy planować potencjalne problemy endogeniczności przed zbieraniem danych, uwzględniać miary potencjalnie ważnych zmiennych kontrolnych i rozważać wykorzystanie wielu podejść pomiarowych.\nAnaliza powinna obejmować testowanie endogeniczności gdy to możliwe, stosowanie odpowiednich metod statystycznych dla konkretnej sytuacji i dokumentowanie założeń oraz ograniczeń.\nRaportowanie musi jasno opisywać potencjalne problemy endogeniczności, wyjaśniać jak zostały one rozwiązane i omawiać implikacje dla wniosków.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#regresja-wieloraka",
    "href": "correg_pl.html#regresja-wieloraka",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.7 Regresja Wieloraka (*)",
    "text": "16.7 Regresja Wieloraka (*)\n\n16.7.1 Rozszerzenie do Wielu Predyktorów\nModel regresji wielorakiej rozszerza nasz prosty model o kilka predyktorów:\nModel Populacyjny: Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k + \\varepsilon\nOszacowanie Próbkowe: \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 + ... + \\hat{\\beta}_kX_k\nStwórzmy przykład z wieloma predyktorami:\n\n# Generate sample data with two predictors\nset.seed(105)\nn &lt;- 100\nX1 &lt;- rnorm(n, mean = 50, sd = 10)\nX2 &lt;- rnorm(n, mean = 20, sd = 5)\nY &lt;- 10 + 0.5*X1 + 0.8*X2 + rnorm(n, 0, 5)\n\ndata_multiple &lt;- data.frame(Y = Y, X1 = X1, X2 = X2)\n\n# Fit multiple regression model\nmodel_multiple &lt;- lm(Y ~ X1 + X2, data = data_multiple)\n\n# Create 3D visualization using scatter plots\np1 &lt;- ggplot(data_multiple, aes(x = X1, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Y vs X1\")\n\np2 &lt;- ggplot(data_multiple, aes(x = X2, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Y vs X2\")\n\ngrid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nPrzykład Regresji Wielorakiej\n\n\n\n# Print model summary\nsummary(model_multiple)\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = data_multiple)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8598  -3.6005   0.1166   3.0892  14.6102 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.77567    4.01351   2.934  0.00418 ** \nX1           0.45849    0.05992   7.651 1.47e-11 ***\nX2           0.81639    0.11370   7.180 1.42e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.122 on 97 degrees of freedom\nMultiple R-squared:  0.5062,    Adjusted R-squared:  0.4961 \nF-statistic: 49.72 on 2 and 97 DF,  p-value: 1.367e-15\n\n\n\n\n16.7.2 Interpretacja Współczynników\nW regresji wielorakiej, każdy \\hat{\\beta}_k reprezentuje oczekiwaną zmianę w Y przy jednostkowym wzroście X_k, przy utrzymaniu wszystkich innych zmiennych na stałym poziomie.\n\n# Create prediction grid for X1 (holding X2 at its mean)\nX1_grid &lt;- seq(min(X1), max(X1), length.out = 100)\npred_data_X1 &lt;- data.frame(\n  X1 = X1_grid,\n  X2 = mean(X2)\n)\npred_data_X1$Y_pred &lt;- predict(model_multiple, newdata = pred_data_X1)\n\n# Create prediction grid for X2 (holding X1 at its mean)\nX2_grid &lt;- seq(min(X2), max(X2), length.out = 100)\npred_data_X2 &lt;- data.frame(\n  X1 = mean(X1),\n  X2 = X2_grid\n)\npred_data_X2$Y_pred &lt;- predict(model_multiple, newdata = pred_data_X2)\n\n# Plot partial effects\np3 &lt;- ggplot() +\n  geom_point(data = data_multiple, aes(x = X1, y = Y)) +\n  geom_line(data = pred_data_X1, aes(x = X1, y = Y_pred), \n            color = \"red\", size = 1) +\n  theme_minimal() +\n  labs(title = \"Efekt Cząstkowy X1\",\n       subtitle = paste(\"(X2 utrzymane na średniej =\", round(mean(X2), 2), \")\"))\n\np4 &lt;- ggplot() +\n  geom_point(data = data_multiple, aes(x = X2, y = Y)) +\n  geom_line(data = pred_data_X2, aes(x = X2, y = Y_pred), \n            color = \"red\", size = 1) +\n  theme_minimal() +\n  labs(title = \"Efekt Cząstkowy X2\",\n       subtitle = paste(\"(X1 utrzymane na średniej =\", round(mean(X1), 2), \")\"))\n\ngrid.arrange(p3, p4, ncol = 2)\n\n\n\n\nEfekty Cząstkowe w Regresji Wielorakiej\n\n\n\n\n\n\n16.7.3 Współliniowość\nWspółliniowość występuje, gdy predyktory są silnie skorelowane. Zobaczmy jej efekty:\n\n# Generate data with multicollinearity\nset.seed(106)\nX1_new &lt;- rnorm(n, mean = 50, sd = 10)\nX2_new &lt;- 2*X1_new + rnorm(n, 0, 5)  # X2 silnie skorelowane z X1\nY_new &lt;- 10 + 0.5*X1_new + 0.8*X2_new + rnorm(n, 0, 5)\n\ndata_collinear &lt;- data.frame(Y = Y_new, X1 = X1_new, X2 = X2_new)\n\n# Fit model with multicollinearity\nmodel_collinear &lt;- lm(Y ~ X1 + X2, data = data_collinear)\n\n# Calculate VIF\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nThe following object is masked from 'package:psych':\n\n    logit\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nvif_results &lt;- vif(model_collinear)\n\n# Plot correlation\nggplot(data_collinear, aes(x = X1, y = X2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Korelacja między Predyktorami\",\n       subtitle = paste(\"Korelacja =\", \n                       round(cor(X1_new, X2_new), 3)))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nEfekty Współliniowości",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#tematy-zaawansowane",
    "href": "correg_pl.html#tematy-zaawansowane",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.8 Tematy Zaawansowane",
    "text": "16.8 Tematy Zaawansowane\n\n16.8.1 Efekty Interakcji\nEfekty interakcji pozwalają na to, by wpływ jednego predyktora zależał od innego:\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3(X_1 \\times X_2) + \\varepsilon\n\n# Generate data with interaction\nset.seed(107)\nX1_int &lt;- rnorm(n, mean = 0, sd = 1)\nX2_int &lt;- rnorm(n, mean = 0, sd = 1)\nY_int &lt;- 1 + 2*X1_int + 3*X2_int + 4*X1_int*X2_int + rnorm(n, 0, 1)\n\ndata_int &lt;- data.frame(X1 = X1_int, X2 = X2_int, Y = Y_int)\nmodel_int &lt;- lm(Y ~ X1 * X2, data = data_int)\n\n# Create interaction plot\nX1_levels &lt;- quantile(X1_int, probs = c(0.25, 0.75))\nX2_seq &lt;- seq(min(X2_int), max(X2_int), length.out = 100)\n\npred_data &lt;- expand.grid(\n  X1 = X1_levels,\n  X2 = X2_seq\n)\npred_data$Y_pred &lt;- predict(model_int, newdata = pred_data)\npred_data$X1_level &lt;- factor(pred_data$X1, \n                            labels = c(\"Niskie X1\", \"Wysokie X1\"))\n\nggplot(pred_data, aes(x = X2, y = Y_pred, color = X1_level)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Efekt Interakcji\",\n       subtitle = \"Wpływ X2 zależy od poziomu X1\",\n       color = \"Poziom X1\")\n\n\n\n\nWizualizacja Efektów Interakcji\n\n\n\n\n\n\n16.8.2 Wyrazy Wielomianowe\nGdy zależności są nieliniowe, możemy dodać wyrazy wielomianowe:\nY = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\varepsilon\n\n# Generate data with quadratic relationship\nset.seed(108)\nX_poly &lt;- seq(-3, 3, length.out = 100)\nY_poly &lt;- 1 - 2*X_poly + 3*X_poly^2 + rnorm(length(X_poly), 0, 2)\ndata_poly &lt;- data.frame(X = X_poly, Y = Y_poly)\n\n# Fit linear and quadratic models\nmodel_linear &lt;- lm(Y ~ X, data = data_poly)\nmodel_quad &lt;- lm(Y ~ X + I(X^2), data = data_poly)\n\n# Add predictions\ndata_poly$pred_linear &lt;- predict(model_linear)\ndata_poly$pred_quad &lt;- predict(model_quad)\n\n# Plot\nggplot(data_poly, aes(x = X, y = Y)) +\n  geom_point(alpha = 0.5) +\n  geom_line(aes(y = pred_linear, color = \"Liniowy\"), size = 1) +\n  geom_line(aes(y = pred_quad, color = \"Kwadratowy\"), size = 1) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme_minimal() +\n  labs(title = \"Dopasowanie Liniowe vs Kwadratowe\",\n       color = \"Typ Modelu\")\n\n\n\n\nPrzykład Regresji Wielomianowej",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#praktyczne-wskazówki-do-analizy-regresji",
    "href": "correg_pl.html#praktyczne-wskazówki-do-analizy-regresji",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.9 Praktyczne Wskazówki do Analizy Regresji",
    "text": "16.9 Praktyczne Wskazówki do Analizy Regresji\n\n16.9.1 Proces Budowy Modelu\n\nEksploracja Danych\n\n\n# Generate example dataset\nset.seed(109)\nn &lt;- 100\ndata_example &lt;- data.frame(\n  x1 = rnorm(n, mean = 50, sd = 10),\n  x2 = rnorm(n, mean = 20, sd = 5),\n  x3 = runif(n, 0, 100)\n)\ndata_example$y &lt;- 10 + 0.5*data_example$x1 + 0.8*data_example$x2 - \n                 0.3*data_example$x3 + rnorm(n, 0, 5)\n\n# Correlation matrix plot\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nggpairs(data_example) +\n  theme_minimal() +\n  labs(title = \"Analiza Eksploracyjna Danych\",\n       subtitle = \"Macierz korelacji i rozkłady\")\n\n\n\n\nPrzykład Eksploracji Danych\n\n\n\n\n\nWybór Zmiennych\n\n\n# Fit models with different variables\nmodel1 &lt;- lm(y ~ x1, data = data_example)\nmodel2 &lt;- lm(y ~ x1 + x2, data = data_example)\nmodel3 &lt;- lm(y ~ x1 + x2 + x3, data = data_example)\n\n# Compare models\nmodels_comparison &lt;- data.frame(\n  Model = c(\"y ~ x1\", \"y ~ x1 + x2\", \"y ~ x1 + x2 + x3\"),\n  R_kwadrat = c(summary(model1)$r.squared,\n                summary(model2)$r.squared,\n                summary(model3)$r.squared),\n  Skorygowany_R_kwadrat = c(summary(model1)$adj.r.squared,\n                    summary(model2)$adj.r.squared,\n                    summary(model3)$adj.r.squared)\n)\n\nknitr::kable(models_comparison, digits = 3,\n             caption = \"Podsumowanie Porównania Modeli\")\n\n\nPodsumowanie Porównania Modeli\n\n\nModel\nR_kwadrat\nSkorygowany_R_kwadrat\n\n\n\n\ny ~ x1\n0.323\n0.316\n\n\ny ~ x1 + x2\n0.433\n0.421\n\n\ny ~ x1 + x2 + x3\n0.893\n0.890\n\n\n\nProces Wyboru Zmiennych\n\n\n\n\n16.9.2 Typowe Pułapki i Rozwiązania\n\nWartości Odstające i Punkty Wpływowe\n\n\n# Create data with outlier\nset.seed(110)\nx_clean &lt;- rnorm(50, mean = 0, sd = 1)\ny_clean &lt;- 2 + 3*x_clean + rnorm(50, 0, 0.5)\ndata_clean &lt;- data.frame(x = x_clean, y = y_clean)\n\n# Add outlier\ndata_outlier &lt;- rbind(data_clean,\n                      data.frame(x = 4, y = -10))\n\n# Fit models\nmodel_clean &lt;- lm(y ~ x, data = data_clean)\nmodel_outlier &lt;- lm(y ~ x, data = data_outlier)\n\n# Plot\nggplot() +\n  geom_point(data = data_clean, aes(x = x, y = y), color = \"blue\") +\n  geom_point(data = data_outlier[51,], aes(x = x, y = y), \n             color = \"red\", size = 3) +\n  geom_line(data = data_clean, \n            aes(x = x, y = predict(model_clean), \n                color = \"Bez Wartości Odstającej\")) +\n  geom_line(data = data_outlier, \n            aes(x = x, y = predict(model_outlier), \n                color = \"Z Wartością Odstającą\")) +\n  theme_minimal() +\n  labs(title = \"Wpływ Wartości Odstających na Regresję\",\n       color = \"Model\") +\n  scale_color_manual(values = c(\"blue\", \"red\"))\n\n\n\n\nIdentyfikacja i Obsługa Wartości Odstających\n\n\n\n\n\nWzorce Brakujących Danych\n\n\n# Create data with missing values\nset.seed(111)\ndata_missing &lt;- data_example\ndata_missing$x1[sample(1:n, 10)] &lt;- NA\ndata_missing$x2[sample(1:n, 15)] &lt;- NA\ndata_missing$x3[sample(1:n, 20)] &lt;- NA\n\n# Visualize missing patterns\nlibrary(naniar)\nvis_miss(data_missing) +\n  theme_minimal() +\n  labs(title = \"Wzorce Brakujących Danych\")\n\n\n\n\nWzorce Brakujących Danych\n\n\n\n\n\nHeteroskedastyczność\n\n\n# Generate heteroscedastic data\nset.seed(112)\nx_hetero &lt;- seq(-3, 3, length.out = 100)\ny_hetero &lt;- 2 + 1.5*x_hetero + rnorm(100, 0, abs(x_hetero)/2)\ndata_hetero &lt;- data.frame(x = x_hetero, y = y_hetero)\n\n# Fit model\nmodel_hetero &lt;- lm(y ~ x, data = data_hetero)\n\n# Plot\np1 &lt;- ggplot(data_hetero, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Dane Heteroskedastyczne\")\n\np2 &lt;- ggplot(data_hetero, aes(x = fitted(model_hetero), \n                             y = residuals(model_hetero))) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Wykres Reszt\",\n       x = \"Wartości dopasowane\",\n       y = \"Reszty\")\n\ngrid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWykrywanie i Wizualizacja Heteroskedastyczności",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#najlepsze-praktyki",
    "href": "correg_pl.html#najlepsze-praktyki",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.10 Najlepsze Praktyki",
    "text": "16.10 Najlepsze Praktyki\n\n16.10.1 Walidacja Modelu\n\n# Simple cross-validation example\nset.seed(113)\n\n# Create training and test sets\ntrain_index &lt;- sample(1:nrow(data_example), 0.7*nrow(data_example))\ntrain_data &lt;- data_example[train_index, ]\ntest_data &lt;- data_example[-train_index, ]\n\n# Fit model on training data\nmodel_train &lt;- lm(y ~ x1 + x2 + x3, data = train_data)\n\n# Predict on test data\npredictions &lt;- predict(model_train, newdata = test_data)\nactual &lt;- test_data$y\n\n# Calculate performance metrics\nrmse &lt;- sqrt(mean((predictions - actual)^2))\nmae &lt;- mean(abs(predictions - actual))\nr2 &lt;- cor(predictions, actual)^2\n\n# Plot predictions vs actual\ndata_validation &lt;- data.frame(\n  Przewidywane = predictions,\n  Rzeczywiste = actual\n)\n\nggplot(data_validation, aes(x = Rzeczywiste, y = Przewidywane)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Walidacja Modelu: Przewidywane vs Rzeczywiste\",\n       subtitle = sprintf(\"RMSE = %.2f, MAE = %.2f, R² = %.2f\", \n                         rmse, mae, r2))\n\n\n\n\nPrzykład Walidacji Krzyżowej\n\n\n\n\n\n\n16.10.2 Prezentacja Wyników\nPrzykład profesjonalnej tabeli wyników regresji:\n\n# Create regression results table\nlibrary(broom)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nmodel_final &lt;- lm(y ~ x1 + x2 + x3, data = data_example)\nresults &lt;- tidy(model_final, conf.int = TRUE)\n\nkable(results, digits = 3,\n      caption = \"Podsumowanie Wyników Regresji\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nPodsumowanie Wyników Regresji\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n9.116\n2.835\n3.216\n0.002\n3.489\n14.743\n\n\nx1\n0.497\n0.039\n12.756\n0.000\n0.419\n0.574\n\n\nx2\n0.905\n0.086\n10.468\n0.000\n0.734\n1.077\n\n\nx3\n-0.324\n0.016\n-20.322\n0.000\n-0.356\n-0.292",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#podsumowanie-1",
    "href": "correg_pl.html#podsumowanie-1",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.11 Podsumowanie",
    "text": "16.11 Podsumowanie\n\n16.11.1 Kluczowe Wnioski\n\nZawsze zaczynaj od eksploracyjnej analizy danych\nSprawdzaj założenia przed interpretacją wyników\nBądź świadomy typowych pułapek:\n\nWartości odstające\nBrakujące dane\nWspółliniowość\nHeteroskedastyczność\n\nWaliduj swój model używając:\n\nWykresów diagnostycznych\nWalidacji krzyżowej\nAnalizy reszt\n\nPrezentuj wyniki jasno i kompletnie\n\n\n\nLiteratura Uzupełniająca\nDla głębszego zrozumienia:\n\nWooldridge, J.M. “Wprowadzenie do Ekonometrii: Współczesne Ujęcie”\nFox, J. “Analiza Regresji Stosowana i Uogólnione Modele Liniowe”\nAngrist, J.D. i Pischke, J.S. “W Większości Nieszkodliwa Ekonometria”\nStock & Watson “Wprowadzenie do Ekonometrii”\n\n\n\n\n\n\n\nZrozumienie Regresji Metodą Najmniejszych Kwadratów (OLS): Podsumowanie dla Studentów\n\n\n\nAnaliza regresji to podstawowa metoda statystyczna, która bada i modeluje zależności między zmiennymi w celu zrozumienia, jak zmiany w jednej lub kilku zmiennych niezależnych wpływają na zmienną zależną.\nW swojej istocie analiza regresji pomaga odpowiedzieć na pytania dotyczące przyczyny i skutku, przewidywania oraz prognozowania. Na przykład, przedsiębiorstwo może wykorzystać analizę regresji do zrozumienia, jak wydatki na reklamę wpływają na sprzedaż lub jak liczba godzin szkoleń pracowników przekłada się na produktywność.\n\n16.11.2 Regresja jako Model Stochastyczny\nW modelowaniu matematycznym spotykamy dwa podstawowe podejścia do opisywania relacji między zmiennymi:\n\nmodele deterministyczne,\nmodele stochastyczne.\n\n\n16.11.2.1 Modele Deterministyczne a Stochastyczne\nModel deterministyczny zakłada precyzyjną, ustaloną relację między danymi wejściowymi a wyjściowymi. W takich modelach, znając dane wejściowe, możemy z całkowitą pewnością obliczyć dokładny wynik. Weźmy pod uwagę klasyczne równanie fizyczne dotyczące drogi:\n\\text{Droga} = \\text{Prędkość} × \\text{Czas}\nPrzy określonych wartościach prędkości i czasu, równanie to zawsze da tę samą drogę. Nie ma tu miejsca na jakąkolwiek zmienność wyniku.\nW przeciwieństwie do tego, analiza regresji uwzględnia naturalną zmienność danych. Podstawowa struktura modelu regresji to:\nY = f(X) + \\epsilon\nGdzie:\n\nY reprezentuje wynik, który chcemy przewidzieć\nf(X) reprezentuje systematyczną relację między naszymi predyktorami (X) a wynikiem\n\\epsilon reprezentuje losową zmienność naturalnie występującą w rzeczywistych danych\n\n\n\n\n16.11.3 Czym jest Prosta Regresja Liniowa?\nRozważmy związek między czasem poświęconym na naukę (predyktor, zmienna niezależna) a wynikami egzaminów (zmienna objaśniana, outcome/response variable). Prosta regresja liniowa tworzy optymalną linię prostą przechodzącą przez punkty danych, modelującą tę relację. Linia ta służy dwóm celom: prognozowaniu oraz kwantyfikacji związku między zmiennymi.\nZależność matematyczna jest wyrażona jako:\n Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i \nUżywając naszego przykładu z czasem nauki do zilustrowania każdego składnika:\n\nY_i reprezentuje zmienną zależną (wyniki egzaminów)\nX_i reprezentuje zmienną niezależną (godziny nauki)\n\\beta_0 reprezentuje wyraz wolny - oczekiwany wynik egzaminu przy zerowym czasie nauki\n\\beta_1 reprezentuje nachylenie - zmianę wyniku egzaminu na każdą dodatkową godzinę nauki\n\\epsilon_i reprezentuje składnik błędu, uwzględniający nieobserwowane czynniki\n\nNaszym celem jest znalezienie oszacowań \\hat{\\beta}_0 i \\hat{\\beta}_1, które najlepiej przybliżają prawdziwe parametry \\beta_0 i \\beta_1. Dopasowana linia regresji jest wtedy wyrażona jako:\n \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1X_i \n\n\n16.11.4 Metoda Najmniejszych Kwadratów (MNK/OLS)\nMetoda “Najmniejszych Kwadratów” określa optymalne wartości dla \\hat{\\beta}_0 i \\hat{\\beta}_1 poprzez metodyczny proces. Choć współczesne oprogramowanie statystyczne wykonuje te obliczenia automatycznie, zrozumienie podstawowego procesu jest kluczowe:\n\nRozpocznij od wstępnego oszacowania linii regresji\n\nZazwyczaj zaczynamy od linii poziomej na poziomie \\bar{Y} (średnia Y)\nSłuży to jako punkt odniesienia, reprezentujący brak związku między X a Y\n\nOblicz odległości pionowe (reszty) od każdego obserwowanego punktu do tej linii\n\nTe reszty \\hat{\\epsilon}_i = e_i = Y_i - \\hat{Y}_i reprezentują nasze błędy predykcji\nDodatnia reszta oznacza, że nasza linia niedoszacowuje prawdziwej wartości\nUjemna reszta oznacza, że nasza linia przeszacowuje prawdziwą wartość\n\nPodnieś te odległości do kwadratu, aby:\n\nZapewnić, że dodatnie i ujemne odchylenia się nie znoszą\nPrzypisać większą wagę większym odchyleniom\n\nZsumuj wszystkie kwadraty odległości, aby otrzymać Sumę Kwadratów Błędów (SSE)\n\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\nReprezentuje to całkowite kwadratowe odchylenie obserwowanych wartości od naszych przewidywanych wartości\n\nSystematycznie dostosowuj nachylenie i wyraz wolny linii, aby zminimalizować SSE:\n\n\nProces polega na stopniowym modyfikowaniu nachylenia i położenia linii, obserwując jak zmienia się suma kwadratów odchyłek (SSE) [formalnie wymaga to zastosowania pojęć z rachunku różniczkowego]\nGdy SSE przestaje się zmniejszać przy kolejnych niewielkich zmianach parametrów linii, oznacza to znalezienie optymalnego dopasowania\nTo miejsce odpowiada najmniejszej możliwej wartości SSE, czyli najlepszemu dopasowaniu linii do danych\n\nBardziej formalnie, MNK (Metoda Najmniejszych Kwadratów) znajduje optymalne wartości dla \\beta_0 i \\beta_1 poprzez minimalizację sumy kwadratów błędów:\n\\min_{\\beta_0, \\beta_1} \\sum \\epsilon_i^2 = \\min_{\\beta_0, \\beta_1} \\sum(y_i - \\hat{y_i})^2 = \\min_{\\beta_0, \\beta_1} \\sum(y_i - (\\beta_0 + \\beta_1x_i))^2\nRozwiązanie tego problemu minimalizacji daje nam:\n\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = \\frac{cov(X, Y)}{var(X)} \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\nWarto zauważyć, że w powyższych wzorach \\hat{\\beta}_1 reprezentuje nachylenie linii regresji i jest wyrażone jako stosunek kowariancji zmiennych X i Y do wariancji zmiennej X. Natomiast \\hat{\\beta}_0 to wyraz wolny (punkt przecięcia z osią Y), który możemy obliczyć używając średnich wartości zmiennych X i Y oraz już oszacowanego nachylenia. Symbol \\bar{x} oznacza średnią wartość zmiennej X, a \\bar{y} średnią wartość zmiennej Y.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.11.5 Estymacja Parametrów\nFormuły do oszacowania parametrów wykorzystują podstawowe miary statystyczne:\n \\hat{\\beta}_1 = \\frac{Cov(X,Y)}{Var(X)} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2} \nTa formuła zawiera dwa istotne pojęcia statystyczne:\n\nKowariancja (Cov) kwantyfikuje wspólny ruch X i Y\nWariancja (Var) mierzy rozproszenie wartości X wokół ich średniej\n\\bar{X} i \\bar{Y} reprezentują odpowiednio średnie X i Y\n\nPo obliczeniu \\hat{\\beta}_1 określamy \\hat{\\beta}_0:\n \\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X} \nZapewnia to, że linia regresji przechodzi przez punkt średnich (\\bar{X}, \\bar{Y}).\n\n\n16.11.6 Dekompozycja Wariancji w Regresji\nW analizie regresji rozkładamy wariancję w naszych danych na odrębne składniki, aby zrozumieć, jak dobrze nasz model wyjaśnia zmienną zależną. Przyjrzyjmy się każdemu składnikowi wariancji szczegółowo.\n\n16.11.6.1 Przegląd Składników Wariancji\nCałkowita Suma Kwadratów (SST, z ang. Total Sum of Squares) reprezentuje całkowitą zmienność obecną w naszej zmiennej zależnej. Regresyjna Suma Kwadratów (SSR, z ang. Regression Sum of Squares) obejmuje zmienność wyjaśnioną przez nasz model. Resztowa Suma Kwadratów (SSE, z ang. Error Sum of Squares) przedstawia pozostałą, niewyjaśnioną zmienność. Przeanalizujmy każdy z tych elementów.\n\n\n16.11.6.2 Całkowita Suma Kwadratów (SST)\nSST mierzy całkowitą zmienność w zmiennej zależnej, obliczając, jak bardzo każda obserwacja odbiega od średniej ogólnej:\n SST = \\sum(y_i - \\bar{y})^2 \nW tym wzorze y_i reprezentuje każdą pojedynczą obserwację, a \\bar{y} reprezentuje średnią wszystkich obserwacji. Kwadraty różnic ujmują całkowitą rozpiętość naszych danych wokół ich wartości średniej.\n\n\n16.11.6.3 Regresyjna Suma Kwadratów (SSR)\nSSR określa ilościowo zmienność, którą nasz model wyjaśnia poprzez zmienne objaśniające:\n SSR = \\sum(\\hat{y}_i - \\bar{y})^2 \nTutaj \\hat{y}_i reprezentuje wartości przewidywane przez nasz model. Ten wzór mierzy, jak daleko nasze przewidywania odbiegają od średniej, reprezentując zmienność uchwyconą przez nasz model.\n\n\n16.11.6.4 Resztowa Suma Kwadratów (SSE)\nSSE mierzy zmienność, która pozostaje niewyjaśniona przez nasz model:\n SSE = \\sum(y_i - \\hat{y}_i)^2 \nTen wzór oblicza różnice między wartościami rzeczywistymi (y_i) a wartościami przewidywanymi (\\hat{y}_i), pokazując nam, ile zmienności nasz model nie zdołał wyjaśnić.\n\n\n16.11.6.5 Podstawowa Zależność\nTe trzy składniki są powiązane poprzez kluczowe równanie:\n SST = SSR + SSE \nTo równanie pokazuje, że całkowita zmienność (SST) równa się sumie zmienności wyjaśnionej (SSR) i niewyjaśnionej (SSE). Ta zależność stanowi podstawę do pomiaru wydajności modelu.\n\n\n16.11.6.6 Współczynnik Determinacji (R²)\nStatystyka R², wyprowadzona z tych składników, mówi nam, jaką proporcję zmienności wyjaśnia nasz model:\n R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST} \nGdy R² równa się 0,75, oznacza to, że nasz model wyjaśnia 75% zmienności w zmiennej zależnej. Pozostałe 25% reprezentuje zmienność niewyjaśnioną.\n\n\n16.11.6.7 Istotne Aspekty\nZrozumienie tych składników pomaga nam:\n\nOcenić wydajność modelu poprzez proporcję wyjaśnionej zmienności\nZidentyfikować, ile zmienności pozostaje niewyjaśnione\nPodejmować decyzje dotyczące ulepszania modelu\nPorównywać moc wyjaśniającą różnych modeli\n\n\n\n\n\n\n\n\n\n\n\n\n\n16.11.7 Współczynnik Determinacji (R^2)\nR^2 służy jako miara dopasowania modelu, reprezentująca proporcję wariancji wyjaśnionej przez naszą regresję:\n R^2 = \\frac{SSR}{SST} = \\frac{SSR}{SSR + SSE} = 1 - \\frac{SSE}{SST} \nAby zinterpretować R^2:\n\nR^2 równe 0,75 wskazuje, że model wyjaśnia 75% wariancji (np. wyników egzaminów)\nR^2 równe 0,20 sugeruje, że predyktory (np. godziny nauki) odpowiadają za 20% zmienności wyników\nR^2 równe 1,00 reprezentuje doskonałą predykcję (rzadko obserwowane w praktyce)\nR^2 równe 0,00 wskazuje na brak mocy wyjaśniającej\n\nUwaga: Niższa wartość R^2 niekoniecznie oznacza gorszy model - może po prostu odzwierciedlać złożoność badanej relacji.\n\n\n16.11.8 Kluczowy Wniosek\nRegresja liniowa ustala relacje między zmiennymi poprzez identyfikację optymalnego dopasowania liniowego przez punkty danych. Ta optymalizacja zachodzi poprzez minimalizację kwadratów reszt. Współczynnik determinacji (R^2) kwantyfikuje następnie moc wyjaśniającą modelu poprzez porównanie wyjaśnionej zmienności z całkowitą zmiennością w naszych danych.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#dodatek-a.1-obliczanie-kowariancji-korelacji-pearsona-i-spearmana-oraz-modelowanie-ols---przykład-z-obliczeniami",
    "href": "correg_pl.html#dodatek-a.1-obliczanie-kowariancji-korelacji-pearsona-i-spearmana-oraz-modelowanie-ols---przykład-z-obliczeniami",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.12 Dodatek A.1: Obliczanie Kowariancji, Korelacji Pearsona i Spearmana, oraz modelowanie OLS - przykład z obliczeniami",
    "text": "16.12 Dodatek A.1: Obliczanie Kowariancji, Korelacji Pearsona i Spearmana, oraz modelowanie OLS - przykład z obliczeniami\nStudentka politologii bada związek między wielkością okręgu wyborczego (DM) a wskaźnikiem dysproporcjonalności Gallaghera (GH) w wyborach parlamentarnych w 10 losowo wybranych demokracjach.\nDane dotyczące wielkości okręgu wyborczego (\\text{DM}) i indeksu Gallaghera:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18,2\n\n\n3\n16,7\n\n\n4\n15,8\n\n\n5\n15,3\n\n\n6\n15,0\n\n\n7\n14,8\n\n\n8\n14,7\n\n\n9\n14,6\n\n\n10\n14,55\n\n\n11\n14,52\n\n\n\n\n16.12.1 Krok 1: Obliczanie Podstawowych Statystyk\nObliczanie średnich:\nDla \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nSzczegółowe obliczenia:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6,5\nDla indeksu Gallaghera (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nSzczegółowe obliczenia:\n18,2 + 16,7 + 15,8 + 15,3 + 15,0 + 14,8 + 14,7 + 14,6 + 14,55 + 14,52 = 154,17 \\bar{y} = \\frac{154,17}{10} = 15,417\n\n\n16.12.2 Krok 2: Szczegółowe Obliczenia Kowariancji\nPełna tabela robocza ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n-4,5\n2,783\n-12,5235\n20,25\n7,7451\n\n\n2\n3\n16,7\n-3,5\n1,283\n-4,4905\n12,25\n1,6461\n\n\n3\n4\n15,8\n-2,5\n0,383\n-0,9575\n6,25\n0,1467\n\n\n4\n5\n15,3\n-1,5\n-0,117\n0,1755\n2,25\n0,0137\n\n\n5\n6\n15,0\n-0,5\n-0,417\n0,2085\n0,25\n0,1739\n\n\n6\n7\n14,8\n0,5\n-0,617\n-0,3085\n0,25\n0,3807\n\n\n7\n8\n14,7\n1,5\n-0,717\n-1,0755\n2,25\n0,5141\n\n\n8\n9\n14,6\n2,5\n-0,817\n-2,0425\n6,25\n0,6675\n\n\n9\n10\n14,55\n3,5\n-0,867\n-3,0345\n12,25\n0,7517\n\n\n10\n11\n14,52\n4,5\n-0,897\n-4,0365\n20,25\n0,8047\n\n\nSuma\n65\n154,17\n0\n0\n-28,085\n82,5\n12,8442\n\n\n\nObliczanie kowariancji: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28,085}{9} = -3,120556\n\n\n16.12.3 Krok 3: Obliczanie Odchylenia Standardowego\nDla \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82,5}{9}} = \\sqrt{9,1667} = 3,026582\nDla Gallaghera (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12,8442}{9}} = \\sqrt{1,4271} = 1,194612\n\n\n16.12.4 Krok 4: Obliczanie Korelacji Pearsona\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3,120556}{3,026582 \\times 1,194612} = \\frac{-3,120556}{3,615752} = -0,863044\n\n\n16.12.5 Krok 5: Obliczanie Korelacji Rangowej Spearmana\nPełna tabela rangowa ze wszystkimi obliczeniami:\n\n\n\ni\nX_i\nY_i\nRanga X_i\nRanga Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18,2\n1\n10\n-9\n81\n\n\n2\n3\n16,7\n2\n9\n-7\n49\n\n\n3\n4\n15,8\n3\n8\n-5\n25\n\n\n4\n5\n15,3\n4\n7\n-3\n9\n\n\n5\n6\n15,0\n5\n6\n-1\n1\n\n\n6\n7\n14,8\n6\n5\n1\n1\n\n\n7\n8\n14,7\n7\n4\n3\n9\n\n\n8\n9\n14,6\n8\n3\n5\n25\n\n\n9\n10\n14,55\n9\n2\n7\n49\n\n\n10\n11\n14,52\n10\n1\n9\n81\n\n\nSuma\n\n\n\n\n\n330\n\n\n\nObliczanie korelacji Spearmana: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\n16.12.6 Krok 6: Weryfikacja w R\n\n# Tworzenie wektorów\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Obliczanie kowariancji\ncov(DM, GH)\n\n[1] -3.120556\n\n# Obliczanie korelacji\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\n16.12.7 Krok 7: Podstawowa Wizualizacja\n\nlibrary(ggplot2)\n\n# Tworzenie ramki danych\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Tworzenie wykresu rozrzutu\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Wielkość Okręgu vs Indeks Gallaghera\",\n    x = \"Wielkość Okręgu (DM)\",\n    y = \"Indeks Gallaghera (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n16.12.8 Estymacja OLS i Miary Dopasowania Modelu\n\n\n16.12.9 Krok 1: Obliczanie Estymatorów OLS\nKorzystając z wcześniej obliczonych wartości:\n\n\\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28,085\n\\sum(X_i - \\bar{X})^2 = 82,5\n\\bar{X} = 6,5\n\\bar{Y} = 15,417\n\nObliczanie nachylenia (\\hat{\\beta_1}):\n\\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 ÷ 82,5 = -0,3404\nObliczanie wyrazu wolnego (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 × 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nZatem równanie regresji OLS ma postać: \\hat{Y} = 17,6296 - 0,3404X\n\n\n16.12.10 Krok 2: Obliczanie Wartości Dopasowanych i Reszt\nPełna tabela ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n16,9488\n1,2512\n1,5655\n7,7451\n2,3404\n\n\n2\n3\n16,7\n16,6084\n0,0916\n0,0084\n1,6461\n1,4241\n\n\n3\n4\n15,8\n16,2680\n-0,4680\n0,2190\n0,1467\n0,7225\n\n\n4\n5\n15,3\n15,9276\n-0,6276\n0,3939\n0,0137\n0,2601\n\n\n5\n6\n15,0\n15,5872\n-0,5872\n0,3448\n0,1739\n0,0289\n\n\n6\n7\n14,8\n15,2468\n-0,4468\n0,1996\n0,3807\n0,0290\n\n\n7\n8\n14,7\n14,9064\n-0,2064\n0,0426\n0,5141\n0,2610\n\n\n8\n9\n14,6\n14,5660\n0,0340\n0,0012\n0,6675\n0,7241\n\n\n9\n10\n14,55\n14,2256\n0,3244\n0,1052\n0,7517\n1,4184\n\n\n10\n11\n14,52\n13,8852\n0,6348\n0,4030\n0,8047\n2,3439\n\n\nSuma\n65\n154,17\n154,17\n0\n3,2832\n12,8442\n9,5524\n\n\n\nObliczenia dla wartości dopasowanych:\nDla X = 2:\nŶ = 17,6296 + (-0,3404 × 2) = 16,9488\n\nDla X = 3:\nŶ = 17,6296 + (-0,3404 × 3) = 16,6084\n\n[... kontynuacja dla wszystkich wartości]\n\n\n16.12.11 Krok 3: Obliczanie Miar Dopasowania\nSuma kwadratów reszt (SSE): SSE = \\sum e_i^2\nSSE = 3,2832\nCałkowita suma kwadratów (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12,8442\nSuma kwadratów regresji (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9,5524\nWeryfikacja dekompozycji: SST = SSR + SSE\n12,8442 = 9,5524 + 3,2832 (w granicach błędu zaokrąglenia)\nObliczanie współczynnika determinacji R-kwadrat: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR² = 9,5524 ÷ 12,8442\n   = 0,7438\n\n\n16.12.12 Krok 4: Weryfikacja w R\n\n# Dopasowanie modelu liniowego\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# Podsumowanie statystyk\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 4.67e-10 ***\nDM          -0.34042    0.07053  -4.827  0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Ręczne obliczenie R-kwadrat\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\n16.12.13 Krok 5: Analiza Reszt\n\n# Tworzenie wykresów reszt\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\n16.12.14 Krok 6: Wykres Wartości Przewidywanych vs Rzeczywistych\n\n# Tworzenie wykresu wartości przewidywanych vs rzeczywistych\nggplot(data.frame(\n  Rzeczywiste = GH,\n  Przewidywane = fitted(model)\n), aes(x = Przewidywane, y = Rzeczywiste)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Wartości Przewidywane vs Rzeczywiste\",\n    x = \"Przewidywany Indeks Gallaghera\",\n    y = \"Rzeczywisty Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n16.12.15 Modele z Transformacją Logarytmiczną\n\n\n16.12.16 Krok 1: Transformacja Danych\nNajpierw obliczamy logarytmy naturalne zmiennych:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18,2\n0,6931\n2,9014\n\n\n2\n3\n16,7\n1,0986\n2,8154\n\n\n3\n4\n15,8\n1,3863\n2,7600\n\n\n4\n5\n15,3\n1,6094\n2,7278\n\n\n5\n6\n15,0\n1,7918\n2,7081\n\n\n6\n7\n14,8\n1,9459\n2,6946\n\n\n7\n8\n14,7\n2,0794\n2,6878\n\n\n8\n9\n14,6\n2,1972\n2,6810\n\n\n9\n10\n14,55\n2,3026\n2,6777\n\n\n10\n11\n14,52\n2,3979\n2,6757\n\n\n\n\n\n16.12.17 Krok 2: Porównanie Różnych Specyfikacji Modelu\nSzacujemy trzy alternatywne specyfikacje:\n\nModel log-liniowy: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nModel liniowo-logarytmiczny: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nModel log-log: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Tworzenie zmiennych transformowanych\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Dopasowanie modeli\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Porównanie wartości R-kwadrat\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Liniowy\", \"Log-liniowy\", \"Liniowo-logarytmiczny\", \"Log-log\"),\n  R_kwadrat = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Wyświetlenie porównania\nmodels_comparison\n\n                  Model R_kwadrat\n1               Liniowy 0.7443793\n2           Log-liniowy 0.7670346\n3 Liniowo-logarytmiczny 0.9141560\n4               Log-log 0.9288088\n\n\n\n\n16.12.18 Krok 3: Porównanie Wizualne\n\n# Tworzenie wykresów dla każdego modelu\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowy\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-liniowy\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowo-logarytmiczny\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-log\") +\n  theme_minimal()\n\n# Układanie wykresów w siatkę\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n16.12.19 Krok 4: Analiza Reszt dla Najlepszego Modelu\nNa podstawie wartości R-kwadrat, analiza reszt dla najlepiej dopasowanego modelu:\n\n# Wykresy reszt dla najlepszego modelu\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\n16.12.20 Krok 5: Interpretacja Najlepszego Modelu\nWspółczynniki modelu liniowo-logarytmicznego:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 4.94e-11 ***\nlog_DM       -2.0599     0.2232   -9.23 1.54e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 1.539e-05\n\n\nInterpretacja: - \\hat{\\beta_0} reprezentuje oczekiwany Indeks Gallaghera, gdy ln(DM) = 0 (czyli gdy DM = 1) - \\hat{\\beta_1} reprezentuje zmianę Indeksu Gallaghera związaną z jednostkowym wzrostem ln(DM)\n\n\n16.12.21 Krok 6: Predykcje Modelu\n\n# Tworzenie wykresu predykcji dla najlepszego modelu\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Model Liniowo-logarytmiczny: Indeks Gallaghera vs ln(Wielkość Okręgu)\",\n    x = \"ln(Wielkość Okręgu)\",\n    y = \"Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n16.12.22 Krok 7: Analiza Elastyczności\nDla modelu log-log współczynniki bezpośrednio reprezentują elastyczności. Obliczenie średniej elastyczności dla modelu liniowo-logarytmicznego:\n\n# Obliczenie elastyczności przy wartościach średnich\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelastycznosc &lt;- beta1 * (1/mean_GH)\nelastycznosc\n\n    log_DM \n-0.1336136 \n\n\nWartość ta reprezentuje procentową zmianę Indeksu Gallaghera przy jednoprocentowej zmianie Wielkości Okręgu.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#dodatek-a.2-porównanie-popularnych-miar-korelacji-pearson-spearman-i-kendall",
    "href": "correg_pl.html#dodatek-a.2-porównanie-popularnych-miar-korelacji-pearson-spearman-i-kendall",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.13 Dodatek A.2: Porównanie Popularnych Miar Korelacji: Pearson, Spearman i Kendall",
    "text": "16.13 Dodatek A.2: Porównanie Popularnych Miar Korelacji: Pearson, Spearman i Kendall\n\n16.13.1 Zbiór Danych\n\ndane &lt;- data.frame(\n  x = c(2, 4, 5, 3, 8),\n  y = c(3, 5, 4, 4, 7)\n)\n\n\n\n16.13.2 Korelacja Pearsona\n r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}} \n\n16.13.2.1 Obliczenia krok po kroku:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n2\n3\n-2,4\n-1,6\n3,84\n5,76\n2,56\n\n\n2\n4\n5\n-0,4\n0,4\n-0,16\n0,16\n0,16\n\n\n3\n5\n4\n0,6\n-0,6\n-0,36\n0,36\n0,36\n\n\n4\n3\n4\n-1,4\n-0,6\n0,84\n1,96\n0,36\n\n\n5\n8\n7\n3,6\n2,4\n8,64\n12,96\n5,76\n\n\nSuma\n22\n23\n0\n0\n12,8\n21,2\n9,2\n\n\n\n\\bar{x} = 4,4 \\bar{y} = 4,6\n r = \\frac{12,8}{\\sqrt{21,2 \\times 9,2}} = \\frac{12,8}{\\sqrt{195,04}} = \\frac{12,8}{13,97} = 0,92 \n\n\n\n16.13.3 Korelacja Spearmana\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} \n\n16.13.3.1 Obliczenia krok po kroku:\n\n\n\ni\nx_i\ny_i\nRanga x_i\nRanga y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n3\n1\n1\n0\n0\n\n\n2\n4\n5\n3\n5\n-2\n4\n\n\n3\n5\n4\n4\n2,5\n1,5\n2,25\n\n\n4\n3\n4\n2\n2,5\n-0,5\n0,25\n\n\n5\n8\n7\n5\n4\n1\n1\n\n\nSuma\n\n\n\n\n\n7,5\n\n\n\n \\rho = 1 - \\frac{6(7,5)}{5(25-1)} = 1 - \\frac{45}{120} = 0,82 \n\n\n\n16.13.4 Tau Kendalla\n \\tau = \\frac{\\text{liczba par zgodnych} - \\text{liczba par niezgodnych}}{\\frac{1}{2}n(n-1)} \n\n16.13.4.1 Obliczenia krok po kroku:\n\n\n\nPara (i,j)\nx_i,x_j\ny_i,y_j\nx_j-x_i\ny_j-y_i\nWynik\n\n\n\n\n(1,2)\n2,4\n3,5\n+2\n+2\nZ\n\n\n(1,3)\n2,5\n3,4\n+3\n+1\nZ\n\n\n(1,4)\n2,3\n3,4\n+1\n+1\nZ\n\n\n(1,5)\n2,8\n3,7\n+6\n+4\nZ\n\n\n(2,3)\n4,5\n5,4\n+1\n-1\nN\n\n\n(2,4)\n4,3\n5,4\n-1\n-1\nZ\n\n\n(2,5)\n4,8\n5,7\n+4\n+2\nZ\n\n\n(3,4)\n5,3\n4,4\n-2\n0\nN\n\n\n(3,5)\n5,8\n4,7\n+3\n+3\nZ\n\n\n(4,5)\n3,8\n4,7\n+5\n+3\nZ\n\n\n\nLiczba par zgodnych = 8 Liczba par niezgodnych = 2  \\tau = \\frac{8-2}{10} = 0,74 \n\n\n\n16.13.5 Weryfikacja w R\n\ncat(\"Pearson:\", round(cor(dane$x, dane$y, method=\"pearson\"), 2), \"\\n\")\n\nPearson: 0.92 \n\ncat(\"Spearman:\", round(cor(dane$x, dane$y, method=\"spearman\"), 2), \"\\n\")\n\nSpearman: 0.82 \n\ncat(\"Kendall:\", round(cor(dane$x, dane$y, method=\"kendall\"), 2), \"\\n\")\n\nKendall: 0.74 \n\n\n\n\n16.13.6 Interpretacja Wyników\n\nKorelacja Pearsona (r = 0,92)\n\nSilna dodatnia korelacja liniowa\nWskazuje na bardzo silny liniowy związek między zmiennymi\n\nKorelacja Spearmana (ρ = 0,82)\n\nRównież silna dodatnia korelacja\nNieco niższa niż Pearsona, co sugeruje pewne odchylenia od monotoniczności\n\nTau Kendalla (τ = 0,74)\n\nNajniższa z trzech wartości, ale wciąż wskazuje na silną zależność\nBardziej odporna na wartości odstające\n\n\n\n\n16.13.7 Porównanie Miar\n\nRóżnice w wartościach:\n\nPearson (0,92) - najwyższa wartość, silna liniowość\nSpearman (0,82) - uwzględnia tylko uporządkowanie\nKendall (0,74) - najbardziej konserwatywna miara\n\nPraktyczne zastosowanie:\n\nWszystkie miary potwierdzają silną dodatnią zależność\nRóżnice między miarami wskazują na nieznaczne odchylenia od idealnej liniowości\nKendall daje najbardziej ostrożną ocenę siły związku",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-b.",
    "href": "correg_pl.html#appendix-b.",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.14 Appendix B.",
    "text": "16.14 Appendix B.\n(…)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-c.-przykłady",
    "href": "correg_pl.html#appendix-c.-przykłady",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.15 Appendix C. Przykłady",
    "text": "16.15 Appendix C. Przykłady",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "href": "correg_pl.html#descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.16 Descriptive Statistics and OLS Example - Income and Voter Turnout",
    "text": "16.16 Descriptive Statistics and OLS Example - Income and Voter Turnout\nBackground\nIn preparation for the 2024 municipal elections, the Amsterdam Electoral Commission conducted research on voter participation patterns across different city neighborhoods. A key question emerged:\nDoes economic prosperity of a neighborhood correlate with civic engagement, specifically voter turnout?\nData Collection\nSample: 5 representative neighborhoods in Amsterdam\nTime Period: Data from the 2022 municipal elections\nVariables:\n\nIncome: Average annual household income per capita (thousands €)\nTurnout: Percentage of registered voters who voted in the election\n\n\n16.16.1 Initial R Output for Reference\n\n# Data\nincome &lt;- c(50, 45, 56, 40, 60)  # thousands €\nturnout &lt;- c(60, 56, 70, 50, 75) # %\n\n# Full model check\nmodel &lt;- lm(turnout ~ income)\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n\n\n\n16.16.2 Dispersion Measures\nMeans:\n\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} = \\frac{50 + 45 + 56 + 40 + 60}{5} = \\frac{251}{5} = 50.2\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} = \\frac{60 + 56 + 70 + 50 + 75}{5} = \\frac{311}{5} = 62.2\n\n# Verification\nmean(income)  # 50.2\n\n[1] 50.2\n\nmean(turnout) # 62.2\n\n[1] 62.2\n\n\nVariances:\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X: (-0.2, -5.2, 5.8, -10.2, 9.8)\ns^2_X = \\frac{0.04 + 27.04 + 33.64 + 104.04 + 96.04}{4} = \\frac{260.8}{4} = 65.2\nDeviations for Y: (-2.2, -6.2, 7.8, -12.2, 12.8)\ns^2_Y = \\frac{4.84 + 38.44 + 60.84 + 148.84 + 163.84}{4} = \\frac{416.8}{4} = 104.2\n\n# Verification\nvar(income)  # 65.2\n\n[1] 65.2\n\nvar(turnout) # 104.2\n\n[1] 104.2\n\n\n\n\n16.16.3 Covariance and Correlation\nCovariance:\ns_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-0.2 \\times -2.2) = 0.44 (-5.2 \\times -6.2) = 32.24 (5.8 \\times 7.8) = 45.24 (-10.2 \\times -12.2) = 124.44 (9.8 \\times 12.8) = 125.44\ns_{XY} = \\frac{327.8}{4} = 81.95\n\n# Verification\ncov(income, turnout) # 81.95\n\n[1] 81.95\n\n\nCorrelation:\nr_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{81.95}{\\sqrt{65.2}\\sqrt{104.2}} = 0.994\n\n# Verification\ncor(income, turnout) # 0.994\n\n[1] 0.9942402\n\n\n\n\n16.16.4 OLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{81.95}{65.2} = 1.2571429\nIntercept:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\nStep by step:\n\n1.2571429 \\times 50.2 = 63.1085714\n\\hat{\\beta_0} = 62.2 - 63.1085714 = -0.9085714\n\n\n# Verification\ncoef(model)  # Exact coefficients from R\n\n(Intercept)      income \n -0.8964724   1.2569018 \n\n\n\n\n16.16.5 Detailed Decomposition of Variance and R-squared\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = -0.9085714 + 1.2571429X\nThe predicted values \\hat{Y} for each X value:\nFor X = 50:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (50)\n \\hat{Y} = -0.9085714 + 62.857145 \\hat{Y} = 61.9485736\nFor X = 45:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (45)\n \\hat{Y} = -0.9085714 + 56.5714305 \\hat{Y} = 55.6535591\nFor X = 56:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (56)\n \\hat{Y} = -0.9085714 + 70.4200024 \\hat{Y} = 69.5114310\nFor X = 40:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (40)\n \\hat{Y} = -0.9085714 + 50.2657160 \\hat{Y} = 49.3571446\nFor X = 60:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (60)\n \\hat{Y} = -0.9085714 + 75.4285740 \\hat{Y} = 74.5200026\n\n# Verification of predicted values\ny_hat &lt;- -0.9085714 + 1.2571429 * income\ndata.frame(\n  X = income,\n  Y = turnout,\n  Y_hat = y_hat,\n  row.names = 1:5\n)\n\n   X  Y    Y_hat\n1 50 60 61.94857\n2 45 56 55.66286\n3 56 70 69.49143\n4 40 50 49.37714\n5 60 75 74.52000\n\n\nStep 2: Calculate SST (Total Sum of Squares)\nSST = \\sum(Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = 62.2\n(60 - 62.2)^2 = (-2.2)^2 = 4.84 (56 - 62.2)^2 = (-6.2)^2 = 38.44 (70 - 62.2)^2 = (7.8)^2 = 60.84 (50 - 62.2)^2 = (-12.2)^2 = 148.84 (75 - 62.2)^2 = (12.8)^2 = 163.84\nSST = 4.84 + 38.44 + 60.84 + 148.84 + 163.84 = 416.8\nStep 3: Calculate SSR (Regression Sum of Squares)\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\n(61.9485736 - 62.2)^2 = (-0.2514264)^2 = 0.0632151 (55.6535591 - 62.2)^2 = (-6.5464409)^2 = 42.8558689 (69.5114310 - 62.2)^2 = (7.3114310)^2 = 53.4570178 (49.3571446 - 62.2)^2 = (-12.8428554)^2 = 164.9389370 (74.5200026 - 62.2)^2 = (12.3200026)^2 = 151.7824640\nSSR = 413.0975028\nStep 4: Calculate SSE (Error Sum of Squares)\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n(60 - 61.9485736)^2 = (-1.9485736)^2 = 3.7969384 (56 - 55.6535591)^2 = (0.3464409)^2 = 0.1200212 (70 - 69.5114310)^2 = (0.4885690)^2 = 0.2387198 (50 - 49.3571446)^2 = (0.6428554)^2 = 0.4132631 (75 - 74.5200026)^2 = (0.4799974)^2 = 0.2303975\nSSE = 4.7024972\nStep 5: Verify decomposition\nSST = SSR + SSE 416.8 = 413.0975028 + 4.7024972\nStep 6: Calculate R-squared\nR^2 = \\frac{SSR}{SST} = \\frac{413.0975028}{416.8} = 0.9916\n\n# Verification\nsummary(model)$r.squared  # Should match our calculation\n\n[1] 0.9885135\n\n\n\n\n16.16.6 Visualization\n\nlibrary(ggplot2)\ndf &lt;- data.frame(income = income, turnout = turnout)\n\nggplot(df, aes(x = income, y = turnout)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Voter Turnout vs Income per Capita\",\n    x = \"Income per Capita (thousands €)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\n16.16.7 Interpretation\nThe analysis shows:\n\nA very strong positive correlation (r = 0.994) between income and voter turnout\nThe regression equation \\hat{Y} = -0.9085714 + 1.2571429X indicates that:\n\nFor each €1,000 increase in income, turnout increases by about 1.26 percentage points\nThe intercept (-0.9086) has little practical meaning as income is never zero\n\nThe R-squared of 0.9916 indicates that 99.16% of the variance in turnout is explained by income",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "href": "correg_pl.html#anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.17 Anxiety Levels and Cognitive Performance: A Laboratory Study",
    "text": "16.17 Anxiety Levels and Cognitive Performance: A Laboratory Study\n\n16.17.1 Data and Context\nIn a psychology experiment, researchers measured the relationship between anxiety levels (measured by galvanic skin response, GSR) and cognitive performance (score on a working memory task).\n\n# Data\nanxiety &lt;- c(2.1, 3.4, 4.2, 5.1, 5.8, 6.4, 7.2, 8.0)  # GSR readings\nperformance &lt;- c(92, 88, 84, 78, 74, 70, 65, 62)      # Working memory scores\n\n# Initial model check\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8993 -0.6660  0.2162  0.6106  1.5262 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 105.3248     1.3189   79.86 2.60e-10 ***\nanxiety      -5.4407     0.2359  -23.06 4.35e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 6 degrees of freedom\nMultiple R-squared:  0.9888,    Adjusted R-squared:  0.987 \nF-statistic: 531.9 on 1 and 6 DF,  p-value: 4.355e-07\n\n\n\n\n16.17.2 Descriptive Statistics\nMeans: \\bar{X} = \\frac{2.1 + 3.4 + 4.2 + 5.1 + 5.8 + 6.4 + 7.2 + 8.0}{8} = \\frac{42.2}{8} = 5.275\n\\bar{Y} = \\frac{92 + 88 + 84 + 78 + 74 + 70 + 65 + 62}{8} = \\frac{613}{8} = 76.625\n\n# Verification\nmean(anxiety)\n\n[1] 5.275\n\nmean(performance)\n\n[1] 76.625\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(2.1 - 5.275) = -3.175\n(3.4 - 5.275) = -1.875\n(4.2 - 5.275) = -1.075\n(5.1 - 5.275) = -0.175\n(5.8 - 5.275) = 0.525\n(6.4 - 5.275) = 1.125\n(7.2 - 5.275) = 1.925\n(8.0 - 5.275) = 2.725\n\nSquared deviations:\n10.08063 + 3.51563 + 1.15563 + 0.03063 + 0.27563 + 1.26563 + 3.70563 +\n7.42563 = 27.45500\ns^2_X = \\frac{27.45500}{7} = 3.922143\nSimilarly for Y: Deviations:\n15.375, 11.375, 7.375, 1.375, -2.625, -6.625, -11.625, -14.625\ns^2_Y = \\frac{236.875 + 129.391 + 54.391 + 1.891 + 6.891 + 43.891 + 135.141 + 213.891}{7} = \\frac{822.362}{7} = 117.4803\n\n# Verification\nvar(anxiety)\n\n[1] 3.922143\n\nvar(performance)\n\n[1] 117.4107\n\n\n\n\n16.17.3 Covariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-3.175 × 15.375) = -48.815625\n(-1.875 × 11.375) = -21.328125\n(-1.075 × 7.375) = -7.928125\n(-0.175 × 1.375) = -0.240625\n(0.525 × -2.625) = -1.378125\n(1.125 × -6.625) = -7.453125\n(1.925 × -11.625) = -22.378125\n(2.725 × -14.625) = -39.853125\nSum = -149.375\ns_{XY} = \\frac{-149.375}{7} = -21.33929\n\n# Verification\ncov(anxiety, performance)\n\n[1] -21.33929\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-21.33929}{\\sqrt{3.922143}\\sqrt{117.4803}} = -0.9932\n\n# Verification\ncor(anxiety, performance)\n\n[1] -0.9944073\n\n\n\n\n16.17.4 OLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-21.33929}{3.922143} = -5.4407\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-5.4407 × 5.275 = -28.6997\n\\hat{\\beta_0} = 76.625 - (-28.6997) = 105.3247\n\n\n# Verification\ncoef(model)\n\n(Intercept)     anxiety \n 105.324804   -5.440721 \n\n\n\n\n16.17.5 4. R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}): \\hat{Y} = 105.3247 - 5.4407X\n\n# Predicted values\ny_hat &lt;- 105.3247 - 5.4407 * anxiety\ndata.frame(\n  Anxiety = anxiety,\n  Performance = performance,\n  Predicted = y_hat,\n  row.names = 1:8\n)\n\n  Anxiety Performance Predicted\n1     2.1          92  93.89923\n2     3.4          88  86.82632\n3     4.2          84  82.47376\n4     5.1          78  77.57713\n5     5.8          74  73.76864\n6     6.4          70  70.50422\n7     7.2          65  66.15166\n8     8.0          62  61.79910\n\n\nStep 2: Sum of Squares\nSST = \\sum(Y_i - \\bar{Y})^2 = 822.362\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 816.3094\nSSE = \\sum(Y_i - \\hat{Y}_i)^2 = 6.0526\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{816.3094}{822.362} = 0.9926\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.9888459\n\n\n\n\n16.17.6 Visualization\n\nlibrary(ggplot2)\n\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Cognitive Performance vs. Anxiety Levels\",\n    x = \"Anxiety (GSR)\",\n    y = \"Performance Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n16.17.7 Interpretation\n\nStrong negative correlation (r = -0.993) between anxiety and cognitive performance\nFor each unit increase in GSR (anxiety), performance decreases by approximately 5.44 points\nThe model explains 99.26% of the variance in performance scores\nThe relationship appears to be strongly linear, suggesting a reliable anxiety-performance relationship\nThe high intercept (105.32) represents the theoretical maximum performance at zero anxiety\n\n\n\n16.17.8 Study Limitations\n\nSmall sample size (n=8)\nPossible other confounding variables\nLimited range of anxiety levels\nCross-sectional rather than longitudinal data",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "href": "correg_pl.html#district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.18 District Magnitude and Electoral Disproportionality: A Comparative Analysis",
    "text": "16.18 District Magnitude and Electoral Disproportionality: A Comparative Analysis\n\n16.18.1 Data Generating Process\nLet’s set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\n16.18.2 Descriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\n16.18.3 Covariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 × 3.2833) = -18.6057\n(-3.6667 × 2.0833) = -7.6387\n(-1.6667 × 3.4833) = -5.8056\n(1.3333 × -1.6167) = -2.1556\n(3.3333 × -3.2167) = -10.7223\n(6.3333 × -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\n16.18.4 OLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 × 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\n16.18.5 R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\n16.18.6 Visualization - True vs. Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + ε\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + ε\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\n16.18.7 Observations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (ε)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\n16.18.8 Interpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\n16.18.9 Study Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\n16.18.10 Limitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "probability_en.html",
    "href": "probability_en.html",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "",
    "text": "17.1 Probability: Preliminary Concepts\nImagine you’re trying to decide whether to bring an umbrella to class tomorrow. You check the weather forecast, which says there’s a 30% chance of rain. But what does this number really mean?\nThis is where probability comes in - it’s a mathematical way to measure how likely something is to happen.\nA probability represents the likelihood or chance of an event occurring, expressed as a number between 0 and 1 (or as a percentage between 0% and 100%).\nBefore we dive into probability theory, let’s establish some foundational concepts that we’ll use throughout this course.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#probability-preliminary-concepts",
    "href": "probability_en.html#probability-preliminary-concepts",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "",
    "text": "17.1.1 Basic Set Concepts\nBefore we can understand probability, we need to grasp some fundamental concepts from set theory. A set is simply a collection of distinct objects.\nA set can be defined by:\n\nListing all elements: A = \\{1, 2, 3\\}\nDescribing a property: B = \\{\\text{x | x is a positive integer less than 4}\\}\n\nThe empty set \\emptyset contains no elements.\nIf A and B are sets:\n\nIf A is a subset of B, we write A \\subseteq B\nIf x is an element of A, we write x \\in A\n\nFor example, if B = \\{1, 2, 3\\}:\n\n\\{1, 2\\} is a subset of B (written \\{1, 2\\} \\subseteq B)\n1 is an element of B (written 1 \\in B)\n\n\n\n\n\n\n\nNote\n\n\n\nThe proper subset notation uses a strict subset symbol. If A is a proper subset of B, we write:\nA \\subset B\nThis means that A is a subset of B AND A \\neq B (A is not equal to B).\nIn contrast, A \\subseteq B allows for the possibility that A = B.\nA set is a fundamental mathematical concept - it’s a collection of distinct objects where order doesn’t matter and duplicates are not allowed. In other words, each element either belongs to the set or it doesn’t, with no concept of “how many times” it belongs.\nFormally, if x \\in A (meaning x is an element of set A), then adding another copy of x has no effect on A. This gives us identities like:\n\\{1, 2, 2, 3\\} = \\{1, 2, 3\\} = \\{3, 1, 2\\}\nThis distinguishes sets from other mathematical collections:\n\nLists/Sequences: Order matters and duplicates are allowed\n\n[1, 2, 2, 3] ≠ [1, 2, 3]\n[1, 2, 3] ≠ [3, 2, 1]\n\nMultisets: Order doesn’t matter but duplicates are allowed\n\n{1, 2, 2, 3}ₘ ≠ {1, 2, 3}ₘ\n{1, 2, 2, 3}ₘ = {3, 2, 1, 2}ₘ\n\n\nThis unique property of sets - that membership is binary (an element either belongs or doesn’t) - makes them particularly useful in mathematics for describing collections where we only care about whether something is present, not how many times it appears or in what order.\n\n\n\n\n17.1.2 Set Operations\nBasic set (events) operations (given two sets A and B):\n\nUnion (A \\cup B): Elements in either A OR B (or both)\nIntersection (A \\cap B): Elements in BOTH A AND B\nComplement (A^c or A^{'}): Elements NOT in A\nDifference (A \\setminus B): Elements in A but NOT in B\n\nThese operations follow important laws like:\n(A \\cup B)^c = A^c \\cap B^c (DeMorgan’s Law)\n(A \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C) (Distributive Law)\nSets and the associated operations are easy to visualize in terms of Venn diagrams, as illustrated in the figure below:\n\n\n\nhttp://athenasc.com/probbook.html\n\n\nExamples of Venn diagrams:\n\nThe shaded region is S \\cap T.\nThe shaded region is S \\cup T.\nThe shaded region is S \\cap T^c.\nHere, T \\subset S. The shaded region is the complement of S.\nThe sets S, T, and U are disjoint.\nThe sets S, T, and U form a partition of the universal set \\Omega.\n\nThe Universal Set (often denoted as \\Omega, U, or S):\n\nIn Set Theory:\n\n\nSet containing all elements in a given context\nAll other sets are its subsets\nComplement of set A is A' = \\Omega - A\n\n\nIn Probability:\n\n\nCalled the sample space S or \\Omega\nContains all possible outcomes\nHas probability P(\\Omega) = 1\n\nKey Properties:\n\nA \\subseteq \\Omega\nA \\cup A' = \\Omega\nA \\cap A' = \\emptyset\n\\Omega' = \\emptyset\n\\emptyset' = \\Omega\n\nExamples:\n\nDie roll: \\Omega = \\{1,2,3,4,5,6\\}\nCoin flip: \\Omega = \\{H,T\\}\n\n\n\n\n\n\n\nSet Theory as a Language for Probability\n\n\n\nSet theory provides the mathematical framework for probability theory. Here are the key parallels:\n\n\n\n\n\n\n\n\nSet Theory\nProbability Theory\nDescription\n\n\n\n\n\\Omega (Universal set)\nSample space (S)\nAll possible outcomes\n\n\nx \\in A (Element)\nOutcome\nSingle result\n\n\nA \\subseteq \\Omega (Subset)\nEvent\nCollection of outcomes\n\n\n\\emptyset (Empty set)\nImpossible event\nCannot occur (P(\\emptyset) = 0)\n\n\n\\Omega (Universal set)\nCertain event\nMust occur (P(\\Omega) = 1)\n\n\nA \\cup B (Union)\nEither A OR B\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\n\nA \\cap B (Intersection)\nBoth A AND B\nP(A \\cap B) = P(A)P(B) (if independent)\n\n\nA' (Complement)\nNot A\nP(A') = 1 - P(A)\n\n\nA \\cap B = \\emptyset\nMutually exclusive\nP(A \\cap B) = 0\n\n\n\n\n\n\n\n\n\n\n\nCardinality of Sets\n\n\n\nIn set theory, we denote cardinality (the number of elements in a set) using vertical bars: |A|\nKey points:\n\n|A| means “number of elements in set A”\nFor a finite set like A = \\{1, 2, 3\\}, we have |A| = 3\nEmpty set has cardinality zero: |\\emptyset| = 0\nFor two sets A and B:\n\nUnion (no overlap): |A \\cup B| = |A| + |B|\nUnion (with overlap): |A \\cup B| = |A| + |B| - |A \\cap B|\nCartesian product: |A \\times B| = |A| \\times |B|\n\n\nExample:\nIf A = \\{\\spadesuit, \\clubsuit, \\heartsuit, \\diamondsuit\\} and B = \\{K, Q, J\\}, then:\n\n|A| = 4\n|B| = 3\n|A \\times B| = 12 (all possible combinations)\n\nThe cardinality of a set is denoted by |A| or #A. Here are the calculations:\n\n|\\{apple, orange, watermelon\\}| = 3 (Each element is distinct)\n|\\{1, 1, 1, 1, 1\\}| = 1 (In a set, duplicates are counted only once)\n|[0, 1]| = \\aleph_1 (This is an uncountably infinite interval of real numbers)\n|\\{1, 2, 3, \\cdots\\}| = \\aleph_0 (This is countably infinite)\n|\\{\\emptyset, \\{1\\}, \\{2\\}, \\{1, 2\\}\\}| = 4 (Each element is a distinct set)\n|\\{\\emptyset, \\{1\\}, \\{1, 1\\}, \\{1, 1, 1\\}, \\cdots\\}| = 2 (After removing duplicates: \\{\\emptyset, \\{1\\}\\} since \\{1\\} = \\{1, 1\\} = \\{1, 1, 1\\} = \\cdots)\n\n\n\n\n\n17.1.3 Understanding Set Relations: Elements vs Subsets (*)\nThe difference between an element belonging to a set and one set being a subset of another.\n\n17.1.3.1 The “Belongs To” Relationship (\\in)\nWhen we say an element belongs to a set (written as x \\in A), we’re describing membership of a single item in a collection. Think of a classroom: each individual student belongs to (is a member of) the class. They are elements of the set “class.”\nConsider a deck of cards and let H be the set of all hearts:\nH = \\{2♥, 3♥, 4♥, 5♥, 6♥, 7♥, 8♥, 9♥, 10♥, J♥, Q♥, K♥, A♥\\}\nWe can say:\n\nA♥ \\in H (true, because the ace of hearts is one of the hearts)\nK♠ \\notin H (false, because the king of spades is not a heart)\n\\{A♥\\} \\notin H (false, this is a set containing the ace of hearts, not the card itself)\n\n\n\n17.1.3.2 The “Is Contained In” Relationship (\\subseteq)\nA subset relationship (written as A \\subseteq B) describes when one set is entirely contained within another set. Every element of the smaller set must appear in the larger set. This is different from set membership (\\in), which describes when a single element belongs to a set.\nTo understand the distinction, let’s look at some examples:\nConsider the following sets:\n\nA = \\{1, 2\\}\nB = \\{1, 2, 3, 4\\}\nC = \\{1\\}\n\nFor set membership (\\in):\n\n1 \\in A (the number 1 is an element of set A)\n\\{1\\} \\notin A (the set containing 1 is not an element of A)\n2 \\in B (the number 2 is an element of B)\n\nFor subset relationships (\\subseteq):\n\nA \\subseteq B (all elements of A are in B)\nC \\subseteq A (all elements of C are in A)\n\\{1\\} \\subseteq A (the set containing 1 is a subset of A)\n\nA key insight is that while 1 \\in A is true (1 is an element of A), \\{1\\} \\in A is false (the set containing 1 is not an element of A). However, \\{1\\} \\subseteq A is true (the set containing 1 is a subset of A).\nThink of it this way: membership (\\in) asks “Is this single thing in the set?” while subset (\\subseteq) asks “Is every element of this smaller set found in the larger set?”\nAnother helpful example is with the empty set \\emptyset:\n\n\\emptyset \\subseteq A for any set A (the empty set is a subset of every set)\nBut \\emptyset \\notin A unless A specifically contains the empty set as an element\n\nExercise (https://www.alextsun.com/files/Prob_Stat_for_CS_Book.pdf). Using the given sets:\n\nA = \\{1, 3\\}\nB = \\{3, 1\\}\nC = \\{1, 2\\}\nD = \\{\\emptyset, \\{1\\}, \\{2\\}, \\{1, 2\\}, 1, 2\\}\n\nDetermine whether the following are true or false:\n\n1 \\in A : TRUE (1 is an element of A)\n1 \\subseteq A : FALSE (1 is not a set, so subset relation doesn’t apply)\n\\{1\\} \\subseteq A : TRUE (every element of the set {1} is an element of A)\n\\{1\\} \\in A : FALSE (A doesn’t contain any sets as elements)\n3 \\notin C : TRUE (3 is not an element of C)\nA \\in B : FALSE (B doesn’t contain any sets as elements)\nA \\subseteq B : TRUE (A and B contain the same elements)\nC \\in D : TRUE (the set {1,2} appears in D, but not the set C itself)\nC \\subseteq D : TRUE (all elements of C (1 and 2) are also elements of D)\n\\emptyset \\in D : TRUE (empty set is listed as an element of D)\n\\emptyset \\subseteq D : TRUE (True, by definition, the empty set is a subset of any set. This is because if this were not the case, there would have to be an element of \\emptyset which was not in D. But there are no elements in \\emptyset, so the statement is true.)\nA = B : TRUE (they contain the same elements)\n\\emptyset \\subseteq \\emptyset : TRUE (empty set is a subset of itself; the empty set is a subset of any set)\n\\emptyset \\in \\emptyset : FALSE (empty set contains no elements)",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#set-theory-and-power-sets-event-space",
    "href": "probability_en.html#set-theory-and-power-sets-event-space",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "17.2 Set Theory and Power Sets (Event Space)",
    "text": "17.2 Set Theory and Power Sets (Event Space)\nThe power set of a set, denoted as \\mathcal{F}(S) or 2^{|S|}, is the set of all possible subsets of S, including the empty set and S itself.\nThis concept is crucial in probability theory because it helps us understand the relationship between the sample space S (all possible outcomes) and the event space (all possible events (i.e. all possible subsets of S) we might want to consider).\nLet’s explore this with a simple example. Consider flipping a single coin where: S = \\{H, T\\} (our sample space)\nThe power set would be:\n\\mathcal{F}(S) = \\{\\emptyset, \\{H\\}, \\{T\\}, \\{H,T\\}\\}\nEach element in the power set represents a possible event. For instance:\n\n\\emptyset: The impossible event (e.g., the coin landing neither heads nor tails)\n\\{H\\}: The event of getting heads\n\\{T\\}: The event of getting tails\n\\{H,T\\}: The certain event (the coin must land either heads or tails)\n\nFor a set with n elements, its power set will have 2^n elements. This is because for each element, we have two choices: include it or not include it in a subset.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#counting-rules-in-probability-the-power-of-and-or",
    "href": "probability_en.html#counting-rules-in-probability-the-power-of-and-or",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "17.3 Counting Rules in Probability: The Power of AND & OR",
    "text": "17.3 Counting Rules in Probability: The Power of AND & OR\nA fundamental challenge in probability is counting possible outcomes. Two key rules help us solve these problems:\n\n17.3.1 The Multiplication Rule for Independent Events (“AND” Situations)\nWhen we need a sequence of independent choices where we must make ALL choices, we multiply the number of possibilities for each choice. This principle applies when we need option A AND option B AND option C, etc.\nFor example, consider creating a password with exactly three characters in this order:\n\nFirst character must be a letter (26 choices)\nSecond character must be a digit (10 choices)\nThird character must be a symbol (@, #, $, or % - so 4 choices)\n\nTotal possible passwords = 26 × 10 × 4 = 1,040\nThis is like filling three slots where each slot has its own set of valid options. Each new requirement multiplies our total possibilities.\n\n\n17.3.2 The Addition Rule for Mutually Exclusive Events (“OR” Situations)\nWhen there are multiple valid ways to achieve a goal, and we can use ANY ONE of these ways, we add the number of possibilities. This applies when we accept option A OR option B OR option C, etc.\nFor example, if a password must be EITHER:\n\nA 3-letter word (26³ possibilities) OR\nA 4-digit number (10⁴ possibilities)\n\nTotal possibilities = 26³ + 10⁴ = 17,576 + 10,000 = 27,576\nThink of this as having separate paths to success - we count how many ways each path offers and sum them up.\n\n\n17.3.3 Combining the Rules\nMany real problems require both multiplication and addition.\nExample 1. For instance, if a password must be EITHER:\n\nA letter followed by two digits (26 × 10 × 10 possibilities) OR\nThree symbols (4 × 4 × 4 possibilities)\n\nTotal = (26 × 10 × 10) + (4 × 4 × 4) = 2,600 + 64 = 2,664\nUnderstanding when to multiply (AND situations) versus when to add (OR situations) is key to solving counting problems correctly.\nExample 2. Calvin wants to reach Milwaukee and has these options:\n\nFirst leg (home → Chicago): 3 bus services OR 2 train services\nSecond leg (Chicago → Milwaukee): 2 bus services OR 3 train services\n\nTo solve this:\n\nFirst leg options = 3 + 2 = 5 ways\nSecond leg options = 2 + 3 = 5 ways\nTotal routes = 5 × 5 = 25 possibilities\n\nWhy multiply at the end? Because for EACH way of reaching Chicago, Calvin can use ANY of the ways to reach Milwaukee. This creates 25 unique combinations like:\n\nBus 1 → Bus 1\nBus 1 → Train 1\nBus 2 → Bus 2 …and so on.\n\nThe key is recognizing whether you’re dealing with sequential choices (multiply) or alternative options (add) at each step. Master this distinction, and you’ll solve complex counting problems with ease.\n\n\n\n\n\nflowchart TD\n    Start[Problem: Count Possible Outcomes] --&gt; Q1{\"Are we counting outcomes\\nthat happen in sequence?\"}\n    \n    Q1 --&gt;|Yes| M1[Multiplication Rule:\\nMultiply choices for each step]\n    Q1 --&gt;|No| Q2{\"Are we counting different\\nways to achieve same result?\"}\n    \n    M1 --&gt; ME1[Examples of Sequential Choices]\n    ME1 --&gt; MC1[\"Password: letter then number\\n26 letters × 10 numbers\\n= 260 possibilities\"]\n    MC1 --&gt; MC2[\"Travel: bus then train\\n3 bus routes × 2 train routes\\n= 6 possible journeys\"]\n    \n    Q2 --&gt;|Yes| Q3{\"Do options overlap?\"}\n    \n    Q3 --&gt;|No| A1[Simple Addition Rule:\\nAdd all possibilities]\n    Q3 --&gt;|Yes| A2[\"Extended Addition Rule:\\nAdd - Overlap\"]\n    \n    A1 --&gt; AE1[Examples of Non-Overlapping Options]\n    AE1 --&gt; AC1[\"Coin toss: H or T\\n1 + 1 = 2 outcomes\"]\n    AC1 --&gt; AC2[\"License type: Car or Motorcycle\\n100 + 50 = 150 types\"]\n    \n    A2 --&gt; AE2[Examples of Overlapping Options]\n    AE2 --&gt; AC3[\"Students in Sports or Music:\\n45 + 35 - 15 in both\\n= 65 students\"]\n    \n    classDef start fill:#2d5a8c,stroke:#333,color:#fff,stroke-width:2px\n    classDef question fill:#d4426e,stroke:#333,color:#fff,stroke-width:2px\n    classDef rule fill:#156b45,stroke:#333,color:#fff,stroke-width:2px\n    classDef example fill:#4a4a4a,stroke:#333,color:#fff\n    \n    class Start start\n    class Q1,Q2,Q3 question\n    class M1,A1,A2 rule\n    class ME1,AE1,AE2,MC1,MC2,AC1,AC2,AC3 example\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n    subgraph Addition_Rules[Addition Rule Examples]\n        direction TB\n        subgraph Exclusive[Mutually Exclusive Example]\n            direction TB\n            A1[\"Coin Flip\"] --&gt; AH((Heads))\n            A1 --&gt; AT((Tails))\n            AT --&gt; AR[\"Total = 1 + 1 = 2\\nNo overlap possible\"]\n            AH --&gt; AR\n        end\n        \n        subgraph Overlapping[Overlapping Sets Example]\n            direction TB\n            O1[\"Students in\\nClubs\"] --&gt; OS[\"Science Club\\n25 students\"] & OA[\"Art Club\\n20 students\"]\n            OS & OA --&gt; OI[\"Both Clubs\\n8 students\"]\n            OI --&gt; OT[\"Total = 25 + 20 - 8\\n= 37 students\"]\n        end\n    end\n    \n    classDef default fill:#f5f5f5,stroke:#333,color:#000\n    classDef set fill:#e6e6e6,stroke:#333,color:#000\n    classDef result fill:#d9d9d9,stroke:#333,color:#000\n    classDef option fill:#ffffff,stroke:#333,color:#000\n    classDef example fill:#f0f0f0,stroke:#333,color:#000\n    \n    class A1,O1 set\n    class AR,OT result\n    class AH,AT option\n    class Exclusive,Overlapping example\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n    subgraph Multiplication_Rule[Multiplication Rule Examples]\n        direction TB\n        M1[\"Choose Breakfast\"] --&gt; MA[\"Drink\\n(3 options)\"] & MB[\"Food\\n(2 options)\"]\n        MA --&gt; MA1((Coffee)) & MA2((Tea)) & MA3((Juice))\n        MB --&gt; MB1((Toast)) & MB2((Cereal))\n        MA1 & MA2 & MA3 --&gt; MR[\"Total Combinations:\\n3 drinks × 2 foods\\n= 6 possible breakfasts\"]\n        MB1 & MB2 --&gt; MR\n        \n        subgraph Tree_Example[Tree Diagram]\n            direction TB\n            T1[\"PIN First Digit\\n(0-9)\"] --&gt; T2[\"Second Digit\\n(0-9)\"]\n            T2 --&gt; T3[\"10 × 10 = 100\\ntotal combinations\"]\n        end\n    end\n    \n    classDef default fill:#f5f5f5,stroke:#333,color:#000\n    classDef set fill:#e6e6e6,stroke:#333,color:#000\n    classDef result fill:#d9d9d9,stroke:#333,color:#000\n    classDef option fill:#ffffff,stroke:#333,color:#000\n    classDef example fill:#f0f0f0,stroke:#333,color:#000\n    \n    class MA,MB set\n    class MR,T3 result\n    class MA1,MA2,MA3,MB1,MB2 option\n    class Tree_Example example\n\n\n\n\n\n\n\n\n\n\n\n\nThe Inclusion-Exclusion Principle (*)\n\n\n\nThe Inclusion-Exclusion Principle states that for two sets A and B:\n|A \\cup B| = |A| + |B| - |A \\cap B|\nThis means: The size of their union equals the sum of their individual sizes, minus their intersection (to avoid double counting shared elements).\nFor three sets A, B, and C, the principle extends to:\n|A \\cup B \\cup C| = |A| + |B| + |C| - |A \\cap B| - |B \\cap C| - |A \\cap C| + |A \\cap B \\cap C|\nThis pattern continues for more sets, alternating between adding and subtracting intersections of increasing size.\nA simple example:\n\nSet A: Students who play soccer (20 students)\nSet B: Students who play basketball (15 students)\n8 students play both sports\nTotal students in either sport = 20 + 15 - 8 = 27 students\n\nThe principle is essential in probability theory, combinatorics, and set theory. It helps us correctly count elements when sets overlap, avoiding the common error of double-counting shared elements.\n\n\n\nhttps://en.wikipedia.org/wiki/Inclusion%E2%80%93exclusion_principle",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#probability-theory-basic-concepts-and-rules",
    "href": "probability_en.html#probability-theory-basic-concepts-and-rules",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "17.4 Probability Theory: Basic Concepts and Rules",
    "text": "17.4 Probability Theory: Basic Concepts and Rules\n\n\n\n\n\n\nCore Concepts\n\n\n\nProbability theory provides a rigorous foundation for quantifying uncertainty and analyzing random phenomena.\n\n\n\n17.4.1 Random Experiments\nA random experiment is any procedure that has a well-defined set of possible outcomes but whose specific result cannot be predicted with certainty.\nProperties:\n\nRepeatable under identical conditions\nKnown possible outcomes\nUnpredictable specific results\n\n\n\n17.4.2 Sample Space (S)\n\nComplete set of all possible outcomes of a random experiment\nDenoted by S (or \\Omega)\nProperties:\n\nMutually exclusive outcomes\nCollectively exhaustive\n\n\nExamples:\n\nCoin flip: S = \\{H, T\\}\nDie roll: S = \\{1, 2, 3, 4, 5, 6\\}\n\n\n\n17.4.3 The Event Space: What Can Happen in an Experiment\nEvents are subsets of the sample space S. This means we can use standard set operations to work with them in a precise, mathematical way.\nThe event space \\mathcal{F} is a collection of all events (outcomes or sets of outcomes) that we can assign probabilities to in an experiment. It must follow three fundamental rules:\n\nComplete Space Rule\n\nThe entire sample space S must be in \\mathcal{F}\nThis means all possible outcomes together form a valid event\n\nComplement Rule\n\nIf event A is in \\mathcal{F}, then “not A” (written as A^c) must also be in \\mathcal{F}\nExample: If “getting heads” is an event, “not getting heads” must also be an event\n\nUnion Rule\n\nIf we have any sequence of events A_1, A_2, ... in \\mathcal{F}, their union must also be in \\mathcal{F}\nThis means we can combine valid events to form new valid events\n\n\n\n\n\n\n\n\nDiscrete vs. Continuous Probability: Understanding the Two Types of Random Events\n\n\n\nIn probability, we encounter two fundamentally different types of random events: those we can count (discrete) and those we can measure (continuous). This distinction shapes how we calculate and interpret probabilities.\n\n17.4.3.1 Discrete Probability\nWhat: Events that can be counted with whole numbers - Like counting marbles, rolling dice, or flipping coins - Has “gaps” between possible values\nKey Examples:\n\nRolling a die\n\nPossible outcomes: 1, 2, 3, 4, 5, or 6\nNothing in between (can’t roll a 2.5)\nCan say: P(\\text{rolling a 6}) = \\frac{1}{6}\n\nNumber of customers per hour\n\nCould be 0, 1, 2, 3, …\nCan’t have 2.7 customers\nCan say: P(\\text{exactly 5 customers}) = 0.1\n\n\n\n\n17.4.3.2 Continuous Probability\nWhat: Events measured on a continuous scale - Like measuring height, time, or temperature - Values flow smoothly with no gaps\nKey Examples:\n\nPerson’s height\n\nCould be 170cm, 170.1cm, 170.11cm, …\nCan measure with increasing precision\nMust use ranges: P(170 \\leq \\text{height} \\leq 171)\n\nTime until next bus arrives\n\nCould be 5 mins, 5.1 mins, 5.01 mins, …\nInfinitely divisible\nMust use ranges: P(\\text{waiting time} \\leq 10 \\text{ mins})\n\n\n\n\n17.4.3.3 Critical Differences\n\nIndividual Values\n\nDiscrete: Can have positive probability\n\nP(\\text{rolling a 6}) = \\frac{1}{6} &gt; 0\n\nContinuous: Always have zero probability\n\nP(\\text{height} = 170.000...) = 0\n\n\nHow We Calculate\n\nDiscrete: Can sum individual probabilities\nContinuous: Must use ranges and integrals\n\n\n\n\n17.4.3.4 Real-World Application\nThink about a pizza delivery:\n\nDiscrete: Number of toppings (1, 2, 3, …)\nContinuous: Delivery time (15.7 minutes, 15.73 minutes, …)\n\n\n\n17.4.3.5 Why Understanding This Matters\n\nHelps choose appropriate probability tools\nGuides how we collect and analyze data\nDetermines how we express uncertainty\nShapes how we make predictions\n\nThis foundation helps us tackle real-world probability problems with the right approach!\n\n\n\nLet’s examine a simple coin flip:\n\nSample space: S = \\{H, T\\} (Heads or Tails)\nThe complete event space: \\mathcal{F} = \\{\\emptyset, \\{H\\}, \\{T\\}, \\{H,T\\}\\}\n\n\\emptyset : impossible event (no outcomes)\n\\{H\\} : getting Heads\n\\{T\\} : getting Tails\n\\{H,T\\} : getting either Heads or Tails\n\n\nVerifying the rules:\n\nRule 1: \\{H,T\\} (the sample space) is included\nRule 2: For event \\{H\\}, its complement \\{T\\} is included\nRule 3: The union of any events (like \\{H\\} \\cup \\{T\\} = \\{H,T\\}) is included\n\n\n\n\n\n\n\nUnderstanding Outcomes vs Events\n\n\n\nThere’s an important distinction between outcomes (also called simple events) and events:\n\nAn outcome or simple event is a single, indivisible result of an experiment. For example, getting heads on a single coin flip is an outcome.\nAn event is a set of outcomes - it can contain one outcome, multiple outcomes, or even no outcomes (the empty set). For example, “getting at least one head when flipping two coins” is an event containing multiple outcomes.\n\nLet’s illustrate this with two coin flips where:\nS = \\{HH, HT, TH, TT\\} (our sample space)\nThe power set (all possible events) would contain 2^4 = 16 events:\n\n\\emptyset (impossible event)\nSingle outcomes: \\{HH\\}, \\{HT\\}, \\{TH\\}, \\{TT\\}\nPairs of outcomes: \\{HH,HT\\}, \\{HH,TH\\}, \\{HH,TT\\}, \\{HT,TH\\}, \\{HT,TT\\}, \\{TH,TT\\}\nTriples: \\{HH,HT,TH\\}, \\{HH,HT,TT\\}, \\{HH,TH,TT\\}, \\{HT,TH,TT\\}\nComplete sample space: \\{HH,HT,TH,TT\\}\n\n\n\n\n\n\n\n\n\nEvent Types\n\n\n\n\nSimple Events: Single outcomes\nCompound Events: Multiple outcomes\nSure (or Certain) Event: Sample space S\nImpossible Event: Empty set \\emptyset\n\n\n\n\n\n17.4.4 Probability Measure and Probability Axioms: Assigning Numbers to Events\nA probability measure P is a way to quantify how likely events are to occur. It takes any event from our event space \\mathcal{F} and assigns it a number between 0 and 1.\nThis assignment follows three essential rules/axioms. These axioms, introduced by Andrey Kolmogorov in 1933, serve as the foundation for all probability calculations:\n\nNon-Negativity Rule\n\nFor any event A, its probability must be at least 0: P(A) \\geq 0\nWe can never have a negative probability\nExample: If we roll a die, the probability of getting a 6 is \\frac{1}{6} (it cannot be negative)\n\nTotal Probability Rule\n\nThe probability of all possible outcomes must equal 1: P(S) = 1\nSomething must happen - the probabilities of all possibilities add up to 100%\nExample: For a fair coin, P(\\text{heads}) + P(\\text{tails}) = 0.5 + 0.5 = 1\n\nAddition Rule for Non-Overlapping Events\n\nIf events cannot happen together (they’re “disjoint”), the probability of their union equals the sum of their individual probabilities\nWritten formally: P(A_1 \\cup A_2 \\cup ...) = P(A_1) + P(A_2) + ...\nExample: In drawing a card, P(\\text{getting ace}) = P(\\text{ace of hearts}) + P(\\text{ace of diamonds}) + P(\\text{ace of clubs}) + P(\\text{ace of spades})\n\n\nThese rules/axioms ensure that our probability assignments make logical sense and match our intuitive understanding of chance and likelihood.\n\n\n\n\n\n\nImportant Terms in Probability Theory\n\n\n\nLet’s clarify some closely related but distinct concepts:\n\nProbabilistic Model consists of two fundamental elements:\n\nA sample space \\Omega (or S): the set of all possible outcomes\nA probability law that assigns probabilities to events (subsets of \\Omega)\n\nProbability Measure (P):\n\nThe formal mathematical function that maps events to numbers in [0,1]\nMust satisfy the three axioms (non-negativity, normalization, additivity)\nExample: P(A) gives the probability of event A occurring\n\nProbability Distribution:\n\nThe specific assignment of probabilities to all possible outcomes\nDescribes how probability is distributed across the sample space\nExample: For a fair die, {1: 1/6, 2: 1/6, 3: 1/6, 4: 1/6, 5: 1/6, 6: 1/6}\n\nProbability Law:\n\nOften used as a synonym for probability distribution\nCan also refer to the underlying rule generating the probabilities\nExample: “Each face of a fair die has equal probability”\n\n\nIn practice, these terms are interrelated: The probability measure implements the probability law, which determines the probability distribution, all within the context of a probabilistic model.\n\n\n\n\n\n\n\n\nVisualizing Sample Spaces\n\n\n\nIn probability theory and statistics, being able to visualize sample spaces is crucial for understanding possible outcomes and their relationships. We’ll explore three main approaches to visualizing sample spaces:\n\nVenn Diagrams\nTree Diagrams\nGrid/Matrix Diagrams\n\n\n17.4.4.1 Venn Diagrams\nVenn diagrams provide a powerful visual tool for understanding sample spaces.\n\nA Venn diagram is a graphical representation of sets and their relationships using (overlapping or disjoint) circles or other shapes.\nThink of each shape in a Venn diagram as a container that holds items with specific characteristics. Where these shapes overlap, we find items that share characteristics of multiple groups.\n\nIn probability theory, our sample space (usually denoted by Ω or S) represents all possible outcomes of an experiment. When we draw a Venn diagram, the rectangular frame represents this entire sample space (a universal set), with a probability of 1. Any event is then a subset of this space.\n\n\n\n17.4.4.2 Tree Diagrams\nTree diagrams are particularly useful for visualizing sequential events and their outcomes. Here’s a tree diagram showing a simple probability experiment: We toss a fair coin twice.\n\n\n\n\n\ngraph LR\n    Start[Start] --&gt; H1[H]\n    Start --&gt; T1[T]\n    H1 --&gt; H2[H]\n    H1 --&gt; T2[T]\n    T1 --&gt; H3[H]\n    T1 --&gt; T3[T]\n    \n    H2 --&gt; HH([HH: 1/4])\n    T2 --&gt; HT([HT: 1/4])\n    H3 --&gt; TH([TH: 1/4])\n    T3 --&gt; TT([TT: 1/4])\n    \n    linkStyle 0,2,3 stroke:#1e88e5,stroke-width:2px\n    linkStyle 4,5 stroke:#ff5252,stroke-width:2px\n    linkStyle 1 stroke:#ff5252,stroke-width:2px\n    \n    style Start fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style H1 fill:#bbdefb,stroke:#1e88e5,stroke-width:2px\n    style T1 fill:#ffcdd2,stroke:#ff5252,stroke-width:2px\n    style H2 fill:#bbdefb,stroke:#1e88e5,stroke-width:2px\n    style T2 fill:#ffcdd2,stroke:#ff5252,stroke-width:2px\n    style H3 fill:#bbdefb,stroke:#1e88e5,stroke-width:2px\n    style T3 fill:#ffcdd2,stroke:#ff5252,stroke-width:2px\n    \n    style HH fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style HT fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style TH fill:#f5f5f5,stroke:#333,stroke-width:2px\n    style TT fill:#f5f5f5,stroke:#333,stroke-width:2px\n\n\n\n\n\n\n\n\n17.4.4.3 Grid/Matrix Diagrams\nGrid diagrams are excellent for showing combinations of events.\nScenario: We have 7 balls in the bag:\n\n4 red balls (R₁, R₂, R₃, R₄)\n3 black balls (B₁, B₂, B₃)\nWe’ll draw 2 balls without replacement\n\nLet’s visualize the entire sample space using a grid where each cell represents selecting two balls in order (first draw → columns, second draw ↓ rows):\n\n\n\nFirst Draw →\nR₁\nR₂\nR₃\nR₄\nB₁\nB₂\nB₃\n\n\n\n\nSecond Draw ↓\n\n\n\n\n\n\n\n\n\nR₁\nX\n⚫\n⚫\n⚫\n⚪\n⚪\n⚪\n\n\nR₂\n⚫\nX\n⚫\n⚫\n⚪\n⚪\n⚪\n\n\nR₃\n⚫\n⚫\nX\n⚫\n⚪\n⚪\n⚪\n\n\nR₄\n⚫\n⚫\n⚫\nX\n⚪\n⚪\n⚪\n\n\nB₁\n⚪\n⚪\n⚪\n⚪\nX\n⚫\n⚫\n\n\nB₂\n⚪\n⚪\n⚪\n⚪\n⚫\nX\n⚫\n\n\nB₃\n⚪\n⚪\n⚪\n⚪\n⚫\n⚫\nX\n\n\n\nWhere:\n\nX: Impossible (same ball twice)\n⚫: Both same color (both red in upper-left, both black in lower-right)\n⚪: Different colors (red-black or black-red)\n\nFrom this grid:\n\nBoth red = 12 outcomes (⚫ in upper-left quadrant)\nBoth black = 6 outcomes (⚫ in lower-right quadrant)\nRed then black = 12 outcomes (⚪ in lower-left quadrant)\nBlack then red = 12 outcomes (⚪ in upper-right quadrant)\nTotal possible outcomes = 42 (remove seven diagonal X’s from 7 × 7 grid)\n\nTotal count verification: 12 + 6 + 12 + 12 = 42 outcomes\nNote: Each outcome is determined by reading first draw (column) then second draw (row).\n\n17.4.4.3.1 Grid/Matrix Diagrams with Unordered Pairs\nFor this modified scenario where order doesn’t matter, we need to adjust our counting since (R₁,B₁) and (B₁,R₁) would be considered the same outcome.\nScenario: We have 7 balls in the bag:\n\n4 red balls (R₁, R₂, R₃, R₄)\n3 black balls (B₁, B₂, B₃)\nWe’ll draw 2 balls without replacement\nOrder does NOT matter\n\nBecause order doesn’t matter, we only need to look at half of the grid, excluding the diagonal.\n\n\n\nFirst Draw →\nR₁\nR₂\nR₃\nR₄\nB₁\nB₂\nB₃\n\n\n\n\nSecond Draw ↓\n\n\n\n\n\n\n\n\n\nR₁\nX\n⚫\n⚫\n⚫\n⚪\n⚪\n⚪\n\n\nR₂\n–\nX\n⚫\n⚫\n⚪\n⚪\n⚪\n\n\nR₃\n–\n–\nX\n⚫\n⚪\n⚪\n⚪\n\n\nR₄\n–\n–\n–\nX\n⚪\n⚪\n⚪\n\n\nB₁\n–\n–\n–\n–\nX\n⚫\n⚫\n\n\nB₂\n–\n–\n–\n–\n–\nX\n⚫\n\n\nB₃\n–\n–\n–\n–\n–\n–\nX\n\n\n\nWhere:\n\nX: Impossible (same ball twice)\n⚫: Both same color\n⚪: Different colors\n–: Redundant (already counted in upper half)\n\nFrom this grid:\n\nBoth red = 6 outcomes (⚫ in upper-left quadrant)\nBoth black = 3 outcomes (⚫ in lower-right quadrant)\nOne red and one black = 12 outcomes (⚪ only counted once)\nTotal possible outcomes = 21 (half of the ordered outcomes: 42 ÷ 2)\n\nTotal count verification: 6 + 3 + 12 = 21 unordered outcomes\nNote: Each unordered pair {R₁,B₁} is counted only once, whereas in the ordered scenario we counted both (R₁,B₁) and (B₁,R₁).\n\n\n\n\n\n\n17.4.5 Discrete Sample Spaces & Probability\nWhen we analyze random events like coin flips or dice rolls, we need a way to list all possible outcomes. This is where discrete sample spaces come in. Let’s break this down step by step:\n\n17.4.5.1 Discrete Sample Spaces (S or \\Omega)\nThink of a sample space as a container holding all possible outcomes of a random event/experiment. In discrete sample spaces, we can count these outcomes one by one, like counting marbles in a bag. We write it as:\n\nFor finite events: S = \\{s_1, s_2, ..., s_n\\} [classical (‘naive’) probability]\nFor infinite but countable events: S = \\{s_1, s_2, ...\\}\n\n\n\n17.4.5.2 Three Essential Rules\n\nMust include everything possible (no missing outcomes)\nNo overlap between outcomes (each is unique)\nMust be countable (you can list them out)\n\n\n\n17.4.5.3 Examples\n1. Equally Likely Outcomes (uniform probability distribution)\nThese are scenarios where each basic outcome has the same probability:\n\nFair die: S = \\{1, 2, 3, 4, 5, 6\\}, each with probability \\frac{1}{6}\nFair deck: S = \\{\\text{52 cards}\\}, each with probability \\frac{1}{52}\nFair coin: S = \\{\\text{H}, \\text{T}\\}, each with probability \\frac{1}{2}\n\n2. Events with Different Probabilities\nSome (discrete) cases:\nBinomial Scenarios (Counting Successes)\n\nExample: Number of successes in n trials, each with probability p\nSample Space: S = \\{0, 1, 2, ..., n\\}\nOutcomes have different probabilities based on:\n\nNumber of ways to get k successes (\\binom{n}{k})\nProbability of each arrangement (p^k(1-p)^{n-k})\n\n\nGeometric Scenarios (Waiting for Success)\n\nExample: Number of trials until first success, probability p per trial\nSample Space: S = \\{1, 2, 3, ...\\}\nProbability decreases with number of trials\nP(\\text{first success on trial }n) = (1-p)^{n-1}p\n\nNote: The term “success” in probability simply means the outcome we’re tracking - it could be any event of interest. Each trial has two possible outcomes: success (probability p) or failure (probability 1-p).\n\n\n17.4.5.4 How to Assign Probabilities\n1. Equal Chances (Classical Probability) When all outcomes are equally likely:\nP(\\text{one outcome}) = \\frac{1}{\\text{total outcomes}}\nExample: Rolling a fair die\n\nProbability of rolling a 4 = \\frac{1}{6}\n\n2. Mathematical Models (Probability Distribution Functions) For more complex situations, we use specific formulas (functions):\n\nMultiple trials (Binomial): P(X=k) = \\binom{n}{k}p^k(1-p)^{n-k}\nFirst success (Geometric): P(X=k) = p(1-p)^{k-1}\n\n3. Using Data (Empirical/Experimental Probability) When we have actual observations:\nP(\\text{outcome}) = \\frac{\\text{times outcome occurred}}{\\text{total observations}}\nExample: If you flip a coin 100 times and get 53 heads: P(\\text{heads}) = \\frac{53}{100} = 0.53\n\n\n\n\n\n\nTip\n\n\n\nClassical (“Naive”) Probability: Why Single Outcomes Have Equal Probabilities\nLet’s prove this step by step:\n\nStart with n equally likely outcomes: s_1, s_2, ..., s_n\nWe know the total probability must be 1 (rule 2 above)\nCall the probability of each outcome p\nSince outcomes are equally likely, each has the same probability p\nAdding up all outcomes: p + p + ... + p (n times) = 1\nTherefore: np = 1\nSolving for p: p = \\frac{1}{n}\n\nThis gives us the following rule under the assumption of equally likely outcomes:\nP(\\text{single outcome}) = \\frac{1}{\\text{number of possible outcomes}}\n\n\nImportant Considerations\n\nClassical probability (equal likelihood) is a special case\nMany real-world phenomena follow specific probability distributions (a probability distribution is the mathematical function that gives the probabilities of occurrence of possible outcomes for an experiment.)\nThe type of probability assignment depends on the context:\n\nPhysical symmetry → Classical probability\nRepeated independent trials → Binomial distribution\nRare events → Poisson distribution\nWaiting times → Geometric distribution\n(…)\n\nAll these cases work within discrete sample spaces\nThe probabilities must always sum to 1 over the entire sample space\n\nThis framework helps us systematically study discrete random variables and their probabilities, whether they follow uniform or non-uniform distributions.\n\n\n\n\n\n\nWarning\n\n\n\nThere is no single, universal formula for calculating probabilities across all probability spaces and situations.\nDifferent Types of Probability Spaces\n\nClassical (Finite, Equally Likely)\n\nOnly here we can use: P(A) = \\frac{\\text{favorable outcomes}}{\\text{total outcomes}}\nLimited to finite, equally likely cases\n\nGeneral Discrete\n\nMust specify individual probabilities\nExample: Loaded die needs experimental/empirical determination\nSum of probabilities must equal 1\n\nContinuous\n\nUses calculus and density functions\nProbabilities found by integration\nExample: P(a \\leq X \\leq b) = \\int_a^b f(x)dx\nNo universal formula for density function\n\nMixed/Hybrid\n\nCombines discrete and continuous elements\nDifferent methods needed for different parts\n\n\nWhy No Universal Formula?\n\nDifferent types of randomness need different mathematical tools\nNature of outcomes (discrete/continuous) affects calculation method\nPrior knowledge or assumptions shape probability calculation\nSome probabilities must be found empirically (frequentist/statistical probability) rather than calculated\n\n\n\n\n\n\n\n\n\nFrequentist/Statistical/Empirical Probability & the Law of Large Numbers\n\n\n\nFrequentist probability defines probability as the long-term relative frequency of an event’s occurrence in repeated trials under identical conditions:\nP(A) = \\lim_{n \\to \\infty} \\frac{\\text{number of times A occurs}}{n}\nThe Law of Large Numbers states that as we increase the number of trials, the observed frequency converges to the true probability:\n\nIf true probability of heads is 0.5\nIn 10 flips: might get 7 heads (frequency = 0.7)\nIn 1000 flips: might get 495 heads (frequency ≈ 0.495)\nAs trials → ∞, frequency → 0.5\n\nKey Characteristics:\n\nRequires repeatable experiments under identical conditions\nObjective approach - probability viewed as physical property\nCannot handle one-time events\nFoundation for classical statistical inference\n\nLimitation: We can never perform infinite trials, so we estimate probabilities from large but finite samples.\n\n\n\n\n\n\n\n\nClassical Probability: Equal-Likelihood in Discrete Sample Spaces\n\n\n\nClassical probability, also known as Laplace probability, applies when all outcomes in a finite sample space are equally likely to occur. This framework, developed by Pierre-Simon Laplace, provides a simple yet powerful way to calculate probabilities when symmetry exists.\n\n17.4.6 Mathematical Foundation\nFor any event A in sample space S, the classical probability is calculated as:\nP(A) = \\frac{\\text{number of favorable outcomes}}{\\text{number of possible outcomes}} = \\frac{|A|}{|S|}\nThis definition automatically satisfies Kolmogorov’s probability axioms:\n\nNon-negativity: P(A) \\geq 0 for all A \\subseteq S\nNormalization: P(S) = \\frac{|S|}{|S|} = 1\nAdditivity: For disjoint events A and B, P(A \\cup B) = P(A) + P(B)\n\n\n\n17.4.7 Key Requirements\nTwo essential conditions must be met:\n\nThe sample space S must be finite\nEach elementary outcome s \\in S must be equally likely, with probability P(\\{s\\}) = \\frac{1}{|S|}\n\n\n\n17.4.8 Common Applications\n\nFair Dice\n\nRolling a six: P(6) = \\frac{1}{6}\nRolling an even number: P(2,4,6) = \\frac{3}{6} = \\frac{1}{2}\nRolling a number greater than 4: P(5,6) = \\frac{2}{6} = \\frac{1}{3}\n\nPlaying Cards\n\nDrawing a heart: P(♥) = \\frac{13}{52} = \\frac{1}{4}\nDrawing a face card: P(J,Q,K) = \\frac{12}{52} = \\frac{3}{13}\nDrawing a red ace: P(\\text{A♥,A♦}) = \\frac{2}{52} = \\frac{1}{26}\n\nMultiple Coin Flips\n\nTwo fair coins: S = \\{HH, HT, TH, TT\\}\nP(\\text{exactly one head}) = \\frac{|\\{HT,TH\\}|}{|\\{HH,HT,TH,TT\\}|} = \\frac{2}{4} = \\frac{1}{2}\nP(\\text{at least one head}) = \\frac{|\\{HH,HT,TH\\}|}{|\\{HH,HT,TH,TT\\}|} = \\frac{3}{4}\n\n\n\n\n17.4.9 Limitations and Considerations\n\nEqually Likely Assumption\n\nThis framework fails for biased coins, loaded dice, or any scenario where outcomes aren’t equally likely\nIn such cases, we need empirical probability or other probability measures\n\nFinite Space Requirement\n\nCannot directly apply to infinite sample spaces\nRequires modification for continuous probability spaces\n\nSymmetry Assessment\n\nPhysical symmetry (as in fair dice) often suggests equal likelihood\nBut physical symmetry alone doesn’t guarantee equal probabilities in practice\n\n\n\n\n17.4.10 Connection to Other Probability Concepts\nClassical probability serves as a foundation for understanding more complex probability concepts:\n\nForms the basis for combinatorial probability\nProvides intuition for uniform distributions\nHelps in understanding probability density functions in continuous spaces\n\nNote: While classical probability is intuitive and mathematically elegant, real-world applications often require more general probability frameworks to handle non-uniform probabilities and infinite sample spaces.\n\n\n\n\n\n\n17.4.11 Classical (or ‘Naive’) Probability: The Equal-Likelihood Special Case in Discrete Sample Spaces\nClassical probability applies to finite sample spaces where all outcomes are equally likely to occur. The probability formula is:\nP(\\text{event}) = \\frac{\\text{number of favorable outcomes}}{\\text{total number of possible outcomes}}\nRequirements:\n\nFinite sample space (finite number of outcomes)\nAll outcomes equally likely\nTotal probability sums to 1\n\nExamples:\n\nFair die roll:\n\nP(\\text{rolling a 3}) = \\frac{1}{6}\nP(\\text{rolling an even number}) = \\frac{3}{6} = \\frac{1}{2}\n\nDrawing from a standard deck:\n\nP(\\text{drawing an ace}) = \\frac{4}{52} = \\frac{1}{13}\nP(\\text{drawing a heart}) = \\frac{13}{52} = \\frac{1}{4}\n\n\nKey Limitation:\nClassical probability fails when outcomes are not equally likely (e.g., loaded die) or when the sample space is infinite.\nThis is why it’s called “naive” probability - it assumes a simple, idealized situation where simple counting is sufficient.\n\n17.4.11.1 Starting Assumptions\nWhen developing classical probability theory, we begin with a probability experiment that has two key properties:\n\nThe sample space S is finite, with cardinality |S| = n\nAll elementary outcomes are equally likely\n\nWe can write our sample space explicitly as: S = \\{s_1, s_2, ..., s_n\\}\nThis leads us to classical (or Laplace) probability, which we’ll derive rigorously in the following section. This special case provides a foundation for understanding more complex probability scenarios and helps build crucial probabilistic intuition.\n\n\n17.4.11.2 Core Axioms of Probability Theory\nTo derive classical probability, we start with the aforementioned key probability axioms. These form the mathematical foundation for all probability theory, including the classical or ‘naive’ probability:\n\nFor any event A, P(A) \\geq 0 (Non-negativity)\nP(S) = 1 (Total probability)\nFor disjoint events A and B, P(A \\cup B) = P(A) + P(B) (Additivity)\n\n\n\n17.4.11.3 Deriving the Classical (‘Naive’) Probability Formula (Equal-Likelihood Case)\n\n17.4.11.3.1 Starting Point\nIn a fair game or unbiased experiment where all outcomes are equally likely, we can derive the famous “number of favorable outcomes divided by total outcomes” formula.\n\n\n17.4.11.3.2 The Setup\nConsider a finite sample space S with n outcomes: \\{s_1, s_2, ..., s_n\\}\nEqual-Likelihood Assumption:\n\nEach outcome has the same probability p\nMathematically: P(\\{s_1\\}) = P(\\{s_2\\}) = ... = P(\\{s_n\\}) = p\n\n\n\n17.4.11.3.3 Step-by-Step Derivation\n\nUse Total Probability Axiom\n\nAll probabilities must sum to 1\nFor our n equally likely outcomes:\n\nP(S) = P(\\{s_1\\}) + P(\\{s_2\\}) + ... + P(\\{s_n\\}) = 1 \\underbrace{p + p + ... + p}_{n \\text{ terms}} = 1 np = 1 p = \\frac{1}{n} = \\frac{1}{|S|}\nCalculate Probability of Any Event A\n\nLet event A contain k outcomes\nBy the addition rule for disjoint events: P(A) = \\underbrace{p + p + ... + p}_{k \\text{ terms}} P(A) = k \\cdot \\frac{1}{|S|} = \\frac{k}{|S|} = \\frac{|A|}{|S|}\n\nThe Classical Probability Formula P(A) = \\frac{|A|}{|S|} = \\frac{\\text{number of favorable outcomes}}{\\text{total number of possible outcomes}}\n\n\n\n\n17.4.11.4 Examples to Illustrate\n\nFair Die Roll\n\nS = \\{1,2,3,4,5,6\\}, so |S| = 6\nFor getting an even number, A = \\{2,4,6\\}, so |A| = 3\nP(A) = \\frac{3}{6} = \\frac{1}{2}\n\nDrawing a Card\n\n|S| = 52 (total cards)\nFor drawing a king, |A| = 4\nP(\\text{king}) = \\frac{4}{52} = \\frac{1}{13}\n\n\n\n\n17.4.11.5 Important Limitations\nThis formula only works when:\n\nSample space is finite (we can count outcomes)\nAll outcomes are equally likely\nEach outcome is distinct and well-defined\n\nIf any of these conditions fail (like with a loaded die), we need different methods to calculate probabilities.\n\n\n17.4.11.6 Understanding the Result\nThis derivation reveals several profound insights about classical probability:\nThe familiar “counting formula” (‘Naive’ probability) isn’t just an intuitive rule - it follows necessarily from our axioms combined with the equal-likelihood assumption. When we say outcomes are equally likely, we’re forced mathematically to assign each elementary outcome a probability of \\frac{1}{|S|}. This isn’t a choice but a requirement of the axioms.\nFor any event A, its probability is determined entirely by comparing two cardinalities: the size of the event (|A|) relative to the size of the sample space (|S|).\n\n\n\n\n\n\nFair Coin Toss Probability Space\n\n\n\nA fair coin toss experiment is defined by:\nSample Space:\n\\Omega = \\{H, T\\}\nEvent Space (collection of all possible events):\n\\mathcal{F} = \\{\\{H\\}, \\{T\\}, \\{H, T\\}, \\emptyset\\}\nProbability Measure:\n\nP(\\{H\\}) = P(\\{T\\}) = \\frac{1}{2} (probability of heads or tails)\nP(\\Omega) = P(\\{H, T\\}) = 1 (certainty)\nP(\\emptyset) = 0 (impossible event)\n\nKey Points:\n\nThis is a simple probability space that satisfies the axioms:\n\nP(A) \\geq 0 for all events A\nP(\\Omega) = 1\nP(A \\cup B) = P(A) + P(B) for disjoint events\n\nThe event space \\mathcal{F} includes:\n\nIndividual outcomes: \\{H\\} and \\{T\\}\nThe entire sample space: \\{H, T\\}\nThe empty set: \\emptyset\n\nThe probabilities are equal (\\frac{1}{2}) because it’s a fair coin\n\nThis example illustrates a complete probability space with its three components: sample space (\\Omega), event space (\\mathcal{F}), and probability measure (P).\n\n\n\n\n17.4.11.7 Example Application\nLet’s solidify this understanding by working through a concrete example. Consider rolling a fair six-sided die and finding P(\\text{even number}):\n\nFirst, identify the sample space: S = \\{1, 2, 3, 4, 5, 6\\}, giving us |S| = 6\nThen, identify the event: A = \\{2, 4, 6\\}, giving us |A| = 3\nApply the formula: P(\\text{even number}) = \\frac{|A|}{|S|} = \\frac{3}{6} = \\frac{1}{2}\n\n\n\n\n\n\n\nKey Questions Before Calculating Probabilities\n\n\n\nBefore we can correctly calculate probabilities in any discrete scenario, we must answer two fundamental questions:\n\nDoes Order Matter?\n\nThe importance of order fundamentally changes how we count outcomes. Consider selecting two cards from a deck:\n\nIf we’re playing poker, order doesn’t matter - getting an ace and then a king is the same hand as getting a king and then an ace.\nIf we’re performing a magic trick where we need specific cards in sequence, order matters - getting an ace then a king is different from getting a king then an ace.\n\nWhen order matters, we’re dealing with permutations. When order doesn’t matter, we’re dealing with combinations. This distinction dramatically affects the number of possible outcomes and, consequently, our probability calculations.\n\nIs Sampling With or Without Replacement?\n\nAfter selecting an item, do we put it back before the next selection? This question fundamentally changes the probability structure:\n\nWith replacement: Each selection has the same probability distribution as the first selection. Drawing a red ball and replacing it means the probability of drawing red on the next try remains unchanged.\nWithout replacement: Each selection changes the probability distribution for subsequent selections. Drawing a red ball and not replacing it means there are fewer red balls available for the next draw.\n\nThese sampling schemes lead to different probability models:\n\nWith replacement leads to independent events and often simpler calculations\nWithout replacement leads to dependent events and requires conditional probability\n\n\n\n\n\n\n\n\n\nUnderstanding When to ADD vs MULTIPLY Probabilities\n\n\n\n\n17.4.12 The Key Principle\n\nADD when events represent different ways (paths) to achieve the same outcome\nMULTIPLY when events must occur in sequence (one after another)\n\nExample 1: Single Die Roll Consider events:\n\nA: “rolling an even number” = {2, 4, 6}\nB: “rolling a number &gt; 4” = {5, 6}\n\nP(A or B) requires ADDITION because we want any outcome satisfying either condition:\n\nP(A or B) = P(A) + P(B) - P(A and B)\n= 3/6 + 2/6 - 1/6 = 4/6\n\nExample 2: Two Coin Flips\nFor P(at least one heads):\n\nADD different successful paths: P(HT or TH or HH)\n= 1/4 + 1/4 + 1/4 = 3/4\n\nFor P(two heads):\n\nMULTIPLY along path: P(H) × P(H)\n= 1/2 × 1/2 = 1/4\n\n\n\n17.4.13 Why This Works\n\nAddition combines different ways to succeed\nMultiplication reflects narrowing down possibilities with each sequential requirement\n\n\n\n\n\n\ngraph LR\n    Start[Start] --&gt; H1[H]\n    Start --&gt; T1[T]\n    H1 --&gt; H2[H]\n    H1 --&gt; T2[T]\n    T1 --&gt; H3[H]\n    T1 --&gt; T3[T]\n\n\n\n\n\n\n\n\n\n\n\n\n17.4.14 Understanding Classical Probability Through the Urn Example\nIn our urn with 3 green and 2 red balls:\n\nP(\\text{green}) = \\frac{3}{5}\nP(\\text{red}) = \\frac{2}{5}\nP(\\text{green}) + P(\\text{red}) = 1\n\nThe classical definition of probability assumes:\n\nA finite sample space \\Omega with equally likely outcomes (‘fair’ experiment)\nFor an event A, probability is defined as: P(A) = \\frac{\\text{favorable outcomes}}{\\text{total outcomes}}\n\nIn our urn with 3 green and 2 red balls, these assumptions manifest as:\n\nSample space \\Omega = \\{b_1, b_2, b_3, b_4, b_5\\} where each ball is equally likely\nFor green: P(\\text{green}) = \\frac{|\\text{green balls}|}{|\\Omega|} = \\frac{3}{5}\nFor red: P(\\text{red}) = \\frac{|\\text{red balls}|}{|\\Omega|} = \\frac{2}{5}\n\nKey probability axioms are demonstrated:\n\nNon-negativity: P(\\text{green}), P(\\text{red}) \\geq 0\nNormalization: P(\\Omega) = P(\\text{green}) + P(\\text{red}) = 1\nAdditivity: Since green and red are disjoint events, P(\\text{green or red}) = P(\\text{green}) + P(\\text{red})\n\nREMARK: Many probabilistic situations have the property that they involve a number of different possible outcomes, all of which are equally likely. For example, Heads and Tails on a coin are equally likely to be tossed, the numbers 1 through 6 on a die are equally likely to be rolled, and the ten balls in the above box are all equally likely to be picked.\n‘Naive’ (classical) probability definition assumes uniform probability measure (all outcomes equally likely), and finite uniform sample space.\nWhen considering shapes or elements of the same color in an urn or box, treating them as distinguishable allows you to assume a uniform sample space — equally likely outcomes.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#how-to-calculate-basic-probabilities",
    "href": "probability_en.html#how-to-calculate-basic-probabilities",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "17.5 How to Calculate Basic Probabilities",
    "text": "17.5 How to Calculate Basic Probabilities\nLet’s explore some fundamental probability concepts using a simple example with colored balls in an urn/bag. This example will help us understand:\n\nHow to calculate basic probabilities using the tree diagrams\nHow replacement affects probability\nHow the importance of order affects our calculations\nHow to break down probability problems into steps\n\nTree diagrams are powerful tools for visualizing sequential events. Each branch represents a possible outcome, and probabilities multiply along paths.\n\n\n\n\n\n\nUnderstanding Sampling Methods: Sequences vs Sets\n\n\n\nWhen we count possibilities (sample space size = |S|) in probability problems, we need to think about two important questions:\n\nDoes the order of our selections matter? (Like picking a phone PIN where 1234 is different from 4321)\nCan we reuse items we’ve already selected? (Like picking letters where we can reuse them, versus picking students where we can’t pick the same person twice)\n\nConsider sampling from elements \\{A, B, C\\}. The key distinction is whether we care about:\n\nSequences (ordered lists): where position matters\nSets: where only membership matters\n\n\n17.6 Sampling Scenarios\n\n\n\n\n\n\n\n\nSampling Method\nWith Replacement\nWithout Replacement\n\n\n\n\nSequences (Order Matters)\nOrdered lists with repetition:(A,A), (A,B), (A,C)(B,A), (B,B), (B,C)(C,A), (C,B), (C,C)\nOrdered lists without repetition:(A,B), (A,C)(B,A), (B,C)(C,A), (C,B)\n\n\nSets (Order Doesn’t Matter)\nMultisets (sets with repetition):\\{A,A\\}, \\{A,B\\}, \\{A,C\\}\\{B,B\\}, \\{B,C\\}\\{C,C\\}\nSets (no repetition):\\{A,B\\}, \\{A,C\\}\\{B,C\\}\n\n\n\n\n\n17.7 Mathematical Properties\n\nSequences (Order Matters)\n\nElements have positions: a_1, a_2, \\ldots, a_n\nTwo sequences \\mathbf{x}, \\mathbf{y} are equal iff x_i = y_i for all i\nDenoted as ordered tuples: (a_1, a_2, \\ldots, a_n)\n\nSets (Order Doesn’t Matter)\n\nElements have no position, only membership matters\nTwo sets are equal if they contain the same elements\nDenoted with curly braces: \\{a_1, a_2, \\ldots, a_n\\}\n\n\n\n\n17.8 Factorial Notation\nFor any non-negative integer n, the factorial of n (denoted as n!) is defined as:\nn! = n \\cdot (n-1) \\cdot (n-2) \\cdot ... \\cdot 2 \\cdot 1\nSpecial cases:\n\n0! = 1 (by definition)\n1! = 1\n2! = 2 \\cdot 1 = 2\n3! = 3 \\cdot 2 \\cdot 1 = 6\n4! = 4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\n\nThis can be written recursively as:\n\nn! = n \\cdot (n-1)! for n &gt; 0\n0! = 1\n\nHere’s why 0! equals 1:\n\nBy definition, for any positive integer n, n! = n × (n-1)!\nThis means 1! = 1 × 0!\nWe know 1! = 1\nTherefore: 1 = 1 × 0!\nSolving for 0!: 0! = 1\n\nThis definition is also consistent with the combinatorial interpretation - there is exactly one way to arrange zero elements.\n\n\n17.9 Number of Outcomes\nFor n distinct elements, selecting k items:\n\nSequences with Replacement\n\nEach position has n choices\nTotal: n^k outcomes\n\nSequences without Replacement\n\nPermutations: P(n,k) = \\frac{n!}{(n-k)!}\nEach next position has one fewer choice\n\nSets with Replacement\n\nCombinations with repetition allowed\nTotal: \\binom{n+k-1}{k} outcomes\n\nSets without Replacement\n\nCombinations: \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\nEach subset of size k counted once\n\n\n\n\n17.10 Key Relationships\n\nFor sequences vs sets without replacement:\nP(n,k) = k! \\cdot \\binom{n}{k}\nFor any sampling scheme:\n\\text{sequences} \\geq \\text{sets} \\text{with replacement} \\geq \\text{without replacement}\n\n\n\n\n\n17.10.1 Example 1: Drawing Two Balls from an Urn/Bag\nConsider drawing two balls from an urn containing 3 green and 2 red balls.\nFind the probabilities of the following random events:\n\nThe first ball is red and the second one is green (order matters, drawing without replacement)\nThe first ball is red and the second one is green (order matters, drawing with replacement)\nThe balls are of different colors (order doesn’t matter, drawing without replacement)\nThe balls are of different colors (order doesn’t matter, drawing with replacement)\n\nUnderstanding Event Types in Probability:\n\nSimple events represent a single outcome from a single random action, such as drawing one ball from an urn. The probability of a simple event is calculated directly from the number of favorable outcomes divided by the total possible outcomes.\nCompound events involve multiple outcomes or conditions that must occur together. These can occur simultaneously (like rolling two dice at once) or sequentially (like drawing two balls one after another). The key difference lies in whether the events happen at the same time or in sequence.\n\n\nSequential events are a specific type of compound events where outcomes occur in a particular order over time. Our urn example is particularly instructive here because it demonstrates sequential events through the process of drawing balls one after another. This allows us to explore how the probability of the second draw depends on what happened in the first draw (when sampling without replacement).\n\n\n\n\nSample spaces (S) visualized using the grid diagrams\n\n\nTo better understand how the sample space changes based on our sampling method, let’s examine two scenarios:\n\nWith Replacement\n\nWhen we sample with replacement, we return the ball to the urn after the first draw. This means:\n\nThe probability remains constant for each draw\nTotal possible outcomes: 25 (5×5 grid)\nEach outcome has equal probability\nP(\\text{both green}) = \\frac{3}{5} \\times \\frac{3}{5} = \\frac{9}{25}\nP(\\text{both red}) = \\frac{2}{5} \\times \\frac{2}{5} = \\frac{4}{25}\nP(\\text{mixed}) = \\frac{12}{25}\n\n\nWithout Replacement\n\nWhen we sample without replacement, the first draw affects the probability of the second draw:\n\nTotal possible outcomes: 20 (removing diagonal cells where same ball is drawn twice)\nSecond draw probabilities change based on first draw\nP(\\text{both green}) = \\frac{3}{5} \\times \\frac{2}{4} = \\frac{6}{20}\nP(\\text{both red}) = \\frac{2}{5} \\times \\frac{1}{4} = \\frac{2}{20}\nP(\\text{mixed}) = \\frac{12}{20}\n\nThe grid diagram above visualizes both scenarios, where:\n\nGreen cells represent both balls drawn being green\nRed cells represent both balls drawn being red\nOrange cells represent mixed outcomes (one green, one red)\nCrossed-out cells in the “Without Replacement” grid show impossible outcomes\n\nThis visualization helps demonstrate how the sample space and probabilities change between the two sampling methods, while maintaining the fundamental principle that probabilities must sum to 1 in both cases.\n\nDrawing Two Balls Without Replacement\n\nConsider drawing two balls from an urn containing 3 green and 2 red balls. Let’s analyze all scenarios systematically.\n\n\n\n\n\nflowchart TD\n    A([\"Initial State\\n3G, 2R\"]) --&gt; B[\"First: Green\\n3/5\"]\n    A --&gt; C[\"First: Red\\n2/5\"]\n    B --&gt; D[\"Second: Green\\n2/4\"]\n    B --&gt; E[\"Second: Red\\n2/4\"]\n    C --&gt; F[\"Second: Green\\n3/4\"]\n    C --&gt; G[\"Second: Red\\n1/4\"]\n    \n    D --&gt; H[\"GG: 3/5 × 2/4 = 6/20\"]\n    E --&gt; I[\"GR: 3/5 × 2/4 = 6/20\"]\n    F --&gt; J[\"RG: 2/5 × 3/4 = 6/20\"]\n    G --&gt; K[\"RR: 2/5 × 1/4 = 2/20\"]\n\n\n\n\n\n\nLet’s solve for different scenarios:\n\nFirst red, then green (order matters):\nP(R \\text{ then } G) = \\frac{2}{5} \\cdot \\frac{3}{4} = \\frac{6}{20} = 0.3\nDifferent colors (order doesn’t matter):\nP(\\text{different colors}) = P(R \\text{ then } G) + P(G \\text{ then } R)\n= \\frac{2}{5} \\cdot \\frac{3}{4} + \\frac{3}{5} \\cdot \\frac{2}{4} = \\frac{6}{20} + \\frac{6}{20} = \\frac{12}{20} = 0.6\n\n\n\nDrawing With Replacement\n\nWhen we replace the first ball before drawing the second, the probabilities for the second draw remain unchanged:\n\n\n\n\n\nflowchart TD\n    A([\"Initial State\\n3G, 2R\"]) --&gt; B[\"First: Green\\n3/5\"]\n    A --&gt; C[\"First: Red\\n2/5\"]\n    B --&gt; D[\"Second: Green\\n3/5\"]\n    B --&gt; E[\"Second: Red\\n2/5\"]\n    C --&gt; F[\"Second: Green\\n3/5\"]\n    C --&gt; G[\"Second: Red\\n2/5\"]\n    \n    D --&gt; H[\"GG: 3/5 × 3/5 = 9/25\"]\n    E --&gt; I[\"GR: 3/5 × 2/5 = 6/25\"]\n    F --&gt; J[\"RG: 2/5 × 3/5 = 6/25\"]\n    G --&gt; K[\"RR: 2/5 × 2/5 = 4/25\"]\n\n\n\n\n\n\nNow:\n\nFirst red, then green (order matters):\nP(R \\text{ then } G) = \\frac{2}{5} \\cdot \\frac{3}{5} = \\frac{6}{25} = 0.24\nDifferent colors (order doesn’t matter):\nP(\\text{different colors}) = \\frac{2}{5} \\cdot \\frac{3}{5} + \\frac{3}{5} \\cdot \\frac{2}{5} = \\frac{12}{25} = 0.48\n\nKey observations:\n\nWithout replacement:\n\nDifferent orders of the same colors have different probabilities\nThe second draw’s probability depends on the first outcome\n\nWith replacement:\n\nEach draw is independent\nProbabilities multiply directly because sample space remains unchanged\n\n\n\n\n17.10.2 Example 2: The 4 Red and 3 Black Balls Problem\nLet’s solve a real problem using what we learned. We have:\n\n4 red balls (let’s call them R₁, R₂, R₃, R₄)\n3 black balls (B₁, B₂, B₃)\nWe’ll draw 2 balls without replacement\nOrder doesn’t matter (like picking team members)\n\nWe want to find three probabilities:\n\nGetting two red balls\nGetting two black balls\nGetting one of each color\n\n\n17.10.2.1 Method 1: Using Counting Rules\nFirst, let’s count the total possible outcomes:\n\nWe’re picking 2 balls from 7 total balls, order doesn’t matter\nTotal outcomes = \\binom{7}{2} = \\frac{7!}{2!(7-2)!} = \\frac{7 \\times 6}{2 \\times 1} = 21\n\nNow let’s find each probability:\n\nTwo Red Balls\n\n\nWe need to pick 2 red balls from 4 red balls\nThis is like picking 2 team members from 4 people\nNumber of ways = \\binom{4}{2} = \\frac{4 \\times 3}{2 \\times 1} = 6\nProbability = \\frac{6}{21}\n\n\nTwo Black Balls\n\n\nSimilarly, we need to pick 2 black balls from 3 black balls\nNumber of ways = \\binom{3}{2} = \\frac{3 \\times 2}{2 \\times 1} = 3\nProbability = \\frac{3}{21}\n\n\nOne Red and One Black\n\n\nWe need:\n\nOne red ball (we have 4 to choose from)\nOne black ball (we have 3 to choose from)\n\nNumber of ways = 4 \\times 3 = 12\nProbability = \\frac{12}{21}\n\nLet’s verify our work:\n\nAll probabilities should add to 1\n\\frac{6}{21} + \\frac{3}{21} + \\frac{12}{21} = \\frac{21}{21} = 1 ✓\n\nThis matches what we expect - every time we draw two balls, we must get either:\n\nTwo red balls\nTwo black balls\nOne of each color\n\nUnderstanding how to count correctly helps us solve these probability problems systematically and avoid common mistakes like counting the same outcome multiple times.\n\n\n17.10.2.2 Method 2: Tree Diagram Approach\nThe tree diagram helps us visualize the sequential nature of the draws:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[First: Red 4/7]\n    A --&gt; C[First: Black 3/7]\n    B --&gt; D[Second: Red 3/6]\n    B --&gt; E[Second: Black 3/6]\n    C --&gt; F[Second: Red 4/6]\n    C --&gt; G[Second: Black 2/6]\n\n\n\n\n\n\nUsing the tree diagram:\n\nP(both red) = \\frac{4}{7} \\cdot \\frac{3}{6} = \\frac{12}{42} = \\frac{6}{21}\nP(red then black) = \\frac{4}{7} \\cdot \\frac{3}{6} = \\frac{12}{42}\nP(multi-colored) = P(red then black) + P(black then red)\n= \\frac{4}{7} \\cdot \\frac{3}{6} + \\frac{3}{7} \\cdot \\frac{4}{6} = \\frac{24}{42}\n\n\n\n17.10.2.3 Method 3: Grid Diagram Analysis of Two-Ball Draws\nLet’s visualize the ordered sample space using a grid where rows represent the second draw and columns represent the first draw:\n\n\n\nFirst Draw →\nR₁\nR₂\nR₃\nR₄\nB₁\nB₂\nB₃\n\n\n\n\nSecond Draw ↓\n\n\n\n\n\n\n\n\n\nR₁\nX\n⚫\n⚫\n⚫\n⚪\n⚪\n⚪\n\n\nR₂\n⚫\nX\n⚫\n⚫\n⚪\n⚪\n⚪\n\n\nR₃\n⚫\n⚫\nX\n⚫\n⚪\n⚪\n⚪\n\n\nR₄\n⚫\n⚫\n⚫\nX\n⚪\n⚪\n⚪\n\n\nB₁\n⚪\n⚪\n⚪\n⚪\nX\n⚫\n⚫\n\n\nB₂\n⚪\n⚪\n⚪\n⚪\n⚫\nX\n⚫\n\n\nB₃\n⚪\n⚪\n⚪\n⚪\n⚫\n⚫\nX\n\n\n\nWhere:\n\nX: Impossible (same ball drawn twice)\n⚫: Both same color (both red in upper-left, both black in lower-right)\n⚪: Different colors (red-black or black-red)\n\nFrom this grid:\n\nBoth red = 12 outcomes (⚫ in upper-left quadrant)\nBoth black = 6 outcomes (⚫ in lower-right quadrant)\nRed then black = 12 outcomes (⚪ in lower-left quadrant)\nBlack then red = 12 outcomes (⚪ in upper-right quadrant)\nTotal possible outcomes = 42 (all cells minus 7 diagonal X’s)\n\n\n\n17.10.2.4 Analysis of Two-Ball Draws\nFrom the grid, we can count the following outcomes:\n\nBoth red = 12 outcomes (⚫ in upper-left quadrant)\nRed then black = 12 outcomes (⚪ in lower-left quadrant)\nBlack then red = 12 outcomes (⚪ in upper-right quadrant)\nTotal possible outcomes = 42 (all cells minus 7 diagonal X’s)\n\nTherefore:\n\nP(both red) = \\frac{12}{42} = \\frac{2}{7}\nP(red then black) = \\frac{12}{42} = \\frac{2}{7}\nP(multi-colored) = \\frac{24}{42} = \\frac{4}{7} (includes both red-then-black and black-then-red)\n\n\n\n17.10.2.5 Comparing the Methods\nEach method highlights different aspects of the problem:\n\nCounting Rules:\n\nMost efficient for calculation\nHelps understand combinations and arrangements\nMay obscure the actual outcomes\n\nTree Diagram:\n\nShows sequential nature of draws\nMakes conditional probability clear\nVisualizes how probabilities combine\nGood for checking intuition\n\nGrid Diagram:\n\nShows entire sample space explicitly\nMakes it clear why diagonal is impossible\nHelps visualize groups of outcomes\nDemonstrates why we divide by total possibilities\nShows symmetry in the problem",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#sampling-scenarios",
    "href": "probability_en.html#sampling-scenarios",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "17.6 Sampling Scenarios",
    "text": "17.6 Sampling Scenarios\n\n\n\n\n\n\n\n\nSampling Method\nWith Replacement\nWithout Replacement\n\n\n\n\nSequences (Order Matters)\nOrdered lists with repetition:(A,A), (A,B), (A,C)(B,A), (B,B), (B,C)(C,A), (C,B), (C,C)\nOrdered lists without repetition:(A,B), (A,C)(B,A), (B,C)(C,A), (C,B)\n\n\nSets (Order Doesn’t Matter)\nMultisets (sets with repetition):\\{A,A\\}, \\{A,B\\}, \\{A,C\\}\\{B,B\\}, \\{B,C\\}\\{C,C\\}\nSets (no repetition):\\{A,B\\}, \\{A,C\\}\\{B,C\\}",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#mathematical-properties",
    "href": "probability_en.html#mathematical-properties",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "17.7 Mathematical Properties",
    "text": "17.7 Mathematical Properties\n\nSequences (Order Matters)\n\nElements have positions: a_1, a_2, \\ldots, a_n\nTwo sequences \\mathbf{x}, \\mathbf{y} are equal iff x_i = y_i for all i\nDenoted as ordered tuples: (a_1, a_2, \\ldots, a_n)\n\nSets (Order Doesn’t Matter)\n\nElements have no position, only membership matters\nTwo sets are equal if they contain the same elements\nDenoted with curly braces: \\{a_1, a_2, \\ldots, a_n\\}",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#factorial-notation",
    "href": "probability_en.html#factorial-notation",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "17.8 Factorial Notation",
    "text": "17.8 Factorial Notation\nFor any non-negative integer n, the factorial of n (denoted as n!) is defined as:\nn! = n \\cdot (n-1) \\cdot (n-2) \\cdot ... \\cdot 2 \\cdot 1\nSpecial cases:\n\n0! = 1 (by definition)\n1! = 1\n2! = 2 \\cdot 1 = 2\n3! = 3 \\cdot 2 \\cdot 1 = 6\n4! = 4 \\cdot 3 \\cdot 2 \\cdot 1 = 24\n\nThis can be written recursively as:\n\nn! = n \\cdot (n-1)! for n &gt; 0\n0! = 1\n\nHere’s why 0! equals 1:\n\nBy definition, for any positive integer n, n! = n × (n-1)!\nThis means 1! = 1 × 0!\nWe know 1! = 1\nTherefore: 1 = 1 × 0!\nSolving for 0!: 0! = 1\n\nThis definition is also consistent with the combinatorial interpretation - there is exactly one way to arrange zero elements.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#number-of-outcomes",
    "href": "probability_en.html#number-of-outcomes",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "17.9 Number of Outcomes",
    "text": "17.9 Number of Outcomes\nFor n distinct elements, selecting k items:\n\nSequences with Replacement\n\nEach position has n choices\nTotal: n^k outcomes\n\nSequences without Replacement\n\nPermutations: P(n,k) = \\frac{n!}{(n-k)!}\nEach next position has one fewer choice\n\nSets with Replacement\n\nCombinations with repetition allowed\nTotal: \\binom{n+k-1}{k} outcomes\n\nSets without Replacement\n\nCombinations: \\binom{n}{k} = \\frac{n!}{k!(n-k)!}\nEach subset of size k counted once",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#key-relationships",
    "href": "probability_en.html#key-relationships",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "17.10 Key Relationships",
    "text": "17.10 Key Relationships\n\nFor sequences vs sets without replacement:\nP(n,k) = k! \\cdot \\binom{n}{k}\nFor any sampling scheme:\n\\text{sequences} \\geq \\text{sets} \\text{with replacement} \\geq \\text{without replacement}",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#problem-solutions-1",
    "href": "probability_en.html#problem-solutions-1",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "17.11 Problem Solutions (1)",
    "text": "17.11 Problem Solutions (1)\n\n17.11.1 Problem 1: Two-Ball Drawing from an Urn\nAn urn contains 3 red, 2 blue, and 1 yellow balls. Two balls are drawn sequentially without replacement. We need to find the probability that the balls drawn are different colors.\n\n17.11.1.0.1 Initial Conditions\nLet’s first state our starting conditions:\n\nTotal number of balls: n = 3 + 2 + 1 = 6\nDistribution of balls:\n\nRed: n_R = 3\nBlue: n_B = 2\nYellow: n_Y = 1\n\n\n\n\n17.11.1.0.2 Visual Representation\nLet’s visualize all possible outcomes using a tree diagram:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[\"R (3/6)\"]\n    A --&gt; C[\"B (2/6)\"]\n    A --&gt; D[\"Y (1/6)\"]\n    \n    B --&gt; E[\"B (2/5)\"]\n    B --&gt; F[\"Y (1/5)\"]\n    B --&gt; G[\"R (2/5)\"]\n    \n    C --&gt; H[\"R (3/5)\"]\n    C --&gt; I[\"Y (1/5)\"]\n    C --&gt; J[\"B (1/5)\"]\n    \n    D --&gt; K[\"R (3/5)\"]\n    D --&gt; L[\"B (2/5)\"]\n    D --&gt; M[\"Y (0/5)\"]\n    \n    E --&gt; N[\"RB (Success)\"]\n    F --&gt; O[\"RY (Success)\"]\n    G --&gt; P[\"RR (Fail)\"]\n    H --&gt; Q[\"BR (Success)\"]\n    I --&gt; R[\"BY (Success)\"]\n    J --&gt; S[\"BB (Fail)\"]\n    K --&gt; T[\"YR (Success)\"]\n    L --&gt; U[\"YB (Success)\"]\n    M --&gt; V[\"YY (Fail)\"]\n\n\n\n\n\n\n\n\n17.11.1.0.3 Probability Calculation\nLet’s calculate the probability of drawing different colors systematically:\n\nStarting with Red (probability \\frac{3}{6}):\n\nRed → Blue: P(R,B) = \\frac{3}{6} \\cdot \\frac{2}{5} = \\frac{6}{30}\nRed → Yellow: P(R,Y) = \\frac{3}{6} \\cdot \\frac{1}{5} = \\frac{3}{30}\n\nStarting with Blue (probability \\frac{2}{6}):\n\nBlue → Red: P(B,R) = \\frac{2}{6} \\cdot \\frac{3}{5} = \\frac{6}{30}\nBlue → Yellow: P(B,Y) = \\frac{2}{6} \\cdot \\frac{1}{5} = \\frac{2}{30}\n\nStarting with Yellow (probability \\frac{1}{6}):\n\nYellow → Red: P(Y,R) = \\frac{1}{6} \\cdot \\frac{3}{5} = \\frac{3}{30}\nYellow → Blue: P(Y,B) = \\frac{1}{6} \\cdot \\frac{2}{5} = \\frac{2}{30}\n\n\n\n\n17.11.1.0.4 Final Solution\nThe total probability of drawing two different colored balls is the sum of all favorable outcomes:\n\n\\begin{align*}\nP(\\text{different colors}) &= P(R,B) + P(R,Y) + P(B,R) + P(B,Y) + P(Y,R) + P(Y,B) \\\\\n&= \\frac{6}{30} + \\frac{3}{30} + \\frac{6}{30} + \\frac{2}{30} + \\frac{3}{30} + \\frac{2}{30} \\\\\n&= \\frac{22}{30} \\\\\n&= \\frac{11}{15} \\\\\n&\\approx 0.733 \\text{ or } 73.3\\%\n\\end{align*}\n\n\n\n17.11.1.0.5 Verification\nThis result aligns with our intuition because:\n\nThe sample space contains more ways to draw different colors than same colors\nThe complementary probability (drawing same colors) would be \\frac{4}{15} or about 26.7%\nSince same-color draws are limited to RR, BB, and YY combinations, it makes sense that different-color draws are more likely\n\n\n\n\n17.11.2 Problem 2: Die and Coin Probability Exercise\nLet’s analyze the probability of getting heads OR tails OR three dots when flipping both a coin and a die. This problem offers an excellent opportunity to explore probability unions and the importance of careful counting.\n\n17.11.2.0.1 Understanding the Problem Space\nIn our experiment:\n\nWe flip a coin (possible outcomes: heads, tails)\nWe roll a die (possible outcomes: 1, 2, 3, 4, 5, 6 dots)\nThese events occur simultaneously\n\nLet’s start with a visualization:\n\n\n\n\n\ngraph TD\n    A[Experiment] --&gt; B[Coin]\n    A --&gt; C[Die]\n    B --&gt; D[Heads]\n    B --&gt; E[Tails]\n    C --&gt; F[1 dot]\n    C --&gt; G[2 dots]\n    C --&gt; H[3 dots]\n    C --&gt; I[4 dots]\n    C --&gt; J[5 dots]\n    C --&gt; K[6 dots]\n\n\n\n\n\n\n\n\n17.11.2.0.2 Common Mistakes and Overcounting Analysis\nA common first instinct might be to simply add the individual probabilities:\nP(heads) + P(tails) + P(three dots) = \\frac{1}{2} + \\frac{1}{2} + \\frac{1}{6} = \\frac{7}{6}\nThis incorrect approach reveals several important issues:\n\nThe result exceeds 1, which is impossible for a probability\nWe’ve counted many outcomes multiple times\nWe’ve failed to recognize event overlaps\n\nLet’s analyze the overcounting:\n\n\n\n\n\ngraph TD\n    A[Overcounting Analysis] --&gt; B[Heads counted: 6/12]\n    A --&gt; C[Tails counted: 6/12]\n    A --&gt; D[Three dots counted: 2/12]\n    B --&gt; E[Including three with heads: 1/12]\n    C --&gt; F[Including three with tails: 1/12]\n    E --&gt; G[Double counted!]\n    F --&gt; G\n\n\n\n\n\n\n\n\n17.11.2.0.3 Correct Solution Using Set Theory\nLet’s solve this properly using set theory:\n\nSet H: All outcomes with heads\nSet T: All outcomes with tails\nSet 3: All outcomes with three dots\n\nKey insights:\n\nSets H and T are mutually exclusive\nSet 3 is entirely contained within H ∪ T\nTherefore, P(H ∪ T ∪ 3) = P(H ∪ T) = 1\n\nWe can write this formally:\nP(H ∪ T ∪ 3) = P(H) + P(T) - P(H ∩ T) + P(3) - P(3 ∩ (H ∪ T)) = \\frac{1}{2} + \\frac{1}{2} - 0 + \\frac{1}{6} - \\frac{1}{6} = 1\n\n\n17.11.2.0.4 Sample Space Analysis\n\n\n\n\n\ngraph TD\n    A[Total Outcomes: 12] --&gt; B[Heads: 6]\n    A --&gt; C[Tails: 6]\n    B --&gt; D[With three: 1]\n    C --&gt; E[With three: 1]\n    D --&gt; F[Already counted in heads]\n    E --&gt; G[Already counted in tails]\n\n\n\n\n\n\nThis visual representation helps us understand why:\n\nThe sample space has 12 total outcomes (2 × 6)\nThe three-dot outcomes are already included in heads and tails counts\nAdding P(three dots) would lead to double counting\n\n\n\n17.11.2.0.5 Key Learning Points\nThis problem illustrates several fundamental probability concepts:\n\nExhaustive Events: Heads and tails together cover all possible coin outcomes, making additional events redundant unless they introduce new dimensions.\nDouble Counting Protection: The inclusion-exclusion principle helps us avoid counting outcomes multiple times.\nSample Space Structure: Understanding your sample space structure (12 total outcomes) helps verify solution logic.\n\n\n\n\n17.11.3 Problem 3: Laplace’s Two-Draw Probability Problem\nSuppose there are two urns of coloured marbles:\n\nUrn X contains 3 black marbles, 1 white.\nUrn Y contains 1 black marble, 3 white.\n\nI flip a fair coin to decide which urn to draw from, heads for Urn X and tails for Urn Y. Then I draw marbles at random.\nLaplace asked what happens if we do two draws, with replacement. What’s the probability both draws will come up black?\nLet’s solve this fascinating probability problem involving two draws with replacement. This is a particularly interesting case because the replacement aspect affects how we think about sequential probabilities.\n\n17.11.3.0.1 Understanding the Initial Setup\nFirst, let’s clarify our starting conditions:\nUrn X (selected with heads):\n\n3 black marbles, 1 white marble\nTotal: 4 marbles\nP(black|X) = \\frac{3}{4}\n\nUrn Y (selected with tails):\n\n1 black marble, 3 white marbles\nTotal: 4 marbles\nP(black|Y) = \\frac{1}{4}\n\nLet’s visualize this with a tree diagram showing all possible paths:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[Urn X 1/2]\n    A --&gt; C[Urn Y 1/2]\n    \n    B --&gt; D[Draw 1 Black 3/4]\n    B --&gt; E[Draw 1 White 1/4]\n    \n    C --&gt; F[Draw 1 Black 1/4]\n    C --&gt; G[Draw 1 White 3/4]\n    \n    D --&gt; H[Draw 2 Black 3/4]\n    D --&gt; I[Draw 2 White 1/4]\n    \n    E --&gt; J[Draw 2 Black 3/4]\n    E --&gt; K[Draw 2 White 1/4]\n    \n    F --&gt; L[Draw 2 Black 1/4]\n    F --&gt; M[Draw 2 White 3/4]\n    \n    G --&gt; N[Draw 2 Black 1/4]\n    G --&gt; O[Draw 2 White 3/4]\n\n\n\n\n\n\n\n\n17.11.3.0.2 Step-by-Step Solution\nLet’s break this down into manageable steps:\n\nFirst, consider the urn selection:\n\nP(Urn X) = P(heads) = \\frac{1}{2}\nP(Urn Y) = P(tails) = \\frac{1}{2}\n\nFor two black draws from Urn X:\n\nP(black and black|X) = \\frac{3}{4} \\times \\frac{3}{4} = \\frac{9}{16}\nP(X and both black) = \\frac{1}{2} \\times \\frac{9}{16} = \\frac{9}{32}\n\nFor two black draws from Urn Y:\n\nP(black and black|Y) = \\frac{1}{4} \\times \\frac{1}{4} = \\frac{1}{16}\nP(Y and both black) = \\frac{1}{2} \\times \\frac{1}{16} = \\frac{1}{32}\n\nTotal probability (using the law of total probability): P(both black) = P(X and both black) + P(Y and both black) = \\frac{9}{32} + \\frac{1}{32} = \\frac{10}{32} = \\frac{5}{16} ≈ 0.3125 or about 31.25%\n\n\n\n17.11.3.0.3 Key Insights from This Problem\n\nReplacement Matters:\n\nBecause we replace after the first draw, the probabilities remain constant for the second draw\nThis is different from drawing without replacement, where probabilities would change\n\nConditional Independence:\n\nOnce we know which urn we’re using, the draws are independent\nHowever, the draws are not unconditionally independent\n\nLaw of Total Probability:\n\nWe needed to consider both paths (Urn X and Urn Y) to find the total probability\nEach path’s contribution is weighted by the probability of selecting that urn",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#core-probability-rules",
    "href": "probability_en.html#core-probability-rules",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "17.12 Core Probability Rules",
    "text": "17.12 Core Probability Rules\n\n17.12.1 The Complement Rule\nThe complement rule is one of the most fundamental concepts in probability theory. For any event A, there’s always the possibility that A doesn’t occur. We call this the complement of A, written as A' or A^c.\nThe complement rule states:\nP(A') = 1 - P(A)\nThis makes intuitive sense because any outcome must either be in A or in A’ (but not both), and something must happen (the total probability must be 1).\nReal-World Example: Consider a weather forecast that predicts a 70% chance of rain tomorrow. Using the complement rule, we can immediately calculate that there’s a 30% chance it won’t rain:\nP(\\text{no rain}) = 1 - P(\\text{rain}) = 1 - 0.70 = 0.30\nAnother Example: In a game of roulette, what’s the probability of not landing on red? There are 18 red numbers, 18 black numbers, and 2 green numbers (0 and 00) on a roulette wheel. Therefore:\nP(\\text{red}) = \\frac{18}{38} P(\\text{not red}) = 1 - \\frac{18}{38} = \\frac{20}{38}\n\n\n17.12.2 The Addition/Sum Rule\nWhen we want to find the probability of either one event OR another occurring, we use the addition rule. However, we need to be careful about double-counting outcomes that are in both events.\nFor any two events A and B:\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\nThe term P(A \\cap B) represents the probability of both events occurring simultaneously. We subtract it to avoid counting these outcomes twice.\nReal-World Example: In a college class, 65% of students play sports, 45% are in clubs, and 25% do both. What percentage of students are involved in either sports or clubs?\nP(\\text{sports or clubs}) = 65\\% + 45\\% - 25\\% = 85\\%\nFor mutually exclusive events (events that cannot occur simultaneously), P(A \\cap B) = 0, so the formula simplifies to:\nP(A \\cup B) = P(A) + P(B)\nExample: When rolling a die, what’s the probability of rolling either a 1 or a 6? Since these outcomes can’t happen simultaneously:\nP(1 \\text{ or } 6) = P(1) + P(6) = \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{3}\n\n\n17.12.3 Conditional Probability, the Multiplication Rule, and Bayes’ Theorem\n\n17.12.3.1 Understanding Conditional Probability\nConditional probability represents how the probability of one event changes when we have information about another event. Let’s start with a simple example:\nImagine you have a deck of 52 playing cards. What’s the probability of drawing a King given that you’ve drawn a face card (Jack, Queen, or King)?\nTo solve this:\n\nTotal face cards = 12 (4 each of Jack, Queen, King)\nNumber of Kings = 4\nP(\\text{King}|\\text{Face Card}) = \\frac{P(\\text{King} \\cap \\text{Face Card})}{P(\\text{Face Card})} = \\frac{4/52}{12/52} = \\frac{1}{3}\n\nThis illustrates the fundamental formula for conditional probability:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\nIn the context of classical probability, where all outcomes are equally likely, we can express these probabilities in terms of the number of favorable outcomes:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{|A \\cap B|/|S|}{|B|/|S|} = \\frac{|A \\cap B|}{|B|}\nwhere:\n\n|A \\cap B| represents the number of outcomes in both events A and B\n|B| represents the number of outcomes in event B\n|S| represents the total number of outcomes in the sample space\n\nThis is why in our card example: P(\\text{King}|\\text{Face Card}) = \\frac{|\\text{Kings}|}{|\\text{Face Cards}|} = \\frac{4}{12} = \\frac{1}{3}\nThe formula shows that in classical probability, conditional probability is simply the ratio of the number of outcomes favorable to both events to the number of outcomes in the conditioning event.\n\n\n17.12.3.2 Medical Testing Example\nLet’s explore a more practical example involving medical testing. Consider a disease that affects 1% of the population and a test T with the following characteristics:\n\nSensitivity (true positive rate): P(T^+|D^+) = 0.95\nSpecificity (true negative rate): P(T^-|D^-) = 0.98\n\nWe can organize this information in a cross table for a population of 10,000:\n\n\n\nTest/Disease\nD^+ Present\nD^- Absent\nTotal\n\n\n\n\nT^+\n95\n198\n293\n\n\nT^-\n5\n9,702\n9,707\n\n\nTotal\n100\n9,900\n10,000\n\n\n\nUsing this, we can calculate important probabilities like:\n\nPositive Predictive Value: P(D^+|T^+) = \\frac{95}{293} \\approx 0.32\nNegative Predictive Value: P(D^-|T^-) = \\frac{9,702}{9,707} \\approx 0.999\n\nNote that:\n\nP(D^+) = 0.01 (disease prevalence)\nP(T^+|D^-) = 0.02 (false positive rate = 1 - specificity)\nP(T^-|D^+) = 0.05 (false negative rate = 1 - sensitivity)\n\nThese probabilities show how a test with seemingly good characteristics (95% sensitivity and 98% specificity) can still lead to many false positives when the condition being tested for is rare in the population.\n\n\n\n\n\n\nUrn Example with Probability Tree\n\n\n\nConsider an urn containing:\n\n3 blue marbles (B)\n\n2 marked with star (S^+)\n1 unmarked (S^-)\n\n2 red marbles (R)\n\n1 marked with star (S^+)\n1 unmarked (S^-)\n\n\nLet’s calculate the probability of drawing a starred marble given that we drew a blue marble.\nUsing the tree diagram:\n\n\n\n\n\ngraph LR\n    Ω{Ω} --&gt; B[B: 3/5]\n    Ω --&gt; R[R: 2/5]\n    B --&gt; BS+[\"S⁺|B: 2/3\"]\n    B --&gt; BS-[\"S⁻|B: 1/3\"]\n    R --&gt; RS+[\"S⁺|R: 1/2\"]\n    R --&gt; RS-[\"S⁻|R: 1/2\"]\n    \n    style Ω fill:#e6e6ff,stroke:#333,stroke-width:2px,color:#000\n    style B fill:#ccf2ff,stroke:#333,stroke-width:2px,color:#000\n    style R fill:#ffe6e6,stroke:#333,stroke-width:2px,color:#000\n    style BS+ fill:#f2f2f2,stroke:#333,stroke-width:1px,color:#000\n    style BS- fill:#f2f2f2,stroke:#333,stroke-width:1px,color:#000\n    style RS+ fill:#f2f2f2,stroke:#333,stroke-width:1px,color:#000\n    style RS- fill:#f2f2f2,stroke:#333,stroke-width:1px,color:#000\n    \n    linkStyle default stroke:#333,stroke-width:1px\n\n\n\n\n\n\nFrom this tree, we can calculate:\n\nP(B) = \\frac{3}{5}\nP(S^+|B) = \\frac{2}{3} (probability of star given blue)\nP(B \\cap S^+) = \\frac{3}{5} \\cdot \\frac{2}{3} = \\frac{2}{5}\n\nWe can verify the conditional probability formula:\nP(S^+|B) = \\frac{P(B \\cap S^+)}{P(B)} = \\frac{2/5}{3/5} = \\frac{2}{3}\nOther probabilities from this scenario:\n\nP(R) = \\frac{2}{5}\nP(S^+|R) = \\frac{1}{2}\nP(S^+) = P(B)P(S^+|B) + P(R)P(S^+|R) = \\frac{3}{5} \\cdot \\frac{2}{3} + \\frac{2}{5} \\cdot \\frac{1}{2} = \\frac{3}{5}\n\nThis example illustrates how:\n\nThe probability tree helps visualize sequential events\nBranch probabilities multiply along paths\nThe conditional probability formula naturally emerges from the tree structure\nThe law of total probability can be visualized as summing across different paths\n\n\n\n\n\n\n\n\n\nSequential Drawing Example\n\n\n\nConsider drawing two balls from an urn containing 3 blue (B) and 2 red (R) balls without replacement. Let’s find the probability of drawing two blue balls.\n\n\n\n\n\ngraph LR\n    Ω{Start} --&gt; B1[\"B₁: 3/5\"]\n    Ω --&gt; R1[\"R₁: 2/5\"]\n    B1 --&gt; B2[\"B₂|B₁: 2/4\"]\n    B1 --&gt; R2[\"R₂|B₁: 2/4\"]\n    R1 --&gt; B3[\"B₂|R₁: 3/4\"]\n    R1 --&gt; R3[\"R₂|R₁: 1/4\"]\n    \n    style Ω fill:#e6e6ff,stroke:#333,stroke-width:2px,color:#000\n    style B1 fill:#ccf2ff,stroke:#333,stroke-width:2px,color:#000\n    style R1 fill:#ffe6e6,stroke:#333,stroke-width:2px,color:#000\n    style B2 fill:#ccf2ff,stroke:#333,stroke-width:1px,color:#000\n    style R2 fill:#ffe6e6,stroke:#333,stroke-width:1px,color:#000\n    style B3 fill:#ccf2ff,stroke:#333,stroke-width:1px,color:#000\n    style R3 fill:#ffe6e6,stroke:#333,stroke-width:1px,color:#000\n    \n    linkStyle default stroke:#333,stroke-width:1px\n\n\n\n\n\n\nLet’s calculate various probabilities:\n\nTwo blue balls (B₁ and B₂):\n\nP(B_1) = \\frac{3}{5} (first draw)\nP(B_2|B_1) = \\frac{2}{4} (second draw given first was blue)\nP(B_1 \\cap B_2) = \\frac{3}{5} \\cdot \\frac{2}{4} = \\frac{3}{10}\n\nBlue then Red:\n\nP(R_2|B_1) = \\frac{2}{4}\nP(B_1 \\cap R_2) = \\frac{3}{5} \\cdot \\frac{2}{4} = \\frac{3}{10}\n\nRed then Blue:\n\nP(R_1) = \\frac{2}{5}\nP(B_2|R_1) = \\frac{3}{4}\nP(R_1 \\cap B_2) = \\frac{2}{5} \\cdot \\frac{3}{4} = \\frac{3}{10}\n\nTwo red balls:\n\nP(R_2|R_1) = \\frac{1}{4}\nP(R_1 \\cap R_2) = \\frac{2}{5} \\cdot \\frac{1}{4} = \\frac{1}{10}\n\n\nKey observations:\n\nThe probability of second draw depends on first outcome (conditional probability)\nTotal probability = 1: \\frac{3}{10} + \\frac{3}{10} + \\frac{3}{10} + \\frac{1}{10} = 1\nNotice that P(B_1 \\cap R_2) = P(R_1 \\cap B_2) due to symmetry\nThe denominator changes after first draw (4 balls remain)\n\nThis example illustrates how:\n\nProbabilities update based on previous outcomes\nThe multiplication rule applies to sequential events\nSample space reduces after each draw\nOrder can matter in sequential probability calculations\n\n\n\n\n\n\n\n\n\nConditional Probability: A Geometric Perspective\n\n\n\nConditional probability answers the question: “Given that we know event B has occurred, what is the probability that event A will occur?” We write this as P(A|B), read as “the probability of A given B.”\nThe formal definition is:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\nGeometrically, we can visualize this as:\n\nThe original sample space \\Omega represented as a rectangle\nEvent B as a region within \\Omega\nThe intersection A \\cap B as the overlap between regions A and B\nConditional probability as the ratio of the overlap area to the area of B\n\nThis visualization helps understand why we divide by P(B) - we’re essentially creating a new probability space where B is our universe.\n\n\n\n\n\n\n\n\nWhy P(A|B) ≠ P(B|A)?\n\n\n\nImagine these two questions:\n\nWhat’s the probability it’s raining (A) given there are clouds (B)?\nWhat’s the probability there are clouds (B) given it’s raining (A)?\n\nClearly, these are different:\n\nP(A|B): Among all cloudy days, how many are rainy?\nP(B|A): Among all rainy days, how many are cloudy?\n\nP(B|A) would be close to 1 (almost all rainy days have clouds) While P(A|B) might be around 0.3 (not all cloudy days bring rain)\n\n17.12.4 When are they equal?\n\nWhen events are independent:\n\nP(A|B) = P(A) and P(B|A) = P(B)\n\nWhen events have symmetric relationship:\n\nDrawing cards: P(\\text{red}|\\text{face}) = P(\\text{face}|\\text{red})\nBoth equal \\frac{6}{26} = \\frac{3}{13}\n\nWhen applying Bayes: if P(A|B) = P(B|A), then P(A) = P(B)\n\nFrom P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\nIf P(A|B) = P(B|A), then P(A) = P(B)\n\n\n\n\n\n\n\n17.12.4.1 The Law of Total Probability\n\n\n\n\n\n\nSample Space Partitions\n\n\n\nImagine a sample space \\Omega as a complete population where every individual must be classified into exactly one category. This is what a partition does - it divides our universe of possibilities into distinct, non-overlapping groups that together include all possibilities.\nConsider how we might partition a population:\n\nLet A_1 = “Category 1”\nLet A_2 = “Category 2”\nLet A_3 = “Category 3”\n\nThese classifications form a partition because:\n\nComplete Coverage (\\Omega = A_1 \\cup A_2 \\cup ... \\cup A_n):\n\nEvery outcome in the sample space must belong to exactly one category\nNothing can be left unclassified\nThe categories together capture all possibilities\n\nMutual Exclusivity (A_i \\cap A_j = \\emptyset for i \\neq j):\n\nEach outcome belongs to exactly one category\nCategories cannot overlap\nBeing in one category excludes being in any other\n\n\nThis framework becomes powerful when:\n\nCalculating total probability (sum across all categories)\nUpdating beliefs with new information\nBreaking complex problems into manageable pieces\n\nThink of it like organizing a filing system: each document must go into exactly one folder (mutual exclusivity), and every document must be filed somewhere (complete coverage). When we get new information, we might need to update our filing system, but we always maintain these two key properties.\nThe power of partitioning lies in its ability to help us systematically organize possibilities and update probabilities as new information becomes available. This forms the foundation for understanding more complex concepts like the law of total probability and Bayes’ theorem.\n\n\nThe law of total probability is a fundamental bridge between conditional probabilities and overall probabilities. Given a partition \\{A_1, A_2, ..., A_n\\} of the sample space, for any event B:\nP(B) = \\sum_{i=1}^n P(B|A_i)P(A_i)\nVisually, this represents:\n\nBreaking the sample space into disjoint “slices” (the partition)\nFinding the probability of B within each slice (P(B|A_i))\nWeighting each slice by its probability (P(A_i))\nSumming all contributions\n\nExample: In a tech company:\n\n40% of employees are developers (A_1)\n35% are managers (A_2)\n25% are other roles (A_3)\n\nTo find the probability of an employee working remotely (B):\n\n80% of developers work remotely: P(B|A_1) = 0.80\n60% of managers work remotely: P(B|A_2) = 0.60\n40% of other roles work remotely: P(B|A_3) = 0.40\n\nUsing the law of total probability:\nP(B) = (0.80)(0.40) + (0.60)(0.35) + (0.40)(0.25) = 0.64\nSo 64% of all employees work remotely.\n\n\n17.12.4.2 The Multiplication Rule\nThe conditional probability formula can be rearranged to give us the multiplication rule:\nP(A \\cap B) = P(A|B) \\cdot P(B) = P(B|A) \\cdot P(A)\nThis symmetry is crucial because it shows:\n\nWe can compute joint probabilities in two ways\nThe order of conditioning doesn’t matter\nBoth perspectives must yield the same result\n\n\n\n17.12.4.3 Bayes’ Theorem: From Prior to Posterior Beliefs\nLet’s derive Bayes’ theorem starting from the fundamental definitions of conditional probability and using the multiplication rule.\n\n17.12.4.3.1 Starting Point: Conditional Probability\nThe conditional probability formula for two events A and B is:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\nSimilarly, we can write:\nP(B|A) = \\frac{P(A \\cap B)}{P(A)}\n\n\n17.12.4.3.2 Multiplication Rule\nFrom either of these formulas, we can derive the multiplication rule:\nP(A \\cap B) = P(B|A) \\cdot P(A) or equivalently P(A \\cap B) = P(A|B) \\cdot P(B)\n\n\n17.12.4.3.3 Deriving Bayes’ Theorem\n\nStart with the conditional probability formula:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\nUse the multiplication rule to express the intersection:\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n\nThis gives us Bayes’ theorem in its basic form. The denominator P(B) can be expanded using the law of total probability:\nP(B) = P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)\nLeading to the full form:\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)}\nThis derivation shows how Bayes’ theorem emerges naturally from the basic rules of probability, allowing us to “reverse” conditional probabilities and update prior beliefs with new evidence.\nUsing the law of total probability for the denominator:\nP(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c)}\nThe formula has three key components:\n\nPrior probability: P(A) - initial belief about event A\nLikelihood: P(B|A) - probability of evidence B given A is true\nNormalizing constant: P(B) - ensures probabilities sum to 1\n\n\n\n17.12.4.3.4 Example 1: Playing Cards\nLet’s start with a simple example using cards:\nSuppose we draw a card and are told it’s red. What’s the probability it’s a face card?\nGiven:\n\nP(\\text{Face}) = 12/52 = 3/13 (prior)\nP(\\text{Red}|\\text{Face}) = 6/12 = 1/2 (likelihood)\nP(\\text{Red}) = 26/52 = 1/2 (normalizing constant)\n\nUsing Bayes’ theorem: P(\\text{Face}|\\text{Red}) = \\frac{(1/2)(3/13)}{1/2} = \\frac{3}{13}\n\n\n17.12.4.3.5 Example 2: Medical Testing\nA more practical application involves medical diagnostics. Let’s formalize the terminology:\nTesting Framework:\n\nConditions:\n\nD^+: Disease present\nD^-: Disease absent\n\nTest Results:\n\nT^+: Positive test\nT^-: Negative test\n\n\nKey Metrics:\n\nSensitivity: P(T^+|D^+) - True Positive Rate\nSpecificity: P(T^-|D^-) - True Negative Rate\nPPV: P(D^+|T^+) - Positive Predictive Value\nNPV: P(D^-|T^-) - Negative Predictive Value\n\nThese relationships can be visualized in a confusion matrix:\n\n\n\n\nD^+\nD^-\n\n\n\n\nT^+\nTP\nFP\n\n\nT^-\nFN\nTN\n\n\n\nwhere:\n\nTP: True Positives\nTN: True Negatives\nFP: False Positives (Type I error)\nFN: False Negatives (Type II error)\n\nExample Calculation: Consider a test for a rare disease where:\n\nPrevalence: P(D^+) = 0.01 (1%)\nSensitivity: P(T^+|D^+) = 0.95 (95%)\nSpecificity: P(T^-|D^-) = 0.98 (98%)\n\nWhat’s the probability of having the disease given a positive test?\nUsing Bayes’ theorem:\nP(D^+|T^+) = \\frac{P(T^+|D^+) \\cdot P(D^+)}{P(T^+|D^+) \\cdot P(D^+) + P(T^+|D^-) \\cdot P(D^-)}\nP(D^+|T^+) = \\frac{(0.95)(0.01)}{(0.95)(0.01) + (0.02)(0.99)} \\approx 0.32\nThis counterintuitive result (only 32% chance of disease despite a positive test) illustrates the base rate fallacy - when the condition is rare, even a highly accurate test can have a low positive predictive value.\nThis example shows why Bayes’ theorem is crucial in medical decision-making, as it properly accounts for both the test’s accuracy and the disease’s prevalence in the population.\n\n\n17.12.4.3.6 Spam Filtering Example\nEmail spam filters are a perfect real-world application of Bayes’ theorem. Let’s see how it works:\nConsider a single word “lottery” in an email. We want to know: given that an email contains this word, what’s the probability it’s spam?\nLet’s define our events:\n\nS: Email is spam\nW: Email contains the word “lottery”\n\nWe need:\n\nPrior probability: P(S) = 0.30 (30% of all emails are spam)\nLikelihood: P(W|S) = 0.20 (20% of spam emails contain “lottery”)\nFalse positive rate: P(W|S') = 0.001 (0.1% of legitimate emails contain “lottery”)\n\nUsing Bayes’ theorem:\nP(S|W) = \\frac{(0.20)(0.30)}{(0.20)(0.30) + (0.001)(0.70)} \\approx 0.989\nSo if an email contains “lottery,” there’s a 98.9% chance it’s spam!\nReal spam filters:\n\nLook at multiple words and features\nUpdate probabilities continuously based on user feedback\nCombine evidence using the multiplication rule for independent events\nUse logarithms to avoid numerical underflow with many multiplications\n\n\n\n17.12.4.3.7 Weather Forecasting Example\nAnother practical application is weather forecasting. Suppose we want to know if it will rain tomorrow given certain atmospheric conditions:\nLet’s define:\n\nR: It rains tomorrow\nC: Current atmospheric conditions (high pressure system)\n\nGiven:\n\nP(R) = 0.25 (25% chance of rain on any day)\nP(C|R) = 0.10 (10% of rainy days have high pressure)\nP(C|R') = 0.70 (70% of non-rainy days have high pressure)\n\nUsing Bayes’ theorem:\nP(R|C) = \\frac{(0.10)(0.25)}{(0.10)(0.25) + (0.70)(0.75)} \\approx 0.045\nSo given high pressure, there’s only about a 4.5% chance of rain tomorrow.\n\n\n\n17.12.4.4 Key Interconnections and Applications\nThese concepts form a unified framework with wide-ranging applications:\n\nConditional probability provides the foundation for understanding dependent events\nThe multiplication rule enables complex probability calculations\nTotal probability helps break down complex scenarios into manageable pieces\nBayes’ theorem combines these tools to update probabilities with new evidence\n\nModern Applications:\n\nMachine Learning: Naive Bayes classifiers for text categorization\nMedical Diagnosis: Interpreting test results and screening procedures\nQuality Control: Identifying defective products based on test results\nRisk Assessment: Updating risk probabilities with new information\nNatural Language Processing: Sentiment analysis and language modeling\nForensics: Evaluating evidence in legal cases\nRecommender Systems: Predicting user preferences\n\nUnderstanding these relationships helps in:\n\nChoosing the right probabilistic tool for a given problem\nBreaking complex problems into manageable pieces\nAvoiding common probability misconceptions\nMaking better decisions under uncertainty\nBuilding intuition for machine learning algorithms\n\n\n\n\n17.12.5 Independent and Disjoint Events\nUnderstanding the difference between independent and disjoint events is crucial for correctly applying probability rules. The key insight is that these concepts are fundamentally different - in fact, disjoint events are always dependent (except in trivial cases).\n\n17.12.5.1 Independent Events\nEvents A and B are independent if knowing that one occurred doesn’t affect the probability of the other occurring. Mathematically, this means any of these equivalent conditions:\n\nP(A|B) = P(A)\nP(B|A) = P(B)\nP(A \\cap B) = P(A) \\cdot P(B)\n\nExample 1: Flipping a fair coin twice\n\nLet A = “heads on first flip” and B = “heads on second flip”\nP(A) = \\frac{1}{2} and P(B) = \\frac{1}{2}\nP(A \\cap B) = \\frac{1}{4} = P(A) \\cdot P(B)\nTherefore, the flips are independent\n\n\n\n17.12.5.2 Disjoint (Mutually Exclusive) Events\nEvents A and B are disjoint if they cannot occur simultaneously:\nP(A \\cap B) = 0\nExample 2: Rolling a die\n\nLet A = “rolling a 6” and B = “rolling an odd number”\nP(A \\cap B) = 0 (can’t be both 6 and odd)\nThese events are disjoint\n\n\n\n17.12.5.3 Why Disjoint Events are Always Dependent\nLet’s prove that disjoint events (with non-zero probabilities) must be dependent:\n\nFor disjoint events: P(A \\cap B) = 0\nFor events to be independent, we need: P(A \\cap B) = P(A) \\cdot P(B)\nTherefore, for disjoint events to be independent:\n\n0 = P(A) \\cdot P(B) (from 1 and 2)\nThis equation is only true if either P(A) = 0 or P(B) = 0\nBut if either probability is 0, the event is impossible and trivial\n\nFor any non-trivial disjoint events:\n\nAssume P(A) &gt; 0 and P(B) &gt; 0 (considering non-trivial cases)\nThen P(B|A) = \\frac{P(A \\cap B)}{P(A)} = \\frac{0}{P(A)} = 0\nSince P(B) &gt; 0 (by our assumption of non-trivial cases)\nWe have P(B|A) = 0 \\neq P(B)\nThis inequality proves the events are dependent\n\n\nThe assumption P(B) \\neq 0 is crucial because:\n\nIf we allowed P(B) = 0, then P(B|A) = P(B) would be true (both equal to 0)\nThis would mean the events are technically independent\nBut this is a trivial case where B is an impossible event\n\nExample 3: Rolling a die illustrates dependence\n\nLet A = “rolling a 1” and B = “rolling a 2”\nP(A) = \\frac{1}{6} and P(B) = \\frac{1}{6} (both non-zero)\nP(B|A) = 0 (if we rolled a 1, we definitely didn’t roll a 2)\nBut P(B) = \\frac{1}{6}\nTherefore P(B|A) \\neq P(B), showing dependence\n\n\n\n17.12.5.4 Key Insights\n\nIndependence means events don’t affect each other’s probabilities\nDisjoint means events can’t occur together\nThese concepts are almost opposites:\n\nIndependent events can occur together\nDisjoint events must affect each other’s probabilities\n\nThe only case where events can be both independent and disjoint is when at least one event has probability 0 (impossible event)\n\nThis understanding is crucial for correctly applying probability rules and avoiding common misconceptions in probability calculations.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#problem-solutions-2",
    "href": "probability_en.html#problem-solutions-2",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "17.13 Problem Solutions (2)",
    "text": "17.13 Problem Solutions (2)\n\n17.13.1 Problem 1: Colored Balls - At Least One Red\nQuestion: A bag contains 5 red and 3 blue marbles. Two marbles are drawn simultaneously from the bag. What is the probability that at least one marble is red?\nDetailed Solution:\n\n17.13.1.1 Approach 1: Using Tree Diagram and Complement Rule\nThe key idea here is that finding the probability of “at least one red” directly can be complex, but finding its complement - “no red balls” (all blue) - is simpler.\nThen we can use P(\\text{at least one red}) = 1 - P(\\text{no red}).\nLet’s visualize this with a diagram:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[First Draw]\n    B --&gt; C[\"Blue (3/8)\"]\n    B --&gt; D[\"Red (5/8)\"]\n    C --&gt; E[\"Blue (2/7)\"]\n    C --&gt; F[\"Red (5/7)\"]\n    D --&gt; G[\"Blue (3/7)\"]\n    D --&gt; H[\"Red (4/7)\"]\n    \n    E --&gt; I[\"P = 3/8 * 2/7&lt;br/&gt;All Blue\"]\n    F --&gt; J[\"P = 3/8 * 5/7&lt;br/&gt;At least one Red\"]\n    G --&gt; K[\"P = 5/8 * 3/7&lt;br/&gt;At least one Red\"]\n    H --&gt; L[\"P = 5/8 * 4/7&lt;br/&gt;At least one Red\"]\n\n    style I fill:#f9f9f9,stroke:#333\n    style J fill:#f9f9f9,stroke:#333\n    style K fill:#f9f9f9,stroke:#333\n    style L fill:#f9f9f9,stroke:#333\n\n\n\n\n\n\nNow let’s solve using the complement rule:\nP(\\text{at least one red}) = 1 - P(\\text{no red})\nTo get no red marbles, we need to draw both blue marbles:\n\nTotal marbles: 8\nBlue marbles: 3\nP(\\text{first blue}) = \\frac{3}{8}\nP(\\text{second blue}|\\text{first blue}) = \\frac{2}{7}\n\nP(\\text{no red}) = P(\\text{both blue}) = \\frac{3}{8} \\times \\frac{2}{7} = \\frac{6}{56} = \\frac{3}{28}\nTherefore:\nP(\\text{at least one red}) = 1 - \\frac{3}{28} = \\frac{25}{28} \\approx 0.893 or about 89.3%\n\n\n17.13.1.2 Approach 2: Using Combinations\nThe combinations approach involves finding all possible ways to select 2 marbles out of 8, then subtracting the unfavorable outcomes (selecting 2 blue marbles).\nLet’s understand combinations first:\n\nA combination represents the number of ways to select r items from n items where order doesn’t matter\nNotation: C(n,r) or \\binom{n}{r}\nFormula: C(n,r) = \\frac{n!}{r!(n-r)!}\n\nFor this problem:\n\nTotal possible outcomes = C(8,2) = 28 ways to select 2 marbles from 8\nUnfavorable outcomes = C(3,2) = 3 ways to select 2 blue marbles from 3 blue marbles\nFavorable outcomes = C(8,2) - C(3,2) = 28 - 3 = 25\n\nTherefore:\nP(\\text{at least one red}) = \\frac{25}{28} \\approx 0.893 or about 89.3%\nBoth methods give us the same result! The combinations approach is often more elegant for problems involving simultaneous selection, while the tree diagram approach helps visualize the problem better and is particularly useful when events happen in sequence.\n\n\n\n17.13.2 Problem 2: Probability of Drawing Diamonds or Tens\nFrom a standard deck of 52 cards, find the probability of drawing either a diamond or a ten.\n\n17.13.2.1 Setup\nLet’s define our events:\n\nLet D = “drawing a diamond”\nLet T = “drawing a ten”\n\nWe need to find P(D \\cup T)\n\n\n17.13.2.2 Solution Using the Addition Rule\n\nIndividual Probabilities\nFor diamonds:\n\nNumber of diamonds = 13\nP(D) = \\frac{13}{52} = \\frac{1}{4}\n\nFor tens:\n\nNumber of tens = 4\nP(T) = \\frac{4}{52} = \\frac{1}{13}\n\nIntersection\n\nThe ten of diamonds is counted in both events\nP(D \\cap T) = \\frac{1}{52}\n\nAddition Rule\nP(D \\cup T) = P(D) + P(T) - P(D \\cap T)\n= \\frac{13}{52} + \\frac{4}{52} - \\frac{1}{52}\n= \\frac{16}{52} - \\frac{1}{52}\n= \\frac{15}{52}\n\\approx 0.288 or about 28.8%\n\n\n\n17.13.2.3 Verification\nWe can verify this result is reasonable because:\n\nUpper Bound Check\n\nIf we simply added P(D) and P(T): \\frac{13}{52} + \\frac{4}{52} = \\frac{17}{52}\nOur answer must be less than this due to double counting\n\nLower Bound Check\n\nOur answer must be greater than the larger individual probability (\\frac{13}{52})\n\\frac{15}{52} &gt; \\frac{13}{52} ✓\n\n\n\n\n17.13.2.4 Teaching Notes\nThis problem illustrates several important concepts:\n\nAddition Rule Application\n\nWhy we can’t simply add probabilities\nThe role of intersection in avoiding double counting\n\nFraction Arithmetic\n\nWorking with common denominators\nSimplifying fractions (if desired)\n\nSet Theory Visualization\n\nThe problem can be illustrated with a Venn diagram\nShows why subtraction of intersection is necessary\n\nReasonableness Checks\n\nUsing bounds to verify answers\nUnderstanding why certain values are impossible\n\n\n\n\n\n17.13.3 Problem 3a: Introduction to Conditional Probability\nA tech company has 100 employees who work on various projects. The company records show that:\n\n60 employees work on Project A\n45 employees work on Project B\n25 employees work on both projects\n\nThe HR manager randomly selects one employee. Given that this employee works on Project A, what is the probability they also work on Project B?\n\n17.13.3.1 Step-by-Step Solution\n\nDefine Events\n\nLet A = “employee works on Project A”\nLet B = “employee works on Project B”\nWe need to find P(B|A)\n\nReview the Conditional Probability Formula\nP(B|A) = \\frac{P(A \\cap B)}{P(A)}\nIdentify Known Values\n\nTotal employees: n = 100\nNumber working on A: n_A = 60\nNumber working on B: n_B = 45\nNumber working on both: n_{A \\cap B} = 25\n\nCalculate Probabilities\n\nP(A) = \\frac{60}{100} = 0.6\nP(A \\cap B) = \\frac{25}{100} = 0.25\n\nApply the Formula\nP(B|A) = \\frac{0.25}{0.6} = \\frac{25}{60} \\approx 0.417\n\n\n\n17.13.3.2 Interpretation\nThere is about a 41.7% chance that an employee works on Project B, given that they work on Project A. In other words, among the 60 employees who work on Project A, 25 of them (41.7%) also work on Project B.\n\n\n17.13.3.3 Teaching Notes\nThis problem helps students understand:\n\nBasic Concepts\n\nThe difference between joint probability P(A \\cap B) and conditional probability P(B|A)\nWhy P(B|A) is not the same as P(A \\cap B)\nThe role of the denominator P(A) in “restricting the sample space”\n\nVisual Representation\n\nThe problem can be illustrated with a Venn diagram:\n\nOne circle for Project A (60)\nOne circle for Project B (45)\nOverlap shows both projects (25)\nTotal space represents all employees (100)\n\n\nCommon Misconceptions\n\nStudents often confuse P(B|A) with P(A \\cap B)\nThey might think P(B|A) = P(B)\nThey might mix up P(B|A) and P(A|B)\n\nExtensions\n\nCalculate P(A|B) for comparison\nFind the probability of working on exactly one project\nConsider what happens if projects were independent\n\n\n\n\n\n17.13.4 Problem 3b: Colored Balls with Replacement and Addition\nQuestion: A box contains 5 red and 3 green balls. One ball is drawn at random, its color is noted, and it is replaced back. Then one more ball of the same color is added. Then a second ball is drawn. What is the probability that both balls drawn are green?\nDetailed Solution:\nThis is a sequential probability problem where the probability of the second event depends on the outcome of the first. Let’s solve it step by step:\n\nDefine our events:\n\nG₁ = first ball is green\nG₂ = second ball is green\nWe want P(G₁ ∩ G₂)\n\nCalculate P(G₁):\n\nInitially: 3 green balls out of 8 total\nP(G_1) = \\frac{3}{8}\n\nCalculate P(G₂|G₁):\n\nIf first ball was green:\n\nAfter replacement and adding another green: 4 green balls out of 9 total\n\nP(G_2|G_1) = \\frac{4}{9}\n\nApply the multiplication rule:\nP(G_1 \\cap G_2) = P(G_1) \\cdot P(G_2|G_1) = \\frac{3}{8} \\cdot \\frac{4}{9} = \\frac{12}{72} = \\frac{1}{6} \\approx 0.167 or about 16.7%\n\nUnderstanding the Solution:\n\nThe probability is relatively low because we need two specific events to occur in sequence\nThe addition of a ball of the same color as the first draw creates a dependency between the draws\nIf we had simply replaced the first ball without adding another, the draws would have been independent\n\n\n\n17.13.5 Problem 4a: Bayesian Analysis of Medical Test Results\nA medical test for disease D has the following characteristics:\n\nSensitivity (true positive rate): P(T=1|D=1) = 0.95\nSpecificity (true negative rate): P(T=0|D=0) = 0.95\nPrior probability (disease prevalence): P(D=1) = 0.001 (1/1000)\nTest result for Alicia: Positive (T=1)\n\nWe need to find the posterior probability that Alicia has the disease given a positive test result: P(D=1|T=1)\n\n17.13.5.1 Derivation Using Bayes’ Theorem\nStarting with the conditional probability formula:\nP(D=1|T=1) = \\frac{P(T=1|D=1)P(D=1)}{P(T=1)}\nThe denominator P(T=1) can be expanded using the law of total probability:\nP(T=1) = P(T=1|D=1)P(D=1) + P(T=1|D=0)P(D=0)\n\n\n17.13.5.2 Components Analysis\n\nPrior: P(D=1) = 0.001\n\nComplement: P(D=0) = 0.999\n\nLikelihood:\n\nP(T=1|D=1) = 0.95 (sensitivity)\nP(T=0|D=0) = 0.95 (specificity)\nP(T=1|D=0) = 1 - P(T=0|D=0) = 0.05 (false positive rate)\n\nTotal Probability (denominator): P(T=1) = (0.95)(0.001) + (0.05)(0.999) = 0.00095 + 0.04995 = 0.0509\nPosterior Calculation:\n\nP(D=1|T=1) = \\frac{(0.95)(0.001)}{0.0509} = \\frac{0.00095}{0.0509} \\approx 0.0187\n\n\n17.13.5.3 Interpretation\nDespite receiving a positive test result, the probability that Alicia has disease D is only about 1.87%. This counterintuitive result is known as the “Bayesian flip” or “base rate fallacy.”\n\n\n17.13.5.4 Why is the Probability So Low?\n\nBase Rate Consideration:\n\nThe very low prevalence (1/1000) means that in a population of 1000 women:\n\n1 woman has the disease\n999 women don’t have the disease\n\n\nTest Results in Population:\n\nOf the 1 woman with disease:\n\n0.95 will test positive (true positive)\n\nOf the 999 women without disease:\n\nAbout 50 will test positive (false positives)\n\n\nRatio Analysis:\n\nAmong all positive tests (≈51), only about 1 is a true positive\nThis explains why P(D=1|T=1) is so low\n\n\n\n\n17.13.5.5 Teaching Notes\nThis problem illustrates several important concepts:\n\nThe distinction between conditional probabilities:\n\nP(T=1|D=1) (sensitivity)\nP(D=1|T=1) (positive predictive value)\n\nThe crucial role of base rates in Bayesian reasoning\nWhy medical professionals should:\n\nConsider prevalence when interpreting test results\nBe cautious about testing asymptomatic patients\nConsider confirmatory testing for positive results\n\nThe importance of communicating probabilistic information effectively to patients\nThe mathematical relationship between:\n\nPrior probabilities\nTest characteristics (sensitivity/specificity)\nPosterior probabilities\n\n\n\n\n\n17.13.6 Problem 4b: COVID-19 Test Analysis\nQuestion: Given a COVID-19 test with:\n\nSensitivity (P(T=1|D=1)) = 87.5%\nSpecificity (P(T=0|D=0)) = 97.5%\nDisease prevalence (P(D=1)) = 10% Find P(D=1|T=1), the probability that a person with a positive test actually has the disease.\n\nDetailed Solution:\nThis is a perfect application of Bayes’ Theorem. Let’s break it down:\n\nDefine our variables:\n\nD=1: Person has COVID-19\nD=0: Person doesn’t have COVID-19\nT=1: Test is positive\nT=0: Test is negative\n\nGiven information:\n\nP(T=1|D=1) = 0.875 (sensitivity)\nP(T=0|D=0) = 0.975 (specificity)\nP(D=1) = 0.1 (prevalence)\n\nCalculate additional probabilities:\n\nP(D=0) = 1 - P(D=1) = 0.9\nP(T=1|D=0) = 1 - P(T=0|D=0) = 0.025 (false positive rate)\n\nApply Bayes’ Theorem: P(D=1|T=1) = \\frac{P(T=1|D=1) \\cdot P(D=1)}{P(T=1)}\nCalculate P(T=1) using the law of total probability: P(T=1) = P(T=1|D=1)P(D=1) + P(T=1|D=0)P(D=0) = (0.875)(0.1) + (0.025)(0.9) = 0.0875 + 0.0225 = 0.11\nNow we can complete Bayes’ Theorem: P(D=1|T=1) = \\frac{(0.875)(0.1)}{0.11} = \\frac{0.0875}{0.11} \\approx 0.795 or about 79.5%\n\nUnderstanding the Result:\nThis result tells us that even with a positive test, there’s still about a 20.5% chance that the person doesn’t have COVID-19. This might seem surprising, but it’s due to the relatively low prevalence of the disease (10%) in the population. This is known as the base rate fallacy - even a test with good sensitivity and specificity can have a significant false positive rate when the condition being tested for is rare.\n\n\n17.13.7 Problem 5: Conditional Probability: Marble Drawing with Coin Flip\nWe have a probability experiment involving two boxes of marbles and a fair coin:\nBox X1:\n\n2 black marbles\n3 red marbles\nTotal: 5 marbles\n\nBox X2:\n\n1 black marble\n1 red marble\nTotal: 2 marbles\n\nA fair coin is flipped to select the box (heads for X1, tails for X2), then one marble is drawn.\nVisual Representation\nLet’s create a tree diagram to visualize all possible outcomes and their probabilities:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[X1 1/2]\n    A --&gt; C[X2 1/2]\n    \n    B --&gt; D[Black 2/5]\n    B --&gt; E[Red 3/5]\n    \n    C --&gt; F[Black 1/2]\n    C --&gt; G[Red 1/2]\n    \n    D --&gt; H[Black & X1]\n    E --&gt; I[Red & X1]\n    F --&gt; J[Black & X2]\n    G --&gt; K[Red & X2]\n\n\n\n\n\n\n\n17.13.7.1 Solution\nLet’s solve each part step by step:\n\n\n17.13.7.2 P(Black | X1)\nThis is the probability of drawing a black marble given that we selected Box X1.\nP(Black | X1) = \\frac{\\text{Number of black marbles in X1}}{\\text{Total marbles in X1}} = \\frac{2}{5}\nThis is a direct probability from the contents of Box X1. We only consider Box X1’s marbles since we’re given that Box X1 was selected.\n\n\n17.13.7.3 P(Black and X1)\nThis is the probability of both selecting Box X1 and drawing a black marble.\nP(Black and X1) = P(X1) × P(Black | X1) = \\frac{1}{2} \\times \\frac{2}{5} = \\frac{1}{5}\nWe multiply these probabilities because both events must occur (intersection).\n\n\n17.13.7.4 P(Black)\nThis is the total probability of drawing a black marble from either box. We use the law of total probability:\nP(Black) = P(X1) × P(Black | X1) + P(X2) × P(Black | X2) = \\frac{1}{2} \\times \\frac{2}{5} + \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{5} + \\frac{1}{4} = \\frac{4}{20} + \\frac{5}{20} = \\frac{9}{20}\n\n\n17.13.7.5 P(X1 | Black)\nThis is the probability that we selected Box X1 given that we drew a black marble. We use Bayes’ Theorem:\nP(X1 | Black) = \\frac{P(Black | X1) \\times P(X1)}{P(Black)} = \\frac{\\frac{2}{5} \\times \\frac{1}{2}}{\\frac{9}{20}} = \\frac{\\frac{1}{5}}{\\frac{9}{20}} = \\frac{4}{9}\n\n\n17.13.7.6 Key Concepts Demonstrated\n\nConditional Probability: Shown in P(Black | X1), where we consider probability within a subset of outcomes\nMultiplication Rule: Used in finding P(Black and X1), where we multiply probabilities of sequential events\nLaw of Total Probability: Applied in finding P(Black), where we consider all possible ways an event can occur\nBayes’ Theorem: Used to find P(X1 | Black), reversing the direction of conditioning\n\n\n\n\n17.13.8 Problem 6: Probability of Intersecting Events and Independence Analysis\nYou roll a fair die. What is the probability of getting an even number (A) and the number greater or equal to 4 (B)? Are events A and B independent?\nLet’s explore this problem by first understanding what each event means, then calculating their probabilities both separately and together, and finally examining their independence.\n\n17.13.8.1 Understanding the Events\nLet’s first identify what numbers satisfy each condition on a standard six-sided die:\nEvent A (Even numbers): {2, 4, 6} Event B (Numbers ≥ 4): {4, 5, 6}\nWe can visualize this using a Venn diagram:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[\"R (3/6)\"]\n    A --&gt; C[\"B (2/6)\"]\n    A --&gt; D[\"Y (1/6)\"]\n    \n    B --&gt; E[\"B (2/5)\"]\n    B --&gt; F[\"Y (1/5)\"]\n    B --&gt; G[\"R (2/5)\"]\n    \n    C --&gt; H[\"R (3/5)\"]\n    C --&gt; I[\"Y (1/5)\"]\n    C --&gt; J[\"B (1/5)\"]\n    \n    D --&gt; K[\"R (3/5)\"]\n    D --&gt; L[\"B (2/5)\"]\n    D --&gt; M[\"Y (0/5)\"]\n    \n    E --&gt; N[\"RB (Success)\"]\n    F --&gt; O[\"RY (Success)\"]\n    G --&gt; P[\"RR (Fail)\"]\n    H --&gt; Q[\"BR (Success)\"]\n    I --&gt; R[\"BY (Success)\"]\n    J --&gt; S[\"BB (Fail)\"]\n    K --&gt; T[\"YR (Success)\"]\n    L --&gt; U[\"YB (Success)\"]\n    M --&gt; V[\"YY (Fail)\"]\n\n\n\n\n\n\n\n\n17.13.8.2 Calculating P(A ∩ B)\nTo find the probability of getting both an even number AND a number greater than or equal to 4:\n\nFirst, let’s identify the numbers that satisfy both conditions:\n\nMust be even AND ≥ 4\nNumbers that satisfy both: {4, 6}\n\nTherefore: P(A ∩ B) = \\frac{\\text{number of favorable outcomes}}{\\text{total number of possible outcomes}} = \\frac{2}{6} = \\frac{1}{3}\n\n\n\n17.13.8.3 Testing for Independence\nTo determine if events A and B are independent, we need to check if: P(A ∩ B) = P(A) × P(B)\nLet’s calculate each probability:\n\nP(A) = P(even number) = \\frac{3}{6} = \\frac{1}{2}\n\nFavorable outcomes: {2, 4, 6}\n\nP(B) = P(number ≥ 4) = \\frac{3}{6} = \\frac{1}{2}\n\nFavorable outcomes: {4, 5, 6}\n\nP(A) × P(B) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}\nCompare:\n\nP(A ∩ B) = \\frac{1}{3}\nP(A) × P(B) = \\frac{1}{4}\n\n\nSince \\frac{1}{3} \\neq \\frac{1}{4}, events A and B are NOT independent.\n\n\n17.13.8.4 Understanding the Meaning of Dependence\nThis dependence makes intuitive sense because:\n\nKnowing a number is even affects the probability it’s ≥ 4\nIf we know we rolled an even number, there are three possibilities (2, 4, 6)\nWithin these possibilities, the probability of getting ≥ 4 is \\frac{2}{3}, not \\frac{1}{2}\n\nThis illustrates an important principle: events can be dependent even when they don’t seem directly related. The overlap in their outcome spaces creates a subtle but measurable dependence.\n\n\n17.13.8.5 Teaching Extension\nTo deepen understanding, consider this question: How would the independence calculation change if we used “numbers less than 4” instead of “numbers greater than or equal to 4”? This variation helps illustrate how the structure of event spaces influences their independence.\n\n\n\n17.13.9 Problem 7: The Monty Hall Problem - Two Solution Approaches\nLet’s analyze this fascinating probability problem that has puzzled many people, including mathematicians. We’ll solve it using both a tree diagram and conditional probability to build a complete understanding.\n\n17.13.9.1 Problem Statement\nThe Monty Hall problem:\n\nThere are three doors: behind one is a car, behind the others are goats\nYou pick a door\nMonty Hall (who knows what’s behind each door) opens another door, always showing a goat\nYou’re offered the chance to switch to the remaining door\nQuestion: Should you switch? What’s the probability of winning if you switch vs. if you stay?\n\n\n\n17.13.9.2 Approach 1: Tree Diagram Solution\nLet’s visualize all possible scenarios:\n\n\n\n\n\ngraph TD\n    A[Initial Choice] --&gt; B[Car 1/3]\n    A --&gt; C[Goat1 1/3]\n    A --&gt; D[Goat2 1/3]\n    \n    B --&gt; E[Monty Shows Goat2]\n    B --&gt; F[Monty Shows Goat1]\n    \n    C --&gt; G[Monty Must Show Goat2]\n    D --&gt; H[Monty Must Show Goat1]\n    \n    E --&gt; I[Switch loses]\n    F --&gt; J[Switch loses]\n    G --&gt; K[Switch wins]\n    H --&gt; L[Switch wins]\n\n    style I fill:#ffcccc\n    style J fill:#ffcccc\n    style K fill:#ccffcc\n    style L fill:#ccffcc\n\n\n\n\n\n\nAnalyzing the outcomes: 1. If you initially picked the car (1/3 chance): - Monty can show either goat - Switching loses\n\nIf you initially picked a goat (2/3 chance):\n\nMonty must show the other goat\nSwitching wins\n\n\nTherefore:\n\nP(win if stay) = \\frac{1}{3}\nP(win if switch) = \\frac{2}{3}\n\n\n\n17.13.9.3 Approach 2: Conditional Probability Solution\nLet’s use Bayes’ Theorem to solve this. Define events:\n\nC₁: Car is behind Door 1 (your initial choice)\nM₂: Monty opens Door 2 showing a goat\n\nP(Car behind Door 3 | Monty opens Door 2) = ?\nWe can write: P(Car in 3 | M₂) = \\frac{P(M₂|Car in 3) \\times P(Car in 3)}{P(M₂)}\nLet’s calculate each term:\n\nP(Car in 3) = \\frac{1}{3} (prior probability)\nP(M₂|Car in 3) = 1 (Monty must open Door 2)\nP(M₂) = P(M₂|Car in 1) × P(Car in 1) + P(M₂|Car in 2) × P(Car in 2) + P(M₂|Car in 3) × P(Car in 3) = \\frac{1}{2} \\times \\frac{1}{3} + 0 \\times \\frac{1}{3} + 1 \\times \\frac{1}{3} = \\frac{1}{6} + \\frac{1}{3} = \\frac{1}{2}\n\nTherefore:\nP(Car in 3 | M₂) = \\frac{1 \\times \\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}\n\n\n17.13.9.4 Key Insights\n\nWhy Intuition Fails:\n\nPeople often think it’s 50-50 after Monty opens a door\nThis ignores the crucial fact that Monty’s choice is informed, not random\nHis action provides information that should update our probabilities\n\nInformation Value:\n\nMonty’s choice is constrained (must show a goat)\nThis constraint carries information\nThe probability shifts from the initial \\frac{1}{3} to \\frac{2}{3} for switching\n\nSimulation Verification: We could write a simple program to simulate this game thousands of times, and it would confirm these probabilities. The most convincing evidence is often seeing the results empirically.\n\n\n\n\n17.13.10 Problem 8: The Bertrand Box Paradox - A Teaching Analysis\n\n17.13.10.1 Understanding the Problem Setup\nFirst, let’s clearly state what we’re dealing with:\n\nWe have three boxes:\n\nBox 1: Contains two gold coins (GG)\nBox 2: Contains two silver coins (SS)\nBox 3: Contains one gold and one silver coin (GS)\n\nThe process:\n\nWe randomly select a box\nWe randomly draw one coin from the chosen box\nIf we see a gold coin, what’s the probability it came from the gold-only box?\n\n\nMost people intuitively answer \\frac{1}{2}, but let’s discover why this isn’t correct.\n\n\n17.13.10.2 Approach 1: Tree Diagram Analysis\nLet’s visualize all possible paths and outcomes:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[Box GG 1/3]\n    A --&gt; C[Box SS 1/3]\n    A --&gt; D[Box GS 1/3]\n    \n    B --&gt; E[Draw G 1]\n    C --&gt; F[Draw S 1]\n    D --&gt; G[Draw G 1/2]\n    D --&gt; H[Draw S 1/2]\n    \n    E --&gt; I[Saw Gold]\n    G --&gt; I[Saw Gold]\n    F --&gt; J[Saw Silver]\n    H --&gt; J[Saw Silver]\n    \n    style I fill:#FFD700\n    style J fill:#C0C0C0\n\n\n\n\n\n\nFollowing the paths where we see gold:\n\nFrom Box GG (probability = \\frac{1}{3} \\times 1 = \\frac{1}{3})\nFrom Box GS (probability = \\frac{1}{3} \\times \\frac{1}{2} = \\frac{1}{6})\n\nTherefore:\n\nTotal probability of seeing gold = \\frac{1}{3} + \\frac{1}{6} = \\frac{1}{2}\nGiven we saw gold, probability it came from Box GG = \\frac{\\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}\n\n\n\n17.13.10.3 Approach 2: Bayes’ Theorem Solution\nLet’s solve this formally using Bayes’ Theorem:\nP(Box GG | Gold) = \\frac{P(Gold|Box GG) \\times P(Box GG)}{P(Gold)}\nLet’s calculate each component:\n\nP(Gold|Box GG) = 1 (certainty of drawing gold)\nP(Box GG) = \\frac{1}{3} (equal box probabilities)\nP(Gold) = \\frac{1}{3} \\times 1 + \\frac{1}{3} \\times 0 + \\frac{1}{3} \\times \\frac{1}{2} = \\frac{1}{2}\n\nPutting it together:\nP(Box GG | Gold) = \\frac{1 \\times \\frac{1}{3}}{\\frac{1}{2}} = \\frac{2}{3}\n\n\n17.13.10.4 Why This Is Counterintuitive\nThe reason many people get this wrong reveals interesting aspects of how we think about probability:\n\nThe Setup Trick: People often think, “If I see gold, it must be from either Box GG or Box GS, so it’s 50-50.” This ignores the fact that Box GG has twice the opportunity to show gold.\nPrior vs Posterior: The problem shows how observing evidence (seeing gold) updates our prior probability (\\frac{1}{3}) to a posterior probability (\\frac{2}{3}).\nSample Space Structure: Box GG contributes more gold coins to the total sample space of possible draws than Box GS does.\n\n\n\n17.13.10.5 A Teaching Analogy\nThink of it this way: Imagine three people named GG, SS, and GS.\n\nGG always raises both hands when asked\nSS never raises hands\nGS raises one hand\n\nIf you see a raised hand randomly, it’s more likely to belong to GG (who contributes two hands) than GS (who contributes only one).\n\n\n17.13.10.6 Extension for Deeper Understanding\nTo reinforce this concept, consider: How would the probabilities change if we had:\n\nThree coins in each box?\nDifferent prior probabilities for selecting each box?\nThe ability to see both coins but only after selecting a box?",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "probability_en.html#appendix-1.-advanced-counting-in-probability-a-student-guide",
    "href": "probability_en.html#appendix-1.-advanced-counting-in-probability-a-student-guide",
    "title": "17  Introduction to (Discrete) Probability",
    "section": "17.14 Appendix 1. Advanced Counting in Probability: A Student Guide (*)",
    "text": "17.14 Appendix 1. Advanced Counting in Probability: A Student Guide (*)\n\n17.14.1 Poker Hands: A Window into Complex Counting\nPoker hands provide some of the most interesting examples for understanding counting in probability. They’re perfect for learning because they combine multiple counting principles and help us understand common pitfalls. Let’s explore these concepts step by step.\n\n\n17.14.2 Understanding Our Sample Space\nBefore we dive into specific hands, let’s understand what we’re working with. A poker hand consists of 5 cards drawn from a standard 52-card deck. Understanding the sample space is crucial because it forms the foundation of all our probability calculations.\nThe total number of possible poker hands represents how many different ways we can select 5 cards from 52 cards, where the order doesn’t matter (getting ace-king-queen is the same hand as getting king-queen-ace), we can’t reuse cards (we can’t have the ace of spades twice in our hand), and we must take exactly 5 cards (not more, not less).\nThis means we’re dealing with combinations. Let’s calculate this step by step:\n\\binom{52}{5} = \\frac{52!}{5!(52-5)!} = \\frac{52!}{5!(47)!} = \\frac{52 \\cdot 51 \\cdot 50 \\cdot 49 \\cdot 48}{5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1} = 2,598,960\nThis number, 2,598,960, will be our denominator for calculating the probability of any specific poker hand.\n\n\n17.14.3 Understanding Two Pairs: A Careful Counting Approach\nTwo pairs is one of the most interesting hands for understanding counting principles. To get two pairs, we need:\n\nTwo cards of one rank\nTwo cards of another rank\nOne card of a third rank (the kicker)\n\nLet’s build this hand step by step, being careful to understand each choice we make:\nFirst, let’s select our ranks. We might think we should just choose two ranks from 13 for our pairs using \\binom{13}{2}, but this approach hides some important subtleties. Instead, let’s think about the actual process of constructing the hand:\n\nWe have 13 possible ranks for our first pair\nAfter choosing the first pair’s rank, we have 12 ranks left for our second pair\nAfter choosing both pair ranks, we have 11 ranks left for our kicker\n\nFor each rank we’ve chosen, we need to select specific cards:\n\nFor our first pair: we choose 2 cards from the 4 available cards of that rank: \\binom{4}{2} = 6 ways\nFor our second pair: again \\binom{4}{2} = 6 ways\nFor our kicker: we choose 1 card from 4: \\binom{4}{1} = 4 ways\n\nNow, here’s where many students get confused: Does it matter which pair we count “first” and which we count “second”? The answer reveals a deep truth about counting in probability.\nLet’s use a concrete example. Suppose we want two pairs with Aces and Kings, and a Two as our kicker. We could:\n\nChoose Aces as our first pair, then Kings as our second pair\nChoose Kings as our first pair, then Aces as our second pair\n\nThese lead to the exact same hand type, but we need to count both paths to this hand because they represent different ways of constructing it. It’s similar to how we can make a sandwich by putting either cheese slice on first - the order of construction matters for counting all possibilities, even though the final sandwich is the same.\nThis is why our final formula multiplies all these independent choices:\n13 (first pair rank) × 12 (second pair rank) × 11 (kicker rank) × \\binom{4}{2} (first pair cards) × \\binom{4}{2} (second pair cards) × \\binom{4}{1} (kicker card)\nEach term represents a separate decision we make in constructing the hand. While the order of these decisions doesn’t affect the final hand we get, we need to account for all possible ways to arrive at each hand to get the correct total.\nLet’s calculate the total probability:\nP(\\text{two pairs}) = \\frac{13 \\cdot 12 \\cdot 11 \\cdot \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}}{2,598,960} = \\frac{123,552}{2,598,960} \\approx 0.0475\nThis means about 4.75% of all possible poker hands are two pairs.\n\n\n17.14.4 Understanding Full House: A Different Counting Challenge\nA full house gives us a perfect contrast to two pairs. While both hands involve multiple cards of the same rank, the counting process reveals important differences in how we approach probability problems.\nIn a full house, we need: - Three cards of one rank (called “three of a kind”) - Two cards of another rank (a pair)\nLet’s think about why counting a full house is different from counting two pairs. With two pairs, we had to be careful about the order of selecting our pairs. With a full house, we have a natural order: we must choose our three of a kind first (because it’s distinct from the pair), then choose our pair.\nLet’s count step by step:\n\nFor the three of a kind:\n\nChoose the rank: 13 possible ranks\nChoose which three cards of that rank: \\binom{4}{3} = 4 ways\n\nFor the pair:\n\nChoose the rank: 12 remaining ranks\nChoose which two cards of that rank: \\binom{4}{2} = 6 ways\n\n\nMultiplying these together:\n13 (three of a kind rank) × \\binom{4}{3} (specific three cards) × 12 (pair rank) × \\binom{4}{2} (specific pair cards)\n= 13 \\cdot 4 \\cdot 12 \\cdot 6 = 3,744\nTherefore:\nP(\\text{full house}) = \\frac{3,744}{2,598,960} \\approx 0.0014\nAbout 0.14% of all poker hands are full houses, making them significantly rarer than two pairs (4.75%). This makes intuitive sense - it’s harder to get three of the same rank plus a pair than to get two pairs plus a kicker.\n\n\n17.14.5 The Birthday Problem: A Beautiful Probability Surprise\nThe birthday problem provides a fascinating connection to our poker probability work, while teaching us something profound about the nature of counting. The classic question is: “How many people need to be in a room for there to be a 50% chance that at least two share a birthday?”\nMost people guess around 183 (half of 365), but the actual answer is just 23 people! Let’s understand why this connects to our previous counting work and why the answer is so surprising.\nFirst, let’s think about what makes this problem different from our poker calculations:\n\nIn poker, we were looking for specific combinations (like two pairs)\nIn the birthday problem, we’re looking for any match at all\n\nThis is similar to the difference between asking: - “What’s the probability of drawing the ace of spades and king of hearts specifically?” - “What’s the probability of drawing any two cards of different ranks?”\nThe second question has many more ways to succeed.\nLet’s solve the birthday problem step by step:\n\nFirst, it’s easier to calculate the probability of no matches\nThen we can subtract from 1 to get the probability of at least one match\n\nFor 23 people, we calculate no matches like this: - First person can have any birthday: \\frac{365}{365} - Second person needs a different birthday: \\frac{364}{365} - Third person needs a different birthday: \\frac{363}{365} And so on until person 23.\nThis gives us:\nP(\\text{no matches}) = \\frac{365}{365} \\cdot \\frac{364}{365} \\cdot \\frac{363}{365} \\cdot ... \\cdot \\frac{343}{365}\n= \\frac{365!}{(365-23)! \\cdot 365^{23}} \\approx 0.492\nTherefore:\nP(\\text{at least one match}) = 1 - 0.492 \\approx 0.508\nThis teaches us something profound about probability: when we’re looking for any match among many possibilities (like in the birthday problem), we often get much higher probabilities than when we’re looking for specific matches (like in poker hands).\n\n\n17.14.6 Lottery Mathematics\nLet’s apply everything we’ve learned to understand lottery probabilities. Consider a typical “6/49” lottery where players choose 6 numbers from 1-49. This gives us a perfect opportunity to apply our counting principles in a real-world context.\nThe fundamental question is: What’s the probability of winning the jackpot (matching all 6 numbers)?\nThis is a combination problem because: - Order doesn’t matter (matching 1-2-3-4-5-6 is the same as matching 6-5-4-3-2-1) - We can’t use the same number twice - We need exactly 6 numbers\nTherefore:\nP(\\text{jackpot}) = \\frac{1}{\\binom{49}{6}} = \\frac{1}{13,983,816}\nThis tiny probability (about 0.0000000715) shows why lottery wins are so rare. But modern lotteries have multiple prize tiers, which gives us a chance to explore more interesting probability calculations.\nConsider matching 5 numbers plus a bonus number. For this, we need to: 1. Match 5 of the 6 winning numbers: \\binom{6}{5} ways to choose which 5 2. Match 1 of the remaining 43 numbers with the bonus: \\binom{43}{1} ways\nTherefore:\nP(\\text{5 + bonus}) = \\frac{\\binom{6}{5} \\cdot \\binom{43}{1}}{\\binom{49}{6}} = \\frac{6 \\cdot 43}{13,983,816} \\approx 0.0000184\nThis shows us how breaking down complex probability problems into simpler parts helps us solve them systematically.\n\n\n17.14.7 Appendix 2. Alternative Approaches to Poker Hand Probabilities (*)\nUnderstanding different ways to calculate the same probability deepens our insight into counting principles. Let’s explore several methods for finding the probabilities of two pairs and full house, seeing how each approach highlights different aspects of the problem.\n\n17.14.7.1 Multiple Paths to Two Pairs Probability\nLet’s start with two pairs. We’ve seen one method, but there are several valid approaches:\nMethod 1: Sequential Selection (Our Original Approach) We build the hand step by step: 1. Choose first pair’s rank: 13 ways 2. Choose second pair’s rank: 12 ways 3. Choose kicker’s rank: 11 ways 4. Choose specific cards for first pair: \\binom{4}{2} ways 5. Choose specific cards for second pair: \\binom{4}{2} ways 6. Choose specific card for kicker: \\binom{4}{1} ways\nThis gives us: P(\\text{two pairs}) = \\frac{13 \\cdot 12 \\cdot 11 \\cdot \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}}{2,598,960}\nMethod 2: Complementary Counting We can find two pairs probability by subtracting the probability of all other possible hands from 1. However, this is more complex than direct counting because we need to know the probabilities of all other poker hands. Still, it serves as a good verification:\nP(\\text{two pairs}) = 1 - P(\\text{high card}) - P(\\text{one pair}) - P(\\text{three of a kind}) - P(\\text{straight}) - P(\\text{flush}) - P(\\text{full house}) - P(\\text{four of a kind}) - P(\\text{straight flush})\nMethod 3: Using Permutations with Adjustment We can use permutations and then adjust for overcounting:\n\nChoose an ordered arrangement of two ranks for pairs: P(13,2) = 13 \\cdot 12\nChoose kicker rank: 11 ways\nChoose specific cards for pairs and kicker: \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}\nDivide by 2 to account for the fact that the order of pairs doesn’t matter\n\nThis gives: P(\\text{two pairs}) = \\frac{P(13,2) \\cdot 11 \\cdot \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}}{2 \\cdot 2,598,960}\nMethod 4: Combination-Based Approach with Multiplication Principle We can separate rank selection from card selection:\n\nFirst, select three ranks: \\binom{13}{3} ways\nFrom these three ranks, designate two for pairs and one for kicker: \\binom{3}{2} ways\nFor each pair rank, select two cards: \\binom{4}{2} \\cdot \\binom{4}{2} ways\nFor the kicker rank, select one card: \\binom{4}{1} ways\n\nThis gives us: P(\\text{two pairs}) = \\frac{\\binom{13}{3} \\cdot \\binom{3}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{2} \\cdot \\binom{4}{1}}{2,598,960}\n\n\n17.14.7.2 Alternative Approaches to Full House Probability\nThe full house probability can also be calculated in several ways:\nMethod 1: Direct Sequential Selection (Our Original Approach) 1. Choose rank for three of a kind: 13 ways 2. Choose specific three cards: \\binom{4}{3} ways 3. Choose rank for pair: 12 ways 4. Choose specific two cards: \\binom{4}{2} ways\nLeading to: P(\\text{full house}) = \\frac{13 \\cdot \\binom{4}{3} \\cdot 12 \\cdot \\binom{4}{2}}{2,598,960}\nMethod 2: Using Combinations with Distribution We can think about it as: 1. Choose two ranks from 13: \\binom{13}{2} ways 2. Designate which rank gets three cards: 2 ways (since either rank could be the three of a kind) 3. Choose specific cards: \\binom{4}{3} \\cdot \\binom{4}{2} ways\nThis gives: P(\\text{full house}) = \\frac{\\binom{13}{2} \\cdot 2 \\cdot \\binom{4}{3} \\cdot \\binom{4}{2}}{2,598,960}\nMethod 3: Using the Multiplication Principle with Sets Think about constructing the hand as selecting two sets of cards: 1. First set: three cards of the same rank from 13 ranks - Choose rank: 13 ways - Choose three cards: \\binom{4}{3} ways 2. Second set: two cards of the same rank from 12 remaining ranks - Choose rank: 12 ways - Choose two cards: \\binom{4}{2} ways\nThis yields the same result: P(\\text{full house}) = \\frac{13 \\cdot \\binom{4}{3} \\cdot 12 \\cdot \\binom{4}{2}}{2,598,960}\nEach method illuminates different aspects of the counting process: - Sequential selection helps us understand the step-by-step construction of hands - Combination-based approaches highlight the underlying structure of the selections - Permutation-based methods with adjustment show how overcounting can be handled systematically\nThe fact that all these methods yield the same result serves as a powerful verification tool. When solving complex probability problems, being able to approach the solution in multiple ways not only confirms our answer but also deepens our understanding of the underlying counting principles.\n\n\n\n17.14.8 Appendix 3: Occupancy Problems and Statistical Physics (*)\nUnderstanding how objects can be distributed into containers forms the foundation for both probability theory and statistical mechanics. Let’s explore this connection, starting with basic counting principles and building up to physical applications.\n\n17.14.8.1 The Basic Occupancy Problem\nImagine we have n identical balls and k distinct boxes. How many ways can we distribute the balls? This simple question leads us to three fundamentally different scenarios that mirror important physical systems:\n\nUnrestricted occupancy (Bose-Einstein statistics)\n\nEach box can hold any number of balls\nThe balls are indistinguishable\nLike photons in quantum states\n\nMaximum one per box (Fermi-Dirac statistics)\n\nEach box can hold at most one ball\nThe balls are indistinguishable\nLike electrons in atomic orbitals\n\nAll arrangements count separately (Maxwell-Boltzmann statistics)\n\nEach box can hold any number of balls\nThe balls are distinguishable\nLike classical gas molecules\n\n\n\n\n17.14.8.2 Stars and Bars: Understanding Unrestricted Occupancy\nLet’s start with the Bose-Einstein case. The “stars and bars” method provides a beautiful way to visualize and count these arrangements.\nImagine n=5 balls and k=3 boxes. We can represent any arrangement as a sequence of stars and bars:\n\nStars (*) represent balls\nBars (|) separate different boxes\n\nFor example:\n\n** | ** | * represents 2 balls in first box, 2 in second, 1 in third\n***** | | represents all 5 balls in first box, none in others\n| ***** | represents all 5 balls in middle box\n\nThe key insight is that we need: - n stars (one for each ball) - k-1 bars (to create k sections)\nTherefore, we’re really just choosing positions for the k-1 bars among n+(k-1) total positions. This gives us:\n\\text{Number of arrangements} = \\binom{n+k-1}{k-1} = \\binom{n+k-1}{n}\n\n\n17.14.8.3 From Counting to Physics\nNow let’s see how these counting principles reveal deep physical truths:\n\nBose-Einstein Statistics (Unrestricted, Indistinguishable)\n\nThink of photons in a laser\nMany particles can occupy same energy state\nTotal arrangements: \\binom{n+k-1}{k-1}\nExample: Light in a cavity\n\nFermi-Dirac Statistics (Restricted, Indistinguishable)\n\nThink of electrons in atoms\nMaximum one particle per state\nTotal arrangements: \\binom{k}{n} if n \\leq k, 0 otherwise\nExample: Electron configuration in atoms\n\nMaxwell-Boltzmann Statistics (Classical, Distinguishable)\n\nThink of gas molecules\nParticles are distinct\nTotal arrangements: k^n\nExample: Air molecules in a room\n\n\n\n\n17.14.8.4 An Intuitive Bridge to Physics\nTo understand why these statistics matter, consider three real scenarios:\n\nPhotons in a Laser (Bose-Einstein) Imagine shining a laser into a mirror cavity. Photons are happy to bunch together in the same quantum state - they’re “social particles.” This is why lasers can produce intense, coherent light.\nElectrons in an Atom (Fermi-Dirac) Electrons are “antisocial” - they refuse to share quantum states (Pauli exclusion principle). This explains atomic structure and why matter is mostly empty space.\nGas Molecules in a Room (Maxwell-Boltzmann) Air molecules bounce around randomly, and we can tell them apart (in principle). This gives us the familiar gas laws and diffusion.\n\n\n\n17.14.8.5 The Power of the Star and Bars Method\nThe stars and bars visualization helps us understand more complex problems. For instance, if we have restrictions on box occupancy:\n\nAt least one ball per box:\n\nFirst put one ball in each box\nThen distribute remaining balls freely\nFormula: \\binom{n-k+k-1}{k-1} = \\binom{n-1}{k-1}\n\nMaximum capacity per box:\n\nUse inclusion-exclusion principle\nSubtract arrangements that violate constraints\nMore complex but same underlying principle\n\n\n\n\n17.14.8.6 Connection to Partition Problems\nThis same framework helps us solve other important problems:\n\nInteger Partitions How many ways can we write n as a sum of positive integers?\n\nLike distributing n balls into unlimited boxes\nEach box represents a different term in the sum\n\nCompositions How many ways can we write n as an ordered sum?\n\nLike distinguishable boxes\nOrder matters here\n\n\nThis connection between simple counting and profound physical phenomena shows the deep unity of mathematics and physics. The same principles that help us count poker hands and lottery combinations govern the behavior of the universe at its most fundamental level.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Introduction to (Discrete) Probability</span>"
    ]
  },
  {
    "objectID": "rv_pdf_en.html",
    "href": "rv_pdf_en.html",
    "title": "19  Random Variables and Probability Distributions",
    "section": "",
    "text": "19.2 Random Variables: Making Outcomes Measurable\nA random variable is a way to assign numbers to the outcomes of a random experiment. Think of it as a function that converts outcomes into numbers, making them measurable and easier to analyze.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "rv_pdf_en.html#models-for-different-data-generating-processes-dgps",
    "href": "rv_pdf_en.html#models-for-different-data-generating-processes-dgps",
    "title": "19  Random Variables and Probability Distributions",
    "section": "19.1 Models for Different Data Generating Processes (DGPs)",
    "text": "19.1 Models for Different Data Generating Processes (DGPs)\nDifferent random experiments follow different patterns. We model these using specific probability distributions:\n\nBernoulli: For single yes/no experiments\n\nExample: Single coin flip\nDGP Assumption: Two outcomes with fixed success probability\n\nBinomial: For counting successes in fixed trials\n\nExample: Number of heads in 10 coin flips\nDGP Assumption: Independent trials with same success probability\n\nPoisson: For counting rare events in an interval\n\nExample: Number of customer arrivals per hour\nDGP Assumption: Events occur independently at constant rate\n\n\n💡 Important Note: These are mathematical models based on assumptions about how data is generated. Their usefulness depends on how well these assumptions match reality.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "rv_pdf_en.html#random-variables-making-outcomes-measurable",
    "href": "rv_pdf_en.html#random-variables-making-outcomes-measurable",
    "title": "19  Random Variables and Probability Distributions",
    "section": "",
    "text": "19.2.1 Example: Rolling a Die\n\nOutcome space: {⚀, ⚁, ⚂, ⚃, ⚄, ⚅}\nRandom variable X: “number of dots showing”\nX converts outcomes to numbers: X(⚀) = 1, X(⚁) = 2, …, X(⚅) = 6\n\n\n\n19.2.2 Example: Flipping a Coin\n\nOutcome space: {Heads, Tails}\nRandom variable Y: “1 if heads, 0 if tails”\nY converts outcomes to numbers: Y(Heads) = 1, Y(Tails) = 0\n\n\n\n19.2.3 Properties of Discrete Random Variables\n\nEach possible value has a probability\nAll probabilities must be ≥ 0\nSum of all probabilities must equal 1",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "rv_pdf_en.html#understanding-probability-distributions",
    "href": "rv_pdf_en.html#understanding-probability-distributions",
    "title": "19  Random Variables and Probability Distributions",
    "section": "19.3 Understanding Probability Distributions",
    "text": "19.3 Understanding Probability Distributions\nA probability distribution is a mathematical description of the probabilities of different possible outcomes in a random experiment. It tells us:\n\nWhat values can occur (the support of the distribution)\nHow likely each value is to occur (the probability of each outcome)\nHow the probabilities are spread across the possible values\n\nFor discrete random variables, we can represent the distribution as:\n\nA probability mass function (PMF) that gives P(X = x) for each possible value x\nA cumulative distribution function (CDF) that gives P(X ≤ x) for each possible value x\n\nKey Properties of Any Probability Distribution:\n\nAll probabilities must be between 0 and 1: 0 \\leq P(X = x) \\leq 1 for all x\nThe sum of all probabilities must equal 1: \\sum P(X = x) = 1\n\nLet’s explore three fundamental discrete distributions:\n\n19.3.1 Uniform Distribution\nThe uniform distribution represents complete randomness - all outcomes are equally likely.\n\n19.3.1.1 Properties\n\nEach outcome has equal probability\nFor n possible outcomes, P(X = x) = 1/n for each x\nMean: E(X) = \\frac{a + b}{2} where a is minimum and b is maximum\nVariance: Var(X) = \\frac{(b-a+1)^2 - 1}{12} for discrete uniform\n\n\n\n19.3.1.2 Example: Fair Die\n\n# Visualization of uniform distribution (die roll)\nlibrary(ggplot2)\n\ndie_data &lt;- data.frame(\n  outcome = 1:6,\n  probability = rep(1/6, 6)\n)\n\nggplot(die_data, aes(x = factor(outcome), y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  geom_text(aes(label = sprintf(\"1/6\")), vjust = -0.5) +\n  labs(title = \"Probability Distribution of a Fair Die Roll\",\n       x = \"Outcome\",\n       y = \"Probability\") +\n  theme_minimal() +\n  ylim(0, 0.5)\n\n\n\n\n\n\n\n\n\n\n\n19.3.2 Bernoulli Distribution\nThe Bernoulli distribution is the simplest probability distribution, modeling a single “yes/no” trial.\n\n19.3.2.1 Properties\n\nOnly two possible outcomes: 0 (failure) or 1 (success)\nControlled by single parameter p (probability of success)\nMean: E(X) = p\nVariance: Var(X) = p(1-p)\n\n\n\n19.3.2.2 Example: Biased Coin\n\n# Visualization of Bernoulli distribution (p = 0.7)\nbernoulli_data &lt;- data.frame(\n  outcome = c(\"Failure (0)\", \"Success (1)\"),\n  probability = c(0.3, 0.7)\n)\n\nggplot(bernoulli_data, aes(x = outcome, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"lightgreen\") +\n  geom_text(aes(label = scales::percent(probability)), vjust = -0.5) +\n  labs(title = \"Bernoulli Distribution (p = 0.7)\",\n       x = \"Outcome\",\n       y = \"Probability\") +\n  theme_minimal() +\n  ylim(0, 1)\n\n\n\n\n\n\n\n\n\n\n\n19.3.3 Binomial Distribution\nThe binomial distribution models the number of successes in n independent Bernoulli trials.\n\n19.3.3.1 Properties\n\nParameters: n (number of trials) and p (probability of success)\nPossible values: 0 to n successes\nMean: E(X) = np\nVariance: Var(X) = np(1-p)\nProbability mass function: P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\n\n\n19.3.3.2 Understanding the Formula\nThe binomial probability formula has three parts:\n\n\\binom{n}{k} - Number of ways to get k successes in n trials\np^k - Probability of k successes\n(1-p)^{n-k} - Probability of (n-k) failures\n\n\n\n19.3.3.3 Visualizing Binomial Distributions\n\nlibrary(ggplot2)\n# Function to calculate binomial probabilities\nbinomial_probs &lt;- function(n, p) {\n  k &lt;- 0:n\n  probs &lt;- dbinom(k, n, p)\n  data.frame(k = k, probability = probs)\n}\n\n# Create plots for different parameters\nggplot(binomial_probs(10, 0.5), aes(x = k, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"salmon\") +\n  labs(title = \"Binomial(n=10, p=0.5)\",\n       x = \"Number of Successes\",\n       y = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(binomial_probs(10, 0.2), aes(x = k, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue\") +\n  labs(title = \"Binomial(n=10, p=0.2)\",\n       x = \"Number of Successes\",\n       y = \"Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n19.3.3.4 Effect of Parameters\nThe shape of the binomial distribution changes with n and p:\n\nEffect of n (number of trials):\n\nLarger n → more possible outcomes\nLarger n → distribution becomes more “bell-shaped”\n\nEffect of p (probability of success):\n\np = 0.5 → symmetric distribution\np &lt; 0.5 → right-skewed distribution\np &gt; 0.5 → left-skewed distribution\n\n\n\n\n19.3.3.5 Real-World Applications\n\nQuality Control\n\nn = number of items inspected\np = probability of defect\nX = number of defective items found\n\nClinical Trials\n\nn = number of patients\np = probability of recovery\nX = number of patients who recover\n\n\n\n\n\n19.3.4 Comparing the Distributions\nKey differences between our three distributions:\n\nUniform Distribution\n\nEqual probability for all outcomes\nUsed when all outcomes are equally likely\nExample: rolling a fair die\n\nBernoulli Distribution\n\nSpecial case of binomial with n = 1\nOnly two possible outcomes\nExample: single coin flip\n\nBinomial Distribution\n\nCounts successes in multiple trials\nCombines multiple Bernoulli trials\nExample: number of heads in 10 coin flips\n\n\n\n\n19.3.5 Interactive R Code for Exploration\n\n# Function to compare distributions\ncompare_distributions &lt;- function(n = 10, p = 0.5) {\n  # Binomial probabilities\n  x &lt;- 0:n\n  binom_probs &lt;- dbinom(x, n, p)\n  \n  # Create data frame\n  df &lt;- data.frame(\n    x = x,\n    probability = binom_probs\n  )\n  \n  # Create plot\n  ggplot(df, aes(x = x, y = probability)) +\n    geom_bar(stat = \"identity\", fill = \"steelblue\", alpha = 0.7) +\n    geom_text(aes(label = round(probability, 3)), vjust = -0.5) +\n    labs(title = sprintf(\"Binomial Distribution (n=%d, p=%.2f)\", n, p),\n         x = \"Number of Successes\",\n         y = \"Probability\") +\n    theme_minimal()\n}\n\n# Try different parameters\ncompare_distributions(n = 5, p = 0.5)\n\n\n\n\n\n\n\ncompare_distributions(n = 10, p = 0.3)\n\n\n\n\n\n\n\ncompare_distributions(n = 20, p = 0.7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplying Binomial Distribution: ESP Probability Analysis\n\n\n\nExtrasensory perception (ESP), also known as a sixth sense, is a claimed ability to perceive information through mental means rather than the five physical senses.\nProblem: A man claims to have extrasensory perception (ESP). To test this claim, a fair coin is flipped 10 times and the man is asked to predict each outcome in advance. He gets 7 out of 10 predictions correct. Calculate the probability of obtaining 7 or more correct predictions out of 10 trials by chance alone (assuming no ESP).\nRemark: Under the null hypothesis of no ESP, the probability of each correct prediction is \\frac{1}{2}.\n\n19.3.6 Preliminary Concepts\n\n19.3.6.1 Bernoulli Variable\nA Bernoulli random variable represents a trial with exactly two possible outcomes: success (1) or failure (0). Each trial has probability p of success and 1-p of failure.\nIn our case, each coin flip prediction is a Bernoulli trial where success means a correct prediction.\n\n\n19.3.6.2 Binomial Distribution\nThe binomial distribution extends the Bernoulli concept to model the sum of n independent Bernoulli trials. It describes the probability of obtaining exactly k successes in these n trials. The probability mass function is:\nP(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\nwhere:\n\nn is the number of trials\nk is the number of successes\np is the probability of success on each trial\n\\binom{n}{k} is the binomial coefficient\n\n\n\n19.3.6.3 Binomial Coefficient\nThe binomial coefficient \\binom{n}{k} represents the number of ways to choose k items from n items, regardless of order. It is calculated as:\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\nFor example, \\binom{10}{7} = \\frac{10!}{7!(10-7)!} = \\frac{10!}{7!3!} = 120\n\n\n\n19.3.7 Problem Solution\nGiven: - n = 10 coin flips - p = \\frac{1}{2} (probability of correct guess without ESP) - Need P(X \\geq 7) = P(X = 7) + P(X = 8) + P(X = 9) + P(X = 10)\nStep 1: Calculate P(X = 7) P(X = 7) = \\binom{10}{7}(\\frac{1}{2})^7(\\frac{1}{2})^3 = 120 \\cdot (\\frac{1}{128}) \\cdot (\\frac{1}{8}) = 120 \\cdot \\frac{1}{1024} = \\frac{120}{1024} \\approx 0.117\nStep 2: Calculate P(X = 8) P(X = 8) = \\binom{10}{8}(\\frac{1}{2})^8(\\frac{1}{2})^2 = 45 \\cdot (\\frac{1}{256}) \\cdot (\\frac{1}{4}) = 45 \\cdot \\frac{1}{1024} = \\frac{45}{1024} \\approx 0.044\nStep 3: Calculate P(X = 9) P(X = 9) = \\binom{10}{9}(\\frac{1}{2})^9(\\frac{1}{2})^1 = 10 \\cdot (\\frac{1}{512}) \\cdot (\\frac{1}{2}) = 10 \\cdot \\frac{1}{1024} = \\frac{10}{1024} \\approx 0.010\nStep 4: Calculate P(X = 10) P(X = 10) = \\binom{10}{10}(\\frac{1}{2})^{10}(\\frac{1}{2})^0 = 1 \\cdot (\\frac{1}{1024}) \\cdot 1 = \\frac{1}{1024} \\approx 0.001\nStep 5: Sum all probabilities P(X \\geq 7) = \\frac{120 + 45 + 10 + 1}{1024} = \\frac{176}{1024} \\approx 0.172\n\n\n19.3.8 Interpretation\nThere is approximately a 17.2% chance that someone without ESP would correctly guess 7 or more coin flips out of 10 by pure chance. This is a relatively high probability, suggesting that getting 7 out of 10 correct predictions is not strong evidence for ESP. Generally, we would want a much smaller probability (e.g., &lt; 5% or &lt; 1%) before considering the results statistically significant evidence for ESP.\n\n\n\n\n\n\n\n\n\nInductive Derivation of the Binomial Distribution Formula\n\n\n\n\nRandom Variable (X): A function that maps outcomes from a sample space to real numbers. In our case:\n\nX = number of successes in n trials\nX maps “Success” to 1 and “Failure” to 0 for each trial\nFor n trials, X takes values in {0, 1, 2, …, n}\n\nProbability Distribution: A function P(X = k) that assigns probabilities to each possible value k of the random variable X, where:\n\nP(X = k) ≥ 0 for all k\n\\sum_{k=0}^n P(X = k) = 1\n\n\nLet’s denote success as S and failure as F, with:\n\nProbability of success p = P(\\text{Success}) = P(X = 1) for a single trial\nProbability of failure q = 1-p = P(\\text{Failure}) = P(X = 0) for a single trial\n\n\n19.3.9 1. One Trial\n\n\n\nOutcome\nWays\nProbability\n\n\n\n\nS\n1\np\n\n\nF\n1\nq\n\n\n\nTotal outcomes: p + q = 1\n\n\n19.3.10 2. Two Trials\n\n\n\n# Successes\nWays\nCombinations\nProbability\n\n\n\n\n2\nSS\n1\np^2\n\n\n1\nSF, FS\n2\n2pq\n\n\n0\nFF\n1\nq^2\n\n\n\nNotice: \\binom{2}{k} gives us the number of ways (k = 0,1,2)\nTotal probability: p^2 + 2pq + q^2 = 1\n\n\n19.3.11 3. Three Trials\n\n\n\n# Successes\nWays\nCombinations\nProbability\n\n\n\n\n3\nSSS\n1\np^3\n\n\n2\nSSF, SFS, FSS\n3\n3p^2q\n\n\n1\nSFF, FSF, FFS\n3\n3pq^2\n\n\n0\nFFF\n1\nq^3\n\n\n\nHere, \\binom{3}{k} gives us the combinations (k = 0,1,2,3)\nTotal probability: p^3 + 3p^2q + 3pq^2 + q^3 = 1\n\n\n19.3.12 Illustrating Coordinates for 3 Choose 2\nFor n = 3 and k = 2 successes, \\binom{3}{2} = 3 gives us these success position coordinates:\n\n\n\nSuccess Positions\nSequence\nCoordinate Interpretation\n\n\n\n\n(1,2)\nSSF\nSuccesses in positions 1,2\n\n\n(2,3)\nFSS\nSuccesses in positions 2,3\n\n\n(1,3)\nSFS\nSuccesses in positions 1,3\n\n\n\nThis demonstrates why \\binom{3}{2} = 3 - it counts all possible ways to choose 2 positions from 3 available positions.\n\n\n19.3.13 4. Four Trials\n\n\n\n\n\n\n\n\n\n\n# Successes\nWays\nCombinations\nCoordinate Positions\nProbability\n\n\n\n\n4\nSSSS\n1\n(1,2,3,4)\np^4\n\n\n3\nSSSF, SSFS, SFSS, FSSS\n4\n(1,2,3), (1,2,4), (1,3,4), (2,3,4)\n4p^3q\n\n\n2\nSSFF, SFSF, SFFS, FSSF, FSFS, FFSS\n6\n(1,2), (1,3), (1,4), (2,3), (2,4), (3,4)\n6p^2q^2\n\n\n1\nSFFF, FSFF, FFSF, FFFS\n4\n(1), (2), (3), (4)\n4pq^3\n\n\n0\nFFFF\n1\n()\nq^4\n\n\n\nNote how \\binom{4}{k} gives us the combinations:\n\n\\binom{4}{4} = 1 (one way to choose all positions)\n\\binom{4}{3} = 4 (four ways to choose 3 positions)\n\\binom{4}{2} = 6 (six ways to choose 2 positions)\n\\binom{4}{1} = 4 (four ways to choose 1 position)\n\\binom{4}{0} = 1 (one way to choose no positions)\n\nTotal probability: p^4 + 4p^3q + 6p^2q^2 + 4pq^3 + q^4 = 1\n\n\n19.3.14 Pattern Recognition and Generalization\n\nFor n trials:\n\nEach outcome has exactly n positions\nFor k successes, we need k positions filled with S and (n-k) positions with F\n\\binom{n}{k} gives us the number of ways to choose k positions for successes\nEach success contributes p, each failure contributes q\n\nTherefore, for k successes in n trials:\n\nWays = \\binom{n}{k} (binomial coefficient)\nProbability of each way = p^k q^{n-k}\n\n\n\n\n19.3.15 Binomial Distribution Formula\nThe probability of exactly k successes in n trials is:\n\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\nwhere:\n\n\\binom{n}{k} counts the ways to arrange k successes in n positions\np^k accounts for k successes\n(1-p)^{n-k} accounts for (n-k) failures\n\n\n\n19.3.16 Coordinate System Interpretation\nThe binomial coefficient provides a systematic way to count all possible position combinations for successes. For example:\n\nIn the 3-trial case with 2 successes (\\binom{3}{2} = 3):\n\nPosition 1 means success in first trial\nPosition 2 means success in second trial\nPosition 3 means success in third trial\nThe coordinates (1,2), (2,3), (1,3) represent all possible ways to place 2 successes in 3 positions\n\nIn the 4-trial case with 2 successes (\\binom{4}{2} = 6):\n\nWe get six distinct pairs of positions: (1,2), (1,3), (1,4), (2,3), (2,4), (3,4)\nEach pair represents a unique way to distribute 2 successes across 4 trials\nThe remaining positions automatically become failures\n\n\nThis coordinate system demonstrates why the binomial coefficient is the correct counting tool: it systematically accounts for all possible ways to distribute k successes across n positions, where each position must be either a success or failure.\n\n\n\n\n19.3.16.1 Understanding the Binomial Coefficient\nThe binomial coefficient \\binom{n}{k} counts the number of ways to choose k items from n items, where order doesn’t matter. It relates to the multiplication rule through this key insight:\n\nFirst, count all possible ordered sequences (using multiplication rule)\nThen, divide by number of ways to arrange the k items (to remove order)\n\nFor example, to choose 2 items from 4:\n\nFirst position can be filled in 4 ways\nSecond position can be filled in 3 ways\nTotal ordered sequences = 4 × 3 = 12\nNumber of ways to arrange 2 items = 2 × 1 = 2\nTherefore \\binom{4}{2} = \\frac{4 × 3}{2 × 1} = 6",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "inference_en.html",
    "href": "inference_en.html",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "",
    "text": "21.1 Statistical Hypothesis Testing (introduction)\nStatistical inference is how we draw conclusions about a population from a sample. It’s like being a detective: we never have all the information, but we can make educated guesses based on the evidence we have.",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_en.html#statistical-hypothesis-testing-introduction",
    "href": "inference_en.html#statistical-hypothesis-testing-introduction",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "",
    "text": "Key Steps in Statistical Hypothesis Testing (general framework)\n\n\n\n\nInitial Suspicion/Research Question\n\nWe suspect some effect/relationship/difference\nThis guides our research design and analysis\n\nData Collection\n\nWe collect appropriate amount of data\nSample size depends on expected effect and required precision\n\nResult Observation\n\nWe observe and summarize our data\nLook at relevant patterns in the data\n\nHypothesis System\n\nH₀: no effect/no difference (“status quo”)\nH₁: effect exists (one or two-sided)\nChoice of direction based on research question\n\nP-value Approach\n\nConsider: how likely are our results (or more extreme) if H₀ is true?\nChoose appropriate probability model based on data type\nCalculate this probability (p-value)\n\nDecision Making\n\nCompare p-value to significance level (typically α = 0.05)\nSmall p-value suggests results unlikely under H₀\n\nConclusion\n\nIf p ≤ α, reject H₀\nConclude evidence against null hypothesis\nConsider practical significance\n\n\n\n\n\n\n\n\n\n\nThe Logic of Statistical Hypothesis Testing: A Probabilistic Proof by Contradiction\n\n\n\n\n21.1.1 Research Context\nA person claims to possess ESP (extrasensory perception) abilities that enable them to predict coin flips. This scenario illustrates the fundamental logic of statistical hypothesis testing.\n\nTask: Predict 100 coin flips before each flip occurs\nObserved Result: 70 correct predictions out of 100 attempts\nKey Consideration: High success could indicate either ESP ability OR a biased coin\n\n\n\n21.1.2 The Core Logic\nStatistical hypothesis testing follows a logic similar to proof by contradiction in mathematics:\n\nWe start by assuming what we want to disprove (the null hypothesis)\nCalculate the probability of our observed data under this assumption\nIf this probability is extremely small, we reject the initial assumption\n\n\n\n21.1.3 Statistical Framework\n\n21.1.3.1 1. The Null Hypothesis Mechanism\nThe null hypothesis (H₀) serves as our “assumption to be disproven” and typically represents:\n\nNo effect\nNo difference\nPure chance\nThe status quo\n\nIn our ESP case: Random guessing (p = 0.5)\n\n\n21.1.3.2 2. The Alternative Hypothesis\nThe alternative hypothesis (H₁) represents what we suspect might be true:\n\nAn effect exists\nA difference exists\nNot due to chance\nA deviation from status quo\n\nIn our ESP case: Better than random guessing (p &gt; 0.5)\n\n\n21.1.3.3 3. The Decision Rule\nWe establish a conventional cutoff point (α) that defines “extremely unlikely”:\n\nTypically set at α = 0.05 (5%)\nRepresents the threshold for “rare enough” to reject H₀\nA conventional threshold, not a mathematical necessity\n\n\n\n21.1.3.4 4. The P-value Mechanism\nThe p-value quantifies the logical argument:\n\nAssuming H₀ is true\nWhat’s the probability of our observed result or more extreme?\nSmall p-value means either:\n\nH₀ is false (our desired conclusion)\nA rare event occurred under H₀\n\n\n\nP-value (statistical significance): In statistical hypothesis significance testing, the p-value is the probability of obtaining test results (outcomes) at least as extreme as the result actually observed, under the assumption that the null hypothesis (H0) is correct. A very small p-value means that, if the null hypothesis were true, the probability of observing data as extreme as or more extreme than what we actually observed would be very small (the empirical outcome “contradicts” H0). The smaller the p-value, the stronger the statistical evidence against the null hypothesis, leading us to reject H0 at predetermined significance levels (cut-off or threshold probability) such as 0.05 or 0.01, while recognizing that these thresholds are conventions rather than mathematically derived boundaries.\n\n\n\n\n21.1.4 Application to ESP Testing\nStatistical Hypotheses: \n\\begin{align*}\nH_0&: p = 0.5 \\text{ (random guessing)} \\\\\nH_1&: p &gt; 0.5 \\text{ (better than guessing)}\n\\end{align*}\n\nProbability Calculation:\nFor 70 successes out of 100 trials:\n\\text{P-value} = P(X \\geq 70) = \\sum_{k=70}^{100} \\binom{100}{k}(0.5)^k(0.5)^{100-k} \\approx 0.0000393\nDecision Framework: \n\\text{Decision Rule} = \\begin{cases}\n\\text{Reject H}_0 & \\text{if p-value} &lt; 0.05 \\\\\n\\text{Fail to reject H}_0 & \\text{if p-value} \\geq 0.05\n\\end{cases}\n\n0.0000393 &lt; 0.05 (significance level)\nThis means that under the null hypothesis (pure guessing):\n\nThe probability of getting 70 or more correct predictions by chance is about 0.00393%\nSuch extreme results would occur by chance only about 4 times in 100,000 trials\nThis is far below our conventional significance level of 0.05 (5%)\n\n\n\n21.1.5 The General Pattern\n\nAssume the null (like assuming not-A in proof by contradiction)\nCalculate probability of data under null\nIf probability &lt; α:\n\nReject null\nAccept alternative\nConclude evidence supports our suspicion\n\n\n\n\n21.1.6 Key Distinctions from Mathematical Proof\n\nProbabilistic rather than deterministic\nConclusions are “supported” rather than “proven”\nUses conventional thresholds (α)\nAlways includes uncertainty\n\n\n\n\n\nThe binomial test is a hypothesis test used when you have binary (two-outcome) trials, where each trial is independent and has the same probability of success. It tests whether the observed proportion of successes differs significantly from an expected probability under the null hypothesis. For example: Testing whether a coin is fair by checking if the proportion of heads in 100 flips differs significantly from the expected probability of 0.5 under the null hypothesis.\n\n\n\n\n\n\n\nWhat is a P-value?\n\n\n\nA p-value is a probability that captures how extreme our observed data is relative to a null hypothesis:\n\nThe p-value is the probability of obtaining the observed outcome, or a more extreme one in the direction of the alternative hypothesis, assuming the null hypothesis (H₀) is true.\n\n\n21.1.7 Key Components\n\nObserved outcome: The actual value or statistic computed from our sample data.\nMore extreme outcomes: Additional outcomes that provide even stronger evidence against H₀: \n\\begin{cases}\n\\text{Values } \\leq \\text{ observed} & \\text{for } H_1\\text{: parameter &lt; value} \\\\\n\\text{Values } \\geq \\text{ observed} & \\text{for } H_1\\text{: parameter &gt; value} \\\\\n\\text{Values in both tails} & \\text{for } H_1\\text{: parameter } \\neq \\text{ value}\n\\end{cases}\n\nNull hypothesis assumption: All probabilities are calculated using the parameter value specified in H₀.\n\n\n\n21.1.8 One-Tailed vs Two-Tailed Tests\nThe choice between one-tailed and two-tailed tests depends on your alternative hypothesis and the context of your research question:\nOne-Tailed Tests:\n\nUsed when H₁ specifies a direction (&lt; or &gt;)\nP-value calculated from one tail of the distribution\nHigher power but requires strong directional justification\nExample hypotheses: \n\\begin{align*}\nH_0&: \\mu = \\mu_0 \\\\\nH_1&: \\mu &gt; \\mu_0 \\text{ (right-tailed) or } \\mu &lt; \\mu_0 \\text{ (left-tailed)}\n\\end{align*}\n\n\nTwo-Tailed Tests:\n\nUsed when H₁ is non-directional (≠)\nP-value includes both tails of the distribution\nMore conservative, standard choice when direction uncertain\nParticularly suitable for symmetric distributions like the normal distribution\nExample hypotheses: \n\\begin{align*}\nH_0&: \\mu = \\mu_0 \\\\\nH_1&: \\mu \\neq \\mu_0\n\\end{align*}\n\n\n\n\n21.1.9 Example: Binomial Test\nTesting if a politician is overestimating 98% support (p = 0.98) when observing 13 supporters in n = 15 people. In this context, a one-tailed test is most appropriate because the research question is inherently directional (overestimating implies p &lt; 0.98).\n\n\\begin{align*}\nH_0&: p = 0.98 \\\\\nH_1&: p &lt; 0.98\n\\end{align*}\n\nP-value calculation: \n\\begin{align*}\n\\text{p-value} &= P(X \\leq 13 \\mid H_0) \\\\\n&= 1 - P(X \\geq 14 \\mid p = 0.98) \\\\\n&= 0.0353\n\\end{align*}\n\n\n\n21.1.10 Common Misconceptions to Avoid\n\nP-value ≠ Probability H₀ is true\n1 - p-value ≠ Probability H₁ is true\nLarge p-value ≠ H₀ is true\nOne-tailed tests aren’t automatically “better” despite higher power\nThe choice between one-tailed and two-tailed tests should be based on the research context, not just statistical convenience\n\nRemember: The p-value quantifies evidence against H₀ but should be considered alongside practical significance and effect size.\n\n\n\n\n21.1.11 The Method of Proof by Contradiction\n\n21.1.11.1 In Mathematics\nProof that \\sqrt{2} is Irrational\nInitial Assumption\nIf \\sqrt{2} is rational, then \\sqrt{2} = \\frac{p}{q} where:\n\np and q are integers\nq \\neq 0\np and q have no common factors\n\nAlgebraic Steps\n\nStarting with \\sqrt{2} = \\frac{p}{q}\nSquare both sides: 2 = \\frac{p^2}{q^2}\nMultiply both sides by q^2: 2q^2 = p^2\n\nProperties of p and q\n\nSince 2q^2 = p^2, p^2 is even\nIf p^2 is even, then p is even\nTherefore p = 2k for some integer k\nSubstituting p = 2k into 2q^2 = p^2: 2q^2 = (2k)^2 = 4k^2\nTherefore q^2 = 2k^2\nThus q^2 is even, so q is even\n\nContradiction\n\nWe proved both p and q are even\nThis contradicts our assumption that p and q have no common factors\nTherefore, \\sqrt{2} cannot be rational\n\nThus, \\sqrt{2} is irrational.\n\n\n21.1.11.2 In Statistics\nWe use a similar but probabilistic approach:\n\nMake assumption (null hypothesis)\nSee if data contradicts this assumption\nIf contradiction is strong enough, reject assumption\n\nKey Difference: We deal with probability, not certainty.\n\n\n\n21.1.12 The Null Hypothesis Framework\n\n21.1.12.1 Step 1: State the Hypotheses\nFor a coin example:\n\nH₀ (Null): Coin is fair (p = 0.5)\nH₁ (Alternative): Coin is not fair (p ≠ 0.5)\n\nThink of H₀ as the “innocent until proven guilty” assumption.\n\n\n21.1.12.2 Step 2: Collect Evidence\nSuppose we flip coin 100 times:\n\nExpected under H₀: About 50 heads\nActually observe: 70 heads\n\n\n\n21.1.12.3 Step 3: Assess Evidence\nAsk: “If coin were truly fair (H₀ true), how likely is this result?”\nThis is like asking in a legal case:\n\n“If defendant were innocent, how do we explain the evidence?”\nVery improbable evidence suggests innocence might be false\n\n\n\n\n\n\n\nUnderstanding Error Types in Hypothesis Testing\n\n\n\nIn hypothesis testing, we can make two types of errors:\n\n21.1.13 Type I Error (False Positive)\n\nDefinition: Rejecting H₀ when it is actually true\nProbability: α (significance level)\nExample in Justice System: Convicting an innocent person\nExample in Medicine: Diagnosing healthy patient as sick\n\n\n\n21.1.14 Type II Error (False Negative)\n\nDefinition: Failing to reject H₀ when it is actually false\nProbability: β\nPower = 1 - β (probability of correctly rejecting false H₀)\nExample in Justice System: Letting guilty person go free\nExample in Medicine: Missing an actual disease\n\n\n\n21.1.15 Trade-off Between Errors\n\nDecreasing α (being more conservative) increases β\nDecreasing β (increasing power) requires either:\n\nLarger sample size\nLarger effect size\nHigher α\n\n\n\n\n21.1.16 Practical Implications\n\n\n\nContext\nType I Concern\nType II Concern\nTypical α\n\n\n\n\nCriminal Justice\nConvict innocent\nFree guilty\n0.001\n\n\nMedical Testing\nUnnecessary treatment\nMiss disease\n0.01\n\n\n\n\n\n\n\n\n\n\n\n\nHistorical Note: Jerzy Neyman (1894-1981)\n\n\n\nThe framework of statistical hypothesis testing as we know it today was largely developed by Jerzy Neyman, a Polish mathematician and statistician, in collaboration with Egon Pearson. Born in Bendery, Imperial Russia (now Moldova), Neyman made fundamental contributions to statistics that transformed both theoretical foundations and practical applications.\nHis most significant contributions include:\n\nDevelopment of the formal hypothesis testing framework, introducing the concepts of Type I and Type II errors\nCreation of confidence intervals as a way to express uncertainty in estimation\nPioneering the potential outcomes framework in causal inference\nAdvancement of sampling theory\n\nThe potential outcomes framework, first introduced by Neyman in his 1923 master’s thesis on agricultural experiments, revolutionized how we think about causality in statistics. This framework, later rediscovered and expanded by Donald Rubin (hence sometimes called the Neyman-Rubin causal model), introduced the concept of comparing potential outcomes that would occur under different treatments. For each unit, Neyman conceived of multiple potential outcomes, only one of which could be observed - a fundamental concept now known as the “fundamental problem of causal inference.”\nHis approach to statistical inference differed notably from R.A. Fisher’s significance testing, leading to important debates that helped shape modern statistical theory and practice.\nThe potential outcomes framework he introduced has become particularly influential in modern causal inference, epidemiology, and social sciences research. The impact of his contributions continues to be felt in how we approach statistical inference, experimental design, and causal analysis today.\n\n\n\n\n\n21.1.17 The Logic of P-values\n\n21.1.17.1 What is a p-value?\np-value = P(seeing this evidence or more extreme | H₀ is true)\nLike asking:\n\n“How surprising is this evidence if H₀ is true?”\n“Could this easily happen by chance?”\n\n\n\n21.1.17.2 Example: Step by Step\nObserve 8 heads in 10 flips:\n\nAssume H₀: p = 0.5 (fair coin)\nCalculate: P(X ≥ 8) = P(8 heads) + P(9 heads) + P(10 heads) = \\binom{10}{8}(0.5)^8(0.5)^2 + \\binom{10}{9}(0.5)^9(0.5)^1 + \\binom{10}{10}(0.5)^{10} ≈ 0.055\nInterpret:\n\nAbout 5.5% chance of seeing this under H₀\nModerately unusual, but not extremely so\n\n\n\n\n21.1.17.3 Common Misunderstandings\n\n“p-value is probability H₀ is true”\n\nNo: It’s probability of data, assuming H₀\nLike P(evidence|innocent), not P(innocent|evidence)\n\n“Small p-value proves H₀ false”\n\nNo: Only suggests H₀ unlikely\nLike strong evidence, but not proof\n\n“Large p-value proves H₀ true”\n\nNo: Just fails to provide evidence against H₀\nLike “not guilty” vs “proven innocent”\n\n\n\n\n\n\n\n\nBeyond Simple Significance Testing\n\n\n\n\n21.1.18 Why Not Just Use 5%?\nHistorical reasons:\n\nR.A. Fisher suggested as benchmark\nPre-computed tables used 5%\nBecame convention, not logical necessity\n\n\n\n21.1.19 Better Approach:\nConsider p-value on continuous scale:\n\np = 0.049 vs p = 0.051 are virtually same\nContext matters:\n\nMedical trials might need p &lt; 0.001\nMarket research might accept p &lt; 0.1\n\n\n\n\n21.1.20 Practical Significance\nAlways consider:\n\nEffect size (how big is the difference?)\nPractical importance\nCosts and benefits of decisions\nSample size and power",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_en.html#problem-solutions-part-1-the-binomial-tests-one-tailedsided-tests",
    "href": "inference_en.html#problem-solutions-part-1-the-binomial-tests-one-tailedsided-tests",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "21.2 Problem Solutions – part 1: the binomial tests (one-tailed/sided tests)",
    "text": "21.2 Problem Solutions – part 1: the binomial tests (one-tailed/sided tests)\n\nThe binomial test is a statistical hypothesis test used when you have binary (two-outcome) trials, where each trial is independent and has the same probability of success. It tests whether the observed proportion of successes differs significantly from an expected probability under the null hypothesis. For example: Testing whether a coin is fair by checking if the proportion of heads in 100 flips differs significantly from the expected probability of 0.5 under the null hypothesis.\n\n\nProblem 1: Binomial Test for Candidate Support\n\n21.2.0.1 Problem Statement\nAn election candidate believes she has the support of 50% (p = 0.5) of the residents in a particular town. A researcher suspects this might be an underestimation and conducts a survey. The researcher asks 10 people whether they support the candidate or not; 7 people say that they do (70% in a sample).\nCalculate the p-value and decide whether there is enough evidence to reject H0 using data from the sample (assuming the critical probability = 5%).\n\n\n21.2.0.2 Setup\nHypotheses:\n\nH_0: p = 0.5 (null hypothesis: true proportion (support) is 50%)\nH_1: p &gt; 0.5 (alternative hypothesis: true proportion is greater than 50%)\n\nData:\n\nSample size: n = 10\nObserved successes: x = 7 (70% support)\nHypothesized proportion: p_0 = 0.5\nSignificance level: \\alpha = 0.05 (5%)\n\n\n\n21.2.0.3 Finding the P-value\nFor a one-sided test, the p-value is the probability of observing 7 or more successes out of 10 trials, assuming H_0 is true. Using the binomial distribution:\nP(X \\geq 7) = P(X = 7) + P(X = 8) + P(X = 9) + P(X = 10)\n\n\n\n\n\n\nWhy One-Tailed Test?\n\n\n\nThis is a one-tailed/sided test because we’re specifically interested in whether the candidate is under-estimating her support. In statistical terms:\n\nIf she believes support is 50% but it’s actually 40%, she’s over-estimating her support\nIf she believes support is 50% but it’s actually 60%, she’s under-estimating her support\n\nOur research question only concerns under-estimation, so we only need to consider evidence in that direction (values greater than 50%). This is reflected in our alternative hypothesis H_1: p &gt; 0.5.\n\n21.2.0.4 Why Not Just P(X = 7)?\nWe can’t just calculate P(X = 7) because:\n\nThe p-value represents the probability of observing results as extreme or more extreme than what we saw, assuming H_0 is true\n“More extreme” means results that provide even stronger evidence against H_0 in the direction of H_1\nSince H_1 suggests higher proportions than 50%, outcomes of 8, 9, or 10 supporters out of 10 would be even stronger evidence against H_0\n\nTherefore, we must sum:\nP(X \\geq 7) = P(X = 7) + P(X = 8) + P(X = 9) + P(X = 10)\nIf we only calculated P(X = 7) = 0.1172, we would ignore these other possible outcomes that also support H_1, leading to an incorrect p-value.\n\n\n\nFor each value k, we use the binomial probability formula:\nP(X = k) = \\binom{n}{k} p_0^k (1-p_0)^{n-k}\nLet’s calculate each term:\n\nP(X = 7) = \\binom{10}{7} (0.5)^7 (0.5)^3 = 120 \\cdot (0.5)^{10} = 0.1172\nP(X = 8) = \\binom{10}{8} (0.5)^8 (0.5)^2 = 45 \\cdot (0.5)^{10} = 0.0439\nP(X = 9) = \\binom{10}{9} (0.5)^9 (0.5)^1 = 10 \\cdot (0.5)^{10} = 0.0098\nP(X = 10) = \\binom{10}{10} (0.5)^{10} (0.5)^0 = 1 \\cdot (0.5)^{10} = 0.0010\n\nP-value = 0.1172 + 0.0439 + 0.0098 + 0.0010 = 0.1719 (17.19%)\n\n\n21.2.0.5 Decision\nSince the p-value (0.1719) is greater than the significance level (0.05), we fail to reject the null hypothesis.\n\n\n21.2.0.6 Interpretation\nThere is not enough evidence to conclude that the candidate is under-estimating her support. While the sample shows 70% support (higher than 50%), this difference could reasonably occur by chance even if the true support was only 50%. The relatively small sample size (n = 10) makes it harder to detect real differences.\n\n\n\nProblem 2: Binomial Test for Candidate Support (2)\n\n21.2.0.7 Problem Statement\nAn election candidate believes she has the support of 40% (p = 0.4) of the residents in a particular town. A researcher suspects this might be an overestimation and conducts a survey. The researcher asks 20 people whether they support the candidate or not; 3 people say that they do (15% in a sample). Calculate the p-value and decide whether there is enough evidence to reject H0 using data from the sample (assuming the critical probability = 5%).\n\n\n21.2.0.8 Setup\nHypotheses:\n\nH_0: p = 0.4 (null hypothesis: true proportion is 40%)\nH_1: p &lt; 0.4 (alternative hypothesis: true proportion is less than 40%)\n\nData:\n\nSample size: n = 20\nObserved successes: x = 3 (15% support)\nHypothesized proportion: p_0 = 0.4\nSignificance level: \\alpha = 0.05 (5%)\n\n\n\n21.2.0.9 Why One-Tailed Test?\nThis is a one-tailed test because we’re specifically interested in whether the candidate is over-estimating her support. We only care about evidence suggesting the true proportion is less than 40%, leading to a left-tailed test.\n\n\n21.2.0.10 Finding the P-value\nFor this left-tailed test, the p-value is the probability of observing 3 or fewer successes out of 20 trials, assuming H_0 is true. Using the binomial distribution:\nP(X \\leq 3) = P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3)\nFor each value k, we use the binomial probability formula: P(X = k) = \\binom{n}{k} p_0^k (1-p_0)^{n-k}\n\nWe want to find P(X ≤ 3) when X follows B(20, 0.4) Using the binomial formula:\n\nP(X = k) = \\binom{n}{k}p^k(1-p)^{n-k}\n\nCalculate the combinations \\binom{20}{k} for k = 0, 1, 2, 3:\n\n\\binom{20}{0} = 1\n\\binom{20}{1} = 20\n\\binom{20}{2} = \\frac{20 \\times 19}{2 \\times 1} = 190\n\\binom{20}{3} = \\frac{20 \\times 19 \\times 18}{3 \\times 2 \\times 1} = 1,140\n\nCalculate each probability:\n\nFor k = 0: P(X = 0) = \\binom{20}{0}(0.4)^0(0.6)^{20} = 1 \\times 1 \\times 0.6^{20} \\approx 0.0000366\nFor k = 1: P(X = 1) = \\binom{20}{1}(0.4)^1(0.6)^{19} = 20 \\times 0.4 \\times 0.6^{19} \\approx 0.0004875\nFor k = 2: P(X = 2) = \\binom{20}{2}(0.4)^2(0.6)^{18} = 190 \\times 0.16 \\times 0.6^{18} \\approx 0.0030874\nFor k = 3: P(X = 3) = \\binom{20}{3}(0.4)^3(0.6)^{17} = 1,140 \\times 0.064 \\times 0.6^{17} \\approx 0.0123497\n\nSum all probabilities: P(X \\leq 3) = \\sum_{k=0}^3 P(X = k) = 0.0000366 + 0.0004875 + 0.0030874 + 0.0123497 = 0.0159612\nDecision rule:\n\n\nReject H₀ if p-value &lt; α\nSince 0.0159612 &lt; 0.05, we reject H₀.\n\n\n\n21.2.0.11 Decision\nSince the p-value is less than the significance level (0.05), we reject the null hypothesis.\n\n\n21.2.0.12 Interpretation\nThere is sufficient evidence at the 5% significance level to conclude that the candidate is overestimating her support. The sample shows only 15% support, which is significantly lower than the candidate’s belief of 40%. The probability of observing such low support (3 or fewer out of 20) would be only about 1.6% if the true support were actually 40%.\n\n\n\nProblem 3: Binomial Test for Candidate Support (3)\n\n21.2.0.13 Problem Statement\nA political candidate claims that 40% of residents in a town support her campaign (p = 0.4). A researcher suspects this might be an overestimation and conducts a survey. In a random sample of 12 residents, 1 person expresses support for the candidate. Test whether there is sufficient evidence to conclude that the candidate is overestimating her support level, using a significance level of 5%.\n\n\n21.2.0.14 Hypotheses\n\n\\begin{align*}\nH_0&: p = 0.4 \\text{ (The candidate's claim is correct)} \\\\\nH_1&: p &lt; 0.4 \\text{ (The candidate is overestimating support)}\n\\end{align*}\n\n\n\n21.2.0.15 Given Information\n\nSample size: n = 12\nNumber of successes: x = 1\nHypothesized proportion: p_0 = 0.4\nSignificance level: \\alpha = 0.05\nObserved proportion: \\hat{p} = \\frac{1}{12} \\approx 0.083\n\n\n\n21.2.0.16 Solution\n\nFor a left-tailed test, we calculate the probability of observing 1 or fewer successes under H_0.\nUsing the binomial probability formula: P(X \\leq 1) = \\sum_{k=0}^{1} \\binom{12}{k}(0.4)^k(0.6)^{12-k}\nWe find: P(X \\leq 1) = 0.0196\nDecision Rule:\n\nReject H_0 if p-value &lt; \\alpha\nSince 0.0196 &lt; 0.05, we reject H_0\n\n\n\n\n21.2.0.17 Conclusion\nAt a 5% significance level, there is sufficient evidence to conclude that the candidate is overestimating her support. The sample proportion (8.3%) is substantially lower than the claimed 40% support, and this difference is statistically significant (p = 0.0196).\n\n\n21.2.0.18 Statistical Power Consideration\nWhile the sample size (n = 12) is relatively small, we were still able to detect a significant difference. This is because the observed difference between the claimed proportion (40%) and sample proportion (8.3%) was quite large. However, a larger sample size would provide more reliable results and better estimation of the true support proportion.\n\n\n\nProblem 4: Binomial Test for Candidate Support (4)\nAn election candidate claims that 20% of residents in a town support her campaign. A researcher believes the candidate might be over-estimating her support and wants to test this claim. In a random sample of 12 residents, 4 people express support for the candidate. Test whether there is sufficient evidence to conclude that the candidate is over-estimating her support level, using a significance level of 5%.\nGiven:\n\nClaimed support: p = 0.2\nSample size: n = 12\nNumber of supporters in sample: x = 4\nSignificance level: α = 0.05\n\n\n21.2.0.19 Solution\nStep 1: State the Hypotheses\nSince we want to test if the candidate is over-estimating (true proportion is less than claimed):\n\n\\begin{align*}\nH_0&: p = 0.2 \\text{ (The candidate's claim is correct)} \\\\\nH_1&: p &lt; 0.2 \\text{ (The candidate is overestimating support)}\n\\end{align*}\n\nStep 2: Choose the Test Statistic\nWe use the number of successes (X) in the sample, where X follows a binomial distribution with n = 12 and p = 0.2 under H₀.\nObserved value: x = 4\nStep 3: Calculate the Test Statistic and P-value\nFor a left-tailed test, we calculate:\nP(X \\leq 4) = \\sum_{k=0}^{4} \\binom{12}{k}(0.2)^k(0.8)^{12-k}\n\n# Calculate p-value\np_value &lt;- pbinom(4, size = 12, prob = 0.2)\nprint(paste(\"P-value =\", round(p_value, 4)))\n\n[1] \"P-value = 0.9274\"\n\n\nThe p-value is 0.9274\nStep 4: Decision Rule\n\nReject H₀ if p-value &lt; α\nSince 0.9274 &gt; 0.05, we fail to reject H₀\n\nStep 5: Interpretation\nAt a 5% significance level, there is not enough evidence to conclude that the candidate is over-estimating her support. In fact, the sample data shows 4/12 ≈ 33.3% support, which is higher than her claimed 20%, going in the opposite direction of our alternative hypothesis.\n\n\n21.2.0.20 Additional Notes\n\nSample Proportion: \\hat{p} = \\frac{x}{n} = \\frac{4}{12} = 0.333\nThe high p-value reflects that the sample proportion (33.3%) is actually higher than the hypothesized value (20%), not lower as we were testing for.\nIf we had suspected under-estimation rather than over-estimation, we should have set up the test with H₁: p &gt; 0.2.\nGiven the small sample size (n = 12), the power of this test to detect true differences is limited.\n\n\n\n21.2.0.21 R Code\nHere’s the complete R code for this analysis:\n\n# Given values\nn &lt;- 12          # sample size\nx &lt;- 4           # number of successes\np0 &lt;- 0.2        # hypothesized proportion\nalpha &lt;- 0.05    # significance level\n\n# Calculate p-value for left-tailed test\np_value &lt;- pbinom(x, size = n, prob = p0)\n\n# Calculate sample proportion\np_hat &lt;- x/n\n\n# Print results\ncat(\"Sample proportion =\", round(p_hat, 3), \"\\n\")\n\nSample proportion = 0.333 \n\ncat(\"P-value =\", round(p_value, 4), \"\\n\")\n\nP-value = 0.9274 \n\ncat(\"Decision: \", ifelse(p_value &lt; alpha, \"Reject H0\", \"Fail to reject H0\"), \"\\n\")\n\nDecision:  Fail to reject H0 \n\n\n\n\n\nProblem 5: Binomial Test for EU Support\n\n21.2.0.22 Problem Statement\nA politician believes that support for his country’s EU membership is about 98% (p = 0.98). A researcher wants to test whether the politician is overestimating this level of support.\nIn a sample of 15 people (n = 15), the researcher observes that 13 people support membership. Let’s define the random variable X as the number of people in the sample who support EU membership. We observed X = 13 “successes” in 15 Bernoulli trials.\nIs there enough evidence to reject the claim that the support is 98%?\n\n\n21.2.0.23 Setup\nHypotheses:\n\nH_0: p = 0.98 (null hypothesis: true proportion is 98%)\nH_1: p &lt; 0.98 (alternative hypothesis: true proportion is less than 98%)\n\nData:\n\nSample size: n = 15\nObserved successes: x = 13 (≈86.7% support)\nHypothesized proportion: p_0 = 0.98\nSignificance level: \\alpha = 0.05 (5%)\n\n\n\n21.2.0.24 Finding the P-value\nFor this left-tailed test, we need P(X \\leq 13). Given the high value of p_0, it’s more efficient to use the complement rule:\nP(X \\leq 13) = 1 - P(X \\geq 14)\n\nThe complement rule in probability states that P(A) = 1 - P(not A), because P(A) + P(not A) = 1. For a left-tailed test, we need P(X ≤ 13). Instead of summing P(X = 0) + P(X = 1) + … + P(X = 13), it’s easier to: calculate P(X ≤ 13) = 1 - P(X &gt; 13), or P(X ≤ 13) = 1 - P(X ≥ 14).\n\nFor this left-tailed test: P(X \\leq 13) = 1 - P(X &gt; 13) = 1 - P(X = 14) - P(X = 15)\nLet’s calculate step by step:\n\nFor X = 14:\n\n\nCalculate combination:\n\n\\binom{15}{14} = \\frac{15!}{14!(15-14)!} = \\frac{15}{1} = 15\n\nCalculate probability:\n\nP(X = 14) = \\binom{15}{14}(0.98)^{14}(0.02)^1 = 15 \\times (0.98)^{14} \\times 0.02 = 15 \\times 0.75051... \\times 0.02 \\approx 0.2252\n\nFor X = 15:\n\n\nCalculate combination:\n\n\\binom{15}{15} = 1\n\nCalculate probability:\n\nP(X = 15) = \\binom{15}{15}(0.98)^{15}(0.02)^0 = 1 \\times (0.98)^{15} \\times 1 = (0.98)^{15} \\approx 0.7395\n\nTherefore:\n\nP(X \\leq 13) = 1 - P(X = 14) - P(X = 15) = 1 - 0.2252 - 0.7395 \\approx 0.0353\n\n\n21.2.0.25 Decision\nSince the p-value (0.0353) is less than the significance level (0.05), we reject the null hypothesis.\n\n\n21.2.0.26 Interpretation\nThere is sufficient evidence to conclude that the politician is overestimating the support for EU membership. While 86.7% support in the sample is still very high, it’s significantly lower than the politician’s claim of 98%. Under the assumption that true support is 98%, the probability of observing 13 or fewer supporters in a sample of 15 people would be only about 3.53%.\nNote on Complement Rule\nThis problem demonstrates the utility of the complement rule in probability calculations. Instead of calculating probabilities for outcomes 0 through 13 (14 calculations), we only needed to calculate probabilities for outcomes 14 and 15 (2 calculations). This is particularly efficient when:\n\nWe have a large number of outcomes in the “tail” we’re interested in\nThe probability is concentrated in the other tail (here, high values due to p_0 = 0.98)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_en.html#problem-solutions-part-2-the-binomial-tests-two-tailedsided-tests",
    "href": "inference_en.html#problem-solutions-part-2-the-binomial-tests-two-tailedsided-tests",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "21.3 Problem Solutions – part 2: the binomial tests (two-tailed/sided tests (*))",
    "text": "21.3 Problem Solutions – part 2: the binomial tests (two-tailed/sided tests (*))\n(…)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_en.html#testing-ols-regression-parameter-significance-using-r",
    "href": "inference_en.html#testing-ols-regression-parameter-significance-using-r",
    "title": "Statistical Hypothesis Testing: A Fundamental Logic",
    "section": "21.4 Testing OLS Regression Parameter Significance Using R (*)",
    "text": "21.4 Testing OLS Regression Parameter Significance Using R (*)\n(…)",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Statistical Hypothesis Testing: A Fundamental Logic</span>"
    ]
  },
  {
    "objectID": "inference_pl.html",
    "href": "inference_pl.html",
    "title": "22  Wprowadzenie do Wnioskowania Statystycznego: Logika Testowania Hipotez Statystycznych",
    "section": "",
    "text": "Wnioskowanie statystyczne to sposób, w jaki wyciągamy wnioski o populacji na podstawie próby. To jak bycie detektywem: nigdy nie mamy wszystkich informacji, ale możemy wyciągać uzasadnione wnioski na podstawie dostępnych dowodów.\n\n22.0.1 Podstawowa Logika\nWyobraź sobie, że podejrzewasz, iż moneta może być nieuczciwa. Jak sprawdzić takie przypuszczenie?\n\nZbierz Dowody:\n\nWykonaj wiele rzutów monetą\nZapisz wyniki\nSprawdź, czy są zgodne z tym, czego oczekiwałbyś od uczciwej monety\n\nPodejmij Decyzję:\n\nJeśli wyniki wyglądają normalnie → kontynuuj założenie, że moneta jest uczciwa\nJeśli wyniki wyglądają bardzo nietypowo → podejrzewaj, że moneta jest nieuczciwa\n\n\n\n\n\n\n\n\nKluczowe Kroki Testowania Hipotez Statystycznych (ogólny schemat)\n\n\n\n\nWstępne Podejrzenie/Pytanie Badawcze\n\nPodejrzewamy istnienie pewnego efektu/związku/różnicy\nTo ukierunkowuje nasze badanie i analizę\n\nZbieranie Danych\n\nGromadzimy odpowiednią ilość danych\nWielkość próby zależy od oczekiwanego efektu i wymaganej precyzji\n\nObserwacja Wyników\n\nObserwujemy i podsumowujemy nasze dane\nSzukamy istotnych wzorców w danych\n\nSystem Hipotez\n\nH₀: brak efektu/brak różnicy (“status quo”)\nH₁: efekt istnieje (jedno- lub dwustronny)\nWybór kierunku zależy od pytania badawczego\n\nPodejście Wartości p\n\nRozważamy: jak prawdopodobne są nasze wyniki (lub bardziej skrajne) jeśli H₀ jest prawdziwa?\nWybieramy odpowiedni model probabilistyczny w zależności od typu danych\nObliczamy to prawdopodobieństwo (wartość p)\n\nPodejmowanie Decyzji\n\nPorównujemy wartość p z poziomem istotności (zazwyczaj α = 0.05)\nMała wartość p sugeruje, że wyniki są mało prawdopodobne przy H₀\n\nWniosek\n\nJeśli p ≤ α, odrzucamy H₀\nWnioskujemy o dowodach przeciwko hipotezie zerowej\nRozważamy znaczenie praktyczne\n\n\n\n\n\n\n\n\n\n\nPodstawowa Logika Testowania Hipotez Statystycznych: Analiza Zdolności ESP\n\n\n\nProblem Badawczy: Testowanie Deklaracji o Posiadaniu Zdolności Pozazmysłowych\nOsoba twierdzi, że posiada zdolności ESP (percepcję pozazmysłową, tzw. szósty zmysł), które pozwalają jej przewidywać wyniki rzutów monetą. Aby naukowo przetestować to twierdzenie, projektujemy eksperyment, w którym moneta jest rzucana 100 razy, a osoba musi przewidzieć każdy wynik przed rzutem.\nOsoba osiąga sukces w 70 na 100 przewidywań. Jednak istnieje subtelne, ale kluczowe zastrzeżenie: wysoki współczynnik sukcesu może wskazywać zarówno na zdolności ESP, JAK I na nieuczciwą monetę.\nDefiniowanie Prawdopodobieństwa Bazowego i Oczekiwanej Skuteczności\nJeśli osoba jedynie zgaduje (brak ESP), każde przewidywanie jest równoważne losowemu zgadywaniu z prawdopodobieństwem sukcesu 0.5. Jeśli rzeczywiście posiada zdolności ESP, spodziewalibyśmy się współczynnika sukcesu przekraczającego 0.5. To stanowi podstawę naszego badania statystycznego.\nUstanawianie Systemu Hipotez Statystycznych\nUstalamy dwie konkurencyjne hipotezy:\n\nHipoteza Zerowa (H₀): Przewidywania opierają się na losowym zgadywaniu przy użyciu uczciwej monety (p = 0.5)\nHipoteza Alternatywna (H₁): Albo osoba posiada zdolności ESP, ALBO moneta jest nieuczciwa (p &gt; 0.5)\n\nWybór Między Testami Jedno- i Dwustronnymi\nW testowaniu hipotez musimy zdecydować, czy testujemy efekt w jednym czy obu kierunkach:\nTest Jednostronny (Nasz Obecny Przypadek):\n\nTestuje efekt tylko w jednym kierunku (tutaj: lepszy niż przypadek)\nWiększa moc wykrywania określonego efektu kierunkowego\nOdpowiedni, gdy interesuje nas tylko jeden kierunek\nPrzykład: Interesuje nas tylko wynik lepszy niż przypadkowy, nie gorszy\n\nTest Dwustronny (Alternatywne Podejście):\n\nTestuje efekt w obu kierunkach (zarówno lepszy jak i gorszy niż przypadek)\nMniejsza moc, ale bardziej kompleksowy\nOdpowiedni, gdy każde odchylenie od hipotezy zerowej jest interesujące\nPrzykład: Testowanie, czy moneta jest nieuczciwa w kierunku orła LUB reszki\n\nWybór Modelu Probabilistycznego: Rozkład Dwumianowy\nNasz test ESP pasuje do modelu prawdopodobieństwa dwumianowego, ponieważ:\n\nKażde przewidywanie jest niezależne\nKażde przewidywanie ma dokładnie dwa możliwe wyniki (poprawne/niepoprawne)\nPrawdopodobieństwo sukcesu pozostaje stałe (0.5 przy H₀)\nZliczamy całkowitą liczbę sukcesów w ustalonej liczbie prób\n\nDla naszego przykładu obliczamy: P(X \\geq 70) = \\sum_{k=70}^{100} \\binom{100}{k}(0.5)^k(0.5)^{100-k}\nObliczanie i Interpretacja Wartości p\nWartość p pomaga nam ocenić, jak zaskakujące byłyby nasze wyniki, gdyby H₀ była prawdziwa:\n\nMierzy prawdopodobieństwo uzyskania 70 lub więcej poprawnych przewidywań na 100 prób przez czysty przypadek\nBardzo mała wartość p sugeruje, że taki sukces byłby rzadki przy losowym zgadywaniu\nKonwencjonalny próg 0.05 oznacza, że wymagamy wyników, które wystąpiłyby przypadkowo rzadziej niż w 5% przypadków\n\nReguły Decyzyjne i Potencjalne Błędy w Testowaniu ESP\n\nBłąd Typu I (Fałszywie Pozytywny):\n\nStwierdzenie, że ktoś ma ESP, gdy miał po prostu szczęście\nOgraniczamy to ryzyko do 5% poprzez poziom istotności\nTo jak błędne potwierdzenie zdolności ESP\n\nBłąd Typu II (Fałszywie Negatywny):\n\nNiewychwycenie rzeczywistych zdolności ESP, lub faktu, że moneta jest obciążona\nBardziej prawdopodobny przy:\n\nWspółczynniku “sukcesu” (prawdopodobieństwo “sukcesu”) niewiele powyżej 0.5\nMałej liczbie prób\nRygorystycznych poziomach istotności\n\n\n\n\n\n\n\n\n\n\n\nObliczenie Wartości p dla Przykładu ESP\n\n\n\nSzczegóły Obliczeniowe\nDla naszego testu ESP z 70 sukcesami na 100 prób, obliczamy:\nP(X \\geq 70) = \\sum_{k=70}^{100} \\binom{100}{k}(0.5)^k(0.5)^{100-k} \\approx 0.0000393\nOznacza to, że przy hipotezie zerowej (czyste zgadywanie):\n\nPrawdopodobieństwo uzyskania 70 lub więcej poprawnych przewidywań przez przypadek wynosi około 0.00393%\nTak skrajne wyniki wystąpiłyby przypadkowo tylko około 4 razy na 100,000 prób\nJest to znacznie poniżej konwencjonalnego poziomu istotności 0.05 (5%)\n\nDecyzja Statystyczna\nPonieważ nasza wartość p (0.0000393) jest znacznie mniejsza niż α = 0.05:\n\nOdrzucamy hipotezę zerową\nWnioskujemy, że istnieją silne dowody statystyczne przeciwko “losowemu zgadywaniu”\nWynik jest uznawany za “wysoce istotny statystycznie”\n\nOstrożna Interpretacja\nMimo że nasz wynik jest istotny statystycznie, powinniśmy rozważyć:\n\nIstotność statystyczna nie dowodzi istnienia ESP (moneta może być nieuczciwa?)\nEksperyment powinien być powtarzalny w kontrolowanych warunkach",
    "crumbs": [
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Wprowadzenie do Wnioskowania Statystycznego: Logika Testowania Hipotez Statystycznych</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\n\n\n\nBlair, G., Coppock, A., & Humphreys, M. (2023). Research design in the social sciences: declaration, diagnosis, and redesign. Princeton University Press. https://book.declaredesign.org/\nBryman, A., 2016. Social research methods. Oxford University Press.\nBueno de Mesquita, Ethan and Anthony Fowler. 2021. Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis. Princeton University Press.\nCausality for Machine Learning. https://ff13.fastforwardlabs.com/\nCetinkaya-Rundel, M., Diez, D.M. and Barr, C.D., 2019 (4th ed.). OpenIntro Statistics: an Open-source Textbook: https://www.openintro.org/book/os/\nClaude [Large language model], 2024. https://www.anthropic.com\nConcepts and Computation: An Introduction to Political Methodology. https://pos3713.github.io/notes/\nHannay, K. (2019). Introduction to statistics and data science. http://khannay.com/StatsBook/\nIsmay, C. and Kim, A.Y., 2019. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. https://moderndive.com/index.html\nNavarro, D.J. and Foxcroft, D.R. (2019). Learning statistics with Jamovi: a tutorial for psychology students and other beginners. (Version 0.70). DOI: 10.24384/hgc3-7p15\nRemler, D.K. and Van Ryzin, G.G., 2014. Research methods in practice: Strategies for description and causation. Sage Publications.\nSanchez, G., Marzban, E. (2020) All Models Are Wrong: Concepts of Statistical Learning. https://allmodelsarewrong.github.io\nSchneider, W. J. (2023). Psycheval: A psychological evaluation toolkit. https://github.com/wjschne/psycheval\nTimbers, T., Campbell, T., & Lee, M. (2022). Data science: A first introduction. Chapman and Hall/CRC. https://datasciencebook.ca/",
    "crumbs": [
      "References"
    ]
  }
]