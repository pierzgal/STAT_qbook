[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Data Analysis: An Introduction (Wprowadzenie do analizy danych społecznych)",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nImportant\n\n\n\nThis is a preliminary draft of a Quarto class notes on social data analysis. Please do not cite or reproduce its contents, as it may contain errors!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "",
    "text": "1.1 What is Data Science?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#what-is-data-science",
    "href": "chapter1.html#what-is-data-science",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "",
    "text": "Important\n\n\n\nStatistics and Data Science are The Art and Science of Learning from Data.\n\n\n\nData science and statistics are powerful tools that help us understand complex phenomena across various social sciences, including political science, economics, and sociology. These complementary fields provide researchers and practitioners with the means to analyze trends, behaviors, and outcomes in society, offering insights that can shape policy and advance our understanding of human interaction.\nStatistics provides the mathematical foundation for analyzing societal trends and outcomes, offering methods for designing studies, summarizing data, and making inferences. Data science expands on this foundation by incorporating computational methods and domain expertise to handle larger datasets and perform more complex analyses.\nTogether, these disciplines allow us to collect and process large datasets, visualize complex information, uncover patterns in social interactions, evaluate policy impacts, and support evidence-based decision-making. Their applications are vast and varied, from studying voting patterns and analyzing economic indicators to researching social inequalities and examining human behavior.\nAs our world becomes increasingly data-driven, the importance of data science and statistics in social sciences continues to grow.\n\n\n\n\n\n\n\nNote\n\n\n\nIn social sciences, data science combines statistical methods, computational tools, and domain expertise to analyze complex social phenomena and human behavior.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-relationship-between-statistics-and-data-science",
    "href": "chapter1.html#the-relationship-between-statistics-and-data-science",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.2 The Relationship Between Statistics and Data Science",
    "text": "1.2 The Relationship Between Statistics and Data Science\nStatistics and data science are closely interrelated fields with significant overlap, especially in social sciences. Rather than strict divisions, it’s more accurate to view them as complementary approaches on a continuum:\n\nTraditional StatisticsModern Data ScienceEvolving Landscape\n\n\n\nRooted in mathematical theories and methods for data analysis\nEmphasizes statistical inference, hypothesis testing, and probability theory\nHistorically central to social sciences for analyzing surveys, experiments, and observational studies\n\n\n\n\nIntegrates statistical methods with computer science and domain expertise\nExpands focus to include machine learning, big data processing, and predictive modeling\nIn social sciences, often tackles large-scale digital data and complex behavioral datasets\n\n\n\n\nBoundaries between statistics and data science are increasingly blurred\nMany techniques and tools are shared across both fields\nSocial scientists often combine traditional statistical approaches with newer data science methods\nThe choice of approach depends on research questions, data characteristics, and specific analytical needs\n\n\n\n\nData science can be seen as an evolution and expansion of traditional statistics, incorporating new technologies and methodologies to handle larger and more complex social science datasets.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#essential-concepts-in-data-science-and-statistics",
    "href": "chapter1.html#essential-concepts-in-data-science-and-statistics",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.3 Essential Concepts in Data Science and Statistics",
    "text": "1.3 Essential Concepts in Data Science and Statistics\n\n1.3.1 Population, Sample and related concepts\n\n\n\n\n\n\nImportant\n\n\n\n\nData: Observations or measurements collected from a sample or population.\nPopulation: The entire set of individuals or items under study at a specific time.\n\nExample: All eligible voters in a country during a specific election year.\n\nSample: A subset of the population that is actually measured. A representative sample is a subset of a larger population that accurately reflects the characteristics of that population. The sample should mirror the population in terms of important traits like age, gender, socioeconomic status, etc. Often uses random sampling methods to avoid bias. Large enough to be statistically significant, but smaller than the full population.\n\nExample: 1500 randomly selected eligible voters surveyed in a pre-election poll.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nData Generating Process (DGP) and Superpopulation: Expanding on Traditional Concepts\nIn traditional statistics, we often work with two key concepts:\n\nPopulation: The entire group we want to study.\nSample: A subset of the population that we actually observe and analyze.\n\nWhile these concepts are fundamental, modern research often requires us to think beyond this dichotomy. This is where Data Generating Process (DGP) and superpopulation come in, extending our understanding of data and populations.\nData Generating Process (DGP):\nThe DGP is the underlying mechanism that produces the data we observe in the real world, whether in our sample or the entire population.\nIntuitive explanation: Think of the DGP as a complex system that takes various inputs and produces observable outcomes. It’s the “black box” that transforms causes into effects, not just for our sample, but for the entire population and beyond.\nExample: Consider a study on voter behavior. The traditional approach might define the population as “all registered voters” and take a sample from this group. The DGP, however, would include factors like demographic characteristics, economic conditions, political events, and media influence that shape voting behavior for all voters, sampled or not.\nSuperpopulation:\nThe superpopulation is a theoretical concept that extends beyond both the sample and the observable population to include all potential outcomes that could occur under similar conditions or processes.\nExamples:\n\nTraditional approach vs. Superpopulation approach:\n\nTraditional: population (all registered voters in a state), sample (1000 surveyed voters)\nSuperpopulation: All possible voters and voting scenarios, including future elections and hypothetical political contexts\n\nWhen sample equals population:\nIn studies of all 50 U.S. states:\n\nTraditional view: No distinction between sample and population\nSuperpopulation view: Considers these 50 states as a “sample” from a theoretical set of all possible state-policy interactions\n\n\nReal-world application: Let’s say researchers are studying the impact of a new urban planning policy across several cities:\n\nTraditional approach:\n\nPopulation: All cities in the country\nSample: The cities included in the study\n\nSuperpopulation approach:\n\nObserved data: The cities in the study\nSuperpopulation: All cities (existing or potential) where similar urban planning principles could be applied\n\n\nThe DGP in this case would be the complex set of factors that determine how urban planning policies affect city outcomes, applicable not just to the sampled cities or even all existing cities, but to the broader concept of “city” itself.\nImportant considerations:\n\nScope and limitations: Researchers should clearly define what units or processes they’re trying to understand, beyond just describing their sample and population.\nGeneralizability: When making claims about a superpopulation, researchers should explicitly state the “scope conditions” - the boundaries within which their findings are expected to hold true.\nContext-specificity: While the superpopulation concept allows for broader inferences than traditional sampling, it’s important to recognize that DGPs can vary across different contexts.\n\nBy incorporating these concepts alongside traditional population-sample thinking, researchers can make more nuanced inferences and be more transparent about the extent to which their findings might apply beyond their specific observed data, while still respecting the foundational principles of statistical inference.\nSummary example: Pizza Quality in New York City\nPopulation: All pizzerias currently operating in New York City. This is a finite, countable group of establishments that exist at the present moment.\nSample: A selection of 50 pizzerias chosen randomly from different boroughs of New York City. These are the specific pizzerias where researchers will taste and rate pizzas.\nSuperpopulation: All possible pizzerias that could exist in New York City, including:\n\nCurrent pizzerias\nFuture pizzerias that haven’t opened yet\nPizzerias that have closed down\nHypothetical pizzerias that might exist under different economic or cultural conditions\n\nThe superpopulation concept allows us to think about pizza quality beyond just the current snapshot of New York pizzerias.\nData Generating Process (DGP): The DGP is the complex set of factors that contribute to the quality of pizza in any given pizzeria. This might include:\n\nIngredients: Quality and source of flour, tomatoes, cheese, etc.\nChef’s skill: Training, experience, and personal touch of the pizza maker\nEquipment: Type and condition of the oven, tools used\nRecipe: Proportions of ingredients, preparation methods\nEnvironmental factors: Humidity, water quality in New York\nCultural influences: Local pizza-making traditions, customer preferences\nEconomic factors: Cost of ingredients, rent prices affecting business decisions\n\nThe DGP is like the “pizza quality recipe” that applies not just to our sample or even the current population, but to all potential pizzerias in the superpopulation.\nIntuitive Breakdown:\n\nIf you visit all current NYC pizzerias and rate them, you’ve assessed the population.\nIf you randomly select 50 pizzerias to visit and rate, you’ve taken a sample.\nIf you consider how pizza quality might vary in all possible NYC pizzerias (past, present, future, and hypothetical), you’re thinking about the superpopulation.\nIf you’re trying to understand all the factors that go into making a quality pizza in NYC, regardless of whether a pizzeria currently exists or not, you’re exploring the Data Generating Process.\n\n\n\n\n\n\n\n\ngraph TD\n    A[Data Generating Process DGP]\n    B(Population)\n    C[Sample]\n    A --&gt;|Generates| B\n    B --&gt;|Sampled from| C\n    C -.-&gt;|Inference| B\n    C -.-&gt;|Inference| A\n    B -.-&gt;|Inference| A\n    \n    classDef dgp fill:#f9f,stroke:#333,stroke-width:4px;\n    classDef pop fill:#bbf,stroke:#333,stroke-width:2px;\n    classDef sam fill:#bfb,stroke:#333,stroke-width:2px;\n    \n    class A dgp;\n    class B pop;\n    class C sam;\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation of the DGP, Population, and Sample Diagram\n\n\n\nThis diagram illustrates the relationships between the Data Generating Process (DGP), population, and sample, including paths of inference:\n\nDirect relationships (solid arrows):\n\nThe DGP generates the population\nSamples are drawn from the population\n\nInference paths (dashed arrows):\n\nFrom Sample to Population: Traditional statistical inference\nFrom Sample to DGP: Inferring about the underlying process from sample data\nFrom Population to DGP: Inferring about the DGP using complete population data\n\n\nFor example, in our study on the effect of electoral rules on voter turnout in Polish municipalities (1998-2010 municipal elections):\n\nWe have data for the entire population of municipalities, so we don’t need to infer from a sample to the population.\nOur focus is on using the complete population data (rightmost dashed arrow) to make inferences about the underlying DGP—the complex processes by which electoral rules influence voter turnout across municipalities.\nThis approach allows us to potentially understand the mechanisms behind how different electoral systems (e.g., proportional representation vs. plurality vote) affect turnout rates, and make informed predictions about how changes in electoral rules might impact future turnout or how these effects might generalize to similar contexts.\n\n\n\n\n\n\nPopulation vs. sample. Retrieved from: https://allmodelsarewrong.github.io/mse.html\n\n\nData forms the foundation of statistical analysis. It can be:\n\nPrimary data: Collected firsthand for a specific purpose\nSecondary data: Obtained from existing sources\n\nExample: In a study of university students’ heights, the population is all university students in the country, while a sample might be 1000 randomly selected students.\n\n\n1.3.2 Variables and Constants\nVariables are characteristics that can take different values across a dataset. They can be:\n\nQuantitative (numeric):\n\nContinuous: Height, weight, temperature\nDiscrete: Number of children, count of errors in a program\n\nQualitative (categorical):\n\nNominal: Blood type, eye color\nOrdinal: Education level, customer satisfaction rating\n\n\nConstants are fixed values that remain unchanged throughout an analysis.\n\n1.3.2.1 Types of Data in Social Sciences\nSocial science research deals with various types of data:\n\nQuantitative Data: Numerical data (e.g., survey responses, economic indicators)\nQualitative Data: Non-numerical data (e.g., interview transcripts, open-ended survey responses)\nBig Data: Large-scale digital traces (e.g., social media posts, online behavior logs)\n\n\n\n\n1.3.3 Population Parameters and Estimands\nPopulation parameters are numerical characteristics of a population. Key points:\n\nThey describe the entire population, not just a sample.\nThey are usually denoted by Greek letters.\nIn most cases, they cannot be directly calculated because we can’t measure the entire population.\nThey are determined by the underlying Data Generating Process (DGP).\n\nCommon population parameters include:\n\nPopulation mean (\\(\\mu\\)): The average value of a variable in the population.\nPopulation variance (\\(\\sigma^2\\)): A measure of variability in the population.\nPopulation proportion (\\(p\\)): The proportion of individuals in the population with a certain characteristic.\n\nAn estimand is the target of estimation - the specific population parameter or function of parameters that we aim to estimate. It defines what we want to know about the population.\n\n\n\n\n\n\nExample: Height of University Students\n\n\n\nConsider the height of all university students in a country:\n\n\\(\\mu\\) (estimand): The true average height of all university students (population mean)\n\\(\\sigma^2\\) (estimand): The true variance of heights in the population\n\nThese parameters are unknown estimands that we aim to estimate using sample data.\n\n\n\n\n1.3.4 Statistic(s) and Estimators\nA statistic (singular) or sample statistic is any quantity computed from values in a sample, which is considered for a statistical purpose.\nWhen a statistic is used for estimating an estimand (population parameter), it is called an estimator. Estimators are functions of sample data that provide approximate values for unknown population parameters.\nExamples of statistics/estimators:\n\nSample mean: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\) (estimates \\(\\mu\\))\nSample variance: \\(s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\\) (estimates \\(\\sigma^2\\))\nSample proportion: \\(\\hat{p} = \\frac{x}{n}\\) (estimates \\(p\\))\n\n\n\n1.3.5 Estimates\nAn estimate is the specific value obtained by applying an estimator to a particular sample. It is a point value that approximates the true estimand (population parameter).\nExample: If we calculate a sample mean height of 68 inches from our data, then 68 inches is our estimate of the estimand \\(\\mu\\) (population mean height).\n\n\n1.3.6 Statistical Models\n\n\n\n\n\n\nNote\n\n\n\nA model in science is a simplified representation of a complex system or phenomenon. It’s designed to help us understand, explain, and make predictions about the real world. Models can take various forms, including mathematical equations, computer simulations, or conceptual frameworks. They allow scientists to focus on key aspects of a system while ignoring less relevant details, making complex problems more manageable and easier to study.\n\n\nStatistical models represent relationships between variables and help in making predictions or inferences about estimands (population parameters).\nExample: A linear regression model \\(y = \\beta_0 + \\beta_1x + \\epsilon\\) describes the relationship between an independent variable \\(x\\) and a dependent variable \\(y\\), where:\n\n\\(y\\) is the dependent variable (e.g. quantity demanded)\n\\(x\\) is the independent variable (e.g. price, income level of the consumer)\n\\(\\beta_0\\) and \\(\\beta_1\\) are parameters, estimands to be estimated\n\\(\\epsilon\\) is the error term, representing unexplained variation\n\nI’ll help you create a callout note about causal inference and counterfactuals for Quarto.\n\n\n\n\n\n\nCausal Inference and Counterfactuals\n\n\n\nIn social sciences, we often want to understand what would have happened if we had done something differently - this hypothetical scenario is called a counterfactual. For instance:\n\nWhat would a person’s income be if they had attended college vs. if they hadn’t?\nHow would voter turnout change if voting was mandatory?\n\nSince we can’t observe both scenarios simultaneously, statistical models help us estimate these counterfactuals by: 1. Controlling for confounding variables 2. Comparing similar groups that differ only in the treatment 3. Using techniques like propensity score matching or instrumental variables\n\n\n\nFundamental problem of causal inference: We can think of causal inference as a PREDICTION problem. How could we predict the counterfactual given that we never observe it?\n\n\nRemember: Correlation ≠ Causation, but careful research design and statistical methods can help us make causal claims.\n\n\n\nConfounding bias and spurious correlation (https://www.bradyneal.com/causal-inference-course) drinking the night before is a common cause of sleeping with shoes on and waking up with a headache :-)\n\n\n\n\n\nReverse causality: https://ff13.fastforwardlabs.com/\n\n\n\n\nThis creates a note-type callout in Quarto that explains the concept succinctly while highlighting key points about counterfactuals and their estimation.\n\n\n1.3.7 Inference\nStatistical inference is the process of drawing conclusions about estimands (population parameters) based on sample data. It involves two main types:\n\nEstimation: Using sample statistics (estimators) to estimate estimands (population parameters)\nHypothesis testing: Making decisions about estimands based on sample evidence\n\n\n\n\n\n\n\nEstimation and Hypothesis Testing\n\n\n\n\nEstimation\n\nEstimation is about determining the likely value of a population parameter based on sample data. In the context of a binomial distribution, we might be interested in estimating the probability of success (p) for a certain event.\nExample: Coin Flipping\nLet’s say we’re flipping a coin 100 times and want to estimate the probability of getting heads.\n\nWe flip the coin 100 times and observe 55 heads.\nOur point estimate for p (probability of heads) would be 55/100 = 0.55\nWe might also calculate a confidence interval, e.g., a 95% confidence interval might be (0.45, 0.65).\n\nThe confidence interval tells us a range where we think the true probability might lie. In plain English, this means: “We’re 95% confident that the true probability of getting heads is between 45% and 65%.”\nThe goal here is to provide our best guess of the true probability of heads, along with a range of plausible values.\nImportant Concepts in Estimation:\n\nBias\n\nBias refers to the tendency of an estimator to systematically overestimate or underestimate the true value of a population parameter (estimand).\n\nAn unbiased estimator is one whose average value (when estimation is repeated multiple times) equals the true value of the parameter.\nBias can be understood as the difference between the average value of the estimator and the true value of the parameter.\n\n\nEfficiency\n\nEfficiency refers to the precision of an estimator. A more efficient estimator produces results closer to the true parameter value, i.e., it has less dispersion in its results.\n\nIt is most often measured by the variance of the estimator (lower variance means higher efficiency)\nFor unbiased estimators, efficiency is often compared using Mean Squared Error (MSE)\n\n\nHypothesis Testing\n\nHypothesis testing, on the other hand, is about making a decision between two competing claims about a population parameter. We typically have a null hypothesis (H0) and an alternative hypothesis (H1).\nExample: Is the Coin Fair?\nUsing the same coin-flipping scenario, let’s say we want to test if the coin is fair (p = 0.5) or biased towards heads (p &gt; 0.5).\n\nNull hypothesis (H0): p = 0.5 (the coin is fair)\nAlternative hypothesis (H1): p &gt; 0.5 (the coin is biased towards heads)\nWe observe 55 heads out of 100 flips\n\nIntroducing p-values and “Probabilistic Proof by Contradiction”\nNow, let’s dive into the concept of p-values and how hypothesis testing works as a kind of “probabilistic proof by contradiction”:\n\nWe start by assuming the null hypothesis (H0) is true. In this case, we assume the coin is fair.\nWe then ask: “If the coin were truly fair, how likely would it be to observe 55 or more heads out of 100 flips?”\nThis probability is called the p-value. It’s the probability of observing our data (or more extreme data) assuming the null hypothesis is true.\nIf this probability (the p-value) is very small, we have a contradiction: we’ve observed something that should be very rare if our assumption (H0) were true.\nWe typically set a threshold called the significance level (often 0.05 or 5%) for what we consider “very small.”\nIf the p-value is less than our chosen significance level, we reject H0. We conclude that our observation is too unlikely under H0, so we favor the alternative hypothesis instead.\nIf the p-value is greater than our significance level, we fail to reject H0. We don’t have enough evidence to conclude the coin is biased.\n\nThis process is like a “probabilistic proof by contradiction” because:\n\nWe start by assuming H0 (like assuming the opposite of what we want to prove in a proof by contradiction).\nWe see if this assumption leads to a very unlikely situation (our observed data).\nIf it does, we reject the assumption (H0) and favor the alternative.\n\nThe p-value quantifies exactly how unlikely our observation is under H0. A very small p-value (like 0.01) means: “If H0 were true, we’d only expect to see data this extreme about 1% of the time.”\nHypothesis testing and estimation are related but distinct statistical procedures; hypothesis testing can be used to make inferences about estimates and can complement estimation in several ways, e.g.:\n\nTesting Point Estimates: Hypothesis testing can be used to evaluate whether a point estimate is significantly different from a hypothesized value. For example, if we estimate that a coin has a 0.55 probability of landing heads, we could use a hypothesis test to determine if this is significantly different from 0.5 (a fair coin).\nParameter Significance: In multivariate models, hypothesis tests (like t-tests in regression) can help determine which estimated parameters are significantly different from zero, providing insight into which variables are important in the model.\n\n\n\n\n\n1.3.8 Relationships Between Concepts\n\nThe Data Generating Process (DGP) determines the actual values of population parameters (estimands).\nEstimands are estimated using statistics calculated from the sample (estimators).\nThe quality of estimators is assessed based on properties such as bias and efficiency in estimating the estimand.\nStatistical models use estimated parameters to describe relationships between variables in the population.\nStatistical inference involves drawing conclusions about estimands based on sample data, utilizing the properties of estimators.\n\n\n\n\n\n\n\nExample: Studying Voting Behavior\n\n\n\n\nPopulation: All eligible voters in a country\nEstimand: \\(p\\) = true proportion of voters supporting a given candidate\nSample: 1000 randomly selected eligible voters\nEstimator: \\(\\hat{p}\\) = proportion of voters in the sample supporting the candidate\nEstimate: Specific value of \\(\\hat{p}\\) calculated from the sample (e.g., 0.52)\nDGP: Complex interaction of factors influencing voting decisions, such as political beliefs, economic conditions, media exposure, and social networks.\n\nUnderstanding the DGP helps researchers interpret why the estimand \\(p\\) has a certain value and how it might change over time. For example, a sudden change in the economy might affect voters’ preferences, thereby changing the value of \\(p\\).\nBias and efficiency in the context of the example:\n\nIf \\(\\hat{p}\\) is an unbiased estimator, it means that when the survey is repeated multiple times with different samples, the average value of \\(\\hat{p}\\) will be close to the true value of \\(p\\).\nThe efficiency of \\(\\hat{p}\\) determines how dispersed the results of individual surveys are around this average. The less dispersion, the more efficient the estimator.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#core-components-of-data-science-in-scientific-research",
    "href": "chapter1.html#core-components-of-data-science-in-scientific-research",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.4 Core Components of Data Science in Scientific Research",
    "text": "1.4 Core Components of Data Science in Scientific Research\n\nData CollectionData ProcessingExploratory Data Analysis (EDA)Statistical InferenceMachine LearningData Visualization and CommunicationReproducibility and Open Science\n\n\n\nExperimental methods: Controlled studies where researchers manipulate variables to observe effects\nObservational studies: Gathering data by watching and recording without interfering\nSurveys and interviews: Collecting information directly from people through questions\nDigital data collection: Gathering data from online sources, sensors, or computer systems\nEthical considerations: Ensuring research respects participants’ rights and well-being\n\n\n\n\nData cleaning: Removing errors and inconsistencies from raw data\nHandling missing values: Addressing gaps in the dataset that could affect analysis\nData transformation: Converting data into formats suitable for analysis, like changing text to numbers\n\n\n\n\nDescriptive statistics: Summarizing data with measures like mean, median, and standard deviation\nData visualization: Creating graphs and charts to visually represent data patterns\nPattern identification: Discovering trends or relationships in the data\n\n\n\n\nHypothesis testing: Using data to evaluate claims about populations\nRegression analysis: Examining relationships between variables and making predictions\nCausal inference: Determining if one variable directly influences another\n\n\n\n\nSupervised learning: Training models to predict outcomes using data with known answers\nUnsupervised learning: Finding hidden patterns in data without predefined categories\nNatural Language Processing (NLP): Teaching computers to understand and analyze human language\n\n\n\n\nEffective visualizations: Creating clear, informative graphics to represent complex data\nScience communication: Explaining findings to different audiences, from experts to the public\nScientific writing: Preparing research papers and reports to share results\n\n\n\n\nVersion control: Tracking changes in data and code throughout the research process\nOpen data practices: Sharing research data and methods for verification and further study\nReproducible workflows: Documenting research steps so others can repeat the study",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#tools-for-data-science-in-social-sciences",
    "href": "chapter1.html#tools-for-data-science-in-social-sciences",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.5 Tools for Data Science in Social Sciences",
    "text": "1.5 Tools for Data Science in Social Sciences\nIn this course, we’ll use R for our data analysis, as it’s widely used in social science research.\n\n1.5.1 R for Social Science Data Analysis\nR offers powerful capabilities for social science research, from data manipulation to advanced statistical modeling.\n\n\nKliknij, aby pokazać/ukryć kod R\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nKliknij, aby pokazać/ukryć kod R\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate example data with a Simpson's Paradox\nn &lt;- 1000\ndata &lt;- tibble(\n  age_group = sample(c(\"Young\", \"Middle\", \"Old\"), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),\n  education_years = case_when(\n    age_group == \"Young\" ~ rnorm(n, mean = 10, sd = 1),\n    age_group == \"Middle\" ~ rnorm(n, mean = 13, sd = 1),\n    age_group == \"Old\" ~ rnorm(n, mean = 16, sd = 1)\n  ),\n  income = case_when(\n    age_group == \"Young\" ~ 70000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Middle\" ~ 50000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Old\" ~ 30000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000)\n  )\n)\n\n# Basic data summary\nsummary(data)\n\n\n  age_group         education_years      income     \n Length:1000        Min.   : 6.628   Min.   :34068  \n Class :character   1st Qu.:10.913   1st Qu.:51508  \n Mode  :character   Median :13.004   Median :63376  \n                    Mean   :12.986   Mean   :63307  \n                    3rd Qu.:14.934   3rd Qu.:75023  \n                    Max.   :18.861   Max.   :96620  \n\n\nKliknij, aby pokazać/ukryć kod R\n# Correlation analysis\ncor(data %&gt;% select(education_years, income))\n\n\n                education_years     income\neducation_years       1.0000000 -0.8152477\nincome               -0.8152477  1.0000000\n\n\nKliknij, aby pokazać/ukryć kod R\n# Overall trend (Simpson's Paradox)\noverall_plot &lt;- ggplot(data, aes(x = education_years, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Overall Relationship between Education and Income\",\n       subtitle = \"Simpson's Paradox: Appears negative\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Trend by age group (Resolving Simpson's Paradox)\ngrouped_plot &lt;- ggplot(data, aes(x = education_years, y = income, color = age_group)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Relationship between Education and Income by Age Group\",\n       subtitle = \"Resolving Simpson's Paradox: Positive relationship within groups\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Statistical analysis\nmodel_overall &lt;- lm(income ~ education_years, data = data)\nmodel_by_age &lt;- lm(income ~ education_years + age_group, data = data)\n\n# Print results\nprint(overall_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(grouped_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_overall))\n\n\n\nCall:\nlm(formula = income ~ education_years, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24451  -5439    235   5262  34328 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     121814.7     1339.5   90.94   &lt;2e-16 ***\neducation_years  -4505.4      101.3  -44.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7976 on 998 degrees of freedom\nMultiple R-squared:  0.6646,    Adjusted R-squared:  0.6643 \nF-statistic:  1978 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_by_age))\n\n\n\nCall:\nlm(formula = income ~ education_years + age_group, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-14827  -3369    118   3356  16388 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      48270.8     2028.4  23.797  &lt; 2e-16 ***\neducation_years   1135.5      154.6   7.345 4.26e-13 ***\nage_groupOld    -19942.8      593.2 -33.619  &lt; 2e-16 ***\nage_groupYoung   20461.1      600.7  34.064  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4950 on 996 degrees of freedom\nMultiple R-squared:  0.8711,    Adjusted R-squared:  0.8707 \nF-statistic:  2244 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\n# Calculate and print correlations\noverall_cor &lt;- cor(data$education_years, data$income)\ngroup_cors &lt;- data %&gt;%\n  group_by(age_group) %&gt;%\n  summarize(correlation = cor(education_years, income))\n\nprint(\"Overall correlation:\")\n\n\n[1] \"Overall correlation:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(overall_cor)\n\n\n[1] -0.8152477\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(\"Correlations by age group:\")\n\n\n[1] \"Correlations by age group:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(group_cors)\n\n\n# A tibble: 3 × 2\n  age_group correlation\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Middle          0.185\n2 Old             0.291\n3 Young           0.223\n\n\nThis example demonstrates basic data manipulation, summary statistics, and visualization using R, which are common tasks in social science research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#causal-inference-vs.-observational-studies",
    "href": "chapter1.html#causal-inference-vs.-observational-studies",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.6 Causal Inference vs. Observational Studies",
    "text": "1.6 Causal Inference vs. Observational Studies\nIn social sciences and beyond, understanding the relationship between variables is crucial. Two key approaches to this are causal inference and observational studies, each with its own strengths and limitations.\n\nCausal InferenceObservational StudiesKey Distinction: Correlation vs. Causation\n\n\n\nAims to establish cause-and-effect relationships\nOften involves experimental designs or advanced statistical techniques\nSeeks to answer “What if?” questions and determine the impact of interventions\nExamples: Randomized controlled trials, quasi-experimental designs, instrumental variables\n\n\n\n\nExamine relationships between variables without direct intervention\nRely on data collected from natural settings or existing datasets\nCan identify correlations and patterns but struggle to establish causation\nExamples: Cohort studies, case-control studies, cross-sectional surveys\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember: Correlation Does Not Imply Causation\n\n\n\nA fundamental principle in research is that correlation between two variables does not necessarily imply a causal relationship. This concept is crucial when interpreting results from observational studies.\n\nCorrelation: Measures the strength and direction of a relationship between variables\nCausation: Indicates that changes in one variable directly cause changes in another\n\nWhile strong correlations can suggest potential causal links, additional evidence and rigorous methods are required to establish causality.\n\n\n\nChallenges in Establishing CausalityMethods to Strengthen Causal ClaimsImportance in Social Sciences\n\n\n\nConfounding variables: Unmeasured factors that affect both the presumed cause and effect\nReverse causality: The presumed effect might actually be causing the presumed cause\nSelection bias: Non-random selection of subjects into study groups\n\n\n\n\nRandomized controlled trials (when ethical and feasible)\nNatural experiments or quasi-experimental designs\nPropensity score matching\nDifference-in-differences analysis\nInstrumental variable approaches\nDirected acyclic graphs (DAGs) for visualizing causal relationships\n\n\n\nUnderstanding the distinction between causal inference and observational studies is crucial in social sciences, where ethical considerations often limit experimental manipulation. Researchers must carefully design studies and interpret results to avoid misleading conclusions about causality.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#models-in-science-from-deterministic-to-stochastic",
    "href": "chapter1.html#models-in-science-from-deterministic-to-stochastic",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.7 Models in Science: From Deterministic to Stochastic (*)",
    "text": "1.7 Models in Science: From Deterministic to Stochastic (*)\nModels are essential tools in scientific research, helping scientists to represent, understand, and predict complex phenomena. This section explores the main types of models used in science, along with examples of their applications. It’s important to note that these categories often overlap, and many scientific models incorporate multiple aspects.\n\n1.7.1 Mathematical Models\nMathematical models use equations and mathematical concepts to describe and analyze systems or phenomena. They can be further divided into several subcategories, though it’s important to note that some complex models may incorporate elements from multiple categories:\n\n1.7.1.1 a. Deterministic Models\nDeterministic models provide precise predictions based on a set of variables, without incorporating randomness at the macroscopic level.\nExample: Newton’s laws of motion, which can precisely predict the motion of objects under known forces in classical mechanics.\n\n\n1.7.1.2 b. Stochastic Models\nStochastic models incorporate randomness and probability. However, it’s crucial to distinguish between two fundamentally different types of stochastic models:\n\n1.7.1.2.1 i. Classical Stochastic Models\nThese models deal with randomness arising from incomplete information or complex interactions in classical systems. The underlying system is deterministic, but practical limitations in measurement or computation lead to the use of probabilistic descriptions.\nExample: Regression models in statistics, where the randomness represents unexplained variation or measurement error:\n\\[y = β_0 + β_1x + ε\\]\nWhere:\n\n\\(y\\) is the dependent variable (e.g. quantity demanded)\n\\(x\\) is the independent variable (e.g. price, income level of the consumer)\n\\(β_0\\) and \\(β_1\\) are parameters\n\\(ε\\) is the error term, representing unexplained variation\n\n\n\n1.7.1.2.2 ii. Quantum Stochastic Models\nThese models deal with the fundamental, irreducible randomness inherent in quantum mechanical systems. This randomness is not due to lack of information, but is a core feature of quantum reality.\nExample: The Standard Model in particle physics, which describes particle interactions using quantum field theory. For instance, the decay of a particle is inherently probabilistic:\n\\[P(t) = e^{-t/τ}\\]\nWhere:\n\n\\(P(t)\\) is the probability that the particle has not decayed after time t\n\\(τ\\) is the mean lifetime of the particle\n\n\n\n\n1.7.1.3 c. Computer Simulation Models\nComputer simulations use algorithms and computational methods based on mathematical models to simulate complex systems and predict their behavior over time. These can be deterministic or stochastic.\nExample: Climate models that simulate the Earth’s climate system, incorporating factors such as atmospheric composition, ocean currents, and solar radiation to project future climate scenarios.\n\n\n\n1.7.2 Conceptual Models\nConceptual models are abstract representations of systems or processes, often using diagrams or flowcharts to illustrate relationships between components.\nExample: The water cycle model in Earth sciences, which illustrates the continuous movement of water within the Earth and atmosphere through processes such as evaporation, precipitation, and runoff.\n\n\n1.7.3 Physical Models\nPhysical models are tangible representations of objects or systems, often scaled down or simplified versions of the real thing.\nExample: Wind tunnel models in aerodynamics research, used to study the effects of air moving past solid objects and optimize designs for aircraft, vehicles, or buildings.\n\n\n1.7.4 Theoretical Models\nTheoretical models are abstract frameworks based on fundamental principles and hypotheses, often used to explain observed phenomena or predict new ones. These models frequently employ mathematical formulations and can be deterministic or stochastic in nature.\nExample: The theory of evolution by natural selection, which provides a framework for understanding the diversity and adaptation of life forms over time.\n\n\n1.7.5 Conclusion\nThese various forms of models play crucial roles in scientific research, each offering unique advantages for understanding and predicting natural phenomena. Scientists often use multiple types of models in conjunction to gain comprehensive insights into complex systems and processes.\nIt’s important to recognize that these categories are not mutually exclusive and often overlap:\n\nMathematical models form the foundation for many other types of models, including computer simulations and some theoretical models.\nComputer simulation models are essentially mathematical models implemented through computational methods, and can be either deterministic or stochastic.\nTheoretical models often employ mathematical formulations and may be implemented as computer simulations.\nPhysical models may be designed based on mathematical models and can be used to validate computer simulations.\n\nThe choice of model type often depends on the specific research question, the nature of the system being studied, the available data, and the computational resources at hand. As science progresses, the boundaries between these model types continue to blur, leading to increasingly sophisticated and interdisciplinary approaches to modeling complex phenomena.\nIt’s crucial to distinguish between different types of stochastic models. Classical stochastic models, such as those used in regression analysis, deal with randomness arising from incomplete information or complex interactions in otherwise deterministic systems. In contrast, quantum stochastic models, like those in particle physics, deal with fundamental, irreducible randomness inherent in quantum mechanical systems. This distinction reflects the profound differences between classical and quantum paradigms in physics and highlights the diverse ways in which probability is used in scientific modeling.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#understanding-spurious-correlations-confounders-and-colliders",
    "href": "chapter1.html#understanding-spurious-correlations-confounders-and-colliders",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.8 Understanding Spurious Correlations, Confounders, and Colliders (*)",
    "text": "1.8 Understanding Spurious Correlations, Confounders, and Colliders (*)\nIn this tutorial, we’ll explore three important concepts in statistical analysis: spurious correlations, confounders, and colliders. Understanding these concepts is crucial for avoiding misinterpretation of data and drawing incorrect conclusions from statistical analyses.\nLet’s start by loading the necessary libraries:\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nset.seed(123) # for reproducibility\n\n\n1.8.1 Spurious Correlations\nSpurious correlations are relationships between variables that appear to be causal but are actually coincidental or caused by an unseen third factor.\n\n\n1.8.2 Example: Ice Cream Sales and Drowning Incidents\nLet’s create a dataset that shows a spurious correlation between ice cream sales and drowning incidents:\n\nn &lt;- 100\nspurious_data &lt;- tibble(\n  temperature = rnorm(n, mean = 25, sd = 5),\n  ice_cream_sales = 100 + 5 * temperature + rnorm(n, sd = 10),\n  drowning_incidents = 1 + 0.5 * temperature + rnorm(n, sd = 2)\n)\n\nggplot(spurious_data, aes(x = ice_cream_sales, y = drowning_incidents)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Spurious Correlation: Ice Cream Sales vs. Drowning Incidents\",\n       x = \"Ice Cream Sales\", y = \"Drowning Incidents\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis plot shows a positive correlation between ice cream sales and drowning incidents. However, this relationship is spurious. The real cause for both is the temperature:\n\nggplot(spurious_data, aes(x = temperature)) +\n  geom_point(aes(y = ice_cream_sales), color = \"blue\") +\n  geom_point(aes(y = drowning_incidents * 10), color = \"red\") +\n  geom_smooth(aes(y = ice_cream_sales), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(aes(y = drowning_incidents * 10), method = \"lm\", se = FALSE, color = \"red\") +\n  scale_y_continuous(\n    name = \"Ice Cream Sales\",\n    sec.axis = sec_axis(~./10, name = \"Drowning Incidents\")\n  ) +\n  labs(title = \"Temperature as the Common Cause\",\n       x = \"Temperature\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n1.8.3 Confounders\nA confounder is a variable that influences both the dependent variable and independent variable, causing a spurious association.\n\n\n1.8.4 Example: Education, Income, and Age\n\nlibrary(tidyverse)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nn &lt;- 1000\nconfounder_data &lt;- tibble(\n  age = runif(n, 25, 65),\n  education = round(10 + 0.1 * age + rnorm(n, sd = 2)),\n  income = 20000 + 1000 * education + 500 * age + rnorm(n, sd = 5000)\n)\n\n# Without controlling for age\nmodel_naive &lt;- lm(income ~ education, data = confounder_data)\n# Controlling for age\nmodel_adjusted &lt;- lm(income ~ education + age, data = confounder_data)\n\n# Create age groups for visualization\nconfounder_data &lt;- confounder_data %&gt;%\n  mutate(age_group = cut(age, breaks = 3, labels = c(\"Young\", \"Middle\", \"Old\")))\n\n# Visualize\nggplot(confounder_data, aes(x = education, y = income)) +\n  geom_point(aes(color = age), alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1.2) +\n  geom_smooth(aes(group = age_group, color = as.numeric(age_group)), \n              method = \"lm\", se = FALSE, linewidth = 1) +\n  scale_color_viridis_c(name = \"Age\", \n                        breaks = c(30, 45, 60), \n                        labels = c(\"Young\", \"Middle\", \"Old\")) +\n  labs(title = \"Education vs Income, Confounded by Age\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCompare the coefficients:\n\nsummary(model_naive)$coefficients[\"education\", \"Estimate\"]\n\n[1] 2328.718\n\nsummary(model_adjusted)$coefficients[\"education\", \"Estimate\"]\n\n[1] 1101.783\n\n\nThe effect of education on income is overestimated when we don’t control for age.\n\n\n1.8.5 Colliders\nA collider is a variable that is influenced by both the independent variable and the dependent variable. Controlling for a collider can introduce a spurious correlation.\n\n\n1.8.6 Example: Job Satisfaction, Salary, and Work-Life Balance\nLet’s create a dataset where work-life balance is a collider between job satisfaction and salary:\n\nn &lt;- 1000\ncollider_data &lt;- tibble(\n  job_satisfaction = rnorm(n),\n  salary = rnorm(n),\n  work_life_balance = -0.5 * job_satisfaction - 0.5 * salary + rnorm(n, sd = 0.5)\n)\n\n# Without controlling for work-life balance\nmodel_correct &lt;- lm(salary ~ job_satisfaction, data = collider_data)\n\n# Incorrectly controlling for work-life balance\nmodel_collider &lt;- lm(salary ~ job_satisfaction + work_life_balance, data = collider_data)\n\n# Visualize\nggplot(collider_data, aes(x = job_satisfaction, y = salary, color = work_life_balance)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_color_viridis_c() +\n  labs(title = \"Job Satisfaction vs Salary, Work-Life Balance as Collider\",\n       x = \"Job Satisfaction\", y = \"Salary\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCompare the coefficients:\n\nsummary(model_correct)$coefficients[\"job_satisfaction\", \"Estimate\"]\n\n[1] 0.02063487\n\nsummary(model_collider)$coefficients[\"job_satisfaction\", \"Estimate\"]\n\n[1] -0.4794016\n\n\nControlling for the collider (work-life balance) introduces a spurious correlation between job satisfaction and salary.\n\n\n1.8.7 Conclusion\nUnderstanding spurious correlations, confounders, and colliders is crucial for proper statistical analysis and causal inference. Always consider the underlying causal structure of your data and be cautious about which variables you control for in your analyses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#further-reading",
    "href": "chapter1.html#further-reading",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.9 Further Reading",
    "text": "1.9 Further Reading\n\nPearl, J. (2009). Causality: Models, Reasoning, and Inference. Cambridge University Press.\nHernán, M. A., & Robins, J. M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#ethical-considerations-in-social-science-data-analysis",
    "href": "chapter1.html#ethical-considerations-in-social-science-data-analysis",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.10 Ethical Considerations in Social Science Data Analysis",
    "text": "1.10 Ethical Considerations in Social Science Data Analysis\nEthics play a crucial role in social science research:\n\nPrivacy and Consent: Ensuring participant privacy and informed consent\nData Protection: Securely storing and managing sensitive personal data\nBias and Representation: Addressing sampling bias and ensuring diverse representation\nTransparency: Clearly communicating research methods and limitations\nSocial Impact: Considering the potential societal implications of research findings\n\n\n\n\n\n\n\nImportant\n\n\n\nSocial scientists must carefully consider the ethical implications of their data collection, analysis, and dissemination practices.\n\n\n\n1.10.1 Key Takeaways\n\nData science in social sciences builds upon traditional statistical methods, incorporating new technologies to analyze complex social phenomena.\nUnderstanding concepts like population, sample, and data generating processes is crucial for valid social science research.\nThe data science process in social research involves multiple steps from ethical data collection to the communication of insights.\nR is a powerful tool for social science data analysis, offering a wide range of capabilities.\nEthical considerations should be at the forefront of any social science data project.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-a-classical-vs-quantum-randomness-understanding-the-fundamental-differences",
    "href": "chapter1.html#appendix-a-classical-vs-quantum-randomness-understanding-the-fundamental-differences",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.11 Appendix A: Classical vs Quantum Randomness: Understanding the Fundamental Differences",
    "text": "1.11 Appendix A: Classical vs Quantum Randomness: Understanding the Fundamental Differences\nTo understand how the randomness in quantum mechanics differs from the randomness represented by the error term in regression models, we need to examine their origins, nature, and implications.\n\n1.11.1 Origin of Randomness\n\n1.11.1.1 Classical Randomness (Regression Models)\n\nSource: Incomplete information or complex interactions in an otherwise deterministic system.\nNature: Epistemic uncertainty (due to lack of knowledge).\nExample: In a regression model, \\(y = β_0 + β_1x + ε\\), the error term ε represents unexplained variation.\n\n\n\n1.11.1.2 Quantum Randomness\n\nSource: Fundamental property of quantum systems.\nNature: Ontic uncertainty (inherent to the system, not due to lack of knowledge).\nExample: The exact time of decay of a radioactive atom cannot be predicted, only its probability.\n\n\n\n\n1.11.2 Philosophical Implications\n\n1.11.2.1 Classical Randomness\n\nDeterminism: Underlying reality is deterministic; randomness reflects our ignorance.\nHidden Variables: In principle, if we had complete information, we could predict outcomes precisely.\n\n\n\n1.11.2.2 Quantum Randomness\n\nIndeterminism: Randomness is a fundamental feature of reality, not just our description of it.\nNo Hidden Variables: Even with complete information about a quantum system, some outcomes remain unpredictable (as suggested by Bell’s theorem).\n\n\n\n\n1.11.3 Mathematical Treatment\n\n1.11.3.1 Classical Randomness\n\nProbability Theory: Based on classical probability theory.\nDistribution: Often assumed to follow known distributions (e.g., normal distribution in many regression models).\nCentral Limit Theorem: Applies to large samples of random variables.\n\n\n\n1.11.3.2 Quantum Randomness\n\nQuantum Probability: Based on the mathematical framework of quantum mechanics.\nWave Function: Describes the quantum state and its evolution.\nBorn Rule: Gives probabilities of measurement outcomes from the wave function.\n\n\n\n\n1.11.4 Predictability and Control\n\n1.11.4.1 Classical Randomness\n\nReducible: In principle, can be reduced by gathering more data or improving measurement precision.\nControllable: Systematic errors can be identified and corrected.\n\n\n\n1.11.4.2 Quantum Randomness\n\nIrreducible: Cannot be eliminated even with perfect measurements.\nFundamentally Uncontrollable: The act of measurement itself affects the system (measurement problem).\n\n\n\n\n1.11.5 Practical Implications\n\n1.11.5.1 Classical Randomness\n\nError Reduction: Focus on improving measurement techniques and data collection.\nModel Refinement: Aim to explain more variance and reduce the error term.\n\n\n\n1.11.5.2 Quantum Randomness\n\nInherent Limitation: Accept fundamental limits on predictability.\nProbabilistic Predictions: Focus on accurate probability distributions rather than exact outcomes.\n\n\n\n\n1.11.6 Examples to Understand the Difference\n\n1.11.6.1 Classical Randomness Example\nImagine flipping a coin. Classical physics says the outcome is determined by initial conditions (force applied, air resistance, etc.). The “randomness” comes from our inability to precisely measure and account for all these factors.\n\n\n1.11.6.2 Quantum Randomness Example\nIn the double-slit experiment, individual particles show interference patterns as if they went through both slits simultaneously. The exact path of any individual particle is fundamentally undetermined until measured, and this indeterminacy cannot be resolved by more precise measurements.\n\n\n\n1.11.7 Conclusion\nWhile both types of randomness lead to probabilistic predictions, their fundamental natures are quite different:\n\nClassical randomness in regression models is a reflection of our incomplete knowledge or measurement limitations in an otherwise deterministic system.\nQuantum randomness is a fundamental property of quantum systems, representing an inherent indeterminacy in nature that persists even with perfect knowledge and measurement.\n\nUnderstanding these differences is crucial for correctly interpreting and applying statistical models in different scientific contexts, from social sciences using regression analysis to quantum physics experiments.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-b-large-language-models---understanding-their-stochastic-nature",
    "href": "chapter1.html#appendix-b-large-language-models---understanding-their-stochastic-nature",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.12 Appendix B: Large Language Models - Understanding Their Stochastic Nature",
    "text": "1.12 Appendix B: Large Language Models - Understanding Their Stochastic Nature\nLarge Language Models (LLMs) like GPT-3, BERT, and Claude have revolutionized natural language processing but can make puzzling mistakes, especially in mathematical tasks. This appendix explains LLMs’ functioning, stochastic nature, and compares them to classical statistical models.\n\n1.12.1 LLM Basics and Stochastic Nature\nLLMs are trained on vast text data to predict the probability distribution of the next token in a sequence. They use transformer architectures for processing and generating text. Key aspects of their stochastic nature include:\n\nProbabilistic token selection: LLMs choose each word based on calculated probabilities, not fixed rules.\nTemperature-controlled randomness: A “temperature” parameter adjusts the randomness of selections, balancing creativity and coherence.\nNon-deterministic outputs: The same input can produce different outputs in separate runs.\nContextual ambiguity: LLMs interpret context probabilistically, sometimes leading to misunderstandings.\n\n\n\n1.12.2 Comparison to Classical Statistical Models\nTo understand LLMs better, let’s compare them to Ordinary Least Squares (OLS) regression:\n\n\n\n\n\n\n\n\nAspect\nOLS Regression\nLarge Language Models\n\n\n\n\nBasic Function\nPredicts continuous outcomes based on input variables\nPredicts probability distribution of next token based on previous tokens\n\n\nInput-Output\nContinuous variables, linear relationships\nDiscrete tokens, non-linear relationships\n\n\nPrediction Type\nPoint predictions with confidence intervals\nProbability distributions over possible tokens\n\n\nModel Complexity\nFew parameters\nBillions of parameters\n\n\nInterpretability\nClear coefficient interpretations\nLargely opaque internal workings\n\n\nNoise Handling\nAssumes random noise in outcome variable\nDeals with natural language variability\n\n\nExtrapolation\nLess reliable outside training range\nLess reliable on unfamiliar topics\n\n\n\nBoth models aim to learn input-output mappings based on training data patterns.\n\n\n1.12.3 Implications for Mathematical Tasks\nLLMs’ stochastic nature affects mathematical operations:\n\nVariable outputs for repeated calculations: Each attempt might yield a different result due to probabilistic token selection.\nConfidence doesn’t guarantee correctness: High model confidence can occur even for incorrect answers.\nApproximation rather than exact computation: LLMs pattern-match rather than perform precise calculations.\n\nLimitations in mathematical tasks stem from:\n\nTraining objective mismatch: LLMs are trained for language prediction, not mathematical accuracy.\nLack of explicit mathematical reasoning: They don’t have built-in mathematical rules or operations.\nAbsence of working memory: LLMs can’t reliably store and manipulate intermediate results.\nLimited context window: They may lose track of relevant information in long problems.\nTraining data limitations: Underrepresentation of certain math concepts can lead to poor performance.\nLack of consistency checks: LLMs don’t verify the logical consistency of their outputs.\n\n\n\n1.12.4 Best Practices and Conclusion\nWhen using LLMs for mathematical tasks:\n\nFocus on conceptual explanations, not precise calculations: LLMs excel at explaining concepts but may falter on exact computations.\nVerify results with dedicated software: Always double-check LLM calculations with proper math tools.\nBreak down complex problems: Splitting tasks into smaller steps can improve LLM performance.\nBe aware of rephrasing effects: Different phrasings of the same problem may yield different results.\nUse as assistive tools, not replacements for expertise: LLMs should complement, not substitute, mathematical expertise.\n\nUnderstanding LLMs’ probabilistic nature helps leverage their strengths in language tasks while recognizing their limitations in domains requiring deterministic precision, like mathematics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-c-deterministic-and-stochastic-models",
    "href": "chapter1.html#appendix-c-deterministic-and-stochastic-models",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.13 Appendix C: Deterministic and Stochastic Models (*)",
    "text": "1.13 Appendix C: Deterministic and Stochastic Models (*)\n\n1.13.1 Deterministic Models\nDeterministic models are those where the output is fully determined by the parameter values and the initial conditions. These models are often used in physics and engineering.\n\n\n1.13.2 Example: Uniformly Accelerated Motion\nA classic example of a deterministic model is uniformly accelerated motion, described by the equation:\n\\[x(t) = x_0 + v_0t + \\frac{1}{2}at^2\\]\nWhere:\n\n\\(x(t)\\) is the position at time \\(t\\)\n\\(x_0\\) is the initial position\n\\(v_0\\) is the initial velocity\n\\(a\\) is the acceleration\n\\(t\\) is time\n\nLet’s simulate this in R:\n\n# Uniformly accelerated motion\nsimulate_accelerated_motion &lt;- function(x0, v0, a, t) {\n  x0 + v0 * t + 0.5 * a * t^2\n}\n\n# Generating data\nt &lt;- seq(0, 10, by = 0.1)\nx &lt;- simulate_accelerated_motion(x0 = 0, v0 = 2, a = 1, t = t)\n\n# Plot\nplot(t, x, type = \"l\", xlab = \"Time\", ylab = \"Position\", \n     main = \"Uniformly Accelerated Motion\")\n\n\n\n\n\n\n\n\nThis code will generate a plot of uniformly accelerated motion, which is an intuitive example from Newtonian dynamics. In this case, an object starts moving with an initial velocity and accelerates uniformly, resulting in a parabolic trajectory on the position-time graph.\n\n\n1.13.3 Stochastic Models in Social Sciences\nStochastic models incorporate randomness and are often used in social sciences where there’s inherent uncertainty in the systems being studied.\n\n\n1.13.4 Example: Ordinary Least Squares (OLS) Regression\nOLS is a fundamental stochastic model in social sciences. It’s represented as:\n\\[Y = \\beta_0 + \\beta_1X + \\epsilon\\]\nWhere:\n\n\\(Y\\) is the dependent variable\n\\(X\\) is the independent variable\n\\(\\beta_0\\) and \\(\\beta_1\\) are parameters\n\\(\\epsilon\\) is the error term (stochastic component)\n\nLet’s demonstrate OLS in R:\n\n# Generate some sample data\nset.seed(123)\nX &lt;- rnorm(100)\nY &lt;- 2 + 3*X + rnorm(100, sd = 0.5)\n\n# Fit OLS model\nmodel &lt;- lm(Y ~ X)\n\n# Summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95367 -0.34175 -0.04375  0.29032  1.64520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.94860    0.04878   39.95   &lt;2e-16 ***\nX            2.97376    0.05344   55.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4854 on 98 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.969 \nF-statistic:  3097 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n# Plot\nplot(X, Y, main = \"OLS Regression\")\nabline(model, col = \"red\")\n\n\n\n\n\n\n\n\nThis will fit an OLS model to some simulated data and plot the results.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff\n\n\n\n\n1.13.5 Advanced Stochastic Models: Large Language Models\nLarge Language Models (LLMs) like GPT-3 are complex stochastic models used in natural language processing. While we can’t implement a full LLM in this tutorial, we can discuss its principles.\nLLMs are based on the transformer architecture and use self-attention mechanisms. They’re trained on vast amounts of text data and learn to predict the next token in a sequence.\nThe core of an LLM can be thought of as a conditional probability distribution:\n\\[P(x_t | x_{&lt;t}, \\theta)\\]\nWhere: - \\(x_t\\) is the current token - \\(x_{&lt;t}\\) represents all previous tokens - \\(\\theta\\) are the model parameters\n\n\n\n\n\n\nNote\n\n\n\nTokens in Large Language Models (LLMs) are the basic units of text that the model processes. They can be thought of as pieces of words or punctuation marks. Here are key points about tokens:\nDefinition: Tokens are the smallest units of text that an LLM processes. They can be whole words, parts of words, or even individual characters or punctuation marks. Tokenization: The process of breaking text into tokens is called tokenization. LLMs use specific algorithms to perform this task. Examples:\nThe word “cat” might be a single token. A longer word like “understanding” might be broken into multiple tokens, e.g., “under” and “standing”. Punctuation marks like “.” or “?” are often individual tokens. Common prefixes or suffixes might be their own tokens.\nVocabulary: LLMs have a fixed vocabulary of tokens they recognize. This vocabulary typically ranges from tens of thousands to hundreds of thousands of tokens. Significance: The way text is tokenized can affect how the model understands and generates language. It’s particularly important for handling different languages, rare words, or specialized vocabulary. Context: In the equation for LLMs: \\[P(x_t | x_{&lt;t}, \\theta)\\] Where:\n\\(x_t\\) represents the current token \\(x_{&lt;t}\\) represents all previous tokens in the sequence \\(\\theta\\) represents the model parameters\n\n\nUnlike deterministic models, LLMs produce different outputs even for the same input due to their stochastic nature.\n\n\n1.13.6 Conclusion\nWe’ve explored a range of models from deterministic to highly complex stochastic ones. Each type of model has its place in science, depending on the system being studied and the level of uncertainty involved.\nRemember, the choice between deterministic and stochastic models often depends on the nature of the system you’re studying and the questions you’re trying to answer. Deterministic models are great for systems with well-understood mechanics, while stochastic models shine when dealing with inherent randomness or complex, not fully understood systems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-d-introduction-to-r-rstudio-and-tidyverse",
    "href": "chapter1.html#appendix-d-introduction-to-r-rstudio-and-tidyverse",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.14 Appendix D: Introduction to R, RStudio, and tidyverse",
    "text": "1.14 Appendix D: Introduction to R, RStudio, and tidyverse\nR is a powerful programming language and environment for statistical computing and graphics. It’s widely used in academia, especially in fields like social sciences, for data analysis and visualization.\n\n1.14.0.1 Key features of R:\n\nOpen-source and free\nExtensive package ecosystem\nStrong community support\nExcellent for statistical analysis and data visualization\n\n\n\n1.14.1 Getting Started with RStudio\nRStudio is an Integrated Development Environment (IDE) for R that makes it easier to work with R.\n\n1.14.1.1 Installing R and RStudio\n\nDownload and install R from CRAN\nDownload and install RStudio from RStudio’s website\n\n\n\n1.14.1.2 RStudio Interface\nRStudio has four main panes:\n\nSource Editor: Where you write and edit your R scripts\nConsole: Where you can type R commands and see output\nEnvironment/History: Shows all objects in your workspace and command history\nFiles/Plots/Packages/Help: Multipurpose pane for file management, viewing plots, managing packages, and accessing help\n\n\n\n1.14.1.3 Basic RStudio Features\n\nCreating a new R script: File &gt; New File &gt; R Script\nRunning code: Select code and press Ctrl+Enter (Cmd+Enter on Mac)\nInstalling packages: Tools &gt; Install Packages\nGetting help: Type ?function_name in the console\n\n\n\n\n1.14.2 R Basics\n\n1.14.2.1 Data Types in R\n\n# Numeric\nx &lt;- 10.5\nclass(x)\n\n[1] \"numeric\"\n\n# Integer\ny &lt;- 1L\nclass(y)\n\n[1] \"integer\"\n\n# Character\nname &lt;- \"Alice\"\nclass(name)\n\n[1] \"character\"\n\n# Logical\nis_student &lt;- TRUE\nclass(is_student)\n\n[1] \"logical\"\n\n\n\n\n1.14.2.2 Data Structures\n\n1.14.2.2.1 Vectors\n\n# Create a vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\n\n# Vector operations\nnumbers + 2\n\n[1] 3 4 5 6 7\n\nnumbers * 2\n\n[1]  2  4  6  8 10\n\nmean(numbers)\n\n[1] 3\n\nlength(fruits)\n\n[1] 3\n\n\n\n\n1.14.2.2.2 Matrices\n\n# Create a matrix\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\nprint(m)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n# Matrix operations\nt(m)  # transpose\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nm * 2  # scalar multiplication\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    4    8   12\n\n\n\n\n1.14.2.2.3 Data Frames\n\n# Create a data frame\ndf &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  student = c(TRUE, FALSE, TRUE)\n)\nprint(df)\n\n     name age student\n1   Alice  25    TRUE\n2     Bob  30   FALSE\n3 Charlie  35    TRUE\n\n# Accessing data frame elements\ndf$name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\ndf[1, 2]\n\n[1] 25\n\ndf[df$age &gt; 25, ]\n\n     name age student\n2     Bob  30   FALSE\n3 Charlie  35    TRUE\n\n\n\n\n\n1.14.2.3 Functions\n\n# Define a function\ngreet &lt;- function(name) {\n  paste(\"Hello,\", name, \"!\")\n}\n\n# Use the function\ngreet(\"Alice\")\n\n[1] \"Hello, Alice !\"\n\n# Function with multiple arguments\ncalculate_bmi &lt;- function(weight, height) {\n  bmi &lt;- weight / (height^2)\n  return(bmi)\n}\n\ncalculate_bmi(70, 1.75)\n\n[1] 22.85714\n\n\n\n\n1.14.2.4 Control Structures\n\n# If-else statement\nx &lt;- 10\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n} else {\n  print(\"x is not greater than 5\")\n}\n\n[1] \"x is greater than 5\"\n\n# For loop\nfor (i in 1:5) {\n  print(paste(\"Iteration\", i))\n}\n\n[1] \"Iteration 1\"\n[1] \"Iteration 2\"\n[1] \"Iteration 3\"\n[1] \"Iteration 4\"\n[1] \"Iteration 5\"\n\n# While loop\ncounter &lt;- 1\nwhile (counter &lt;= 5) {\n  print(paste(\"Counter:\", counter))\n  counter &lt;- counter + 1\n}\n\n[1] \"Counter: 1\"\n[1] \"Counter: 2\"\n[1] \"Counter: 3\"\n[1] \"Counter: 4\"\n[1] \"Counter: 5\"\n\n\n\n\n\n1.14.3 Introduction to tidyverse\nThe tidyverse is a collection of R packages designed for data science. These packages share a common philosophy and are designed to work together seamlessly.\n\n1.14.3.1 Key tidyverse Packages\n\nggplot2: for data visualization\ndplyr: for data manipulation\ntidyr: for tidying data\nreadr: for reading rectangular data\npurrr: for functional programming\ntibble: modern reimagining of data frames\n\n\n\n1.14.3.2 Getting Started with tidyverse\n\n# Install tidyverse (run once)\n# install.packages(\"tidyverse\")\n\n# Load tidyverse\nlibrary(tidyverse)\n\n\n\n1.14.3.3 Data Import with readr\n\n# Reading CSV files\ndata &lt;- read_csv(\"social_data.csv\")\n\n# Reading other file formats\nread_tsv(\"data.tsv\")  # Tab-separated values\nread_delim(\"data.txt\", delim = \"|\")  # Custom delimiter\n\n\n\n1.14.3.4 Data Manipulation with dplyr\n\n# Let's use the built-in mtcars dataset\ndata(\"mtcars\")\n\n# Selecting columns\nmtcars %&gt;% \n  select(mpg, cyl, hp)\n\n                     mpg cyl  hp\nMazda RX4           21.0   6 110\nMazda RX4 Wag       21.0   6 110\nDatsun 710          22.8   4  93\nHornet 4 Drive      21.4   6 110\nHornet Sportabout   18.7   8 175\nValiant             18.1   6 105\nDuster 360          14.3   8 245\nMerc 240D           24.4   4  62\nMerc 230            22.8   4  95\nMerc 280            19.2   6 123\nMerc 280C           17.8   6 123\nMerc 450SE          16.4   8 180\nMerc 450SL          17.3   8 180\nMerc 450SLC         15.2   8 180\nCadillac Fleetwood  10.4   8 205\nLincoln Continental 10.4   8 215\nChrysler Imperial   14.7   8 230\nFiat 128            32.4   4  66\nHonda Civic         30.4   4  52\nToyota Corolla      33.9   4  65\nToyota Corona       21.5   4  97\nDodge Challenger    15.5   8 150\nAMC Javelin         15.2   8 150\nCamaro Z28          13.3   8 245\nPontiac Firebird    19.2   8 175\nFiat X1-9           27.3   4  66\nPorsche 914-2       26.0   4  91\nLotus Europa        30.4   4 113\nFord Pantera L      15.8   8 264\nFerrari Dino        19.7   6 175\nMaserati Bora       15.0   8 335\nVolvo 142E          21.4   4 109\n\n# Filtering rows\nmtcars %&gt;% \n  filter(cyl == 4)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n# Arranging data\nmtcars %&gt;% \n  arrange(desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n# Creating new variables\nmtcars %&gt;% \n  mutate(kpl = mpg * 0.425)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb     kpl\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  8.9250\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  8.9250\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1  9.6900\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  9.0950\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  7.9475\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  7.6925\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  6.0775\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 10.3700\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  9.6900\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  8.1600\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  7.5650\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  6.9700\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  7.3525\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  6.4600\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  4.4200\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  4.4200\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  6.2475\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 13.7700\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 12.9200\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 14.4075\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  9.1375\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  6.5875\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  6.4600\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  5.6525\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  8.1600\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 11.6025\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 11.0500\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 12.9200\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  6.7150\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  8.3725\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  6.3750\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2  9.0950\n\n# Summarizing data\nmtcars %&gt;% \n  group_by(cyl) %&gt;% \n  summarize(mean_mpg = mean(mpg),\n            count = n())\n\n# A tibble: 3 × 3\n    cyl mean_mpg count\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1     4     26.7    11\n2     6     19.7     7\n3     8     15.1    14\n\n\n\n\n1.14.3.5 Data Visualization with ggplot2\n\n# Scatter plot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Car Weight vs. Fuel Efficiency\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles per Gallon\")\n\n\n\n\nCar Weight vs. Fuel Efficiency\n\n\n\n\n\n# Bar chart\nmtcars %&gt;% \n  count(cyl) %&gt;% \n  ggplot(aes(x = factor(cyl), y = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Number of Cars by Cylinder Count\",\n       x = \"Number of Cylinders\",\n       y = \"Count\")\n\n\n\n\nNumber of Cars by Cylinder Count\n\n\n\n\n\n# Box plot\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Fuel Efficiency by Number of Cylinders\",\n       x = \"Number of Cylinders\",\n       y = \"Miles per Gallon\")\n\n\n\n\nFuel Efficiency by Number of Cylinders\n\n\n\n\n\n\n\n1.14.4 Additional Resources\n\nR for Data Science\ntidyverse documentation\nRStudio Cheat Sheets\nQuarto Guide\nR Cookbook\n\nRemember to experiment with the code, modify examples, and don’t hesitate to use the built-in R help system (accessed by typing ?function_name in the console) when you encounter unfamiliar functions or concepts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html",
    "href": "rozdzial1.html",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "",
    "text": "2.1 Czym są Statystyka i Nauka o Danych?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#czym-są-statystyka-i-nauka-o-danych",
    "href": "rozdzial1.html#czym-są-statystyka-i-nauka-o-danych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "",
    "text": "Important\n\n\n\nStatystyka i data science to sztuka i nauka (o metodach, technikach lub narzędziach) uczenia się z danych.\n\n\n\nNauka o danych i statystyka to potężne narzędzia, które pomagają nam zrozumieć złożone zjawiska w różnych naukach społecznych, w tym w politologii, ekonomii i socjologii. Te uzupełniające się dziedziny dostarczają badaczom i praktykom środków do analizy trendów, zachowań i wyników w społeczeństwie, oferując wgląd, który może kształtować politykę i pogłębiać nasze zrozumienie ludzkich zachowań.\nStatystyka dostarcza matematycznych podstaw do analizy trendów i wyników społecznych, oferując metody projektowania badań, podsumowywania danych i wyciągania wniosków. Nauka o danych rozszerza tę podstawę, włączając metody obliczeniowe i wiedzę dziedzinową, aby radzić sobie z większymi zbiorami danych i przeprowadzać bardziej złożone analizy.\nRazem te dyscypliny pozwalają nam zbierać i przetwarzać duże zbiory danych, wizualizować złożone informacje, odkrywać wzorce w interakcjach społecznych, oceniać wpływ polityk i wspierać podejmowanie decyzji opartych na dowodach. Ich zastosowania są rozległe i zróżnicowane, od badania wzorców głosowania i analizy wskaźników ekonomicznych po badanie nierówności społecznych i analizę zachowań ludzkich.\nW miarę jak nasz świat staje się coraz bardziej oparty na danych, znaczenie nauki o danych i statystyki w naukach społecznych nadal rośnie.\n\n\n\n\n\n\n\nNote\n\n\n\nW naukach społecznych nauka o danych łączy metody statystyczne, narzędzia obliczeniowe i wiedzę dziedzinową do analizy złożonych zjawisk społecznych i zachowań ludzkich.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#związek-między-statystyką-a-nauką-o-danych",
    "href": "rozdzial1.html#związek-między-statystyką-a-nauką-o-danych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.2 Związek Między Statystyką a Nauką o Danych",
    "text": "2.2 Związek Między Statystyką a Nauką o Danych\nStatystyka i data science to ściśle powiązane dziedziny o znaczącym nakładaniu się, szczególnie w naukach społecznych. Zamiast ścisłego podziału, trafniej jest postrzegać je jako komplementarne podejścia na pewnym kontinuum:\n\nTradycyjna StatystykaNowoczesna Data ScienceEwoluujący Krajobraz\n\n\n\nZakorzeniona w teoriach matematycznych i metodach analizy danych\nKładzie nacisk na wnioskowanie statystyczne, testowanie hipotez i teorię prawdopodobieństwa\nHistorycznie kluczowa w naukach społecznych do analizy badań ankietowych, eksperymentów i badań obserwacyjnych\n\n\n\n\nIntegruje metody statystyczne z nauką o komputerach i wiedzą dziedzinową\nPoszerza fokus o uczenie maszynowe, przetwarzanie big data i modelowanie predykcyjne\nW naukach społecznych często zajmuje się wielkoskalowymi danymi cyfrowymi i złożonymi zbiorami danych behawioralnych\n\n\n\n\nGranice między statystyką a data science są coraz bardziej rozmyte\nWiele technik i narzędzi jest wspólnych dla obu dziedzin\nNaukowcy społeczni często łączą tradycyjne podejścia statystyczne z nowszymi metodami data science\nWybór podejścia zależy od pytań badawczych, charakterystyki danych i konkretnych potrzeb analitycznych\n\n\n\n\nNauka o danych może być postrzegana jako wynik ewolucji i rozszerzenie tradycyjnej statystyki, włączając nowe technologie i metody do obsługi większych i bardziej złożonych zbiorów danych w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#podstawowe-koncepcje-w-nauce-o-danych-i-statystyce",
    "href": "rozdzial1.html#podstawowe-koncepcje-w-nauce-o-danych-i-statystyce",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.3 Podstawowe Koncepcje w Nauce o Danych i Statystyce",
    "text": "2.3 Podstawowe Koncepcje w Nauce o Danych i Statystyce\n\n2.3.1 Dane i Populacje (Data and Populations) oraz pojęcia pokrewne\n\n\n\n\n\n\nImportant\n\n\n\n\nDane: Obserwacje lub pomiary zebrane z próby lub populacji.\nPopulacja: Cały zbiór osób lub elementów badanych w określonym czasie.\n\nPrzykład: Wszyscy uprawnieni wyborcy w kraju podczas konkretnego roku wyborczego.\n\nPróba: Podzbiór populacji, który jest faktycznie mierzony. Reprezentatywna próba to podzbiór większej populacji, który dokładnie odzwierciedla cechy tej populacji. Próba powinna odzwierciedlać populację pod względem ważnych cech, takich jak wiek, płeć, status społeczno-ekonomiczny itp. Często wykorzystuje metody losowego doboru próby, aby uniknąć stronniczości. Jest wystarczająco duża, aby być statystycznie istotna, ale mniejsza niż cała populacja.\n\nPrzykład: 1500 losowo wybranych uprawnionych wyborców ankietowanych w przedwyborczym sondażu.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nProces Generowania Danych (PGD) i Superpopulacja: Rozszerzenie Tradycyjnych Koncepcji\nW tradycyjnej statystyce często pracujemy z dwoma kluczowymi pojęciami:\n\nPopulacja: Cała grupa, którą chcemy badać.\nPróba: Podzbiór populacji, który faktycznie obserwujemy i analizujemy.\n\nChociaż te pojęcia są fundamentalne, współczesne badania często wymagają myślenia wykraczającego poza ten dychotomiczny podział. Tu wkraczają koncepcje Procesu Generowania Danych (PGD) i superpopulacji, rozszerzając nasze rozumienie danych i populacji.\nProces Generowania Danych (PGD; Data Generating Process, DGP):\nPGD to podstawowy mechanizm, który produkuje dane obserwowane w rzeczywistym świecie, zarówno w naszej próbie, jak i w całej populacji.\nIntuicyjne wyjaśnienie: Wyobraź sobie PGD jako złożony system, który przyjmuje różne dane wejściowe i produkuje obserwowalne wyniki. To “czarna skrzynka”, która przekształca przyczyny w skutki, nie tylko dla naszej próby, ale dla całej populacji i poza nią.\nPrzykład: Rozważmy badanie zachowań wyborczych. Tradycyjne podejście mogłoby zdefiniować populację jako “wszyscy zarejestrowani wyborcy” i pobrać z tej grupy próbę. PGD natomiast obejmowałby czynniki takie jak cechy demograficzne, warunki ekonomiczne, wydarzenia polityczne i wpływ mediów, które kształtują zachowania wyborcze wszystkich wyborców, niezależnie od tego, czy zostali uwzględnieni w próbie, czy nie.\nSuperpopulacja:\nSuperpopulacja to teoretyczna koncepcja, która wykracza zarówno poza próbę, jak i obserwowalną populację, obejmując wszystkie potencjalne wyniki, które mogłyby wystąpić w podobnych warunkach lub procesach.\nPrzykłady:\n\nPodejście tradycyjne vs. podejście superpopulacyjne:\n\nTradycyjne: populacja (wszyscy zarejestrowani wyborcy w województwie), próba (1000 ankietowanych wyborców)\nSuperpopulacja: Wszyscy możliwi wyborcy i scenariusze głosowania, w tym przyszłe wybory i hipotetyczne konteksty polityczne\n\nGdy próba równa się populacji:\nW badaniach wszystkich 16 województw Polski:\n\nTradycyjne spojrzenie: Brak rozróżnienia między próbą a populacją\nSpojrzenie superpopulacyjne: Traktuje te 16 województw jako “próbę” z teoretycznego zbioru wszystkich możliwych interakcji między województwami a polityką\n\n\nZastosowanie w rzeczywistości: Załóżmy, że badacze studiują wpływ nowej polityki planowania urbanistycznego w kilku miastach:\n\nPodejście tradycyjne:\n\nPopulacja: Wszystkie miasta w kraju\nPróba: Miasta uwzględnione w badaniu\n\nPodejście superpopulacyjne:\n\nObserwowane dane: Miasta w badaniu\nSuperpopulacja: Wszystkie miasta (istniejące lub potencjalne), w których można by zastosować podobne zasady planowania urbanistycznego\n\n\nPGD (DGP) w tym przypadku byłby złożonym zestawem czynników, które determinują, jak polityki planowania urbanistycznego wpływają na wyniki miast, mające zastosowanie nie tylko do badanych miast czy nawet wszystkich istniejących miast, ale do szerszej koncepcji “miasta” jako takiego.\nWażne kwestie do rozważenia:\n\nZakres i ograniczenia: Badacze powinni jasno określić, jakie jednostki lub procesy starają się zrozumieć, wykraczając poza samo opisanie próby i populacji.\nMożliwość uogólnienia: Przy formułowaniu wniosków dotyczących superpopulacji, badacze powinni wyraźnie określić granice, w których ich ustalenia mają zastosowanie.\nSpecyfika kontekstu: Chociaż koncepcja superpopulacji pozwala na szersze wnioskowanie niż tradycyjne pobieranie próbek, ważne jest, aby zdawać sobie sprawę, że PGD może się różnić w zależności od kontekstu.\n\nPrzykład podsumowujący: Jakość Pizzy w Nowym Jorku\nPopulacja: Wszystkie obecnie działające pizzerie w Nowym Jorku. To skończona, policzalna grupa lokali istniejących w danym momencie.\nPróba: Wybór 50 pizzerii losowo wybranych z różnych dzielnic Nowego Jorku. To konkretne pizzerie, w których badacze będą degustować i oceniać pizze.\nSuperpopulacja: Wszystkie możliwe pizzerie, które mogłyby istnieć w Nowym Jorku, w tym:\n\nObecnie działające pizzerie\nPrzyszłe pizzerie, które jeszcze nie zostały otwarte\nPizzerie, które zostały zamknięte\nHipotetyczne pizzerie, które mogłyby istnieć w innych warunkach ekonomicznych lub kulturowych\n\nKoncepcja superpopulacji pozwala nam myśleć o jakości pizzy wykraczając poza obecny “zrzut ekranu” nowojorskich pizzerii.\nProces Generowania Danych (PGD): PGD to złożony zestaw czynników, które przyczyniają się do jakości pizzy w każdej pizzerii. Może to obejmować:\n\nSkładniki: Jakość i źródło mąki, pomidorów, sera itp.\nUmiejętności szefa kuchni: Szkolenie, doświadczenie i osobiste podejście pizzaiolo\nSprzęt: Rodzaj i stan pieca, używane narzędzia\nPrzepis: Proporcje składników, metody przygotowania\nCzynniki środowiskowe: Wilgotność, jakość wody w Nowym Jorku\nWpływy kulturowe: Lokalne tradycje robienia pizzy, preferencje klientów\nCzynniki ekonomiczne: Koszty składników, ceny wynajmu wpływające na decyzje biznesowe\n\nPGD jest jak “przepis na jakość pizzy”, który ma zastosowanie nie tylko do naszej próby czy nawet obecnej populacji, ale do wszystkich potencjalnych pizzerii w superpopulacji.\nIntuicyjne Wyjaśnienie:\n\nJeśli odwiedzisz wszystkie obecnie działające pizzerie w Nowym Jorku i je ocenisz, zbadałeś populację.\nJeśli losowo wybierzesz 50 pizzerii do odwiedzenia i oceny, pobrałeś próbę.\nJeśli zastanawiasz się, jak jakość pizzy mogłaby się różnić we wszystkich możliwych nowojorskich pizzeriach (przeszłych, obecnych, przyszłych i hipotetycznych), myślisz o superpopulacji.\nJeśli próbujesz zrozumieć wszystkie czynniki, które składają się na jakość pizzy w Nowym Jorku, niezależnie od tego, czy dana pizzeria obecnie istnieje czy nie, badasz Proces Generowania Danych.\n\n\n\n\n\n\n\n\ngraph TD\n    A[Data Generating Process DGP]\n    B(Population)\n    C[Sample]\n    A --&gt;|Generates| B\n    B --&gt;|Sampled from| C\n    C -.-&gt;|Inference| B\n    C -.-&gt;|Inference| A\n    B -.-&gt;|Inference| A\n    \n    classDef dgp fill:#f9f,stroke:#333,stroke-width:4px;\n    classDef pop fill:#bbf,stroke:#333,stroke-width:2px;\n    classDef sam fill:#bfb,stroke:#333,stroke-width:2px;\n    \n    class A dgp;\n    class B pop;\n    class C sam;\n\n\n\n\n\n\n\n\n\n\n\n\nObjaśnienie diagramu PGD, Populacji i Próby\n\n\n\nDiagram przedstawia relacje między Procesem Generującym Dane (PGD), populacją i próbą, wraz ze ścieżkami wnioskowania:\n\nRelacje bezpośrednie (ciągłe strzałki):\n\nPGD generuje populację\nZ populacji pobierane są próby\n\nŚcieżki wnioskowania (przerywane strzałki):\n\nOd Próby do Populacji: Tradycyjne wnioskowanie statystyczne\nOd Próby do PGD: Wnioskowanie o podstawowym procesie na podstawie danych z próby\nOd Populacji do PGD: Wnioskowanie o PGD przy użyciu pełnych danych populacji\n\n\nNa przykład, w naszym badaniu wpływu ordynacji wyborczej na frekwencję w polskich gminach (wybory samorządowe 1998-2010):\n\nDysponujemy danymi dla całej populacji gmin, więc nie musimy wnioskować z próby o populacji.\nSkupiamy się na wykorzystaniu pełnych danych populacyjnych (prawa przerywana strzałka) do wnioskowania o leżącym u podstaw PGD—złożonych procesach, poprzez które ordynacja wyborcza wpływa na frekwencję wyborczą w gminach.\nTakie podejście pozwala nam potencjalnie zrozumieć mechanizmy, dzięki którym różne systemy wyborcze (np. reprezentacja proporcjonalna vs. większościowa) wpływają na poziom frekwencji, oraz formułować uzasadnione przewidywania o tym, jak zmiany w ordynacji wyborczej mogłyby wpłynąć na przyszłą frekwencję lub jak te efekty mogłyby się uogólniać na podobne konteksty.\n\n\n\n\n\n\nPopulacja vs. próba. Retrieved from: https://allmodelsarewrong.github.io/mse.html\n\n\nDane stanowią podstawę analizy statystycznej. Mogą być:\n\nDane pierwotne (Primary data): Zebrane bezpośrednio w określonym celu\nDane wtórne (Secondary data): Uzyskane z istniejących źródeł\n\nPrzykład: W badaniu wzrostu studentów uniwersyteckich, populacją są wszyscy studenci uniwersyteccy w kraju, podczas gdy próba może składać się z 1000 losowo wybranych studentów.\n\n\n2.3.2 Zmienne i Stałe (Variables and Constants)\nZmienne to cechy, które mogą przyjmować różne wartości w zbiorze danych. Mogą być:\n\nIlościowe (Quantitative):\n\nCiągłe (Continuous): Wzrost, waga, temperatura\nDyskretne (Discrete): Liczba dzieci, liczba błędów w programie\n\nJakościowe (Qualitative):\n\nNominalne (Nominal): Grupa krwi, kolor oczu\nPorządkowe (Ordinal): Poziom wykształcenia, ocena satysfakcji klienta\n\n\nStałe to wartości, które pozostają niezmienne w trakcie analizy.\n\n2.3.2.1 Rodzaje Danych w Naukach Społecznych\nBadania w naukach społecznych zajmują się różnymi rodzajami danych:\n\nDane Ilościowe: Dane liczbowe (np. odpowiedzi z ankiet, wskaźniki ekonomiczne)\nDane Jakościowe: Dane nieliczbowe (np. transkrypcje wywiadów, odpowiedzi na pytania otwarte w ankietach)\nBig Data: Dane cyfrowe na dużą skalę (np. posty w mediach społecznościowych, logi zachowań online)\n\n\n\n\n2.3.3 Parametry Populacji i Estymanda (Population Parameters and Estimands)\nParametry populacji to liczbowe charakterystyki populacji. Kluczowe punkty:\n\nOpisują całą populację, nie tylko próbę.\nZwykle oznaczane są greckimi literami.\nW większości przypadków nie mogą być bezpośrednio obliczone, ponieważ nie możemy zmierzyć całej populacji.\nSą determinowane przez podstawowy Proces Generujący Dane (DGP).\n\nTypowe parametry populacji to:\n\nŚrednia populacji (Population mean) (\\(\\mu\\)): Średnia/oczekiwana wartość zmiennej w populacji.\nWariancja populacji (Population variance) (\\(\\sigma^2\\)): Miara zmienności w populacji.\nProporcja populacji (Population proportion) (\\(p\\)): Proporcja osób w populacji posiadających daną cechę.\n\nEstymand (Estimand) to cel estymacji - konkretny parametr populacji lub funkcja parametrów, którą chcemy oszacować. Definiuje to, co chcemy wiedzieć o populacji.\n\n\n\n\n\n\nPrzykład: Wzrost Studentów Uniwersyteckich\n\n\n\nRozważmy wzrost wszystkich studentów uniwersyteckich w kraju:\n\n\\(\\mu\\) (estymand): Prawdziwa średnia wysokość wszystkich studentów uniwersyteckich (średnia populacji)\n\\(\\sigma^2\\) (estymand): Prawdziwa wariancja wysokości w populacji\n\nTe parametry są nieznanymi estymandami, które chcemy oszacować na podstawie danych z próby.\n\n\n\n\n2.3.4 Statystyki i Estymatory (Statistic(s) and Estimators)\nStatystyka (pojedyncza) lub statystyka z próby to dowolna wielkość obliczona na podstawie wartości z próby, która jest rozważana w celu statystycznym.\nGdy statystyka jest używana do oszacowania estymandy (parametru populacji), nazywana jest estymatorem. Estymatory są funkcjami danych z próby, które dostarczają przybliżonych wartości dla nieznanych parametrów populacji.\nPrzykłady statystyk/estymatorów:\n\nŚrednia z próby (Sample mean): \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\) (szacuje \\(\\mu\\))\nWariancja z próby (Sample variance): \\(s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2\\) (szacuje \\(\\sigma^2\\))\nProporcja z próby (Sample proportion): \\(\\hat{p} = \\frac{x}{n}\\) (szacuje \\(p\\))\n\n\n\n2.3.5 Oszacowania (Estimates)\nOszacowanie to konkretna wartość uzyskana przez zastosowanie estymatora do konkretnej próby. Jest to wartość punktowa, która przybliża prawdziwą estymandę (parametr populacji).\nPrzykład: Jeśli obliczamy średnią wysokość z próby wynoszącą 173 cm, to 173 cm jest naszym oszacowaniem estymandy \\(\\mu\\) (średniej wysokości populacji).\n\n\n2.3.6 Modele Statystyczne (Statistical Models)\n\n\n\n\n\n\nNote\n\n\n\nModel w nauce to uproszczona reprezentacja złożonego systemu lub zjawiska. Jest on ta zaprojektowany, aby pomóc nam zrozumieć, wyjaśnić i przewidywać zjawiska zachodzące w rzeczywistym świecie. Modele mogą przybierać różne formy, w tym równania matematyczne, symulacje komputerowe lub ramy koncepcyjne. Pozwalają naukowcom skupić się na kluczowych aspektach systemu, ignorując mniej istotne szczegóły, co sprawia, że złożone problemy stają się łatwiejsze do zrozumienia i badania.\n\n\nModele statystyczne reprezentują relacje między zmiennymi i pomagają w przewidywaniu lub wnioskowaniu o estymandach (parametrach populacji).\nPrzykład: Model regresji liniowej \\(y = \\beta_0 + \\beta_1x + \\epsilon\\) opisuje relację między zmienną niezależną \\(x\\) a zmienną zależną \\(y\\), gdzie:\n\n\\(y\\) to zmienna zależna (np. wielkość popytu na dobro)\n\\(x\\) to zmienna niezależna (np. cena lub dochód konsumenta)\n\\(\\beta_0\\) i \\(\\beta_1\\) to parametry, estymandy do oszacowania\n\\(\\epsilon\\) to składnik błędu, reprezentujący niewyjaśnioną zmienność\n\n\n\n\n\n\n\nWnioskowanie przyczynowe i kontrfakty\n\n\n\nW naukach społecznych często chcemy zrozumieć co by się stało, gdybyśmy podjęli inne działanie - ten hipotetyczny scenariusz nazywamy kontrfaktem/wynikiem kontrfaktycznym. Na przykład:\n\nJakie byłyby zarobki danej osoby, gdyby poszła na studia vs. gdyby nie poszła?\nJak zmieniłaby się frekwencja wyborcza, gdyby głosowanie było obowiązkowe?\n\nPonieważ nie możemy obserwować obu scenariuszy jednocześnie, modele statystyczne pomagają nam oszacować te kontrfakty poprzez:\n\nKontrolowanie zmiennych zakłócających (confounders)\nPorównywanie podobnych grup, które różnią się tylko badanym czynnikiem\nWykorzystanie technik takich jak dopasowanie według współczynnika skłonności czy zmienne instrumentalne\n\n\n\n\nFundamentalny problem wnioskowania przyczynowego: We can think of causal inference as a PREDICTION problem. How could we predict the counterfactual given that we never observe it?\n\n\nPamiętaj: Korelacja ≠ Przyczynowość, ale staranny projekt badawczy i metody statystyczne mogą pomóc nam formułować wnioski przyczynowe.\n\n\n\nConfounding bias and spurious correlation (https://www.bradyneal.com/causal-inference-course) drinking the night before is a common cause of sleeping with shoes on and waking up with a headache :-)\n\n\n\n\n\nReverse causality: https://ff13.fastforwardlabs.com/\n\n\n\n\n\n\n2.3.7 Wnioskowanie (Inference)\nWnioskowanie statystyczne to proces wyciągania wniosków o estymandach (parametrach populacji) na podstawie danych z próby. Obejmuje dwa główne typy:\n\nEstymacja (Estimation): Używanie statystyk z próby (estymatorów) do oszacowania estymand (parametrów populacji)\nTestowanie hipotez (Hypothesis testing): Podejmowanie decyzji o estymandach na podstawie dowodów z próby\n\n\n\n\n\n\n\nEstymacja i testowanie hipotez: wstęp\n\n\n\n\nEstymacja\n\nEstymacja polega na określeniu prawdopodobnej wartości parametru populacji na podstawie danych z próby. W kontekście rozkładu dwumianowego możemy być zainteresowani oszacowaniem prawdopodobieństwa sukcesu (p) dla określonego zdarzenia.\nPrzykład: Rzucanie monetą\nPowiedzmy, że rzucamy monetą 100 razy i chcemy oszacować prawdopodobieństwo wypadnięcia orła.\n\nRzucamy monetą 100 razy i obserwujemy 55 orłów.\nNasze punktowe oszacowanie p (prawdopodobieństwo wypadnięcia orła) wynosi 55/100 = 0,55\nMożemy również obliczyć przedział ufności, np. 95% przedział ufności może wynosić (0,45; 0,65).\n\nPrzedział ufności mówi nam o zakresie, w którym może leżeć prawdziwe prawdopodobieństwo. Mówiąc prościej: “Jesteśmy w 95% pewni, że prawdziwe prawdopodobieństwo wypadnięcia orła mieści się między 45% a 65%.”\nCelem jest tutaj dostarczenie naszego najlepszego oszacowania prawdziwego prawdopodobieństwa wypadnięcia orła, wraz z zakresem prawdopodobnych wartości.\nWażne Pojęcia Teorii Estymacji:\n\nObciążenie (Bias)\n\nObciążenie odnosi się do tendencji estymatora do systematycznego przeszacowania lub niedoszacowania prawdziwej wartości parametru populacji (estymandy).\n\nEstymator nieobciążony to taki, którego średnia wartość (przy wielokrotnym powtórzeniu estymacji) jest równa prawdziwej wartości parametru.\nObciążenie można rozumieć jako różnicę między średnią wartością estymatora a prawdziwą wartością parametru.\n\n\nEfektywność (Efficiency)\n\nEfektywność odnosi się do precyzji estymatora. Bardziej efektywny estymator daje wyniki bliższe prawdziwej wartości parametru, czyli ma mniejsze rozproszenie wyników.\n\nMierzona jest najczęściej wariancją estymatora (im mniejsza wariancja, tym większa efektywność)\nDla nieobciążonych estymatorów efektywność często porównuje się za pomocą Błędu Średniokwadratowego (Mean Squared Error, MSE)\n\n\nTestowanie hipotez\n\nTestowanie hipotez z kolei polega na podejmowaniu decyzji między dwoma konkurencyjnymi twierdzeniami dotyczącymi parametru populacji. Zazwyczaj mamy hipotezę zerową (H0) i hipotezę alternatywną (H1).\nPrzykład: Czy moneta jest uczciwa?\nKorzystając z tego samego scenariusza rzucania monetą, powiedzmy, że chcemy sprawdzić, czy moneta jest uczciwa (p = 0,5), czy też stronnicza na korzyść orła (p &gt; 0,5).\n\nHipoteza zerowa (H0): p = 0,5 (moneta jest uczciwa)\nHipoteza alternatywna (H1): p &gt; 0,5 (moneta jest stronnicza na korzyść orła)\nObserwujemy 55 orłów na 100 rzutów\n\nP-wartość i jak testowanie hipotez działa jako rodzaj “probabilistycznego dowodu nie wprost”:\n\nZaczynamy od założenia, że hipoteza zerowa (H0) jest prawdziwa. W tym przypadku zakładamy, że moneta jest uczciwa.\nNastępnie pytamy: “Jeśli moneta byłaby naprawdę uczciwa, jakie byłoby prawdopodobieństwo zaobserwowania 55 lub więcej orłów na 100 rzutów?”\nTo prawdopodobieństwo nazywa się wartością p. Jest to prawdopodobieństwo zaobserwowania naszych danych (lub bardziej ekstremalnych) przy założeniu, że hipoteza zerowa jest prawdziwa.\nJeśli to prawdopodobieństwo (wartość p) jest bardzo małe, mamy sprzeczność: zaobserwowaliśmy coś, co powinno być bardzo rzadkie, gdyby nasze założenie (H0) było prawdziwe.\nZwykle ustalamy próg zwany poziomem istotności (często 0,05 lub 5%) dla tego, co uważamy za “bardzo małe”.\nJeśli wartość p jest mniejsza niż wybrany poziom istotności, odrzucamy H0. Wnioskujemy, że nasza obserwacja jest zbyt mało prawdopodobna przy H0, więc faworyzujemy hipotezę alternatywną.\nJeśli wartość p jest większa niż nasz poziom istotności, nie odrzucamy H0. Nie mamy wystarczających dowodów, aby stwierdzić, że moneta jest stronnicza.\n\nTen proces jest jak “probabilistyczny dowód nie wprost”, ponieważ:\n\nZaczynamy od założenia H0 (podobnie jak zakładamy przeciwieństwo tego, co chcemy udowodnić w dowodzie nie wprost).\nSprawdzamy, czy to założenie prowadzi do bardzo mało prawdopodobnej sytuacji (naszych zaobserwowanych danych).\nJeśli tak, odrzucamy założenie (H0) i faworyzujemy alternatywę.\n\nWartość p dokładnie określa, jak mało prawdopodobna jest nasza obserwacja przy założeniu H0. Bardzo mała wartość p (np. 0,01) oznacza: “Gdyby H0 była prawdziwa, spodziewalibyśmy się zobaczyć tak ekstremalne dane tylko około 1% czasu.”\nTestowanie hipotez i estymacja to powiązane, ale odrębne procedury statystyczne; testowanie hipotez może być wykorzystane do wyciągania wniosków o oszacowaniach i może uzupełniać estymację na kilka sposobów, np.:\n\nTestowanie oszacowań punktowych: Testowanie hipotez może być wykorzystane do oceny, czy oszacowanie punktowe różni się istotnie od hipotetycznej wartości. Na przykład, jeśli oszacujemy, że moneta ma prawdopodobieństwo 0,55 wypadnięcia orłem, możemy użyć testu hipotezy, aby określić, czy ta wartość różni się istotnie od 0,5 (uczciwa moneta).\nIstotność parametrów: W modelach wielowymiarowych, testy hipotez (takie jak testy t w regresji) mogą pomóc określić, które oszacowane parametry różnią się istotnie od zera, dając wgląd w to, które zmienne są ważne w modelu.\n\n\n\n\n\n2.3.8 Relacje Między Pojęciami\n\nProces Generujący Dane (DGP - Data Generating Process) określa rzeczywiste wartości parametrów populacji (estymand).\nEstymandy są szacowane za pomocą statystyk obliczonych na podstawie próby (estymatorów).\nJakość estymatorów ocenia się na podstawie właściwości takich jak obciążenie i efektywność w szacowaniu estymandy.\nModele statystyczne wykorzystują oszacowane parametry do opisania relacji między zmiennymi w populacji.\nWnioskowanie statystyczne polega na wyciąganiu wniosków o estymandach na podstawie danych z próby, wykorzystując właściwości estymatorów.\n\n\n\n\n\n\n\nPrzykład: Badanie Zachowań Wyborczych\n\n\n\n\nPopulacja: Wszyscy uprawnieni wyborcy w kraju\nEstymanda: \\(p\\) = rzeczywista proporcja wyborców popierających danego kandydata\nPróba: 1000 losowo wybranych uprawnionych wyborców\nEstymator: \\(\\hat{p}\\) = proporcja wyborców z próby popierających kandydata\nOszacowanie: Konkretna wartość \\(\\hat{p}\\) obliczona z próby (np. 0,52)\nDGP: Złożona interakcja czynników wpływających na decyzje wyborcze, takich jak przekonania polityczne, warunki ekonomiczne, ekspozycja na media i sieci społeczne.\n\nZrozumienie DGP pomaga badaczom interpretować, dlaczego estymanda \\(p\\) ma określoną wartość i jak może się zmieniać w czasie. Na przykład, nagła zmiana w gospodarce może wpłynąć na preferencje wyborców, zmieniając tym samym wartość \\(p\\).\nObciążenie i efektywność w kontekście przykładu:\n\nJeśli \\(\\hat{p}\\) jest nieobciążonym estymatorem, oznacza to, że przy wielokrotnym powtórzeniu badania na różnych próbach, średnia wartość \\(\\hat{p}\\) będzie bliska rzeczywistej wartości \\(p\\).\nEfektywność \\(\\hat{p}\\) określa, jak bardzo rozproszone są wyniki poszczególnych badań wokół tej średniej. Im mniejsze rozproszenie, tym estymator jest bardziej efektywny.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#główne-komponenty-nauki-o-danych-w-badaniach-naukowych",
    "href": "rozdzial1.html#główne-komponenty-nauki-o-danych-w-badaniach-naukowych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.4 Główne Komponenty Nauki o Danych w Badaniach Naukowych",
    "text": "2.4 Główne Komponenty Nauki o Danych w Badaniach Naukowych\n\nZbieranie DanychPrzetwarzanie DanychEksploracyjna Analiza Danych (EDA)Wnioskowanie StatystyczneUczenie MaszynoweWizualizacja Danych i KomunikacjaPowtarzalność i Otwarta Nauka\n\n\n\nMetody eksperymentalne: Kontrolowane badania, w których naukowcy manipulują zmiennymi, aby obserwować efekty\nBadania obserwacyjne: Gromadzenie danych poprzez obserwację i rejestrację bez ingerencji\nAnkiety i wywiady: Zbieranie informacji bezpośrednio od ludzi poprzez zadawanie pytań\nCyfrowe zbieranie danych: Gromadzenie danych ze źródeł internetowych, czujników lub systemów komputerowych\nAspekty etyczne: Zapewnienie, że badania respektują prawa i dobro uczestników\n\n\n\n\nCzyszczenie danych: Usuwanie błędów i niespójności z surowych danych\nObsługa brakujących wartości: Radzenie sobie z lukami w zbiorze danych, które mogłyby wpłynąć na analizę\nTransformacja danych: Konwertowanie danych na formaty odpowiednie do analizy, np. zmiana tekstu na liczby\n\n\n\n\nStatystyki opisowe: Podsumowanie danych za pomocą miar takich jak średnia, mediana i odchylenie standardowe\nWizualizacja danych: Tworzenie wykresów i diagramów do wizualnego przedstawienia wzorców w danych\nIdentyfikacja wzorców: Odkrywanie trendów lub zależności w danych\n\n\n\n\nTestowanie hipotez: Wykorzystanie danych do oceny twierdzeń o populacjach\nAnaliza regresji: Badanie zależności między zmiennymi i dokonywanie przewidywań\nWnioskowanie przyczynowe: Określanie, czy jedna zmienna bezpośrednio wpływa na inną\n\n\n\n\nUczenie nadzorowane: Trenowanie modeli do przewidywania wyników przy użyciu danych ze znanymi odpowiedziami\nUczenie nienadzorowane: Znajdowanie ukrytych wzorców w danych bez predefiniowanych kategorii\nPrzetwarzanie języka naturalnego (NLP): Nauczanie komputerów rozumienia i analizy ludzkiego języka\n\n\n\n\nEfektywne wizualizacje: Tworzenie czytelnych, informatywnych grafik do przedstawiania złożonych danych\nKomunikacja naukowa: Wyjaśnianie wyników różnym odbiorcom, od ekspertów po ogół społeczeństwa\nPisanie naukowe: Przygotowywanie artykułów i raportów naukowych w celu dzielenia się wynikami\n\n\n\n\nKontrola wersji: Śledzenie zmian w danych i kodzie w trakcie procesu badawczego\nPraktyki otwartych danych: Udostępnianie danych i metod badawczych do weryfikacji i dalszych badań\nPowtarzalne procesy badawcze: Dokumentowanie kroków badawczych, aby inni mogli powtórzyć badanie",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#narzędzia-do-nauki-o-danych-w-naukach-społecznych",
    "href": "rozdzial1.html#narzędzia-do-nauki-o-danych-w-naukach-społecznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.5 Narzędzia do Nauki o Danych w Naukach Społecznych",
    "text": "2.5 Narzędzia do Nauki o Danych w Naukach Społecznych\nW tym kursie będziemy głównie używać R do naszej analizy danych, ponieważ jest on szeroko stosowany w badaniach nauk społecznych.\n\n2.5.1 R w Analizie Danych Nauk Społecznych\nR oferuje potężne możliwości dla badań w naukach społecznych, od manipulacji danymi po zaawansowane modelowanie statystyczne.\n\n\nKliknij, aby pokazać/ukryć kod R\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nKliknij, aby pokazać/ukryć kod R\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate example data with a Simpson's Paradox\nn &lt;- 1000\ndata &lt;- tibble(\n  age_group = sample(c(\"Young\", \"Middle\", \"Old\"), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),\n  education_years = case_when(\n    age_group == \"Young\" ~ rnorm(n, mean = 10, sd = 1),\n    age_group == \"Middle\" ~ rnorm(n, mean = 13, sd = 1),\n    age_group == \"Old\" ~ rnorm(n, mean = 16, sd = 1)\n  ),\n  income = case_when(\n    age_group == \"Young\" ~ 70000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Middle\" ~ 50000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Old\" ~ 30000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000)\n  )\n)\n\n# Basic data summary\nsummary(data)\n\n\n  age_group         education_years      income     \n Length:1000        Min.   : 6.628   Min.   :34068  \n Class :character   1st Qu.:10.913   1st Qu.:51508  \n Mode  :character   Median :13.004   Median :63376  \n                    Mean   :12.986   Mean   :63307  \n                    3rd Qu.:14.934   3rd Qu.:75023  \n                    Max.   :18.861   Max.   :96620  \n\n\nKliknij, aby pokazać/ukryć kod R\n# Correlation analysis\ncor(data %&gt;% select(education_years, income))\n\n\n                education_years     income\neducation_years       1.0000000 -0.8152477\nincome               -0.8152477  1.0000000\n\n\nKliknij, aby pokazać/ukryć kod R\n# Overall trend (Simpson's Paradox)\noverall_plot &lt;- ggplot(data, aes(x = education_years, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Overall Relationship between Education and Income\",\n       subtitle = \"Simpson's Paradox: Appears negative\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Trend by age group (Resolving Simpson's Paradox)\ngrouped_plot &lt;- ggplot(data, aes(x = education_years, y = income, color = age_group)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Relationship between Education and Income by Age Group\",\n       subtitle = \"Resolving Simpson's Paradox: Positive relationship within groups\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Statistical analysis\nmodel_overall &lt;- lm(income ~ education_years, data = data)\nmodel_by_age &lt;- lm(income ~ education_years + age_group, data = data)\n\n# Print results\nprint(overall_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(grouped_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_overall))\n\n\n\nCall:\nlm(formula = income ~ education_years, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24451  -5439    235   5262  34328 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     121814.7     1339.5   90.94   &lt;2e-16 ***\neducation_years  -4505.4      101.3  -44.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7976 on 998 degrees of freedom\nMultiple R-squared:  0.6646,    Adjusted R-squared:  0.6643 \nF-statistic:  1978 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_by_age))\n\n\n\nCall:\nlm(formula = income ~ education_years + age_group, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-14827  -3369    118   3356  16388 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      48270.8     2028.4  23.797  &lt; 2e-16 ***\neducation_years   1135.5      154.6   7.345 4.26e-13 ***\nage_groupOld    -19942.8      593.2 -33.619  &lt; 2e-16 ***\nage_groupYoung   20461.1      600.7  34.064  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4950 on 996 degrees of freedom\nMultiple R-squared:  0.8711,    Adjusted R-squared:  0.8707 \nF-statistic:  2244 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\n# Calculate and print correlations\noverall_cor &lt;- cor(data$education_years, data$income)\ngroup_cors &lt;- data %&gt;%\n  group_by(age_group) %&gt;%\n  summarize(correlation = cor(education_years, income))\n\nprint(\"Overall correlation:\")\n\n\n[1] \"Overall correlation:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(overall_cor)\n\n\n[1] -0.8152477\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(\"Correlations by age group:\")\n\n\n[1] \"Correlations by age group:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(group_cors)\n\n\n# A tibble: 3 × 2\n  age_group correlation\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Middle          0.185\n2 Old             0.291\n3 Young           0.223\n\n\nTen przykład demonstruje podstawowe operacje na danych, statystyki opisowe i wizualizację danych przy użyciu R.\nCertainly. Here’s the Polish version of the section on causal inference versus observational studies:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wnioskowanie-przyczynowe-a-badania-obserwacyjne",
    "href": "rozdzial1.html#wnioskowanie-przyczynowe-a-badania-obserwacyjne",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.6 Wnioskowanie przyczynowe a badania obserwacyjne",
    "text": "2.6 Wnioskowanie przyczynowe a badania obserwacyjne\nW naukach społecznych i nie tylko, zrozumienie relacji między zmiennymi jest kluczowe. Dwa główne podejścia to wnioskowanie przyczynowe i badania obserwacyjne, każde z własnymi mocnymi stronami i ograniczeniami.\n\nWnioskowanie przyczynoweBadania obserwacyjneKluczowe rozróżnienie: Korelacja vs. Przyczynowość\n\n\n\nDąży do ustalenia związków przyczynowo-skutkowych\nCzęsto obejmuje plany eksperymentalne lub zaawansowane techniki statystyczne\nStara się odpowiedzieć na pytania “Co by było, gdyby?” i określić wpływ interwencji\nPrzykłady: Randomizowane badania kontrolowane, projekty quasi-eksperymentalne, zmienne instrumentalne\n\n\n\n\nBadają relacje między zmiennymi bez bezpośredniej interwencji\nOpierają się na danych zebranych w naturalnych warunkach lub z istniejących zbiorów danych\nMogą identyfikować korelacje i wzorce, ale mają trudności z ustaleniem przyczynowości\nPrzykłady: Badania kohortowe, badania kliniczno-kontrolne, przekrojowe badania ankietowe\n\n\n\n\n\n\n\n\n\n\n\n\n\nPamiętaj: Korelacja nie implikuje przyczynowości\n\n\n\nFundamentalna zasada w badaniach głosi, że korelacja między dwiema zmiennymi niekoniecznie implikuje związek przyczynowy. Ta koncepcja jest kluczowa przy interpretacji wyników badań obserwacyjnych.\n\nKorelacja: Mierzy siłę i kierunek związku między zmiennymi\nPrzyczynowość: Wskazuje, że zmiany w jednej zmiennej bezpośrednio powodują zmiany w drugiej\n\nChociaż silne korelacje mogą sugerować potencjalne związki przyczynowe, do ustalenia przyczynowości wymagane są dodatkowe dowody i rygorystyczne metody.\n\n\n\nWyzwania w ustalaniu przyczynowościMetody wzmacniania twierdzeń przyczynowychZnaczenie w naukach społecznych\n\n\n\nZmienne zakłócające: Niezmierzone czynniki wpływające zarówno na domniemaną przyczynę, jak i skutek\nOdwrotna przyczynowość: Domniemany skutek może w rzeczywistości powodować domniemaną przyczynę\nBłąd selekcji: Nielosowy dobór uczestników do grup badawczych\n\n\n\n\nRandomizowane badania kontrolowane (gdy są etyczne i wykonalne)\nNaturalne eksperymenty lub projekty quasi-eksperymentalne\nDopasowanie według propensity score\nAnaliza różnicy w różnicach\nPodejścia oparte na zmiennych instrumentalnych\nSkierowane grafy acykliczne (DAG) do wizualizacji relacji przyczynowych\n\n\n\nZrozumienie różnicy między wnioskowaniem przyczynowym a badaniami obserwacyjnymi jest kluczowe w naukach społecznych, gdzie względy etyczne często ograniczają manipulacje eksperymentalne. Badacze muszą starannie projektować badania i interpretować wyniki, aby uniknąć wprowadzających w błąd wniosków dotyczących przyczynowości.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#modele-w-nauce-od-deterministycznych-do-stochastycznych",
    "href": "rozdzial1.html#modele-w-nauce-od-deterministycznych-do-stochastycznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.7 Modele w Nauce: Od Deterministycznych do Stochastycznych",
    "text": "2.7 Modele w Nauce: Od Deterministycznych do Stochastycznych\nModele są niezbędnymi narzędziami w badaniach naukowych, pomagając naukowcom reprezentować, rozumieć i przewidywać złożone zjawiska. Ta sekcja omawia główne typy modeli stosowanych w nauce, wraz z przykładami ich zastosowań. Należy pamiętać, że te kategorie często się nakładają, a wiele modeli naukowych łączy w sobie różne aspekty.\n\n2.7.1 Modele Matematyczne\nModele matematyczne wykorzystują równania i koncepcje matematyczne do opisywania i analizowania systemów lub zjawisk. Można je podzielić na kilka podkategorii, choć należy pamiętać, że niektóre złożone modele mogą zawierać elementy z wielu kategorii:\n\n2.7.1.1 a. Modele Deterministyczne\nModele deterministyczne dostarczają precyzyjnych przewidywań na podstawie zestawu zmiennych, bez uwzględniania losowości na poziomie makroskopowym.\nPrzykład: Prawa ruchu Newtona, które mogą precyzyjnie przewidzieć ruch obiektów pod wpływem znanych sił w mechanice klasycznej.\n\n\n2.7.1.2 b. Modele Stochastyczne\nModele stochastyczne uwzględniają losowość i prawdopodobieństwo. Jednak kluczowe jest rozróżnienie dwóch fundamentalnie różnych typów modeli stochastycznych:\n\n2.7.1.2.1 i. Klasyczne Modele Stochastyczne\nTe modele zajmują się losowością wynikającą z niepełnej informacji lub złożonych interakcji w systemach klasycznych. Podstawowy system jest deterministyczny, ale praktyczne ograniczenia w pomiarach lub obliczeniach prowadzą do użycia opisów probabilistycznych.\nPrzykład: Modele regresji w statystyce, gdzie losowość reprezentuje niewyjaśnioną zmienność lub błąd pomiaru:\n\\[y = β_0 + β_1x + ε\\]\nGdzie:\n\n\\(y\\) to zmienna zależna (np. wielkość popytu na dobro)\n\\(x\\) to zmienna niezależna (np. cena lub dochód konsumenta)\n\\(β_0\\) i \\(β_1\\) to parametry\n\\(ε\\) to składnik błędu, reprezentujący niewyjaśnioną zmienność\n\n\n\n2.7.1.2.2 ii. Kwantowe Modele Stochastyczne\nTe modele zajmują się fundamentalną, nieredukowalną losowością nieodłącznie związaną z systemami mechaniki kwantowej. Ta losowość nie wynika z braku informacji, ale jest podstawową cechą rzeczywistości kwantowej.\nPrzykład: Model Standardowy w fizyce cząstek elementarnych, który opisuje interakcje cząstek za pomocą kwantowej teorii pola. Na przykład, rozpad cząstki jest z natury probabilistyczny:\n\\[P(t) = e^{-t/τ}\\]\nGdzie:\n\n\\(P(t)\\) to prawdopodobieństwo, że cząstka nie rozpadła się po czasie t\n\\(τ\\) to średni czas życia cząstki\n\n\n\n\n2.7.1.3 c. Modele Symulacji Komputerowych\nSymulacje komputerowe wykorzystują algorytmy i metody obliczeniowe oparte na modelach matematycznych do symulowania złożonych systemów i przewidywania ich zachowania w czasie. Mogą być deterministyczne lub stochastyczne.\nPrzykład: Modele klimatyczne symulujące system klimatyczny Ziemi, uwzględniające czynniki takie jak skład atmosfery, prądy oceaniczne i promieniowanie słoneczne do prognozowania przyszłych scenariuszy klimatycznych.\n\n\n\n2.7.2 Modele Koncepcyjne\nModele koncepcyjne to abstrakcyjne reprezentacje systemów lub procesów, często wykorzystujące diagramy lub schematy blokowe do ilustrowania relacji między komponentami.\nPrzykład: Model obiegu wody w naukach o Ziemi, który ilustruje ciągły ruch wody w obrębie Ziemi i atmosfery poprzez procesy takie jak parowanie, opady i spływ powierzchniowy.\n\n\n2.7.3 Modele Fizyczne\nModele fizyczne to namacalne reprezentacje obiektów lub systemów, często w formie pomniejszonej lub uproszczonej wersji rzeczywistego obiektu.\nPrzykład: Modele tunelu aerodynamicznego w badaniach aerodynamiki, używane do badania efektów przepływu powietrza wokół obiektów stałych i optymalizacji projektów samolotów, pojazdów lub budynków.\n\n\n2.7.4 Modele Teoretyczne\nModele teoretyczne to abstrakcyjne ramy oparte na fundamentalnych zasadach i hipotezach, często używane do wyjaśniania obserwowanych zjawisk lub przewidywania nowych. Te modele często wykorzystują równania matematyczne i mogą być deterministyczne lub stochastyczne.\nPrzykład: Teoria ewolucji poprzez dobór naturalny, która dostarcza ram do zrozumienia różnorodności i adaptacji form życia w czasie.\n\n\n2.7.5 Podsumowanie\nTe różne formy modeli odgrywają kluczową rolę w badaniach naukowych, każda oferując unikalne zalety dla zrozumienia i przewidywania zjawisk naturalnych. Naukowcy często używają wielu typów modeli jednocześnie, aby uzyskać kompleksowy wgląd w złożone systemy i procesy.\nWażne jest, aby zdawać sobie sprawę, że te kategorie nie są wzajemnie wykluczające i często się nakładają:\n\nModele matematyczne stanowią podstawę dla wielu innych typów modeli, w tym symulacji komputerowych i niektórych modeli teoretycznych.\nModele symulacji komputerowych są zasadniczo modelami matematycznymi implementowanymi za pomocą metod obliczeniowych i mogą być deterministyczne lub stochastyczne.\nModele teoretyczne często wykorzystują sformułowania matematyczne i mogą być implementowane jako symulacje komputerowe.\nModele fizyczne mogą być projektowane na podstawie modeli matematycznych i mogą być używane do walidacji symulacji komputerowych.\n\nWybór typu modelu często zależy od konkretnego pytania badawczego, natury badanego systemu, dostępnych danych oraz zasobów obliczeniowych. W miarę postępu nauki granice między tymi typami modeli coraz bardziej się zacierają, prowadząc do coraz bardziej wyrafinowanych i interdyscyplinarnych podejść do modelowania złożonych zjawisk.\nKluczowe jest rozróżnienie różnych typów modeli stochastycznych. Klasyczne modele stochastyczne, takie jak te używane w analizie regresji, zajmują się losowością wynikającą z niepełnej informacji lub złożonych interakcji w systemach, które są zasadniczo deterministyczne. Z drugiej strony, kwantowe modele stochastyczne, jak te w fizyce cząstek, zajmują się fundamentalną, nieredukowalną losowością nieodłącznie związaną z systemami mechaniki kwantowej. To rozróżnienie odzwierciedla głębokie różnice między klasycznymi a kwantowymi paradygmatami w fizyce i podkreśla różnorodne sposoby, w jakie prawdopodobieństwo jest wykorzystywane w modelowaniu naukowym.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zrozumienie-pozornych-korelacji-zmiennych-zakłócających-i-kolizyjnych",
    "href": "rozdzial1.html#zrozumienie-pozornych-korelacji-zmiennych-zakłócających-i-kolizyjnych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.8 Zrozumienie Pozornych Korelacji, Zmiennych Zakłócających i Kolizyjnych (*)",
    "text": "2.8 Zrozumienie Pozornych Korelacji, Zmiennych Zakłócających i Kolizyjnych (*)\nW tej sekcji zbadamy trzy ważne pojęcia w analizie statystycznej: pozorne korelacje, zmienne zakłócające i zmienne kolizyjne. Zrozumienie tych pojęć jest kluczowe dla uniknięcia błędnej interpretacji danych i wyciągania nieprawidłowych wniosków z analiz statystycznych.\nZacznijmy od załadowania niezbędnych bibliotek:\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nset.seed(123) # dla powtarzalności\n\n\n2.8.1 Pozorne Korelacje\nPozorne korelacje to związki między zmiennymi, które wydają się przyczynowe, ale w rzeczywistości są przypadkowe lub spowodowane przez niewidoczny trzeci czynnik.\n\n2.8.1.1 Przykład: Sprzedaż lodów a przypadki utonięć\nStwórzmy zbiór danych, który pokazuje pozorną korelację między sprzedażą lodów a przypadkami utonięć:\n\nn &lt;- 100\ndane_pozorne &lt;- tibble(\n  temperatura = rnorm(n, mean = 25, sd = 5),\n  sprzedaz_lodow = 100 + 5 * temperatura + rnorm(n, sd = 10),\n  przypadki_utoniec = 1 + 0.5 * temperatura + rnorm(n, sd = 2)\n)\n\nggplot(dane_pozorne, aes(x = sprzedaz_lodow, y = przypadki_utoniec)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Pozorna Korelacja: Sprzedaż Lodów vs Przypadki Utonięć\",\n       x = \"Sprzedaż Lodów\", y = \"Przypadki Utonięć\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTen wykres pokazuje pozytywną korelację między sprzedażą lodów a przypadkami utonięć. Jednak ta relacja jest pozorna. Prawdziwą przyczyną obu zjawisk jest temperatura:\n\nggplot(dane_pozorne, aes(x = temperatura)) +\n  geom_point(aes(y = sprzedaz_lodow), color = \"blue\") +\n  geom_point(aes(y = przypadki_utoniec * 10), color = \"red\") +\n  geom_smooth(aes(y = sprzedaz_lodow), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(aes(y = przypadki_utoniec * 10), method = \"lm\", se = FALSE, color = \"red\") +\n  scale_y_continuous(\n    name = \"Sprzedaż Lodów\",\n    sec.axis = sec_axis(~./10, name = \"Przypadki Utonięć\")\n  ) +\n  labs(title = \"Temperatura jako Wspólna Przyczyna\",\n       x = \"Temperatura\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n2.8.2 Zmienne Zakłócające\nZmienna zakłócająca to zmienna, która wpływa zarówno na zmienną zależną, jak i niezależną, powodując pozorny związek.\n\n2.8.2.1 Przykład: Edukacja, Dochód i Wiek\nStwórzmy zbiór danych, w którym wiek zakłóca relację między edukacją a dochodem:\n\nlibrary(tidyverse)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nn &lt;- 1000\nconfounder_data &lt;- tibble(\n  age = runif(n, 25, 65),\n  education = round(10 + 0.1 * age + rnorm(n, sd = 2)),\n  income = 20000 + 1000 * education + 500 * age + rnorm(n, sd = 5000)\n)\n\n# Without controlling for age\nmodel_naive &lt;- lm(income ~ education, data = confounder_data)\n# Controlling for age\nmodel_adjusted &lt;- lm(income ~ education + age, data = confounder_data)\n\n# Create age groups for visualization\nconfounder_data &lt;- confounder_data %&gt;%\n  mutate(age_group = cut(age, breaks = 3, labels = c(\"Young\", \"Middle\", \"Old\")))\n\n# Visualize\nggplot(confounder_data, aes(x = education, y = income)) +\n  geom_point(aes(color = age), alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1.2) +\n  geom_smooth(aes(group = age_group, color = as.numeric(age_group)), \n              method = \"lm\", se = FALSE, linewidth = 1) +\n  scale_color_viridis_c(name = \"Age\", \n                        breaks = c(30, 45, 60), \n                        labels = c(\"Young\", \"Middle\", \"Old\")) +\n  labs(title = \"Education vs Income, Confounded by Age\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPorównajmy współczynniki:\n\nsummary(model_naive)$coefficients[\"education\", \"Estimate\"]\n\n[1] 2328.718\n\nsummary(model_adjusted)$coefficients[\"education\", \"Estimate\"]\n\n[1] 1101.783\n\n\nEfekt edukacji na dochód jest przeszacowany, gdy nie kontrolujemy wieku.\n\n\n\n2.8.3 Zmienne Kolizyjne\nZmienna kolizyjna to zmienna, na którą wpływają zarówno zmienna niezależna, jak i zmienna zależna. Kontrolowanie zmiennej kolizyjnej może wprowadzić pozorną korelację.\n\n2.8.3.1 Przykład: Satysfakcja z pracy, Wynagrodzenie i Równowaga między pracą a życiem prywatnym\nStwórzmy zbiór danych, w którym równowaga między pracą a życiem prywatnym jest zmienną kolizyjną między satysfakcją z pracy a wynagrodzeniem:\n\nn &lt;- 1000\ndane_kolizyjne &lt;- tibble(\n  satysfakcja_z_pracy = rnorm(n),\n  wynagrodzenie = rnorm(n),\n  rownowaga_praca_zycie = -0.5 * satysfakcja_z_pracy - 0.5 * wynagrodzenie + rnorm(n, sd = 0.5)\n)\n\n# Bez kontrolowania równowagi praca-życie\nmodel_poprawny &lt;- lm(wynagrodzenie ~ satysfakcja_z_pracy, data = dane_kolizyjne)\n\n# Błędne kontrolowanie równowagi praca-życie\nmodel_kolizyjny &lt;- lm(wynagrodzenie ~ satysfakcja_z_pracy + rownowaga_praca_zycie, data = dane_kolizyjne)\n\n# Wizualizacja\nggplot(dane_kolizyjne, aes(x = satysfakcja_z_pracy, y = wynagrodzenie, color = rownowaga_praca_zycie)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_color_viridis_c() +\n  labs(title = \"Satysfakcja z Pracy vs Wynagrodzenie, Równowaga Praca-Życie jako Zmienna Kolizyjna\",\n       x = \"Satysfakcja z Pracy\", y = \"Wynagrodzenie\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPorównajmy współczynniki:\n\nsummary(model_poprawny)$coefficients[\"satysfakcja_z_pracy\", \"Estimate\"]\n\n[1] 0.02063487\n\nsummary(model_kolizyjny)$coefficients[\"satysfakcja_z_pracy\", \"Estimate\"]\n\n[1] -0.4794016\n\n\nKontrolowanie zmiennej kolizyjnej (równowaga praca-życie) wprowadza pozorną korelację między satysfakcją z pracy a wynagrodzeniem.\n\n\n\n2.8.4 Podsumowanie\nZrozumienie pozornych korelacji, zmiennych zakłócających i kolizyjnych jest kluczowe dla prawidłowej analizy statystycznej i wnioskowania przyczynowego. Zawsze rozważ podstawową strukturę przyczynową swoich danych i bądź ostrożny w kwestii tego, które zmienne kontrolujesz w swoich analizach.\n\n\n2.8.5 Dalsza Lektura\n\nPearl, J. (2009). Causality: Models, Reasoning, and Inference. Cambridge University Press.\nHernán, M. A., & Robins, J. M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#etyczne-aspekty-w-analizie-danych-nauk-społecznych",
    "href": "rozdzial1.html#etyczne-aspekty-w-analizie-danych-nauk-społecznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.9 Etyczne Aspekty w Analizie Danych Nauk Społecznych",
    "text": "2.9 Etyczne Aspekty w Analizie Danych Nauk Społecznych\nEtyka odgrywa kluczową rolę w badaniach nauk społecznych:\n\nPrywatność i Zgoda: Zapewnienie prywatności uczestników i świadomej zgody\nOchrona Danych: Bezpieczne przechowywanie i zarządzanie wrażliwymi danymi osobowymi\nBłędy i Reprezentacja: Adresowanie błędów próbkowania i zapewnienie różnorodnej reprezentacji\nPrzejrzystość: Jasne komunikowanie metod badawczych i ograniczeń\nWpływ Społeczny: Rozważanie potencjalnych społecznych implikacji wyników badań\n\n\n\n\n\n\n\nWarning\n\n\n\nNaukowcy społeczni muszą starannie rozważyć etyczne implikacje swoich praktyk zbierania, analizy i rozpowszechniania danych.\n\n\n\n2.9.1 Kluczowe Wnioski\n\nNauka o danych w naukach społecznych bazuje na tradycyjnych metodach statystycznych, włączając nowe technologie do analizy złożonych zjawisk społecznych.\nZrozumienie koncepcji takich jak populacja, próba i procesy generowania danych jest kluczowe dla prawidłowych badań w naukach społecznych.\nProces nauki o danych w badaniach społecznych obejmuje wiele etapów, od etycznego zbierania danych po komunikację wniosków.\nR jest potężnym narzędziem do analizy danych w naukach społecznych, oferującym szeroki zakres możliwości.\nAspekty etyczne powinny być na pierwszym planie każdego projektu związanego z danymi w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-a-losowość-klasyczna-a-kwantowa-zrozumienie-fundamentalnych-różnic",
    "href": "rozdzial1.html#appendix-a-losowość-klasyczna-a-kwantowa-zrozumienie-fundamentalnych-różnic",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.10 Appendix A: Losowość Klasyczna a Kwantowa: Zrozumienie Fundamentalnych Różnic",
    "text": "2.10 Appendix A: Losowość Klasyczna a Kwantowa: Zrozumienie Fundamentalnych Różnic\nAby zrozumieć, jak losowość w mechanice kwantowej różni się od losowości reprezentowanej przez składnik błędu w modelach regresji, musimy przeanalizować ich pochodzenie, naturę i implikacje.\n\n2.10.1 Pochodzenie Losowości\n\n2.10.1.1 Losowość Klasyczna (Modele Regresji)\n\nŹródło: Niekompletna informacja lub złożone interakcje w systemie, który w zasadzie jest deterministyczny.\nNatura: Niepewność epistemiczna (wynikająca z braku wiedzy).\nPrzykład: W modelu regresji, \\(y = β_0 + β_1x + ε\\), składnik błędu ε reprezentuje niewyjaśnioną zmienność.\n\n\n\n2.10.1.2 Losowość Kwantowa\n\nŹródło: Fundamentalna właściwość systemów kwantowych.\nNatura: Niepewność ontyczna (nieodłączna cecha systemu, nie wynika z braku wiedzy).\nPrzykład: Dokładny moment rozpadu atomu radioaktywnego nie może być przewidziany, można określić jedynie jego prawdopodobieństwo.\n\n\n\n\n2.10.2 Implikacje Filozoficzne\n\n2.10.2.1 Losowość Klasyczna\n\nDeterminizm: Podstawowa rzeczywistość jest deterministyczna; losowość odzwierciedla naszą niewiedzę.\nUkryte Zmienne: W zasadzie, gdybyśmy mieli pełną informację, moglibyśmy dokładnie przewidzieć wyniki.\n\n\n\n2.10.2.2 Losowość Kwantowa\n\nIndeterminizm: Losowość jest fundamentalną cechą rzeczywistości, nie tylko naszego jej opisu.\nBrak Ukrytych Zmiennych: Nawet przy pełnej informacji o systemie kwantowym, niektóre wyniki pozostają nieprzewidywalne (co sugeruje twierdzenie Bella).\n\n\n\n\n2.10.3 Ujęcie Matematyczne\n\n2.10.3.1 Losowość Klasyczna\n\nTeoria Prawdopodobieństwa: Oparta na klasycznej teorii prawdopodobieństwa.\nRozkład: Często zakłada się znane rozkłady (np. rozkład normalny w wielu modelach regresji).\nCentralne Twierdzenie Graniczne: Stosuje się do dużych prób zmiennych losowych.\n\n\n\n2.10.3.2 Losowość Kwantowa\n\nPrawdopodobieństwo Kwantowe: Oparte na matematycznych podstawach mechaniki kwantowej.\nFunkcja Falowa: Opisuje stan kwantowy i jego ewolucję.\nReguła Borna: Określa prawdopodobieństwa wyników pomiarów na podstawie funkcji falowej.\n\n\n\n\n2.10.4 Przewidywalność i Kontrola\n\n2.10.4.1 Losowość Klasyczna\n\nRedukowalna: W zasadzie można ją zmniejszyć, zbierając więcej danych lub poprawiając dokładność pomiarów.\nKontrolowalna: Błędy systematyczne można zidentyfikować i skorygować.\n\n\n\n2.10.4.2 Losowość Kwantowa\n\nNieredukowalna: Nie można jej wyeliminować nawet przy idealnych pomiarach.\nFundamentalnie Niekontrolowalna: Sam akt pomiaru wpływa na system (problem pomiaru).\n\n\n\n\n2.10.5 Praktyczne Implikacje\n\n2.10.5.1 Losowość Klasyczna\n\nRedukcja Błędów: Koncentracja na udoskonalaniu technik pomiarowych i zbierania danych.\nUdoskonalanie Modelu: Dążenie do wyjaśnienia większej wariancji i zmniejszenia składnika błędu.\n\n\n\n2.10.5.2 Losowość Kwantowa\n\nNieodłączne Ograniczenie: Akceptacja fundamentalnych granic przewidywalności.\nPrzewidywania Probabilistyczne: Skupienie na dokładnych rozkładach prawdopodobieństwa zamiast na dokładnych wynikach.\n\n\n\n\n2.10.6 Przykłady Pomagające Zrozumieć Różnicę\n\n2.10.6.1 Przykład Losowości Klasycznej\nWyobraź sobie rzut monetą. Fizyka klasyczna mówi, że wynik jest zdeterminowany przez warunki początkowe (przyłożona siła, opór powietrza itp.). “Losowość” wynika z naszej niezdolności do precyzyjnego zmierzenia i uwzględnienia wszystkich tych czynników.\n\n\n2.10.6.2 Przykład Losowości Kwantowej\nW eksperymencie z podwójną szczeliną pojedyncze cząstki wykazują wzory interferencyjne, jakby przechodziły przez obie szczeliny jednocześnie. Dokładna ścieżka każdej pojedynczej cząstki jest fundamentalnie nieokreślona do momentu pomiaru, a tej nieokreśloności nie można rozwiązać przez bardziej precyzyjne pomiary.\n\n\n\n2.10.7 Podsumowanie\nChociaż oba rodzaje losowości prowadzą do probabilistycznych przewidywań, ich fundamentalne natury są zupełnie różne:\n\nLosowość klasyczna w modelach regresji jest odzwierciedleniem naszej niepełnej wiedzy lub ograniczeń pomiarowych w systemie, który w zasadzie jest deterministyczny.\nLosowość kwantowa jest fundamentalną właściwością systemów kwantowych, reprezentującą nieodłączną nieokreśloność w naturze, która utrzymuje się nawet przy doskonałej wiedzy i pomiarze.\n\nZrozumienie tych różnic jest kluczowe dla prawidłowej interpretacji i stosowania modeli statystycznych w różnych kontekstach naukowych, od nauk społecznych wykorzystujących analizę regresji po eksperymenty z fizyki kwantowej.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-b-duże-modele-językowe---zrozumienie-ich-stochastycznej-natury",
    "href": "rozdzial1.html#appendix-b-duże-modele-językowe---zrozumienie-ich-stochastycznej-natury",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.11 Appendix B: Duże Modele Językowe - Zrozumienie Ich Stochastycznej Natury",
    "text": "2.11 Appendix B: Duże Modele Językowe - Zrozumienie Ich Stochastycznej Natury\nDuże Modele Językowe (LLM), takie jak GPT-3, BERT i Claude, zrewolucjonizowały przetwarzanie języka naturalnego, ale mogą popełniać zagadkowe błędy, szczególnie w zadaniach matematycznych. Ten dodatek wyjaśnia funkcjonowanie LLM, ich stochastyczną naturę i porównuje je z klasycznymi modelami statystycznymi.\n\n2.11.1 Podstawy LLM i Ich Stochastyczna Natura\nLLM są trenowane na ogromnych zbiorach danych tekstowych, aby przewidywać rozkład prawdopodobieństwa następnego tokenu w sekwencji. Wykorzystują architektury transformerowe do przetwarzania i generowania tekstu. Kluczowe aspekty ich stochastycznej natury obejmują:\n\nProbabilistyczny wybór tokenów: LLM wybierają każde słowo na podstawie obliczonych prawdopodobieństw, a nie stałych reguł.\nLosowość kontrolowana temperaturą: Parametr “temperatury” dostosowuje losowość wyborów, równoważąc kreatywność i spójność.\nNiedeterministyczne wyniki: Te same dane wejściowe mogą prowadzić do różnych wyników w oddzielnych uruchomieniach.\nKontekstowa niejednoznaczność: LLM interpretują kontekst probabilistycznie, co czasami prowadzi do nieporozumień.\n\n\n\n2.11.2 Porównanie z Klasycznymi Modelami Statystycznymi\nAby lepiej zrozumieć LLM, porównajmy je z regresją Najmniejszych Kwadratów (OLS):\n\n\n\n\n\n\n\n\nAspekt\nRegresja OLS\nDuże Modele Językowe\n\n\n\n\nPodstawowa funkcja\nPrzewiduje ciągłe wyniki na podstawie zmiennych wejściowych\nPrzewiduje rozkład prawdopodobieństwa następnego tokenu na podstawie poprzednich tokenów\n\n\nWejście-Wyjście\nZmienne ciągłe, relacje liniowe\nDyskretne tokeny, relacje nieliniowe\n\n\nTyp predykcji\nPredykcje punktowe z przedziałami ufności\nRozkłady prawdopodobieństwa dla możliwych tokenów\n\n\nZłożoność modelu\nNiewiele parametrów\nMiliardy parametrów\n\n\nInterpretowalność\nJasne interpretacje współczynników\nLargely nieprzejrzyste działanie wewnętrzne\n\n\nObsługa szumu\nZakłada losowy szum w zmiennej wynikowej\nRadzi sobie ze zmiennością języka naturalnego\n\n\nEkstrapolacja\nMniej wiarygodna poza zakresem treningu\nMniej wiarygodna dla nieznanych tematów\n\n\n\nOba modele dążą do nauczenia się mapowania wejścia-wyjścia na podstawie wzorców w danych treningowych.\n\n\n2.11.3 Implikacje dla Zadań Matematycznych\nStochastyczna natura LLM wpływa na operacje matematyczne:\n\nZmienne wyniki dla powtarzanych obliczeń: Każda próba może dać inny wynik ze względu na probabilistyczny wybór tokenów.\nPewność nie gwarantuje poprawności: Wysoka pewność modelu może wystąpić nawet dla niepoprawnych odpowiedzi.\nAproksymacja zamiast dokładnych obliczeń: LLM dopasowują wzorce zamiast wykonywać precyzyjne obliczenia.\n\nOgraniczenia w zadaniach matematycznych wynikają z:\n\nNiedopasowania celu treningu: LLM są trenowane do przewidywania języka, nie dokładności matematycznej.\nBraku jawnego rozumowania matematycznego: Nie mają wbudowanych reguł czy operacji matematycznych.\nBraku pamięci roboczej: LLM nie mogą niezawodnie przechowywać i manipulować wynikami pośrednimi.\nOgraniczonego okna kontekstowego: Mogą tracić istotne informacje w długich problemach.\nOgraniczeń danych treningowych: Niedoreprezentowanie pewnych koncepcji matematycznych może prowadzić do słabych wyników.\nBraku kontroli spójności: LLM nie weryfikują logicznej spójności swoich wyników.\n\n\n\n2.11.4 Najlepsze Praktyki i Wnioski\nPrzy korzystaniu z LLM do zadań matematycznych:\n\nSkup się na wyjaśnieniach koncepcyjnych, nie na dokładnych obliczeniach: LLM doskonale wyjaśniają koncepcje, ale mogą zawodzić w dokładnych obliczeniach.\nWeryfikuj wyniki dedykowanym oprogramowaniem: Zawsze sprawdzaj obliczenia LLM odpowiednimi narzędziami matematycznymi.\nRozbijaj złożone problemy: Podział zadań na mniejsze kroki może poprawić wydajność LLM.\nBądź świadomy efektów przeformułowania: Różne sformułowania tego samego problemu mogą dawać różne wyniki.\nUżywaj jako narzędzi wspomagających, nie zamienników dla ekspertyzy: LLM powinny uzupełniać, a nie zastępować wiedzę matematyczną.\n\nZrozumienie probabilistycznej natury LLM pomaga wykorzystać ich mocne strony w zadaniach językowych, jednocześnie uznając ich ograniczenia w dziedzinach wymagających deterministycznej precyzji, takich jak matematyka.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-c-modele-deterministyczne-a-modele-stochastyczne",
    "href": "rozdzial1.html#appendix-c-modele-deterministyczne-a-modele-stochastyczne",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.12 Appendix C: Modele Deterministyczne a Modele Stochastyczne (*)",
    "text": "2.12 Appendix C: Modele Deterministyczne a Modele Stochastyczne (*)\n\n2.12.1 Modele Deterministyczne\nModele deterministyczne to te, w których wynik jest w pełni określony przez wartości parametrów i warunki początkowe. Modele te są często używane w fizyce i inżynierii.\n\n\n2.12.2 Przykład: Ruch Jednostajnie Przyspieszony\nKlasycznym przykładem modelu deterministycznego jest ruch jednostajnie przyspieszony, opisany równaniem:\n\\[x(t) = x_0 + v_0t + \\frac{1}{2}at^2\\]\nGdzie:\n\n\\(x(t)\\) to położenie w czasie \\(t\\)\n\\(x_0\\) to położenie początkowe\n\\(v_0\\) to prędkość początkowa\n\\(a\\) to przyspieszenie\n\\(t\\) to czas\n\nZasymulujmy to w R:\n\n# Ruch jednostajnie przyspieszony\nsymuluj_ruch_przyspieszony &lt;- function(x0, v0, a, t) {\n  x0 + v0 * t + 0.5 * a * t^2\n}\n\n# Generowanie danych\nt &lt;- seq(0, 10, by = 0.1)\nx &lt;- symuluj_ruch_przyspieszony(x0 = 0, v0 = 2, a = 1, t = t)\n\n# Wykres\nplot(t, x, type = \"l\", xlab = \"Czas\", ylab = \"Położenie\", \n     main = \"Ruch Jednostajnie Przyspieszony\")\n\n\n\n\n\n\n\n\nTen kod wygeneruje wykres ruchu jednostajnie przyspieszonego, który jest intuicyjnym przykładem z dynamiki Newtona. W tym przypadku obiekt zaczyna ruch z początkową prędkością i przyspiesza jednostajnie, co prowadzi do parabolicznej trajektorii na wykresie położenia w funkcji czasu.\n\n\n2.12.3 Modele Stochastyczne w Naukach Społecznych\nModele stochastyczne uwzględniają losowość i są często używane w naukach społecznych, gdzie istnieje nieodłączna niepewność w badanych systemach.\n\n\n2.12.4 Przykład: Regresja Metodą Najmniejszych Kwadratów (OLS)\nOLS to podstawowy model stochastyczny w naukach społecznych. Jest reprezentowany jako:\n\\[Y = \\beta_0 + \\beta_1X + \\epsilon\\]\nGdzie:\n\n\\(Y\\) to zmienna zależna\n\\(X\\) to zmienna niezależna\n\\(\\beta_0\\) i \\(\\beta_1\\) to parametry\n\\(\\epsilon\\) to składnik błędu (komponent stochastyczny)\n\nZademonstrujmy OLS w R:\n\n# Generowanie przykładowych danych\nset.seed(123)\nX &lt;- rnorm(100)\nY &lt;- 2 + 3*X + rnorm(100, sd = 0.5)\n\n# Dopasowanie modelu OLS\nmodel &lt;- lm(Y ~ X)\n\n# Podsumowanie modelu\nsummary(model)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95367 -0.34175 -0.04375  0.29032  1.64520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.94860    0.04878   39.95   &lt;2e-16 ***\nX            2.97376    0.05344   55.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4854 on 98 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.969 \nF-statistic:  3097 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n# Wykres\nplot(X, Y, main = \"Regresja OLS\")\nabline(model, col = \"red\")\n\n\n\n\n\n\n\n\nTo dopasuje model OLS do symulowanych danych i wykreśli wyniki.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff\n\n\n\n\n2.12.5 Zaawansowane Modele Stochastyczne: Duże Modele Językowe\nDuże Modele Językowe (LLM), takie jak GPT-3, to złożone modele stochastyczne używane w przetwarzaniu języka naturalnego. Chociaż nie możemy zaimplementować pełnego LLM w tym tutorialu, możemy omówić jego zasady.\nLLM opierają się na architekturze transformatora i wykorzystują mechanizmy samouwagi. Są trenowane na ogromnych ilościach danych tekstowych i uczą się przewidywać następny token w sekwencji.\nRdzeń LLM można postrzegać jako warunkowy rozkład prawdopodobieństwa:\n\\[P(x_t | x_{&lt;t}, \\theta)\\]\nGdzie:\n\n\\(x_t\\) to aktualny token\n\\(x_{&lt;t}\\) reprezentuje wszystkie poprzednie tokeny\n\\(\\theta\\) to parametry modelu\n\n\n\n\n\n\n\nNote\n\n\n\nTokeny w Dużych Modelach Językowych (LLM) to podstawowe jednostki tekstu, które model przetwarza. Można je postrzegać jako części słów lub znaki interpunkcyjne. Oto kluczowe informacje o tokenach:\nDefinicja: Tokeny to najmniejsze jednostki tekstu, które LLM przetwarza. Mogą to być całe słowa, części słów, a nawet pojedyncze znaki lub znaki interpunkcyjne. Tokenizacja: Proces dzielenia tekstu na tokeny nazywa się tokenizacją. LLM używają specyficznych algorytmów do wykonania tego zadania. Przykłady:\nSłowo “kot” może być pojedynczym tokenem. Dłuższe słowo jak “zrozumienie” może być podzielone na wiele tokenów, np. “zrozum” i “ienie”. Znaki interpunkcyjne jak “.” czy “?” są często oddzielnymi tokenami. Powszechne przedrostki lub przyrostki mogą być własnymi tokenami.\nSłownictwo: LLM mają ustalone słownictwo tokenów, które rozpoznają. To słownictwo zazwyczaj obejmuje od dziesiątek tysięcy do setek tysięcy tokenów. Znaczenie: Sposób tokenizacji tekstu może wpływać na to, jak model rozumie i generuje język. Jest to szczególnie ważne przy obsłudze różnych języków, rzadkich słów lub specjalistycznego słownictwa. Kontekst: W równaniu dla LLM: \\[P(x_t | x_{&lt;t}, \\theta)\\] Gdzie:\n\\(x_t\\) reprezentuje bieżący token \\(x_{&lt;t}\\) reprezentuje wszystkie poprzednie tokeny w sekwencji \\(\\theta\\) reprezentuje parametry modelu\n\n\nW przeciwieństwie do modeli deterministycznych, LLM produkują różne wyniki nawet dla tego samego wejścia ze względu na ich stochastyczną naturę.\n\n\n2.12.6 Podsumowanie\nKażdy rodzaj modelu ma swoje miejsce w nauce, w zależności od badanego systemu i poziomu niepewności.\nPamiętaj, że wybór między modelami deterministycznymi a stochastycznymi często zależy od natury badanego systemu i pytań, na które próbujesz odpowiedzieć. Modele deterministyczne są świetne dla systemów o dobrze zrozumiałej mechanice, podczas gdy modele stochastyczne sprawdzają się przy radzeniu sobie z nieodłączną losowością lub złożonymi, nie w pełni zrozumiałymi systemami.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-d-wprowadzenie-do-r-rstudio-i-tidyverse",
    "href": "rozdzial1.html#appendix-d-wprowadzenie-do-r-rstudio-i-tidyverse",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.13 Appendix D: Wprowadzenie do R, RStudio i tidyverse",
    "text": "2.13 Appendix D: Wprowadzenie do R, RStudio i tidyverse\nR to potężny język programowania i środowisko do obliczeń statystycznych i grafiki. Jest szeroko stosowany w środowisku akademickim, szczególnie w naukach społecznych, do analizy danych i wizualizacji.\n\n2.13.0.1 Kluczowe cechy R:\n\nOtwarty kod źródłowy i darmowy\nRozbudowany ekosystem pakietów\nSilne wsparcie społeczności\nDoskonały do analizy statystycznej i wizualizacji danych\n\n\n\n2.13.1 Pierwsze kroki z RStudio\nRStudio to zintegrowane środowisko programistyczne (IDE) dla R, które ułatwia pracę z R.\n\n2.13.1.1 Instalacja R i RStudio\n\nPobierz i zainstaluj R ze strony CRAN\nPobierz i zainstaluj RStudio ze strony RStudio\n\n\n\n2.13.1.2 Interfejs RStudio\nRStudio ma cztery główne panele:\n\nEdytor źródłowy: Gdzie piszesz i edytujesz skrypty R\nKonsola: Gdzie możesz wpisywać polecenia R i widzieć wyniki\nŚrodowisko/Historia: Pokazuje wszystkie obiekty w twoim obszarze roboczym i historię poleceń\nPliki/Wykresy/Pakiety/Pomoc: Wielofunkcyjny panel do zarządzania plikami, przeglądania wykresów, zarządzania pakietami i dostępu do pomocy\n\n\n\n2.13.1.3 Podstawowe funkcje RStudio\n\nTworzenie nowego skryptu R: Plik &gt; Nowy plik &gt; Skrypt R\nUruchamianie kodu: Zaznacz kod i naciśnij Ctrl+Enter (Cmd+Enter na Macu)\nInstalowanie pakietów: Narzędzia &gt; Instaluj pakiety\nUzyskiwanie pomocy: Wpisz ?nazwa_funkcji w konsoli\n\n\n\n\n2.13.2 Podstawy R\n\n2.13.2.1 Typy danych w R\n\n# Numeryczny\nx &lt;- 10.5\nclass(x)\n\n[1] \"numeric\"\n\n# Całkowity\ny &lt;- 1L\nclass(y)\n\n[1] \"integer\"\n\n# Znakowy\nimie &lt;- \"Alicja\"\nclass(imie)\n\n[1] \"character\"\n\n# Logiczny\njest_studentem &lt;- TRUE\nclass(jest_studentem)\n\n[1] \"logical\"\n\n\n\n\n2.13.2.2 Struktury danych\n\n2.13.2.2.1 Wektory\n\n# Tworzenie wektora\nliczby &lt;- c(1, 2, 3, 4, 5)\nowoce &lt;- c(\"jabłko\", \"banan\", \"wiśnia\")\n\n# Operacje na wektorach\nliczby + 2\n\n[1] 3 4 5 6 7\n\nliczby * 2\n\n[1]  2  4  6  8 10\n\nmean(liczby)\n\n[1] 3\n\nlength(owoce)\n\n[1] 3\n\n\n\n\n2.13.2.2.2 Macierze\n\n# Tworzenie macierzy\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\nprint(m)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n# Operacje na macierzach\nt(m)  # transpozycja\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nm * 2  # mnożenie skalarne\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    4    8   12\n\n\n\n\n2.13.2.2.3 Ramki danych\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(\n  imie = c(\"Alicja\", \"Bartek\", \"Celina\"),\n  wiek = c(25, 30, 35),\n  student = c(TRUE, FALSE, TRUE)\n)\nprint(df)\n\n    imie wiek student\n1 Alicja   25    TRUE\n2 Bartek   30   FALSE\n3 Celina   35    TRUE\n\n# Dostęp do elementów ramki danych\ndf$imie\n\n[1] \"Alicja\" \"Bartek\" \"Celina\"\n\ndf[1, 2]\n\n[1] 25\n\ndf[df$wiek &gt; 25, ]\n\n    imie wiek student\n2 Bartek   30   FALSE\n3 Celina   35    TRUE\n\n\n\n\n\n2.13.2.3 Funkcje\n\n# Definiowanie funkcji\npowitaj &lt;- function(imie) {\n  paste(\"Cześć,\", imie, \"!\")\n}\n\n# Użycie funkcji\npowitaj(\"Alicja\")\n\n[1] \"Cześć, Alicja !\"\n\n# Funkcja z wieloma argumentami\noblicz_bmi &lt;- function(waga, wzrost) {\n  bmi &lt;- waga / (wzrost^2)\n  return(bmi)\n}\n\noblicz_bmi(70, 1.75)\n\n[1] 22.85714\n\n\n\n\n2.13.2.4 Struktury kontrolne\n\n# Instrukcja if-else\nx &lt;- 10\nif (x &gt; 5) {\n  print(\"x jest większe niż 5\")\n} else {\n  print(\"x nie jest większe niż 5\")\n}\n\n[1] \"x jest większe niż 5\"\n\n# Pętla for\nfor (i in 1:5) {\n  print(paste(\"Iteracja\", i))\n}\n\n[1] \"Iteracja 1\"\n[1] \"Iteracja 2\"\n[1] \"Iteracja 3\"\n[1] \"Iteracja 4\"\n[1] \"Iteracja 5\"\n\n# Pętla while\nlicznik &lt;- 1\nwhile (licznik &lt;= 5) {\n  print(paste(\"Licznik:\", licznik))\n  licznik &lt;- licznik + 1\n}\n\n[1] \"Licznik: 1\"\n[1] \"Licznik: 2\"\n[1] \"Licznik: 3\"\n[1] \"Licznik: 4\"\n[1] \"Licznik: 5\"\n\n\n\n\n\n2.13.3 Wprowadzenie do tidyverse\nTidyverse to kolekcja pakietów R zaprojektowanych do nauki o danych. Te pakiety mają wspólną filozofię i są zaprojektowane do bezproblemowej współpracy.\n\n2.13.3.1 Kluczowe pakiety tidyverse\n\nggplot2: do wizualizacji danych\ndplyr: do manipulacji danymi\ntidyr: do porządkowania danych\nreadr: do odczytu danych prostokątnych\npurrr: do programowania funkcyjnego\ntibble: nowoczesne ujęcie ramek danych\n\n\n\n2.13.3.2 Rozpoczęcie pracy z tidyverse\n\n# Instalacja tidyverse (uruchom raz)\n# install.packages(\"tidyverse\")\n\n# Wczytanie tidyverse\nlibrary(tidyverse)\n\n\n\n2.13.3.3 Import danych z readr\n\n# Odczyt plików CSV\ndane &lt;- read_csv(\"dane_spoleczne.csv\")\n\n# Odczyt innych formatów plików\nread_tsv(\"dane.tsv\")  # Wartości oddzielone tabulatorem\nread_delim(\"dane.txt\", delim = \"|\")  # Niestandardowy separator\n\n\n\n2.13.3.4 Manipulacja danymi z dplyr\n\n# Użyjmy wbudowanego zbioru danych mtcars\ndata(\"mtcars\")\n\n# Wybieranie kolumn\nmtcars %&gt;% \n  select(mpg, cyl, hp)\n\n                     mpg cyl  hp\nMazda RX4           21.0   6 110\nMazda RX4 Wag       21.0   6 110\nDatsun 710          22.8   4  93\nHornet 4 Drive      21.4   6 110\nHornet Sportabout   18.7   8 175\nValiant             18.1   6 105\nDuster 360          14.3   8 245\nMerc 240D           24.4   4  62\nMerc 230            22.8   4  95\nMerc 280            19.2   6 123\nMerc 280C           17.8   6 123\nMerc 450SE          16.4   8 180\nMerc 450SL          17.3   8 180\nMerc 450SLC         15.2   8 180\nCadillac Fleetwood  10.4   8 205\nLincoln Continental 10.4   8 215\nChrysler Imperial   14.7   8 230\nFiat 128            32.4   4  66\nHonda Civic         30.4   4  52\nToyota Corolla      33.9   4  65\nToyota Corona       21.5   4  97\nDodge Challenger    15.5   8 150\nAMC Javelin         15.2   8 150\nCamaro Z28          13.3   8 245\nPontiac Firebird    19.2   8 175\nFiat X1-9           27.3   4  66\nPorsche 914-2       26.0   4  91\nLotus Europa        30.4   4 113\nFord Pantera L      15.8   8 264\nFerrari Dino        19.7   6 175\nMaserati Bora       15.0   8 335\nVolvo 142E          21.4   4 109\n\n# Filtrowanie wierszy\nmtcars %&gt;% \n  filter(cyl == 4)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n# Sortowanie danych\nmtcars %&gt;% \n  arrange(desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n# Tworzenie nowych zmiennych\nmtcars %&gt;% \n  mutate(kpl = mpg * 0.425)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb     kpl\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  8.9250\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  8.9250\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1  9.6900\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  9.0950\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  7.9475\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  7.6925\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  6.0775\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 10.3700\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  9.6900\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  8.1600\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  7.5650\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  6.9700\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  7.3525\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  6.4600\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  4.4200\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  4.4200\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  6.2475\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 13.7700\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 12.9200\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 14.4075\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  9.1375\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  6.5875\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  6.4600\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  5.6525\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  8.1600\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 11.6025\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 11.0500\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 12.9200\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  6.7150\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  8.3725\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  6.3750\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2  9.0950\n\n# Podsumowywanie danych\nmtcars %&gt;% \n  group_by(cyl) %&gt;% \n  summarize(srednie_mpg = mean(mpg),\n            liczba = n())\n\n# A tibble: 3 × 3\n    cyl srednie_mpg liczba\n  &lt;dbl&gt;       &lt;dbl&gt;  &lt;int&gt;\n1     4        26.7     11\n2     6        19.7      7\n3     8        15.1     14\n\n\n\n\n2.13.3.5 Wizualizacja danych z ggplot2\n\n# Wykres rozrzutu\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Waga samochodu vs. Zużycie paliwa\",\n       x = \"Waga (1000 funtów)\",\n       y = \"Mile na galon\")\n\n\n\n\nWaga samochodu vs. Zużycie paliwa\n\n\n\n\n\n# Wykres słupkowy\nmtcars %&gt;% \n  count(cyl) %&gt;% \n  ggplot(aes(x = factor(cyl), y = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Liczba samochodów według liczby cylindrów\",\n       x = \"Liczba cylindrów\",\n       y = \"Liczba\")\n\n\n\n\nLiczba samochodów według liczby cylindrów\n\n\n\n\n\n# Wykres pudełkowy\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Zużycie paliwa według liczby cylindrów\",\n       x = \"Liczba cylindrów\",\n       y = \"Mile na galon\")\n\n\n\n\nZużycie paliwa według liczby cylindrów\n\n\n\n\n\n\n\n2.13.4 Dodatkowe zasoby\n\nR for Data Science\nDokumentacja tidyverse\nŚciągawki RStudio\nPrzewodnik Quarto\nR Cookbook\n\nPamiętaj, aby eksperymentować z kodem, modyfikować przykłady i nie wahaj się korzystać z wbudowanego systemu pomocy R (dostępnego przez wpisanie ?nazwa_funkcji w konsoli), gdy napotkasz nieznane funkcje lub koncepcje.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1 Foundations in Number Sets\nBefore diving into data types, it’s essential to understand the basic number sets that form the foundation of our understanding of data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#foundations-in-number-sets",
    "href": "chapter2.html#foundations-in-number-sets",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1.1 Basic Number Sets\n\nNatural Numbers (ℕ): The counting numbers {1, 2, 3, …}\nIntegers (ℤ): Includes natural numbers, their negatives, and zero {…, -2, -1, 0, 1, 2, …}\nRational Numbers (ℚ): Numbers that can be expressed as a fraction of two integers\nReal Numbers (ℝ): All numbers on the number line, including rationals and irrationals\n\n\n\n3.1.2 Properties of Sets\n\nCountable Sets: Sets whose elements can be put in a one-to-one correspondence with the natural numbers. For example, the set of integers is countable.\nUncountable Sets: Sets that are not countable. The set of real numbers is uncountable.\nDiscrete Sets: Sets where each element is separated from other elements by a finite gap. The integers form a discrete set.\nDense Sets: Sets where between any two elements, there is always another element of the set. The rational numbers and real numbers are dense sets.\n\n\n\n\n\n\n\nNote\n\n\n\nUnderstanding these set properties is crucial for grasping the nature of different data types in social sciences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#discrete-vs.-continuous-data-a-nuanced-view",
    "href": "chapter2.html#discrete-vs.-continuous-data-a-nuanced-view",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.2 Discrete vs. Continuous Data: A Nuanced View",
    "text": "3.2 Discrete vs. Continuous Data: A Nuanced View\nIn data science and statistics, we often categorize variables as either discrete or continuous. However, the distinction is not always clear-cut, and some variables exhibit characteristics of both types. This section explores the concepts of discrete and continuous data, their differences, and the interesting cases of variables that can be treated as both or challenge our intuitive understanding.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n3.2.1 Discrete Data\nDiscrete data can only take on specific, countable values. These values are often (but not always) integers.\n\n3.2.1.1 Characteristics of Discrete Data:\n\nCountable\nOften represented by integers\nCan be finite or infinite\nNo values between two adjacent data points\n\n\n\n3.2.1.2 Examples:\n\nNumber of students in a class\nNumber of cars sold by a dealership\nShoe sizes\n\n\n\n\n3.2.2 Continuous Data\nContinuous data can take on any value within a given range, including fractional and decimal values. It’s important to note that continuity is not solely determined by uncountability, but also by density.\n\n3.2.2.1 Characteristics of Continuous Data:\n\nCan be uncountable (like real numbers) or dense (like rational numbers)\nCan be measured to any level of precision (theoretically)\nRepresented by real numbers or dense subsets of real numbers\nThere are always values between any two data points\n\n\n\n3.2.2.2 Examples:\n\nHeight\nWeight\nTemperature\nPercentages (explained further below)\n\n\n\n\n3.2.3 The Discrete-Continuous Spectrum\nIn practice, some variables that are mathematically discrete are often treated as if they are continuous. This dual nature provides flexibility in how these variables can be analyzed and interpreted.\n\n3.2.3.1 Reasons for Treating Discrete Data as Continuous:\n\nDense Granularity\n\nWhen a discrete variable has a large number of possible values within a range, it can approximate continuity.\nExample: Income measured in individual cents. While technically discrete, the large number of possible values makes it behave similarly to a continuous variable.\n\nAnalytical Convenience\n\nContinuous methods often yield reasonable and useful results even for dense discrete variables.\nIt’s often easier to use existing statistical tools if continuity is assumed, as this allows the use of calculus-based methods.\n\nApproximation of Underlying Phenomena\n\nIn some cases, a discrete measurement might be an approximation of an underlying continuous process.\nExample: While we measure time in discrete units (seconds, minutes, hours), time itself is continuous.\n\n\n\n\n3.2.3.2 Examples of Variables with Dual Discrete-Continuous Nature:\n\nAge\n\nDiscrete: Typically measured in whole years\nContinuous: Can be considered as a continuous variable in many analyses, especially when dealing with large populations\n\nPrice and Income\n\nDiscrete: Prices and incomes are actually measured in discrete units (e.g., cents or smallest currency unit)\nContinuous: In economic models and many analyses, prices and incomes are treated as continuous variables due to their dense nature and analytical convenience\n\nTest Scores\n\nDiscrete: Often given as whole numbers\nContinuous: In statistical analyses, test scores might be treated as continuous, especially when the range of possible scores is large\n\n\n\n\n\n3.2.4 Special Case: Percentages and Rational Numbers\nPercentages present an interesting case in the discrete-continuous spectrum:\n\nRational Nature: Percentages are essentially fractions (m/100), making them rational numbers.\nDense but Countable: The set of rational numbers is dense (between any two rationals, there’s another rational) but also countable.\nPractical Continuity: In most practical applications, percentages are treated as continuous due to their dense nature.\nFinite Precision: In reality, percentages are often reported to a limited number of decimal places, creating a finite set of possible values.\n\n\n\n\n\n\n\nPercentages: Bridging Discrete and Continuous\n\n\n\nVariables measured in percentages, such as unemployment rates or voter turnout, challenge our intuitive understanding of discreteness and continuity:\n\nThey are rational numbers (fractions with denominator 100), which are technically countable.\nThey form a dense set within their range (0% to 100%), allowing for values between any two percentages.\nIn practice, they are often treated as continuous variables due to their dense nature and analytical convenience.\nThe precision of measurement (e.g., reporting to one or two decimal places) can impose a discrete structure on what is conceptually a dense set.\n\nThis duality allows for flexible analytical approaches, depending on the specific research context and required precision.\n\n\n\n\n3.2.5 Implications for Data Analysis\nUnderstanding the nuanced nature of variables as discrete, continuous, or somewhere in between has important implications for data analysis:\n\nFlexibility in Modeling: It allows for the use of a wider range of statistical techniques.\nSimplified Calculations: Treating dense discrete data as continuous can simplify calculations and make certain analyses more tractable.\nImproved Interpretability: In some cases, treating discrete data as continuous can lead to more intuitive or useful interpretations of results.\nPotential for Error: It’s important to be aware of when approximations are appropriate and when they might lead to misleading results.\nTheoretical vs. Practical Considerations: While the mathematical nature of the data is important, practical considerations in measurement and analysis often guide how we treat variables.\n\n\n\n3.2.6 Conclusion\nThe distinction between discrete and continuous data is not always rigid in social sciences. Many variables, including those involving money, percentages, or dense measurements, can be viewed through both discrete and continuous lenses. The choice of treatment should be guided by the nature of the data, the goals of the analysis, and the potential implications of the choice. This flexibility, when used thoughtfully, provides powerful tools for social science researchers to gain insights from their data.\n\n\n\n\n\n\nDiscrete vs. Continuous Numerical Data: A Language-Based Analogies\n\n\n\n\n3.2.6.1 The Language Connection\nThink about how you naturally ask questions about quantities:\n\n“How many cookies are in the jar?” (counting)\n“How much water is in the glass?” (measuring)\n\nThis natural language distinction reflects the two fundamental types of numerical data:\n\n\n3.2.6.2 Discrete Data = “How Many?” Questions\n\nLike counting whole objects (countable nouns)\nTakes specific values with gaps between them\nExamples:\n\nNumber of pets: 0, 1, 2, 3… (can’t have 2.5 pets)\nDice rolls: 1, 2, 3, 4, 5, 6\nStudents in a class: 20, 21, 22…\n\n\n🤔 Self-Check: Can you find a value between 2 and 3 students? Why not?\n\n\n3.2.6.3 Continuous Data = “How Much?” Questions\n\nLike measuring quantities (uncountable nouns)\nCan take any value within a range\nExamples:\n\nHeight: 1.7231… meters\nTemperature: 36.8325… °C\nTime: 3.5792… hours\n\n\n🤔 Self-Check: Write down three different values between 1.72 and 1.73 meters\n\n\n3.2.6.4 Quick Recognition Guide\n\nIf you naturally ask “How many?” → Discrete\nIf you naturally ask “How much?” → Continuous\nIf you can measure it more precisely → Continuous\nIf you can only use whole numbers → Discrete\n\n✍️ Practice: Classify these quantities as discrete or continuous\n\nYour age in years: _____\nYour height: _____\nNumber of songs in a playlist: _____\nVolume of water: _____\n\n\n\n\n\n\n3.2.7 R Code Example\nHere’s a simple R code example to illustrate how we might analyze variables treated as continuous:\n\n# Generate some sample data\nset.seed(123)\nages &lt;- round(runif(1000, min = 18, max = 80))\nunemployment_rates &lt;- round(runif(1000, min = 3, max = 10), 1)\n\n# Compare means and medians\ncat(\"Mean age:\", mean(ages), \"\\n\")\n\nMean age: 48.848 \n\ncat(\"Median age:\", median(ages), \"\\n\")\n\nMedian age: 48 \n\ncat(\"Mean unemployment rate:\", mean(unemployment_rates), \"\\n\")\n\nMean unemployment rate: 6.4871 \n\ncat(\"Median unemployment rate:\", median(unemployment_rates), \"\\n\")\n\nMedian unemployment rate: 6.5 \n\n# Linear regression (treating both as continuous)\nincome &lt;- 20000 + 500 * ages + 1000 * unemployment_rates + rnorm(1000, 0, 5000)\nmodel &lt;- lm(income ~ ages + unemployment_rates)\nsummary(model)\n\n\nCall:\nlm(formula = income ~ ages + unemployment_rates)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15354  -3471     54   3485  16684 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        20116.194    717.231   28.05   &lt;2e-16 ***\nages                 503.385      8.983   56.04   &lt;2e-16 ***\nunemployment_rates   989.333     80.004   12.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5053 on 997 degrees of freedom\nMultiple R-squared:  0.7637,    Adjusted R-squared:  0.7632 \nF-statistic:  1611 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n# Plot regression plane\nlibrary(plotly)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nplot_ly(x = ages, y = unemployment_rates, z = income, type = \"scatter3d\", mode = \"markers\") %&gt;%\n  add_trace(x = ages, y = unemployment_rates, z = fitted(model), type = \"scatter3d\", mode = \"lines\")\n\n\n\n\n\nThe 3D plot illustrates how both variables can be treated as continuous in a regression model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#introduction-to-stevens-data-typology",
    "href": "chapter2.html#introduction-to-stevens-data-typology",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.3 Introduction to Stevens’ Data Typology",
    "text": "3.3 Introduction to Stevens’ Data Typology\nStanley S. Stevens, an American psychologist, introduced a classification system for scales of measurement in his 1946 paper “On the Theory of Scales of Measurement.” This system, known as Stevens’ data typology or levels of measurement, has become fundamental in understanding how different types of data should be analyzed and interpreted.\nStevens proposed four levels of measurement:\n\nNominal\nOrdinal\nInterval\nRatio\n\nEach level has specific properties and allows for different types of statistical operations and analyses.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n3.3.1 Nominal Scale\n\n3.3.1.1 Definition\nThe nominal scale is the most basic level of measurement. It uses labels or categories to classify data without any quantitative value or order.\n\n\n3.3.1.2 Properties\n\nCategories are mutually exclusive\nNo inherent order among categories\nNo meaningful arithmetic operations can be performed\n\n\n\n3.3.1.3 Examples\n\nNationality (Polish, English, …)\nBlood types (A, B, AB, O)\nEye color (Blue, Brown, Green, Hazel)\nBinary variables (“Success” versus “Failure”)\n\n\n\n\n3.3.2 Ordinal Scale\n\n3.3.2.1 Definition\nThe ordinal scale categorizes data into ordered categories, but the intervals between categories are not necessarily equal or meaningful.\n\n\n3.3.2.2 Properties\n\nCategories have a defined order\nDifferences between categories are not quantifiable\nArithmetic operations on the numbers are not meaningful\n\n\n\n3.3.2.3 Examples\n\nEducation levels (High School, Bachelor’s, Master’s, PhD)\nLikert scales (Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree)\nSocioeconomic status (Low, Medium, High)\n\n\n\n\n3.3.3 Interval Scale\n\n3.3.3.1 Definition\nThe interval scale has ordered categories with equal intervals between adjacent categories. However, it lacks a true zero point.\n\n\n3.3.3.2 Properties\n\nEqual intervals between adjacent categories\nNo true zero point (zero is arbitrary)\nRatios between values are not meaningful\n\n\n\n3.3.3.3 Examples\n\nTemperature in Celsius or Fahrenheit\nCalendar years\nIQ scores (traditionally treated as interval, though this is debated)\n\n\n\n\n3.3.4 Ratio Scale\n\n3.3.4.1 Definition\nThe ratio scale is the highest level of measurement. It has all the properties of the interval scale plus a true zero point, making ratios between values meaningful.\n\n\n3.3.4.2 Properties\n\nAll properties of interval scales\nTrue zero point\nRatios between values are meaningful\n\n\n\n3.3.4.3 Examples\n\nHeight\nWeight\nAge\nIncome\n\n\n\n\n\n\n\nUnderstanding Operations with Data Types\n\n\n\n\n3.3.5 Temperature Scale Proof (Why Differences Work)\nComplete Proof: 1. Start with basic conversion: \\(F = \\frac{9}{5}C + 32\\) 2. For any two temperatures \\(C_1\\) and \\(C_2\\):\n\n\\(F_1 = \\frac{9}{5}C_1 + 32\\)\n\\(F_2 = \\frac{9}{5}C_2 + 32\\)\n\n\nTemperature difference in Fahrenheit:\n\n\\(\\Delta F = F_2 - F_1\\)\n\\(= (\\frac{9}{5}C_2 + 32) - (\\frac{9}{5}C_1 + 32)\\)\n\\(= \\frac{9}{5}C_2 - \\frac{9}{5}C_1\\) (32s cancel)\n\\(= \\frac{9}{5}(C_2 - C_1)\\)\n\\(= \\frac{9}{5}\\Delta C\\)\n\n\nExample:\n\nIf \\(\\Delta C = 10°C\\)\nThen \\(\\Delta F = \\frac{9}{5}(10) = 18°F\\)\n\n\n\n3.3.6 Why Differences Work for Both Types\nInterval Data (like temperature):\n\nOnly the difference matters\nThe +32 cancels out\nEqual steps stay equal: \\(\\Delta F = \\frac{9}{5}\\Delta C\\)\n\nRatio Data (like length):\n\n20m - 10m = 10m\nIn feet: 65.6ft - 32.8ft = 32.8ft\nConversion: 1m = 3.28ft preserves both\n\nDifferences: 32.8ft = 10m × 3.28\nRatios: 20m/10m = 65.6ft/32.8ft = 2\n\n\n\n\n3.3.7 Why Multiplication Only Works for Ratio\nTry with Temperature (Fails):\n\nTake 10°C × 2\n\nIn Celsius: 20°C\nConvert to F: \\(\\frac{9}{5}(20) + 32 = 68°F\\)\n\nTake 10°C, convert to F, then × 2\n\n10°C → 50°F\n50°F × 2 = 100°F\n\nResult: 68°F ≠ 100°F\n\nMultiplication gives different answers!\n\n\nWith Length (Works):\n\nTake 10m × 2\n\nIn meters: 20m\nConvert to ft: 20 × 3.28 = 65.6ft\n\nTake 10m, convert to ft, then × 2\n\n10m → 32.8ft\n32.8ft × 2 = 65.6ft\n\nResult: 65.6ft = 65.6ft\n\nSame answer both ways!\n\n\n\n\n3.3.8 Mean vs. Variance\nMean (Works for Both):\n\nUses differences only\nExample: \\((x_1 - x_2)\\)\nTemperature differences work same in °C or °F\nLength differences work same in m or ft\n\nVariance (Only Ratio):\n\nUses squared differences: \\((x - \\mu)^2\\)\nTemperature: \\((20°C - 10°C)^2 = 100°C^2\\)\nConvert each: \\((68°F - 50°F)^2 = 324°F^2\\)\nNo consistent relationship between units when squared\n\n\n\n3.3.9 Key Points\n\nDifferences work when equal steps mean the same thing\nMultiplication works only with true zero and consistent ratios\nMean uses differences → works for both\nVariance uses multiplication → only for ratio\n\n\n\n\n\n\n\n\n\n\nMean vs. Variance: Data Type Requirements\n\n\n\n\n3.3.9.1 Temperature Example: Why Variance Fails with Interval Data\n\nCelsius Measurements:\n\nValues: 10°C, 20°C, 30°C\nMean = (10 + 20 + 30)/3 = 20°C\nVariance calculation:\n\n(10-20)² = 100\n(20-20)² = 0\n(30-20)² = 100\nVariance = (100 + 0 + 100)/3 = 66.67°C²\n\n\nSame Temperatures in Fahrenheit:\n\nConverting: (°F = °C × 9/5 + 32)\nValues: 50°F, 68°F, 86°F\nMean = (50 + 68 + 86)/3 = 68°F\nVariance calculation:\n\n(50-68)² = 324\n(68-68)² = 0\n(86-68)² = 324\nVariance = (324 + 0 + 324)/3 = 216°F²\n\n\nWhy This Invalidates Variance for Interval Data:\n\nSame temperatures yield different variances:\n\nCelsius variance = 66.67°C²\nFahrenheit variance = 216°F²\n\nThese are not equivalent measures\nNo consistent conversion factor exists\nThe squared differences depend on the scale chosen\n\n\n\n\n3.3.9.2 Contrast with Ratio Data (Length)\n\nMeters:\n\nValues: 1.5m, 1.6m, 1.7m\nMean = 1.6m\nVariance = 0.01m²\n\nConverting to Feet (1m = 3.28084ft):\n\nValues: 4.92ft, 5.25ft, 5.58ft\nMean = 5.25ft\nVariance = 0.108ft²\nNote: 0.108ft² = 0.01m² × (3.28084)²\n\nWhy This Works:\n\nRatio between variances = square of scale conversion\nPreserves relative relationships\nTrue zero exists (0m = 0ft)\nRatios remain meaningful regardless of unit\n\n\n\n\n3.3.9.3 Key Points\n\nMean remains valid for interval data because:\n\n20°C - 10°C = 10°C converts to 68°F - 50°F = 18°F\nProportional differences preserved\nLinear transformation maintains intervals\n\nVariance fails for interval data because:\n\nSquared differences vary by scale\nNo consistent conversion between units\nResults depend on arbitrary zero point\n\n\n\n\n\n\n\n\n3.3.10 Importance in Research and Analysis\nUnderstanding Stevens’ data typology is crucial for several reasons:\n\nChoosing appropriate statistical tests: The level of measurement determines which statistical analyses are appropriate for a given dataset.\nInterpreting results: The meaning of statistical results depends on the level of measurement of the variables involved.\nDesigning measurement instruments: When creating surveys or other measurement tools, researchers must consider the level of measurement they want to achieve.\nData transformation: Sometimes, data can be transformed from one level to another, but this must be done carefully to avoid misinterpretation.\n\n\n\n3.3.11 Controversies and Limitations\nWhile Stevens’ typology is widely used, it has faced some criticisms:\n\nRigidity: Some argue that the typology is too rigid and that many real-world measurements fall between these categories.\nTreatment of ordinal data: There’s ongoing debate about when it’s appropriate to treat ordinal data as interval for certain analyses.\nPsychological scaling: Some psychological constructs (like intelligence) are difficult to categorize definitively within this system.\n\n\n\n3.3.12 Conclusion\nStevens’ data typology provides a fundamental framework for understanding different types of data and their properties. By recognizing the level of measurement of their variables, researchers can make informed decisions about data collection, analysis, and interpretation. However, it’s important to remember that while this typology is a useful guide, real-world data often requires nuanced consideration and may not always fit neatly into these categories.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not “twice as acidic” as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#likert-scales-and-ordinal-variables-in-psychology",
    "href": "chapter2.html#likert-scales-and-ordinal-variables-in-psychology",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.4 Likert Scales and Ordinal Variables in Psychology",
    "text": "3.4 Likert Scales and Ordinal Variables in Psychology\nLikert scales are widely used in psychology and social sciences to measure attitudes, opinions, and perceptions. Named after psychologist Rensis Likert, these scales typically consist of a series of statements or questions that respondents rate on a scale, often from “Strongly Disagree” to “Strongly Agree.”\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n3.4.1 Why Likert Scales are Ordinal Variables\nLikert scales are considered ordinal variables for several reasons:\n\nOrder without equal intervals: While the responses have a clear order (e.g., “Strongly Disagree” &lt; “Disagree” &lt; “Neutral” &lt; “Agree” &lt; “Strongly Agree”), the intervals between these categories are not necessarily equal.\nSubjective interpretation: The difference between “Strongly Disagree” and “Disagree” may not be the same as the difference between “Agree” and “Strongly Agree” for all respondents.\nLack of true zero point: Likert scales typically don’t have a true zero point, which is a characteristic of interval or ratio scales.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#iq-and-other-psychological-variables-as-ordinal-measures",
    "href": "chapter2.html#iq-and-other-psychological-variables-as-ordinal-measures",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.5 IQ and Other Psychological Variables as Ordinal Measures",
    "text": "3.5 IQ and Other Psychological Variables as Ordinal Measures\nMany psychological measures, including IQ, are often treated as interval scales but are, in fact, ordinal. Here’s why:\n\nIQ Scores:\n\nWhile IQ scores are presented as numbers, the difference between an IQ of 100 and 110 may not represent the same cognitive difference as between 130 and 140.\nThe scale is normalized and adjusted over time, making it difficult to claim true interval properties.\n\nOther Psychological Measures:\n\nDepression scales (e.g., Beck Depression Inventory)\nAnxiety measures (e.g., State-Trait Anxiety Inventory)\nPersonality assessments (e.g., Big Five Inventory)\n\n\nThese measures often use summed Likert-type items or other scoring methods that don’t guarantee equal intervals between scores.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\nLet’s illustrate this with a simulated depression scale:\n\nset.seed(123)\ndepression_scores &lt;- sample(0:27, 100, replace = TRUE)  # Simulating BDI-II scores\n\n# Create categories\ndepression_categories &lt;- cut(depression_scores, \n                             breaks = c(-Inf, 13, 19, 28, Inf),\n                             labels = c(\"Minimal\", \"Mild\", \"Moderate\", \"Severe\"))\n\n# Show distribution\ntable(depression_categories)\n\ndepression_categories\n Minimal     Mild Moderate   Severe \n      46       20       34        0 \n\n# Plot distribution\nbarplot(table(depression_categories), \n        main = \"Distribution of Depression Severity\",\n        xlab = \"Severity Category\",\n        ylab = \"Frequency\")\n\n\n\n\n\n\n\n\nWhile we can order these categories, we can’t assume that the difference between “Minimal” and “Mild” is the same as between “Moderate” and “Severe” in terms of the underlying construct of depression.\n\n3.5.1 Implications for Analysis\nRecognizing these measures as ordinal has important implications for data analysis:\n\nAppropriate statistical tests: Use non-parametric tests (e.g., Mann-Whitney U, Kruskal-Wallis) instead of parametric ones.\nCorrelation analysis: Use Spearman’s rank correlation instead of Pearson’s correlation.\nCentral tendency: Report median and mode rather than mean.\nData visualization: Use methods appropriate for ordinal data, such as bar plots or stacked bar charts.\n\n\n\n3.5.2 Conclusion\nWhile Likert scales and many psychological measures are often treated as interval data for practical reasons, it’s crucial to remember their ordinal nature. This understanding should inform our choice of statistical analyses and interpretations of results in psychological research.\n\n\n\n\n\n\nExercise: Identifying Measurement Scales\n\n\n\nFor each of the following variables, determine the most appropriate scale of measurement (Nominal, Ordinal, Interval, or Ratio). Also evaluate whether the variable is discrete or continuous.\n\nGender: nominal level of measurement, and discrete;\nCustomer satisfaction: Poor, Fair, Good, Excellent\nHeight (questionnaire): “I am: very short, short, average, tall, very tall”\nHeight (inches)\nReaction time (milliseconds)\nPostal codes: e.g., 61548, 61761, 62461, 47424, 65233\nAge (years)\nNationality\nStreet addresses\nMilitary ranks\nLeft-Right political scale placement\nFamily size: 1 child, 2 children, 3 children, …\nIQ score\nShirt size (S, M, L, …)\nMovie ratings (1 star, 2 stars, 3 stars)\nTemperature (Celsius)\nTemperature (Kelvin)\nBlood types: A, B, AB, O\nIncome categories: low, medium, high\nVoter turnout\nPolitical party affiliation\nElectoral district magnitude\n\nRemember to justify your choices for each variable.\nFor instance: In Stevens’ typology of measurement scales, street addresses are nominal data. This is because:\nThey serve purely as labels/identifiers. They have no inherent ordering (123 Main St isn’t “more than” 23 Oak St). You can’t perform meaningful mathematical operations on them.The only valid operation is testing for equality/inequality (is this the same address or different?)\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓*\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\n3.5.3 Notes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html",
    "href": "rozdzial2.html",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "",
    "text": "4.1 Podstawy Zbiorów Liczbowych\nZanim zagłębimy się w typy danych, istotne jest zrozumienie podstawowych zbiorów liczbowych, które stanowią fundament naszego rozumienia danych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "href": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "",
    "text": "4.1.1 Podstawowe Zbiory Liczbowe\n\nLiczby Naturalne (ℕ): Liczby do liczenia {1, 2, 3, …}\nLiczby Całkowite (ℤ): Obejmują liczby naturalne, ich przeciwności i zero {…, -2, -1, 0, 1, 2, …}\nLiczby Wymierne (ℚ): Liczby, które można wyrazić jako ułamek dwóch liczb całkowitych\nLiczby Rzeczywiste (ℝ): Wszystkie liczby na osi liczbowej, w tym wymierne i niewymierne\n\n\n\n4.1.2 Właściwości Zbiorów\n\nZbiory Przeliczalne: Zbiory, których elementy można ustawić w relacji jeden do jednego z liczbami naturalnymi. Na przykład, zbiór liczb całkowitych jest przeliczalny.\nZbiory Nieprzeliczalne: Zbiory, które nie są przeliczalne. Zbiór liczb rzeczywistych jest nieprzeliczalny.\nZbiory Dyskretne: Zbiory, w których każdy element jest oddzielony od innych elementów skończoną przerwą. Liczby całkowite tworzą zbiór dyskretny.\nZbiory Gęste: Zbiory, w których między dowolnymi dwoma elementami zawsze znajduje się inny element zbioru. Liczby wymierne i rzeczywiste są zbiorami gęstymi.\n\n\n\n\n\n\n\nNote\n\n\n\nZrozumienie tych właściwości zbiorów jest kluczowe dla uchwycenia natury różnych typów danych w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#dane-dyskretne-vs.-ciągłe-zniuansowane-spojrzenie",
    "href": "rozdzial2.html#dane-dyskretne-vs.-ciągłe-zniuansowane-spojrzenie",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.2 Dane Dyskretne vs. Ciągłe: Zniuansowane Spojrzenie",
    "text": "4.2 Dane Dyskretne vs. Ciągłe: Zniuansowane Spojrzenie\nW nauce o danych i statystyce często kategoryzujemy zmienne jako dyskretne lub ciągłe. Jednak rozróżnienie to nie zawsze jest jednoznaczne, a niektóre zmienne wykazują cechy obu typów. Ta sekcja bada koncepcje danych dyskretnych i ciągłych, ich różnice oraz interesujące przypadki zmiennych, które można traktować jako oba typy lub które kwestionują nasze intuicyjne rozumienie.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n4.2.1 Dane Dyskretne\nDane dyskretne mogą przyjmować tylko określone, przeliczalne wartości. Te wartości często (ale nie zawsze) są liczbami całkowitymi.\n\n4.2.1.1 Cechy Danych Dyskretnych:\n\nPrzeliczalne\nCzęsto reprezentowane przez liczby całkowite\nMogą być skończone lub nieskończone\nBrak wartości między dwoma sąsiednimi punktami danych\n\n\n\n4.2.1.2 Przykłady:\n\nLiczba studentów w klasie\nLiczba samochodów sprzedanych przez dealera\nRozmiary butów\n\n\n\n\n4.2.2 Dane Ciągłe\nDane ciągłe mogą przyjmować dowolną wartość w danym zakresie, w tym wartości ułamkowe i dziesiętne. Ważne jest, aby zauważyć, że ciągłość nie jest określona wyłącznie przez nieprzeliczalność, ale również przez gęstość.\n\n4.2.2.1 Cechy Danych Ciągłych:\n\nMogą być nieprzeliczalne (jak liczby rzeczywiste) lub gęste (jak liczby wymierne)\nMogą być mierzone z dowolną precyzją (teoretycznie)\nReprezentowane przez liczby rzeczywiste lub gęste podzbiory liczb rzeczywistych\nZawsze istnieją wartości między dowolnymi dwoma punktami danych\n\n\n\n4.2.2.2 Przykłady:\n\nWzrost\nWaga\nTemperatura\nProcenty (wyjaśnione dalej poniżej)\n\n\n\n\n4.2.3 Spektrum Dyskretno-Ciągłe\nW praktyce niektóre zmienne, które matematycznie są dyskretne, często są traktowane tak, jakby były ciągłe. Ta dwoista natura zapewnia elastyczność w analizie i interpretacji tych zmiennych.\n\n4.2.3.1 Powody Traktowania Danych Dyskretnych jako Ciągłych:\n\nGęsta Granularność\n\nGdy zmienna dyskretna ma dużą liczbę możliwych wartości w danym zakresie, może przybliżać ciągłość.\nPrzykład: Dochód mierzony w pojedynczych groszach. Choć technicznie dyskretny, duża liczba możliwych wartości sprawia, że zachowuje się podobnie do zmiennej ciągłej.\n\nWygoda Analityczna\n\nMetody ciągłe często dają rozsądne i użyteczne wyniki nawet dla gęstych zmiennych dyskretnych.\nCzęsto łatwiej jest używać istniejących narzędzi statystycznych, jeśli założymy ciągłość, ponieważ pozwala to na stosowanie metod opartych na rachunku różniczkowym.\n\nPrzybliżenie Zjawisk Bazowych\n\nW niektórych przypadkach dyskretny pomiar może być przybliżeniem bazowego procesu ciągłego.\nPrzykład: Chociaż mierzymy czas w dyskretnych jednostkach (sekundy, minuty, godziny), sam czas jest ciągły.\n\n\n\n\n4.2.3.2 Przykłady Zmiennych o Dwoistej Naturze Dyskretno-Ciągłej:\n\nWiek\n\nDyskretny: Typowo mierzony w pełnych latach\nCiągły: Może być uznany za zmienną ciągłą w wielu analizach, szczególnie przy dużych populacjach\n\nCena i Dochód\n\nDyskretne: Ceny i dochody są w rzeczywistości mierzone w dyskretnych jednostkach (np. grosze lub najmniejsza jednostka waluty)\nCiągłe: W modelach ekonomicznych i wielu analizach ceny i dochody są traktowane jako zmienne ciągłe ze względu na ich gęstą naturę i wygodę analityczną\n\nWyniki Testów\n\nDyskretne: Często podawane jako liczby całkowite\nCiągłe: W analizach statystycznych wyniki testów mogą być traktowane jako ciągłe, szczególnie gdy zakres możliwych wyników jest duży\n\n\n\n\n\n4.2.4 Przypadek Szczególny: Procenty i Liczby Wymierne\nProcenty przedstawiają interesujący przypadek w spektrum dyskretno-ciągłym:\n\nNatura Wymierna: Procenty są zasadniczo ułamkami (m/100), co czyni je liczbami wymiernymi.\nGęste, ale Przeliczalne: Zbiór liczb wymiernych jest gęsty (między dowolnymi dwoma wymiernymi jest inny wymierny), ale także przeliczalny.\nPraktyczna Ciągłość: W większości praktycznych zastosowań procenty są traktowane jako ciągłe ze względu na ich gęstą naturę.\nSkończona Precyzja: W rzeczywistości procenty są często podawane z ograniczoną liczbą miejsc po przecinku, tworząc skończony zbiór możliwych wartości.\n\n\n\n\n\n\n\nProcenty: Łączenie Dyskretnego i Ciągłego\n\n\n\nZmienne mierzone w procentach, takie jak stopy bezrobocia czy frekwencja wyborcza, kwestionują nasze intuicyjne rozumienie dyskretności i ciągłości:\n\nSą liczbami wymiernymi (ułamki z mianownikiem 100), które technicznie są przeliczalne.\nTworzą zbiór gęsty w swoim zakresie (od 0% do 100%), pozwalając na wartości między dowolnymi dwoma procentami.\nW praktyce są często traktowane jako zmienne ciągłe ze względu na ich gęstą naturę i wygodę analityczną.\nPrecyzja pomiaru (np. podawanie do jednego lub dwóch miejsc po przecinku) może narzucić dyskretną strukturę na to, co koncepcyjnie jest zbiorem gęstym.\n\nTa dwoistość pozwala na elastyczne podejścia analityczne, w zależności od konkretnego kontekstu badawczego i wymaganej precyzji.\n\n\n\n\n4.2.5 Implikacje dla Analizy Danych\nZrozumienie zniuansowanej natury zmiennych jako dyskretnych, ciągłych lub gdzieś pomiędzy ma ważne implikacje dla analizy danych:\n\nElastyczność w Modelowaniu: Pozwala na wykorzystanie szerszego zakresu technik statystycznych.\nUproszczone Obliczenia: Traktowanie gęstych danych dyskretnych jako ciągłych może uprościć obliczenia i uczynić niektóre analizy bardziej wykonalnymi.\nLepsza Interpretowalność: W niektórych przypadkach traktowanie danych dyskretnych jako ciągłych może prowadzić do bardziej intuicyjnych lub użytecznych interpretacji wyników.\nPotencjał Błędu: Ważne jest, aby być świadomym, kiedy przybliżenia są odpowiednie, a kiedy mogą prowadzić do mylących wyników.\nRozważania Teoretyczne vs. Praktyczne: Choć matematyczna natura danych jest ważna, praktyczne względy w pomiarze i analizie często kierują tym, jak traktujemy zmienne.\n\n\n\n4.2.6 Wnioski\nRozróżnienie między danymi dyskretnymi a ciągłymi nie zawsze jest sztywne w naukach społecznych. Wiele zmiennych, w tym te dotyczące pieniędzy, procentów czy gęstych pomiarów, można oglądać przez pryzmat zarówno dyskretny, jak i ciągły. Wybór sposobu traktowania powinien być kierowany naturą danych, celami analizy i potencjalnymi implikacjami tego wyboru. Ta elastyczność, gdy jest używana rozważnie, zapewnia potężne narzędzia dla badaczy nauk społecznych do uzyskiwania wglądu w ich dane.\n\n\n\n\n\n\nDane Dyskretne vs. Ciągłe: Analogia Językowa\n\n\n\n\n4.2.6.1 Kluczowe Rozróżnienie Językowe\nW języku polskim mamy precyzyjne rozróżnienie:\n\n“Liczba” → używamy dla rzeczy policzalnych\n“Ilość” → używamy dla rzeczy niepoliczalnych\n\nTo rozróżnienie doskonale odzwierciedla dwa podstawowe typy danych liczbowych:\n\n\n4.2.6.2 Dane Dyskretne = “Liczba czegoś”\n\nUżywamy słowa “liczba” (tak jak mówimy “liczba studentów”)\nWartości są rozdzielone jak pojedyncze elementy\nPrzykłady:\n\nLiczba książek: 0, 1, 2, 3…\nLiczba punktów w teście: 0, 1, 2…\nLiczba mieszkańców: 100, 101, 102…\n\n\n🤔 Czy poprawne jest powiedzenie “ilość studentów” czy “liczba studentów”? (Poprawna forma pomoże Ci rozpoznać typ danych)\n\n\n4.2.6.3 Dane Ciągłe = “Ilość czegoś”\n\nUżywamy słowa “ilość” (tak jak mówimy “ilość wody”)\nWartości płynnie przechodzą jedna w drugą\nPrzykłady:\n\nIlość cieczy: 1,5231… litra\nIlość czasu: 2,3891… godziny\nIlość energii: 5,7123… kWh\n\n\n🤔 Czy mówimy “ilość wody” czy “liczba wody”? (Poprawna forma wskazuje na typ danych)\n\n\n4.2.6.4 Sposób Rozpoznawania\n\nCzy użyłbyś słowa “liczba”? → Dane dyskretne\nCzy użyłbyś słowa “ilość”? → Dane ciągłe\n\n✍️ Ćwiczenie: Uzupełnij poprawnym słowem i określ typ danych\n\n_____ uczniów w klasie (liczba/ilość): typ _____\n_____ deszczu (liczba/ilość): typ _____\n_____ piosenek (liczba/ilość): typ _____\n_____ temperatury (liczba/ilość): typ _____\n\n\n\n\n\n\n4.2.7 Przykład Kodu R\nOto prosty przykład kodu R ilustrujący, jak możemy analizować zmienne traktowane jako ciągłe:\n\n# Generowanie przykładowych danych\nset.seed(123)\nwiek &lt;- round(runif(1000, min = 18, max = 80))\nstopy_bezrobocia &lt;- round(runif(1000, min = 3, max = 10), 1)\n\n# Traktowanie wieku jako dyskretnego\ntabela_wieku &lt;- table(wiek)\nbarplot(tabela_wieku, main = \"Rozkład Wieku (Dyskretny)\", xlab = \"Wiek\", ylab = \"Częstotliwość\")\n\n\n\n\n\n\n\n# Traktowanie wieku jako ciągłego\nhist(wiek, main = \"Rozkład Wieku (Ciągły)\", xlab = \"Wiek\", ylab = \"Częstotliwość\")\n\n\n\n\n\n\n\n# Traktowanie stopy bezrobocia jako dyskretnej\ntabela_stopy &lt;- table(stopy_bezrobocia)\nbarplot(tabela_stopy, main = \"Stopy Bezrobocia (Dyskretne)\", xlab = \"Stopa (%)\", ylab = \"Częstotliwość\")\n\n\n\n\n\n\n\n# Traktowanie stopy bezrobocia jako ciągłej\nhist(stopy_bezrobocia, breaks = 30, main = \"Stopy Bezrobocia (Ciągłe)\", xlab = \"Stopa (%)\", ylab = \"Częstotliwość\")\n\n# Porównanie średnich i median\ncat(\"Średni wiek:\", mean(wiek), \"\\n\")\n\nŚredni wiek: 48.848 \n\ncat(\"Mediana wieku:\", median(wiek), \"\\n\")\n\nMediana wieku: 48 \n\ncat(\"Średnia stopa bezrobocia:\", mean(stopy_bezrobocia), \"\\n\")\n\nŚrednia stopa bezrobocia: 6.4871 \n\ncat(\"Mediana stopy bezrobocia:\", median(stopy_bezrobocia), \"\\n\")\n\nMediana stopy bezrobocia: 6.5 \n\n# Regresja liniowa (traktowanie obu jako ciągłych)\ndochod &lt;- 20000 + 500 * wiek + 1000 * stopy_bezrobocia + rnorm(1000, 0, 5000)\nmodel &lt;- lm(dochod ~ wiek + stopy_bezrobocia)\nsummary(model)\n\n\nCall:\nlm(formula = dochod ~ wiek + stopy_bezrobocia)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15354  -3471     54   3485  16684 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      20116.194    717.231   28.05   &lt;2e-16 ***\nwiek               503.385      8.983   56.04   &lt;2e-16 ***\nstopy_bezrobocia   989.333     80.004   12.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5053 on 997 degrees of freedom\nMultiple R-squared:  0.7637,    Adjusted R-squared:  0.7632 \nF-statistic:  1611 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n# Wykres płaszczyzny regresji\nlibrary(plotly)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n\n\n\n\n\n\nplot_ly(x = wiek, y = stopy_bezrobocia, z = dochod, type = \"scatter3d\", mode = \"markers\") %&gt;%\n  add_trace(x = wiek, y = stopy_bezrobocia, z = fitted(model), type = \"scatter3d\", mode = \"lines\")\n\n\n\n\n\nWykres 3D ilustruje, jak obie zmienne mogą być traktowane jako ciągłe w modelu regresji.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "href": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.3 Wprowadzenie do Typologii Danych Stevensa",
    "text": "4.3 Wprowadzenie do Typologii Danych Stevensa\nStanley S. Stevens, amerykański psycholog, wprowadził system klasyfikacji skal pomiarowych w swoim artykule z 1946 roku “On the Theory of Scales of Measurement”. Ten system, znany jako typologia danych Stevensa lub poziomy pomiaru, stał się fundamentalny dla zrozumienia, jak różne typy danych powinny być analizowane i interpretowane.\nStevens zaproponował cztery poziomy pomiaru:\n\nNominalny\nPorządkowy\nInterwałowy\nIlorazowy\n\nKażdy poziom ma specyficzne właściwości i pozwala na różne rodzaje operacji statystycznych i analiz.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n4.3.1 Skala Nominalna\n\n4.3.1.1 Definicja\nSkala nominalna jest najbardziej podstawowym poziomem pomiaru. Używa etykiet lub kategorii do klasyfikacji danych bez żadnej wartości ilościowej ani porządku.\n\n\n4.3.1.2 Właściwości\n\nKategorie są wzajemnie wykluczające się\nBrak inherentnego porządku między kategoriami\nNie można wykonywać znaczących operacji arytmetycznych\n\n\n\n4.3.1.3 Przykłady\n\nNarodowość (Polak, Niemiec, …)\nGrupy krwi (A, B, AB, O)\nKolor oczu (Niebieskie, Brązowe, Zielone, Piwne)\nZmienne binarne (“Sukces” versus “Niepowodzenie”)\n\n\n\n\n4.3.2 Skala Porządkowa\n\n4.3.2.1 Definicja\nSkala porządkowa kategoryzuje dane w uporządkowane kategorie, ale odstępy między kategoriami niekoniecznie są równe lub znaczące.\n\n\n4.3.2.2 Właściwości\n\nKategorie mają zdefiniowany porządek\nRóżnice między kategoriami nie są kwantyfikowalne\nOperacje arytmetyczne na liczbach nie są znaczące\n\n\n\n4.3.2.3 Przykłady\n\nPoziomy wykształcenia (Szkoła Średnia, Licencjat, Magister, Doktorat)\nSkale Likerta (Zdecydowanie się nie zgadzam, Nie zgadzam się, Neutralnie, Zgadzam się, Zdecydowanie się zgadzam)\nStatus społeczno-ekonomiczny (Niski, Średni, Wysoki)\n\n\n\n\n4.3.3 Skala Interwałowa\n\n4.3.3.1 Definicja\nSkala interwałowa ma uporządkowane kategorie z równymi odstępami między sąsiednimi kategoriami. Jednak brakuje jej prawdziwego punktu zerowego.\n\n\n4.3.3.2 Właściwości\n\nRówne odstępy między sąsiednimi kategoriami\nBrak prawdziwego punktu zerowego (zero jest umowne)\nStosunki między wartościami nie są znaczące\n\n\n\n4.3.3.3 Przykłady\n\nTemperatura w stopniach Celsjusza lub Fahrenheita\nLata kalendarzowe\nWyniki IQ (tradycyjnie traktowane jako interwałowe, chociaż jest to dyskusyjne)\n\n\n\n\n4.3.4 Skala Ilorazowa\n\n4.3.4.1 Definicja\nSkala ilorazowa jest najwyższym poziomem pomiaru. Ma wszystkie właściwości skali interwałowej plus prawdziwy punkt zerowy, co sprawia, że stosunki między wartościami są znaczące.\n\n\n4.3.4.2 Właściwości\n\nWszystkie właściwości skal interwałowych\nPrawdziwy punkt zerowy\nStosunki między wartościami są znaczące\n\n\n\n4.3.4.3 Przykłady\n\nWzrost\nWaga\nWiek\nDochód\n\n\n\n\n\n\n\nDlaczego Niektóre Statystyki Działają (a Inne Nie) dla Skal Interwałowych\n\n\n\n\n4.3.4.4 Kluczowa Idea\nW przypadku skal interwałowych (np. temperatury):\n\nDozwolone jest dodawanie/odejmowanie wartości oraz dzielenie przez liczby\nNiedozwolone jest mnożenie/dzielenie wartości przez siebie\n\n\n\n4.3.4.5 Własności Skali Interwałowej\n\nRówne interwały reprezentują takie same różnice:\n\nRóżnica między 20°C a 25°C (5°C) reprezentuje taką samą zmianę jak między 30°C a 35°C\nProporcje różnic są zachowane: 10°C to dwa razy większa zmiana niż 5°C\n\nPunkt zero jest umowny:\n\n0°C to punkt zamarzania wody, a nie brak temperatury\nTen sam stan fizyczny ma różne wartości: 0°C = 32°F\n\nTransformacja liniowa:\n\nWzór ogólny: \\(y = ax + b\\), gdzie \\(a \\neq 0\\)\nDla temperatury: \\(F = C \\times \\frac{9}{5} + 32\\)\n\n\n\n\n4.3.4.6 Dlaczego Średnia Arytmetyczna Działa\nPrzykład analizy dla dwóch temperatur:\nDane: 20°C i 30°C\n\nMetoda 1: Średnia w Celsjuszach, potem konwersja\n1. Średnia: (20°C + 30°C) ÷ 2 = 25°C\n2. Konwersja: 25°C × (9/5) + 32 = 77°F\n\nMetoda 2: Konwersja na °F, potem średnia\n1. Konwersja: 20°C → 68°F, 30°C → 86°F\n2. Średnia: (68°F + 86°F) ÷ 2 = 77°F\n\nObie metody dają ten sam wynik! ✓\nMatematyczny dowód poprawności:\n\\(\\begin{align*}\n\\bar{F} &= \\frac{F_1 + F_2}{2} = \\frac{(C_1 \\times \\frac{9}{5} + 32) + (C_2 \\times \\frac{9}{5} + 32)}{2} \\\\\n&= \\frac{(C_1 + C_2) \\times \\frac{9}{5} + 64}{2} \\\\\n&= (\\frac{C_1 + C_2}{2}) \\times \\frac{9}{5} + 32 \\\\\n&= \\bar{C} \\times \\frac{9}{5} + 32\n\\end{align*}\\)\nTo działa ponieważ:\n\nUżywamy tylko dozwolonych operacji\nZachowana jest liniowość transformacji\nUmowny punkt zero nie wpływa na wynik\n\n\n\n4.3.4.7 Dlaczego Wariancja Jest Problematyczna\nAnaliza na tym samym przykładzie:\nTe same temperatury: 20°C i 30°C\n\nMetoda 1: Wariancja w Celsjuszach\n1. Średnia: 25°C\n2. Odchylenia: (20 - 25)°C = -5°C, (30 - 25)°C = 5°C\n3. Kwadraty odchyleń: (-5°C)² = 25(°C)², (5°C)² = 25(°C)²\n4. Średnia: (25 + 25)(°C)² ÷ 2 = 25(°C)²\n\nMetoda 2: Wariancja w Fahrenheitach\n1. Konwersja: 20°C → 68°F, 30°C → 86°F\n2. Średnia: 77°F\n3. Odchylenia: (68 - 77)°F = -9°F, (86 - 77)°F = 9°F\n4. Kwadraty odchyleń: (-9°F)² = 81(°F)², (9°F)² = 81(°F)²\n5. Średnia: (81 + 81)(°F)² ÷ 2 = 81(°F)²\n\nProblem: 25(°C)² i 81(°F)² nie są porównywalne!\nMatematyczna analiza problemu:\nDla odchylenia w skali Fahrenheita: \\(\\begin{align*}\n(F_i - \\bar{F})^2 &= [(C_i \\times \\frac{9}{5} + 32) - (\\bar{C} \\times \\frac{9}{5} + 32)]^2 \\\\\n&= [(C_i - \\bar{C}) \\times \\frac{9}{5}]^2 \\\\\n&= (C_i - \\bar{C})^2 \\times (\\frac{9}{5})^2\n\\end{align*}\\)\nTo nie działa dobrze ponieważ:\n\nMnożymy temperatury przez siebie (niedozwolone!)\nPowstają jednostki kwadratowe (°C² lub °F²) bez interpretacji fizycznej\nWyniki w różnych skalach nie są porównywalne\nKwadrat współczynnika skalowania (\\((\\frac{9}{5})^2\\)) nie ma interpretacji fizycznej\n\n\n\n4.3.4.8 Wnioski Teoretyczne\n\nOperacje dozwolone:\n\nDodawanie/odejmowanie (zachowuje różnice)\nMnożenie przez stałe (skalowanie)\nŚrednie arytmetyczne\nPorównywanie różnic temperatur\n\nOperacje niedozwolone:\n\nMnożenie temperatur\nDzielenie temperatur\nŚrednie geometryczne\nWspółczynnik zmienności\n\nImplikacje praktyczne:\n\nDla wariancji i odchylenia standardowego potrzebna ostrożna interpretacja\nLepiej używać miar opartych na różnicach (np. MAD)\nPrzy porównywaniu zmienności warto standaryzować dane\n\n\n\n\n4.3.4.9 Zasada Praktyczna\nJeśli w obliczeniach pojawia się mnożenie wartości ze skali interwałowej przez siebie, należy zachować ostrożność w interpretacji wyników!\n\n\n\n\n\n\n\n\n\nProporcje w Skalach Pomiarowych: Przypadek Temperatury\n\n\n\n\n4.3.4.10 Dwa Rodzaje Proporcji\n\n4.3.4.10.1 A) Proporcje wartości (NIE zachowują się w skali interwałowej):\nWeźmy 80°C i 20°C:\nW Celsjuszach: 80°C/20°C = 4\nW Fahrenheitach: 176°F/68°F ≈ 2.59\nW Kelwinach: 353.15K/293.15K ≈ 1.20\n\nTe same temperatury dają różne proporcje! \n→ Proporcje wartości NIE mają sensu na skalach interwałowych; sens mają tylko na skali ilorazowej\n\n\n4.3.4.10.2 B) Proporcje różnic (zachowują się w skali interwałowej):\nWeźmy dwie pary różnic:\nPara 1: 30°C - 20°C = 10°C\nPara 2: 80°C - 60°C = 20°C\n\nProporcja różnic w Celsjuszach:\n20°C/10°C = 2\n\nTe same temperatury w Fahrenheitach:\nPara 1: 86°F - 68°F = 18°F\nPara 2: 176°F - 140°F = 36°F\n\nProporcja różnic w Fahrenheitach:\n36°F/18°F = 2\n\nProporcja różnic jest taka sama! ✓\n\n\n\n4.3.4.11 Matematyczne Wyjaśnienie\nDla transformacji \\(F = \\frac{9}{5}C + 32\\):\n\nProporcje wartości NIE zachowują się: \\(\\frac{F_1}{F_2} = \\frac{\\frac{9}{5}C_1 + 32}{\\frac{9}{5}C_2 + 32} \\neq \\frac{C_1}{C_2}\\)\nProporcje różnic zachowują się: \\(\\frac{F_1 - F_2}{F_3 - F_4} = \\frac{(\\frac{9}{5}C_1 + 32) - (\\frac{9}{5}C_2 + 32)}{(\\frac{9}{5}C_3 + 32) - (\\frac{9}{5}C_4 + 32)} = \\frac{\\frac{9}{5}(C_1 - C_2)}{\\frac{9}{5}(C_3 - C_4)} = \\frac{C_1 - C_2}{C_3 - C_4}\\)\n\n\n\n4.3.4.12 Dlaczego To Jest Ważne?\n\nDla wartości:\n\nW skali Celsjusza: 40°C nie jest “dwa razy cieplejsze” niż 20°C\nW skali Fahrenheita: 100°F nie jest “dwa razy cieplejsze” niż 50°F\nTylko w Kelwinach proporcje wartości mają sens fizyczny\n\nDla różnic:\n\nWzrost o 20°C jest zawsze dwa razy większy niż wzrost o 10°C\nWzrost o 36°F jest zawsze dwa razy większy niż wzrost o 18°F\nProporcje różnic są niezależne od skali\n\n\n\n\n4.3.4.13 Implikacje dla Statystyk\n\nOperacje bazujące na różnicach (DZIAŁAJĄ):\n\nŚrednia arytmetyczna\nOdchylenie bezwzględne\nRozstęp\n\nOperacje bazujące na proporcjach wartości (NIE DZIAŁAJĄ):\n\nŚrednia geometryczna\nWspółczynnik zmienności\nWariancja (bo używa kwadratu wartości)\n\n\n\n\n\n\n\n\n4.3.5 Znaczenie w Badaniach i Analizie\nZrozumienie typologii danych Stevensa jest kluczowe z kilku powodów:\n\nWybór odpowiednich testów statystycznych: Poziom pomiaru determinuje, które analizy statystyczne są odpowiednie dla danego zbioru danych.\nInterpretacja wyników: Znaczenie wyników statystycznych zależy od poziomu pomiaru zaangażowanych zmiennych.\nProjektowanie narzędzi pomiarowych: Przy tworzeniu ankiet lub innych narzędzi pomiarowych badacze muszą wziąć pod uwagę poziom pomiaru, który chcą osiągnąć.\nTransformacja danych: Czasami dane mogą być przekształcane z jednego poziomu na drugi, ale musi to być robione ostrożnie, aby uniknąć błędnej interpretacji.\n\n\n\n4.3.6 Kontrowersje i Ograniczenia\nChociaż typologia Stevensa jest szeroko stosowana, spotkała się z pewnymi krytykami:\n\nSztywność: Niektórzy twierdzą, że typologia jest zbyt sztywna i że wiele rzeczywistych pomiarów mieści się pomiędzy tymi kategoriami.\nTraktowanie danych porządkowych: Trwa debata na temat tego, kiedy właściwe jest traktowanie danych porządkowych jako interwałowych dla pewnych analiz.\nSkalowanie psychologiczne: Niektóre konstrukty psychologiczne (jak inteligencja) są trudne do jednoznacznego skategoryzowania w ramach tego systemu.\n\n\n\n4.3.7 Podsumowanie\nTypologia danych Stevensa dostarcza fundamentalnych ram dla zrozumienia różnych rodzajów danych i ich właściwości. Rozpoznając poziom pomiaru swoich zmiennych, badacze mogą podejmować świadome decyzje dotyczące gromadzenia danych, analizy i interpretacji. Jednak ważne jest, aby pamiętać, że chociaż ta typologia jest użytecznym przewodnikiem, rzeczywiste dane często wymagają niuansowego podejścia i nie zawsze pasują idealnie do tych kategorii.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not “twice as acidic” as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#skale-likerta-i-ich-charakter-porządkowy",
    "href": "rozdzial2.html#skale-likerta-i-ich-charakter-porządkowy",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.4 Skale Likerta i Ich Charakter Porządkowy",
    "text": "4.4 Skale Likerta i Ich Charakter Porządkowy\n\n4.4.1 Wprowadzenie do Skal Likerta\nSkale Likerta są szeroko stosowane w psychologii i naukach społecznych do pomiaru postaw, opinii i percepcji. Nazwane na cześć psychologa Rensisa Likerta, skale te zazwyczaj składają się z serii stwierdzeń lub pytań, które respondenci oceniają na skali, często od “Zdecydowanie się nie zgadzam” do “Zdecydowanie się zgadzam”.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n4.4.2 Dlaczego Skale Likerta są Zmiennymi Porządkowymi\nSkale Likerta są uważane za zmienne porządkowe z kilku powodów:\n\nPorządek bez równych odstępów: Chociaż odpowiedzi mają wyraźną kolejność (np. “Zdecydowanie się nie zgadzam” &lt; “Nie zgadzam się” &lt; “Neutralnie” &lt; “Zgadzam się” &lt; “Zdecydowanie się zgadzam”), odstępy między tymi kategoriami niekoniecznie są równe.\nSubiektywna interpretacja: Różnica między “Zdecydowanie się nie zgadzam” a “Nie zgadzam się” może nie być taka sama jak różnica między “Zgadzam się” a “Zdecydowanie się zgadzam” dla wszystkich respondentów.\nBrak prawdziwego punktu zerowego: Skale Likerta zazwyczaj nie mają prawdziwego punktu zerowego, co jest cechą charakterystyczną skal interwałowych lub ilorazowych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#iq-i-inne-zmienne-psychologiczne-jako-miary-porządkowe",
    "href": "rozdzial2.html#iq-i-inne-zmienne-psychologiczne-jako-miary-porządkowe",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.5 IQ i Inne Zmienne Psychologiczne jako Miary Porządkowe",
    "text": "4.5 IQ i Inne Zmienne Psychologiczne jako Miary Porządkowe\n\n4.5.1 Przykłady Zmiennych Psychologicznych\nWiele miar psychologicznych, w tym IQ, jest często traktowanych jako skale interwałowe, ale w rzeczywistości są to skale porządkowe. Oto dlaczego:\n\nWyniki IQ:\n\nChociaż wyniki IQ są przedstawiane jako liczby, różnica między IQ 100 a 110 może nie reprezentować takiej samej różnicy poznawczej jak między 130 a 140.\nSkala jest normalizowana i dostosowywana w czasie, co utrudnia stwierdzenie, że ma właściwości prawdziwie interwałowe.\n\nInne Miary Psychologiczne:\n\nSkale depresji (np. Inwentarz Depresji Becka)\nMiary lęku (np. Inwentarz Stanu i Cechy Lęku)\nOceny osobowości (np. Inwentarz Wielkiej Piątki)\n\n\nTe miary często wykorzystują sumowane pozycje typu Likerta lub inne metody punktacji, które nie gwarantują równych odstępów między wynikami.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n4.5.2 Przykład: Symulowana Skala Depresji\nZilustrujmy to na symulowanej skali depresji:\n\nset.seed(123)\nwyniki_depresji &lt;- sample(0:27, 100, replace = TRUE)  # Symulacja wyników BDI-II\n\n# Tworzenie kategorii\nkategorie_depresji &lt;- cut(wyniki_depresji, \n                          breaks = c(-Inf, 13, 19, 28, Inf),\n                          labels = c(\"Minimalna\", \"Łagodna\", \"Umiarkowana\", \"Ciężka\"))\n\n# Pokazanie rozkładu\ntable(kategorie_depresji)\n\nkategorie_depresji\n  Minimalna     Łagodna Umiarkowana      Ciężka \n         46          20          34           0 \n\n# Wykres rozkładu\nbarplot(table(kategorie_depresji), \n        main = \"Rozkład Nasilenia Depresji\",\n        xlab = \"Kategoria Nasilenia\",\n        ylab = \"Częstość\")\n\n\n\n\n\n\n\n\nChociaż możemy uporządkować te kategorie, nie możemy założyć, że różnica między “Minimalna” a “Łagodna” jest taka sama jak między “Umiarkowana” a “Ciężka” w odniesieniu do bazowego konstruktu depresji.\n\n\n4.5.3 Implikacje dla Analizy\nUznanie tych miar za porządkowe ma ważne implikacje dla analizy danych:\n\nOdpowiednie testy statystyczne: Używaj testów nieparametrycznych (np. test U Manna-Whitneya, test Kruskala-Wallisa) zamiast parametrycznych.\nAnaliza korelacji: Używaj korelacji rangowej Spearmana zamiast korelacji Pearsona.\nTendencja centralna: Raportuj medianę i dominantę zamiast średniej.\nWizualizacja danych: Stosuj metody odpowiednie dla danych porządkowych, takie jak wykresy słupkowe lub skumulowane wykresy słupkowe.\n\n\n\n4.5.4 Podsumowanie\nChociaż skale Likerta i wiele miar psychologicznych jest często traktowanych jako dane interwałowe ze względów praktycznych, ważne jest, aby pamiętać o ich porządkowym charakterze. To zrozumienie powinno wpływać na nasz wybór analiz statystycznych i interpretację wyników w badaniach psychologicznych.\n\n\n\n\n\n\nĆwiczenie: Identyfikacja Skal Pomiarowych\n\n\n\nDla każdej z poniższych zmiennych określ najbardziej odpowiednią skalę pomiaru (Nominalna, Porządkowa, Przedziałowa lub Stosunkowa). Czy zmienna jest dyskretna, czy ciągła?\n\nPłeć: skala nominalna; zmienna dyskretna;\nSatysfakcja klienta: Niska, Średnia, Dobra, Doskonała\nWzrost (ankieta): “Jestem: bardzo niski, niski, przeciętnego wzrostu, wysoki, bardzo wysoki”\nWzrost mierzony w centymetrach\nCzas reakcji (w milisekundach)\nKody pocztowe: np. 00-001, 00-950, 80-452, 31-072\nWiek (w latach)\nMarki samochodów\nNarodowość\nLiczba dzieci w rodzinie: 1 dziecko, 2 dzieci, 3 dzieci, …\nWynik testu IQ\nTemperatura (skala Celsjusza)\nTemperatura (skala Kelvina)\nFrekwencja wyborcza\nPrzynależność partyjna\nWielkość okręgu wyborczego\nWspółrzędne w układzie kartezjańskim\nData (względem określonej epoki, np. n.e.)\nWysokość nad poziomem morza\nGrupy krwi: A, B, AB, 0\nKategorie dochodów: niskie, średnie, wysokie\nStopnie wojskowe\n\nPamiętaj, aby uzasadnić swój wybór skali dla każdej zmiennej.\nDla przykładu: W typologii skal pomiarowych Stevensa, adresy uliczne są danymi nominalnymi. Dlaczego?\nPełnią wyłącznie funkcję etykiet/identyfikatorów Nie mają naturalnego uporządkowania (ul. Mickiewicza 5 nie jest “większa” niż ul. Słowackiego 10) Nie można wykonywać na nich sensownych operacji matematycznych Jedyna dozwolona operacja to sprawdzanie równości/nierówności (czy to ten sam adres czy inny?)\nMimo że numery domów są liczbami, w systemie adresowym funkcjonują jako etykiety, a nie wartości ilościowe. Liczba 100 w adresie “ul. Kilińskiego 100” nie jest używana matematycznie - równie dobrze mogłaby to być “ul. Jabłkowa” czy “ul. Zeusa”, jeśli chodzi o jej funkcję w adresie.\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓*\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\n4.5.5 Notes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "",
    "text": "5.1 Introduction to Randomness\nRandomness is a cornerstone concept in statistics and scientific research. It refers to the unpredictability of individual outcomes, even when the overall pattern may be predictable. In the social sciences, understanding randomness is crucial for designing studies, collecting data, and interpreting results.\nConsider flipping a fair coin. While we know that the probability of getting heads is 50%, we can’t predict with certainty the outcome of any single flip. This unpredictability is the essence of randomness.\nExamples of random phenomena in social sciences include:\nUnderstanding randomness helps researchers distinguish between genuine effects and chance occurrences. For instance, if we observe a slight difference in test scores between two groups, randomness helps us determine whether this difference is likely due to a real effect or just chance variation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#introduction-to-randomness",
    "href": "chapter3.html#introduction-to-randomness",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "",
    "text": "Participant Selection: In a psychology experiment studying reaction times, the order in which participants arrive at the lab may be random.\nEconomic Behavior: The daily fluctuations in stock prices often exhibit random patterns, influenced by countless unpredictable factors.\nSocial Interactions: The occurrence of chance encounters between individuals in a community can be considered random events.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-bridging-sample-and-population",
    "href": "chapter3.html#sampling-bridging-sample-and-population",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.2 Sampling: Bridging Sample and Population",
    "text": "5.2 Sampling: Bridging Sample and Population\nSampling is the process of selecting a subset (sample) from a larger group (population) to make inferences about the population. It’s a critical skill in social science research, as studying entire populations is often impractical, too expensive, or sometimes impossible.\nKey Terms:\n\nPopulation: The entire group about which we want to draw conclusions.\nSample: A subset of the population that we actually study.\nSampling Frame: The list or procedure used to identify all members of the population.\n\nExample: Suppose we want to study the job satisfaction of all teachers in the United States (the population). Instead of surveying millions of teachers, we might select a sample of 5,000 teachers from various states, school districts, and grade levels.\nRandomness in sampling helps ensure that the sample is representative of the population, reducing bias and allowing for more accurate inferences. This is why probability sampling methods, which we’ll discuss next, are often preferred in scientific research.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-methods",
    "href": "chapter3.html#sampling-methods",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.3 Sampling Methods",
    "text": "5.3 Sampling Methods\n\n5.3.1 Probability Sampling\nProbability sampling methods involve random selection, giving each member of the population a known, non-zero chance of being selected.\n\nSimple Random Sampling: Each member of the population has an equal chance of being selected.\nExample: To select 100 students from a university with 10,000 students, you could assign each student a number from 1 to 10,000, then use a random number generator to select 100 numbers.\nStratified Random Sampling: The population is divided into subgroups (strata) based on shared characteristics, then samples are randomly selected from each stratum.\nExample: In a national political survey, you might divide the population into strata based on geographic regions (Northeast, Midwest, South, West) and then randomly sample from each region. This ensures representation from all areas of the country.\nCluster Sampling: The population is divided into clusters (usually geographic), some clusters are randomly selected, and all members within those clusters are studied.\nExample: To study high school students’ study habits, you might randomly select 20 high schools from across the country and then survey all students in those schools.\nSystematic Sampling: Selecting every kth item from a list after a random start.\nExample: At a busy shopping mall, you might survey every 20th person who enters the mall, starting with a randomly chosen number between 1 and 20.\n\n\n\n5.3.2 Non-probability Sampling\nNon-probability sampling doesn’t involve random selection. While it can introduce bias, it may be necessary in certain situations, especially when dealing with hard-to-reach populations or when resources are limited.\n\nConvenience Sampling: Selecting easily accessible subjects.\nExample: A researcher studying college students’ sleep patterns might survey students in their own classes or around campus.\nPurposive Sampling: Selecting subjects based on specific characteristics.\nExample: For a study on the experiences of CEOs in the tech industry, a researcher might intentionally seek out and interview CEOs from various tech companies.\nSnowball Sampling: Participants recruit other participants.\nExample: In a study of undocumented immigrants’ access to healthcare, researchers might ask initial participants to refer other potential participants from their community.\nQuota Sampling: Selecting participants to meet specific quotas for certain characteristics.\nExample: In a market research study, researchers might ensure they interview a specific number of people from different age groups, genders, and income levels to match the demographics of the target market.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#making-inferences-from-samples",
    "href": "chapter3.html#making-inferences-from-samples",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.4 Making Inferences from Samples",
    "text": "5.4 Making Inferences from Samples\nStatistical inference is the process of drawing conclusions about a population based on a sample. This allows researchers to estimate characteristics of the entire population (parameters) using characteristics of the sample (statistics).\n\n\n\n\n\n\nNote\n\n\n\nThe Soup Analogy: A Taste of Statistics\n\n\nWhen you taste a spoonful of soup and decide it isn’t salty enough, that’s exploratory/descriptive analysis.\nIf you generalize and conclude that your entire pot of soup needs salt, that’s an inference.\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population).\nIf the soup is not well stirred (heterogeneous population), it doesn’t matter how large a spoon you have (sample size), it will still not accurately represent the whole. If the soup is well stirred (homogeneous population), even a small spoon will suffice to test the soup.\n\n\n\n\n\n\n\n\ngraph TD\n    DGP[Data Generating Process] --&gt;|Generates| A[Population]\n    A --&gt;|Random Selection| B[Sample]\n    B --&gt;|Statistical Inference| C[Estimates & Conclusions]\n    C --&gt;|Generalize back to| A\n    C -.-&gt;|Attempt to understand| DGP\n\n    style DGP fill:#1E90FF,stroke:#000,stroke-width:4px,color:#FFF\n    style A fill:#DC143C,stroke:#000,stroke-width:4px,color:#FFF\n    style B fill:#228B22,stroke:#000,stroke-width:2px,color:#FFF\n    style C fill:#8B4513,stroke:#000,stroke-width:2px,color:#FFF\n    \n    classDef note fill:#F0F0F0,stroke:#000,stroke-width:1px;\n    D[[\"DGP:\n    Underlying process\n    that generates data\"]]\n    E[[\"Population:\n    Entire group of interest\"]]\n    F[[\"Sample:\n    Subset of population\"]]\n    G[[\"Inference:\n    Drawing conclusions\n    about population\n    and DGP\"]]\n    \n    class D,E,F,G note\n    \n    D --&gt; DGP\n    E --&gt; A\n    F --&gt; B\n    G --&gt; C\n\n\n\n\n\n\nKey Concepts:\n\nPoint Estimates: A single value used to estimate a population parameter.\nExample: The mean income of a sample of 1000 workers might be used to estimate the mean income of all workers in a country.\nConfidence Intervals: A range of values likely to contain the true population parameter.\nExample: We might say, “We are 95% confident that the true population mean income falls between $45,000 and $55,000.”\nMargin of Error: The range of values above and below the sample statistic in a confidence interval.\nExample: In political polling, you might see a statement like “Candidate A is preferred by 52% of voters, with a margin of error of ±3%.”\nHypothesis Testing: A method for making decisions about population parameters based on sample data.\nExample: A researcher might test whether there’s a significant difference in test scores between students who study with music and those who study in silence.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-and-non-sampling-errors",
    "href": "chapter3.html#sampling-and-non-sampling-errors",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.5 Sampling and Non-sampling Errors",
    "text": "5.5 Sampling and Non-sampling Errors\nUnderstanding potential errors in research is crucial for interpreting results accurately.\nSampling Error: The difference between a sample statistic and the true population parameter, occurring due to chance variations in the selection of sample members.\nExample: If we estimate the average height of all adult males in a country based on a sample, our estimate will likely differ somewhat from the true average due to sampling error.\nNon-sampling Errors: Errors not due to chance, which can occur in both sample surveys and censuses.\n\nCoverage Error: When the sampling frame doesn’t accurately represent the population.\nExample: A telephone survey that only calls landlines would miss people who only have cell phones, potentially biasing the results.\nNon-response Error: When selected participants fail to respond, potentially introducing bias.\nExample: In a survey about job satisfaction, highly satisfied or highly dissatisfied employees might be more likely to respond, skewing the results.\nMeasurement Error: Inaccuracies in the data collected.\nExample: A poorly worded survey question might be interpreted differently by different respondents, leading to inconsistent data.\nProcessing Error: Mistakes made during data entry, coding, or analysis.\nExample: Accidentally entering “99” instead of “9” for a participant’s response could significantly skew the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sample-size-and-power",
    "href": "chapter3.html#sample-size-and-power",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.6 Sample Size and Power",
    "text": "5.6 Sample Size and Power\nDetermining the appropriate sample size involves balancing the need for precision with available resources.\nSample Size Considerations: - Larger samples generally provide more precise estimates but are more costly and time-consuming to obtain. - The required sample size depends on factors such as the desired level of precision, the variability in the population, and the type of analysis planned.\nExample: To estimate the proportion of voters who support a particular policy with a margin of error of ±3% at a 95% confidence level, you would need a sample size of about 1067 voters (assuming maximum variability).\nStatistical Power: The probability that a study will detect an effect when there is an effect to be detected.\nFactors affecting power: 1. Sample size 2. Effect size (the magnitude of the difference or relationship you’re trying to detect) 3. Chosen significance level (usually 0.05)\nExample: In a study comparing two teaching methods, having a larger sample size would increase the likelihood of detecting a significant difference between the methods, if such a difference exists.\n\n\n\n\n\n\nNote\n\n\n\nWhat is Study Power?\nStudy power is about how likely we are to find something if it really exists. It’s like having a good flashlight when you’re looking for something in the dark - the better your flashlight, the more likely you are to find what you’re looking for.\n\nEffect Size: How big the thing (effect, difference) we’re looking for is.\nSample Size: How many people or things we look at in our study.\nStudy Power: How likely we are to find the effect if it’s really there.\n\nThe Relationship Between Effect Size and Sample Size:\nImagine you’re trying to find coins hidden in sand:\n\nBig Effects (Big Coins):\n\nIf you’re looking for big coins (like quarters), you don’t need to search through as much sand to find them.\nIn research, if the effect is big, you can use a smaller sample.\n\nExample: Testing if a new study method improves test scores by 20 points out of 100.\n\nYou might only need to test 30 students to see this big difference.\n\nSmall Effects (Small Coins):\n\nIf you’re looking for tiny coins (like pennies), you’ll need to search through more sand.\nIn research, if the effect is small, you need a larger sample.\n\nExample: Seeing if using social media affects happiness by a tiny amount.\n\nYou might need to study 500 or more people to detect this small effect.\n\n\nWhy Study Power Matters:\n\nNot Missing Real Effects:\n\nWith low power, you might miss real effects, like using a weak flashlight and missing something that’s actually there.\n\nConfidence in Results:\n\nHigher power gives you more confidence that what you found is real and not just luck.\n\n\nExample:\nLet’s say we want to study if a new teaching method helps students learn better:\n\nSmall Study (Low Power):\n\nWe try the method with just 10 students.\nEven if the method works, with such a small group, it’s hard to tell if improvements are due to the new method or just chance.\n\nLarger Study (Higher Power):\n\nWe use the method with 100 students.\nNow we’re more likely to see if the method really helps because we have more data to look at.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-in-the-digital-age",
    "href": "chapter3.html#sampling-in-the-digital-age",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.7 Sampling in the Digital Age",
    "text": "5.7 Sampling in the Digital Age\nThe advent of big data and digital technologies has transformed sampling practices in many fields.\nBig Data Opportunities and Challenges: - Unprecedented volumes of information available - Potential lack of representativeness - Data quality concerns - Privacy and ethical issues\nExample: Social media data can provide real-time insights into public opinion, but users of a particular platform may not be representative of the general population.\nWeb-based Surveys: - Offer new opportunities for data collection - Face challenges such as coverage bias (not everyone has internet access) and self-selection bias\nExample: An online survey about internet usage habits would inherently exclude people without internet access, potentially biasing the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#ethical-considerations-in-sampling",
    "href": "chapter3.html#ethical-considerations-in-sampling",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.8 Ethical Considerations in Sampling",
    "text": "5.8 Ethical Considerations in Sampling\nEthical sampling practices are crucial in social science research:\n\nInformed Consent: Participants should understand the study’s purpose and agree to participate.\nExample: Before conducting interviews about sensitive topics like mental health, researchers must clearly explain the study’s aims and potential risks to participants.\nPrivacy and Confidentiality: Researchers must protect participants’ personal information.\nExample: In a study on workplace harassment, researchers might use code numbers instead of names to protect participants’ identities.\nRepresentativeness and Inclusivity: Samples should fairly represent diverse populations, including marginalized groups.\nExample: A study on urban housing should make efforts to include participants from various socioeconomic backgrounds, ethnicities, and housing situations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#conclusion",
    "href": "chapter3.html#conclusion",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.9 Conclusion",
    "text": "5.9 Conclusion\nSampling remains a cornerstone of social science research, even in the era of big data. Understanding sampling principles helps researchers design studies, interpret results, and make valid inferences about populations. As we’ve seen, the journey from sample to population involves careful consideration of sampling methods, potential errors, ethical issues, and the ever-evolving landscape of data collection in the digital age.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#statistical-errors---summary",
    "href": "chapter3.html#statistical-errors---summary",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.10 Statistical Errors - Summary",
    "text": "5.10 Statistical Errors - Summary\n\n5.10.1 Systematic Error vs. Random Error\nSystematic errors and random errors are two fundamental types of errors in statistical measurements and experiments.\n\nSystematic Error:\n\nDefinition: Consistent, predictable deviations from the true value\nCharacteristics:\n\nBiases the results in a specific direction\nRepeatable and often constant across measurements\nCan be corrected if identified\n\nExamples:\n\nMiscalibrated measuring instrument\nConsistent rounding error in data entry\nBiased sampling method\n\n\nRandom Error:\n\nDefinition: Unpredictable fluctuations in measurements due to chance\nCharacteristics:\n\nVaries in magnitude and direction\nFollows a probability distribution (often normal)\nCan be reduced by increasing sample size or repeated measurements\n\nExamples:\n\nNatural variations in the phenomenon being measured\nSmall fluctuations in measuring instruments\nHuman errors in reading or recording data\n\n\n\n\n\n5.10.2 Sampling Errors vs. Non-Sampling Errors\nSampling and non-sampling errors are categories of errors that can occur in statistical studies, particularly in survey research.\n\nSampling Errors:\n\nDefinition: Errors that occur due to the sample not perfectly representing the population\nCharacteristics:\n\nInherent in any sample-based study\nCan be estimated and quantified using statistical methods\nDecreases as sample size increases\n\nExamples:\n\nRandom fluctuations in sample statistics\nOver- or under-representation of certain groups in the sample\n\n\nNon-Sampling Errors:\n\nDefinition: All errors in a study that are not related to sampling\nCharacteristics:\n\nCan occur in both sample and census studies\nOften more difficult to quantify and control than sampling errors\nCan introduce bias into results\nCan be either systematic or random\n\nExamples:\n\nResponse errors (e.g., misunderstanding questions, deliberate misreporting)\nNonresponse bias (when certain groups are less likely to respond)\nData processing errors (e.g., coding mistakes, data entry errors)\nCoverage errors (when the sampling frame doesn’t accurately represent the population)\n\n\n\nImportant clarification: Non-sampling errors can indeed be either systematic or random. This is a crucial distinction that should have been included in the original description. Non-sampling errors encompass a wide range of potential errors that are not directly related to the sampling process. Some of these can be systematic (e.g., a miscalibrated measuring instrument), while others can be random (e.g., occasional mistakes in data entry).\nThe distinction between sampling and non-sampling errors is independent of the division between systematic and random errors. In practice, non-sampling errors can fall into both of these categories, which makes their identification and control a particularly important aspect of statistical research.\nUnderstanding these types of errors is crucial for designing robust statistical studies, interpreting results accurately, and making valid inferences about populations based on sample data.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#review-questions-and-exercises",
    "href": "chapter3.html#review-questions-and-exercises",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.11 Review Questions and Exercises",
    "text": "5.11 Review Questions and Exercises\n(…)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html",
    "href": "rozdzial3.html",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "",
    "text": "6.1 Wprowadzenie do Losowości\nLosowość jest fundamentalnym pojęciem w statystyce i badaniach naukowych. Odnosi się do nieprzewidywalności indywidualnych wyników, nawet gdy ogólny wzorzec może być przewidywalny. W naukach społecznych zrozumienie losowości jest kluczowe dla projektowania badań, zbierania danych i interpretacji wyników.\nRozważmy rzut uczciwą monetą. Chociaż wiemy, że prawdopodobieństwo wypadnięcia orła wynosi 50%, nie możemy z pewnością przewidzieć wyniku pojedynczego rzutu. Ta nieprzewidywalność jest istotą losowości.\nPrzykłady losowych zjawisk w naukach społecznych obejmują:\nZrozumienie losowości pomaga badaczom odróżnić rzeczywiste efekty od przypadkowych zdarzeń. Na przykład, jeśli zaobserwujemy niewielką różnicę w wynikach testów między dwiema grupami, losowość pomaga nam określić, czy ta różnica jest prawdopodobnie spowodowana rzeczywistym efektem, czy tylko przypadkową zmiennością.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wprowadzenie-do-losowości",
    "href": "rozdzial3.html#wprowadzenie-do-losowości",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "",
    "text": "Wybór uczestników: W eksperymencie psychologicznym badającym czasy reakcji, kolejność, w jakiej uczestnicy przybywają do laboratorium, może być losowa.\nZachowania ekonomiczne: Codzienne wahania cen akcji często wykazują losowe wzorce, na które wpływa niezliczona ilość nieprzewidywalnych czynników.\nInterakcje społeczne: Występowanie przypadkowych spotkań między osobami w społeczności można uznać za zdarzenia losowe.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#próbkowanie-łączenie-próby-i-populacji",
    "href": "rozdzial3.html#próbkowanie-łączenie-próby-i-populacji",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.2 Próbkowanie: Łączenie Próby i Populacji",
    "text": "6.2 Próbkowanie: Łączenie Próby i Populacji\nPróbkowanie to proces wybierania podzbioru (próby) z większej grupy (populacji) w celu wyciągnięcia wniosków o populacji. Jest to kluczowa umiejętność w badaniach nauk społecznych, ponieważ badanie całych populacji jest często niepraktyczne, zbyt kosztowne lub czasami niemożliwe.\nKluczowe pojęcia:\n\nPopulacja: Cała grupa, o której chcemy wyciągnąć wnioski.\nPróba: Podzbiór populacji, który faktycznie badamy.\nOperat losowania: Lista lub procedura używana do identyfikacji wszystkich członków populacji.\n\nPrzykład: Załóżmy, że chcemy zbadać satysfakcję z pracy wszystkich nauczycieli w Polsce (populacja). Zamiast ankietować setki tysięcy nauczycieli, możemy wybrać próbę 5000 nauczycieli z różnych województw, powiatów i poziomów nauczania.\nLosowość w próbkowaniu pomaga zapewnić, że próba jest reprezentatywna dla populacji, zmniejszając błędy systematyczne i umożliwiając dokładniejsze wnioskowanie. Dlatego metody próbkowania probabilistycznego, które omówimy dalej, są często preferowane w badaniach naukowych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#metody-próbkowania",
    "href": "rozdzial3.html#metody-próbkowania",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.3 Metody Próbkowania",
    "text": "6.3 Metody Próbkowania\n\n6.3.1 Próbkowanie Probabilistyczne\nMetody próbkowania probabilistycznego obejmują losowy wybór, dając każdemu członkowi populacji znaną, niezerową szansę na wybór.\n\nProsty Dobór Losowy: Każdy członek populacji ma równą szansę na wybór.\nPrzykład: Aby wybrać 100 studentów z uniwersytetu liczącego 10 000 studentów, można przypisać każdemu studentowi numer od 1 do 10 000, a następnie użyć generatora liczb losowych do wybrania 100 numerów.\nDobór Losowy Warstwowy: Populacja jest podzielona na podgrupy (warstwy) na podstawie wspólnych cech, a następnie próbki są losowo wybierane z każdej warstwy.\nPrzykład: W ogólnopolskim badaniu politycznym można podzielić populację na warstwy na podstawie regionów geograficznych (np. Polska Zachodnia, Centralna, Wschodnia) i losowo pobierać próbki z każdego regionu. Zapewnia to reprezentację ze wszystkich obszarów kraju.\nDobór Losowy Grupowy: Populacja jest podzielona na skupiska (zwykle geograficzne), niektóre skupiska są losowo wybierane, a wszyscy członkowie w tych skupiskach są badani.\nPrzykład: Aby zbadać nawyki uczenia się uczniów szkół średnich, można losowo wybrać 20 szkół z całego kraju, a następnie przeprowadzić ankietę wśród wszystkich uczniów w tych szkołach.\nDobór Systematyczny: Wybieranie co k-tego elementu z listy po losowym starcie.\nPrzykład: W ruchliwym centrum handlowym można ankietować co 20. osobę wchodzącą do centrum, zaczynając od losowo wybranej liczby między 1 a 20.\n\n\n\n6.3.2 Próbkowanie Nieprobabilistyczne\nPróbkowanie nieprobabilistyczne nie obejmuje losowego wyboru. Chociaż może wprowadzać błędy systematyczne, może być konieczne w niektórych sytuacjach, zwłaszcza w przypadku trudno dostępnych populacji lub gdy zasoby są ograniczone.\n\nDobór Wygodny: Wybieranie łatwo dostępnych podmiotów.\nPrzykład: Badacz studiujący wzorce snu studentów może przeprowadzić ankietę wśród studentów na własnych zajęciach lub na terenie kampusu.\nDobór Celowy: Wybieranie podmiotów na podstawie określonych cech.\nPrzykład: W badaniu doświadczeń prezesów w branży technologicznej badacz może celowo szukać i przeprowadzać wywiady z prezesami różnych firm technologicznych.\nDobór Metodą Kuli Śnieżnej: Uczestnicy rekrutują innych uczestników.\nPrzykład: W badaniu dostępu imigrantów bez dokumentów do opieki zdrowotnej, badacze mogą poprosić początkowych uczestników o polecenie innych potencjalnych uczestników z ich społeczności.\nDobór Kwotowy: Wybieranie uczestników w celu spełnienia określonych kwot dla pewnych cech.\nPrzykład: W badaniu rynku badacze mogą zapewnić, że przeprowadzają wywiady z określoną liczbą osób z różnych grup wiekowych, płci i poziomów dochodów, aby dopasować się do demografii rynku docelowego.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wnioskowanie-z-prób",
    "href": "rozdzial3.html#wnioskowanie-z-prób",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.4 Wnioskowanie z Prób",
    "text": "6.4 Wnioskowanie z Prób\nWnioskowanie statystyczne to proces wyciągania wniosków o populacji na podstawie próby. Pozwala to badaczom oszacować charakterystyki całej populacji (parametry) przy użyciu charakterystyk próby (statystyk).\n\n\n\n\n\n\nNote\n\n\n\nThe Soup Analogy: A Taste of Statistics\n\n\nWhen you taste a spoonful of soup and decide it isn’t salty enough, that’s exploratory/descriptive analysis.\nIf you generalize and conclude that your entire pot of soup needs salt, that’s an inference.\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population).\nIf the soup is not well stirred (heterogeneous population), it doesn’t matter how large a spoon you have (sample size), it will still not accurately represent the whole. If the soup is well stirred (homogeneous population), even a small spoon will suffice to test the soup.\n\n\n\nKluczowe pojęcia:\n\nEstymatory punktowe: Pojedyncza wartość używana do oszacowania parametru populacji.\nPrzykład: Średni dochód z próby 1000 pracowników może być użyty do oszacowania średniego dochodu wszystkich pracowników w kraju.\nPrzedziały ufności: Zakres wartości, który prawdopodobnie zawiera prawdziwy parametr populacji.\nPrzykład: Możemy powiedzieć: “Jesteśmy w 95% pewni, że prawdziwy średni dochód populacji mieści się między 4500 a 5500 złotych”.\nMargines błędu: Zakres wartości powyżej i poniżej statystyki z próby w przedziale ufności.\nPrzykład: W sondażach politycznych można zobaczyć stwierdzenie: “Kandydat A jest preferowany przez 52% wyborców, z marginesem błędu ±3%”.\nTestowanie hipotez: Metoda podejmowania decyzji o parametrach populacji na podstawie danych z próby.\nPrzykład: Badacz może testować, czy istnieje istotna różnica w wynikach testów między uczniami, którzy uczą się przy muzyce, a tymi, którzy uczą się w ciszy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#błędy-próbkowania-i-błędy-niepróbkowe",
    "href": "rozdzial3.html#błędy-próbkowania-i-błędy-niepróbkowe",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.5 Błędy Próbkowania i Błędy Niepróbkowe",
    "text": "6.5 Błędy Próbkowania i Błędy Niepróbkowe\nZrozumienie potencjalnych błędów w badaniach jest kluczowe dla dokładnej interpretacji wyników.\nBłąd próbkowania: Różnica między statystyką z próby a prawdziwym parametrem populacji, występująca z powodu przypadkowych wahań w wyborze członków próby.\nPrzykład: Jeśli oszacujemy średni wzrost wszystkich dorosłych mężczyzn w kraju na podstawie próby, nasze oszacowanie prawdopodobnie będzie się nieco różnić od prawdziwej średniej z powodu błędu próbkowania.\nBłędy niepróbkowe: Błędy nie wynikające z przypadku, które mogą wystąpić zarówno w badaniach próbkowych, jak i spisach.\n\nBłąd pokrycia: Gdy operat losowania nie reprezentuje dokładnie populacji.\nPrzykład: Badanie telefoniczne, które dzwoni tylko na telefony stacjonarne, pominęłoby osoby posiadające tylko telefony komórkowe, potencjalnie wypaczając wyniki.\nBłąd braku odpowiedzi: Gdy wybrani uczestnicy nie odpowiadają, potencjalnie wprowadzając błąd systematyczny.\nPrzykład: W badaniu satysfakcji z pracy, bardzo zadowoleni lub bardzo niezadowoleni pracownicy mogą być bardziej skłonni do odpowiedzi, wypaczając wyniki.\nBłąd pomiaru: Niedokładności w zebranych danych.\nPrzykład: Źle sformułowane pytanie ankietowe może być różnie interpretowane przez różnych respondentów, prowadząc do niespójnych danych.\nBłąd przetwarzania: Błędy popełnione podczas wprowadzania danych, kodowania lub analizy.\nPrzykład: Przypadkowe wprowadzenie “99” zamiast “9” dla odpowiedzi uczestnika mogłoby znacząco wypaczyć wyniki.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wielkość-próby-i-moc-statystyczna",
    "href": "rozdzial3.html#wielkość-próby-i-moc-statystyczna",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.6 Wielkość Próby i Moc Statystyczna",
    "text": "6.6 Wielkość Próby i Moc Statystyczna\nOkreślenie odpowiedniej wielkości próby wymaga zrównoważenia potrzeby precyzji z dostępnymi zasobami.\nRozważania dotyczące wielkości próby: - Większe próby generalnie zapewniają bardziej precyzyjne oszacowania, ale są bardziej kosztowne i czasochłonne do uzyskania. - Wymagana wielkość próby zależy od czynników takich jak pożądany poziom precyzji, zmienność w populacji i rodzaj planowanej analizy.\nPrzykład: Aby oszacować proporcję wyborców popierających konkretną politykę z marginesem błędu ±3% na poziomie ufności 95%, potrzebna byłaby próba około 1067 wyborców (zakładając maksymalną zmienność).\nMoc statystyczna: Prawdopodobieństwo, że badanie wykryje efekt, gdy taki efekt istnieje.\nCzynniki wpływające na moc: 1. Wielkość próby 2. Wielkość efektu (wielkość różnicy lub związku, który próbujemy wykryć) 3. Wybrany poziom istotności (zwykle 0,05)\nPrzykład: W badaniu porównującym dwie metody nauczania, większa wielkość próby zwiększyłaby prawdopodobieństwo wykrycia istotnej różnicy między metodami, jeśli taka różnica istnieje.\n\n\n\n\n\n\nNote\n\n\n\nCzym jest Moc Badania?\nMoc badania dotyczy tego, jak prawdopodobne jest, że znajdziemy coś, jeśli to naprawdę istnieje. To jak posiadanie dobrej latarki, gdy szukasz czegoś w ciemności - im lepsza latarka, tym bardziej prawdopodobne, że znajdziesz to, czego szukasz.\n\nWielkość Efektu: Jak duża jest rzecz (efekt, różnica, itp.), której szukamy.\nWielkość Próby: Ile osób lub rzeczy badamy w naszym studium.\nMoc Badania: Jak prawdopodobne jest, że znajdziemy efekt, jeśli naprawdę istnieje.\n\nZwiązek Między Wielkością Efektu a Wielkością Próby:\nWyobraź sobie, że próbujesz znaleźć monety ukryte w piasku:\n\nDuże Efekty (Duże Monety):\n\nJeśli szukasz dużych monet (jak 5 złotych), nie musisz przeszukiwać tak dużo piasku, aby je znaleźć.\nW badaniach, jeśli efekt jest duży, możesz użyć mniejszej próby.\n\nPrzykład: Testowanie, czy nowa metoda nauki poprawia wyniki testów o 20 punktów na 100.\n\nMoże wystarczyć przetestować 30 uczniów, aby zobaczyć tę dużą różnicę.\n\nMałe Efekty (Małe Monety):\n\nJeśli szukasz maleńkich monet (jak 1 grosz), będziesz musiał przeszukać więcej piasku.\nW badaniach, jeśli efekt jest mały, potrzebujesz większej próby.\n\nPrzykład: Sprawdzanie, czy korzystanie z mediów społecznościowych wpływa na szczęście o niewielką ilość.\n\nMoże być potrzeba zbadania 500 lub więcej osób, aby wykryć ten mały efekt.\n\n\nDlaczego Moc Badania Jest Ważna:\n\nNie Przegapienie Rzeczywistych Efektów:\n\nPrzy niskiej mocy możesz przeoczyć rzeczywiste efekty, jak używanie słabej latarki i przeoczenie czegoś, co faktycznie tam jest.\n\nPewność Wyników:\n\nWyższa moc daje większą pewność, że to, co znalazłeś, jest prawdziwe, a nie tylko przypadkiem.\n\n\nPrzykład:\nZałóżmy, że chcemy zbadać, czy nowa metoda nauczania pomaga uczniom lepiej się uczyć:\n\nMałe Badanie (Niska Moc):\n\nPróbujemy metody z zaledwie 10 uczniami.\nNawet jeśli metoda działa, przy tak małej grupie trudno stwierdzić, czy poprawa wynika z nowej metody, czy to tylko przypadek.\n\nWiększe Badanie (Wyższa Moc):\n\nStosujemy metodę ze 100 uczniami.\nTeraz jest bardziej prawdopodobne, że zobaczymy, czy metoda naprawdę pomaga, ponieważ mamy więcej danych do analizy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#próbkowanie-w-erze-cyfrowej",
    "href": "rozdzial3.html#próbkowanie-w-erze-cyfrowej",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.7 Próbkowanie w Erze Cyfrowej",
    "text": "6.7 Próbkowanie w Erze Cyfrowej\nPojawienie się big data i technologii cyfrowych zmieniło praktyki próbkowania w wielu dziedzinach.\nMożliwości i wyzwania Big Data: - Bezprecedensowe ilości dostępnych informacji - Potencjalny brak reprezentatywności - Problemy z jakością danych - Kwestie prywatności i etyki\nPrzykład: Dane z mediów społecznościowych mogą dostarczyć wglądu w opinię publiczną w czasie rzeczywistym, ale użytkownicy konkretnej platformy mogą nie być reprezentatywni dla ogólnej populacji.\nBadania internetowe: - Oferują nowe możliwości zbierania danych - Stają przed wyzwaniami takimi jak błąd pokrycia (nie każdy ma dostęp do internetu) i błąd samoselekcji\nPrzykład: Ankieta online na temat nawyków korzystania z internetu z natury wykluczałaby osoby bez dostępu do internetu, potencjalnie wypaczając wyniki.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#etyczne-aspekty-próbkowania",
    "href": "rozdzial3.html#etyczne-aspekty-próbkowania",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.8 Etyczne Aspekty Próbkowania",
    "text": "6.8 Etyczne Aspekty Próbkowania\nEtyczne praktyki próbkowania są kluczowe w badaniach nauk społecznych:\n\nŚwiadoma zgoda: Uczestnicy powinni rozumieć cel badania i zgodzić się na udział.\nPrzykład: Przed przeprowadzeniem wywiadów na temat wrażliwych tematów, takich jak zdrowie psychiczne, badacze muszą jasno wyjaśnić cele badania i potencjalne ryzyko uczestnikom.\nPrywatność i poufność: Badacze muszą chronić dane osobowe uczestników.\nPrzykład: W badaniu dotyczącym mobbingu w miejscu pracy, badacze mogą używać kodów numerycznych zamiast nazwisk, aby chronić tożsamość uczestników.\nReprezentatywność i inkluzywność: Próby powinny sprawiedliwie reprezentować zróżnicowane populacje, w tym grupy marginalizowane.\n\nPrzykład: Badanie dotyczące mieszkalnictwa miejskiego powinno dołożyć starań, aby uwzględnić uczestników z różnych środowisk społeczno-ekonomicznych, grup etnicznych i sytuacji mieszkaniowych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#podsumowanie",
    "href": "rozdzial3.html#podsumowanie",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.9 Podsumowanie",
    "text": "6.9 Podsumowanie\nPróbkowanie pozostaje fundamentem badań w naukach społecznych, nawet w erze big data. Zrozumienie zasad próbkowania pomaga badaczom projektować badania, interpretować wyniki i wyciągać trafne wnioski o populacjach. Jak widzieliśmy, droga od próby do populacji wymaga starannego rozważenia metod próbkowania, potencjalnych błędów, kwestii etycznych i stale ewoluującego krajobrazu gromadzenia danych w erze cyfrowej.\nDziękuję za to bardzo trafne pytanie. Ma Pan/Pani rację, i doceniam tę uwagę. Rzeczywiście, tłumaczenie “błąd samplowy vs. niesamplowy” jest bardziej precyzyjne i lepiej oddaje istotę tych pojęć w polskiej terminologii statystycznej. Pozwolę sobie wprowadzić odpowiednie korekty i wyjaśnienia:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#błędy-statystyczne---podsumowanie",
    "href": "rozdzial3.html#błędy-statystyczne---podsumowanie",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.10 Błędy Statystyczne - podsumowanie",
    "text": "6.10 Błędy Statystyczne - podsumowanie\n\n6.10.1 Błąd Systematyczny vs. Błąd Losowy\nBłędy systematyczne i losowe to dwa podstawowe rodzaje błędów w pomiarach i eksperymentach statystycznych.\n\nBłąd Systematyczny:\n\nDefinicja: Konsekwentne, przewidywalne odchylenia od prawdziwej wartości\nCharakterystyka:\n\nZniekształca wyniki w określonym kierunku\nPowtarzalny i często stały w różnych pomiarach\nMoże być skorygowany, jeśli zostanie zidentyfikowany\n\nPrzykłady:\n\nŹle skalibrowany przyrząd pomiarowy\nKonsekwentny błąd zaokrąglania przy wprowadzaniu danych\nStronnicza metoda pobierania próbek\n\n\nBłąd Losowy:\n\nDefinicja: Nieprzewidywalne wahania w pomiarach wynikające z przypadku\nCharakterystyka:\n\nZmienia się co do wielkości i kierunku\nPodąża za rozkładem prawdopodobieństwa (często normalnym)\nMożna go zmniejszyć zwiększając wielkość próby lub powtarzając pomiary\n\nPrzykłady:\n\nNaturalne wahania w badanym zjawisku\nMałe fluktuacje w przyrządach pomiarowych\nBłędy ludzkie przy odczytywaniu lub zapisywaniu danych\n\n\n\n\n\n6.10.2 Błędy Samplowe vs. Błędy Niesamplowe\n\nBłędy Samplowe (lub Błędy Próbkowania):\n\nDefinicja: Błędy wynikające z tego, że próba (sample) nie reprezentuje idealnie populacji\nCharakterystyka:\n\nNieodłączne w każdym badaniu opartym na próbie\nMożna je oszacować i skwantyfikować za pomocą metod statystycznych\nZmniejszają się wraz ze wzrostem wielkości próby\n\nPrzykłady:\n\nLosowe wahania w statystykach (z) próby\nNadreprezentacja lub niedoreprezentowanie niektórych grup w próbie\n\n\nBłędy Niesamplowe:\n\nDefinicja: Wszystkie błędy w badaniu, które nie są związane z próbkowaniem (samplingiem)\nCharakterystyka:\n\nMogą wystąpić zarówno w badaniach próbkowych, jak i pełnych (spisach)\nCzęsto trudniejsze do skwantyfikowania i kontrolowania niż błędy samplowe\nMogą wprowadzać stronniczość do wyników\nMogą być systematyczne lub losowe\n\nPrzykłady:\n\nBłędy odpowiedzi (np. niezrozumienie pytań, celowe błędne raportowanie)\nBłąd braku odpowiedzi (gdy niektóre grupy są mniej skłonne do odpowiedzi)\nBłędy przetwarzania danych (np. błędy kodowania, błędy wprowadzania danych)\nBłędy pokrycia (gdy operat losowania nie reprezentuje dokładnie populacji)\n\n\n\nWażne wyjaśnienie: Błędy niesamplowe mogą być zarówno systematyczne, jak i losowe. To kluczowe rozróżnienie, które powinienem był uwzględnić w pierwotnym opisie. Błędy niesamplowe obejmują szeroki zakres możliwych błędów, które nie są bezpośrednio związane z procesem próbkowania. Niektóre z nich mogą być systematyczne (np. błędnie skalibrowany instrument pomiarowy), podczas gdy inne mogą być losowe (np. przypadkowe błędy przy wprowadzaniu danych).\nRozróżnienie na błędy samplowe i niesamplowe jest niezależne od podziału na błędy systematyczne i losowe. W praktyce, błędy niesamplowe mogą należeć do obu tych kategorii, co czyni ich identyfikację i kontrolę szczególnie ważnym aspektem badań statystycznych.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff\n\n\nKluczowe punkty do zapamiętania:\n\nLosowość jest podstawą wielu metod próbkowania i pomaga zapewnić reprezentatywność próby.\nIstnieją różne metody próbkowania, zarówno probabilistyczne, jak i nieprobabilistyczne, każda z własnymi zaletami i ograniczeniami.\nWnioskowanie statystyczne pozwala nam wyciągać wnioski o populacji na podstawie danych z próby.\nBłędy próbkowania i niepróbkowe mogą wpływać na jakość naszych wniosków, dlatego ważne jest ich zrozumienie i minimalizowanie.\nWielkość próby i moc statystyczna są kluczowe dla zapewnienia wiarygodności wyników badań.\nEra cyfrowa przynosi nowe możliwości i wyzwania w zakresie próbkowania i gromadzenia danych.\nEtyczne aspekty próbkowania, w tym świadoma zgoda, prywatność i reprezentatywność, są nieodłączną częścią procesu badawczego.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "chapter3b.html",
    "href": "chapter3b.html",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "",
    "text": "7.1 Defining Reliability and Validity\nReliability refers to the consistency of a measure. A reliable measurement or study produces similar results under consistent conditions.\nValidity refers to the accuracy of a measure. A valid measurement or study accurately represents what it claims to measure.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#the-four-combinations-of-reliability-and-validity",
    "href": "chapter3b.html#the-four-combinations-of-reliability-and-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.2 The Four Combinations of Reliability and Validity",
    "text": "7.2 The Four Combinations of Reliability and Validity\nThere are four possible combinations of reliability and validity:\n\nHigh Reliability, High Validity\nHigh Reliability, Low Validity\nLow Reliability, High Validity\nLow Reliability, Low Validity\n\nLet’s explore each of these combinations with examples and visualizations.\n\n7.2.1 1. High Reliability, High Validity\nThis is the ideal scenario in research. Measurements are both consistent and accurate.\nExample: A well-calibrated digital scale used to measure weight. It consistently gives the same reading for the same object and accurately represents the true weight.\n\n\n7.2.2 2. High Reliability, Low Validity\nIn this case, measurements are consistent but not accurate.\nExample: A miscalibrated scale that always measures 5 kg too heavy. It gives consistent results (high reliability) but doesn’t represent the true weight (low validity).\n\n\n7.2.3 3. Low Reliability, High Validity\nHere, measurements are accurate on average but inconsistent.\nExample: A scale that fluctuates around the true weight. Sometimes it’s a bit over, sometimes a bit under, but on average, it’s correct.\n\n\n7.2.4 4. Low Reliability, Low Validity\nThis is the worst-case scenario, where measurements are neither consistent nor accurate.\nExample: A broken scale that gives random readings unrelated to the true weight.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#visualizing-reliability-and-validity",
    "href": "chapter3b.html#visualizing-reliability-and-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.3 Visualizing Reliability and Validity",
    "text": "7.3 Visualizing Reliability and Validity\nTo better understand these concepts, let’s create visualizations using ggplot2 in R. We’ll simulate measurement data for each scenario and plot them.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generate data for each scenario\nn &lt;- 100\ntrue_value &lt;- 50\n\ndata &lt;- tibble(\n  high_rel_high_val = rnorm(n, mean = true_value, sd = 1),\n  high_rel_low_val = rnorm(n, mean = true_value + 5, sd = 1),\n  low_rel_high_val = rnorm(n, mean = true_value, sd = 5),\n  low_rel_low_val = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenario\", values_to = \"measurement\")\n\n# Create the scatterplot\nscatter_plot &lt;- ggplot(data, aes(x = id, y = measurement, color = scenario)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = true_value, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Scatterplots of Measurements\",\n       subtitle = \"Dashed line represents the true value\",\n       x = \"Measurement ID\",\n       y = \"Measured Value\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Create the histogram\nhist_plot &lt;- ggplot(data, aes(x = measurement, fill = scenario)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = true_value, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Histograms of Measurements\",\n       subtitle = \"Red dashed line represents the true value\",\n       x = \"Measured Value\",\n       y = \"Count\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Combine the plots\ncombined_plot &lt;- scatter_plot / hist_plot +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Reliability and Validity in Measurements\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Display the combined plot\ncombined_plot\n\n\n\n\n\n\n\n\n\n7.3.1 Interpreting the Visualizations\n\nHigh Reliability, High Validity: Points cluster tightly around the true value (dashed line).\nHigh Reliability, Low Validity: Points cluster tightly, but consistently above the true value.\nLow Reliability, High Validity: Points scatter widely but center around the true value.\nLow Reliability, Low Validity: Points scatter randomly with no clear pattern or relation to the true value.\n\nUnderstanding reliability and validity is crucial in data science and research. High reliability ensures consistent measurements, while high validity ensures accurate representations of what we intend to measure. By considering both aspects, researchers can design more robust studies and draw more meaningful conclusions from their data.\nWhen conducting your own research or analyzing others’ work, always consider: - How reliable are the measurements? - How valid is the approach for measuring the intended concept? - Do the methods used support both reliability and validity?\nBy keeping these questions in mind, you’ll be better equipped to produce and interpret high-quality research in data science.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#types-of-reliability",
    "href": "chapter3b.html#types-of-reliability",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.4 Types of Reliability",
    "text": "7.4 Types of Reliability\nReliability can be assessed in several ways, each focusing on a different aspect of consistency:\n\nTest-Retest Reliability: This measures the consistency of a test over time. It involves administering the same test to the same group of participants at different times and comparing the results.\nInter-Rater Reliability: This assesses the degree of agreement among different raters or observers. It’s crucial when subjective judgments are involved in data collection.\nInternal Consistency: This evaluates how well different items on a test or scale measure the same construct. Cronbach’s alpha is a common measure of internal consistency.\nParallel Forms Reliability: This involves creating two equivalent forms of a test and administering them to the same group. The correlation between the two sets of scores indicates reliability.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#types-of-validity",
    "href": "chapter3b.html#types-of-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.5 Types of Validity",
    "text": "7.5 Types of Validity\nValidity is a multifaceted concept, with several types that researchers need to consider:\n\nContent Validity: This ensures that a measure covers all aspects of the construct it aims to measure. It’s often assessed by expert judgment.\nConstruct Validity: This evaluates whether a test measures the intended theoretical construct. It includes:\n\nConvergent Validity: The degree to which the measure correlates with other measures of the same construct.\nDiscriminant Validity: The extent to which the measure does not correlate with measures of different constructs.\n\nCriterion Validity: This assesses how well a measure predicts an outcome. It includes:\n\nConcurrent Validity: How well the measure correlates with other measures of the same construct at the same time.\nPredictive Validity: How well the measure predicts future outcomes.\n\nFace Validity: Face validity describes how test subjects perceive the test and whether - from their point of view - it is adequate for the purpose it is supposed to serve. A lack of face validity, even though the test may be valid from the perspective of a specific purpose, can contribute to a decrease in motivation among test subjects, which directly affects the results achieved or may lead to rejection of the test. While not a scientific measure, it can be important for participant buy-in.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#internal-vs.-external-validity",
    "href": "chapter3b.html#internal-vs.-external-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.6 Internal vs. External Validity",
    "text": "7.6 Internal vs. External Validity\nThese concepts are crucial in experimental design and the generalizability of research findings:\n\n7.6.1 Internal Validity\nInternal validity refers to the extent to which a study establishes a causal relationship between the independent and dependent variables. It answers the question: “Did the experimental treatment actually cause the observed effects?”\nFactors that can threaten internal validity include: - History: External events occurring between pre-test and post-test - Maturation: Natural changes in participants over time - Testing effects: Changes due to taking a pre-test - Instrumentation: Changes in the measurement tool or observers - Selection bias: Non-random assignment to groups - Attrition: Loss of participants during the study\n\n\n7.6.2 External Validity\nExternal validity refers to the extent to which the results of a study can be generalized to other situations, populations, or settings. It addresses the question: “To what extent can the findings be applied beyond the specific context of the study?”\nFactors that can affect external validity include: - Population validity: How well the sample represents the larger population - Ecological validity: How well the study setting represents real-world conditions - Temporal validity: Whether the results hold true across time",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#consistency-in-research",
    "href": "chapter3b.html#consistency-in-research",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.7 Consistency in Research",
    "text": "7.7 Consistency in Research\nConsistency is closely related to reliability but extends beyond just measurement. In research, consistency refers to the overall coherence and stability of results across different contexts, methods, or studies.\nKey aspects of consistency in research include:\n\nReplicability: The ability to reproduce study results using the same methods and data.\nRobustness: The stability of findings across different analytical approaches or slight variations in methodology.\nConvergence: The alignment of results from different studies or methods investigating the same phenomenon.\nLongitudinal Consistency: The stability of findings over time, especially important in longitudinal studies.\n\nEnsuring consistency in research involves: - Using standardized procedures and measures - Thoroughly documenting methods and analytical decisions - Conducting replication studies - Meta-analyses to synthesize findings across multiple studies",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#balancing-reliability-validity-and-consistency",
    "href": "chapter3b.html#balancing-reliability-validity-and-consistency",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.8 Balancing Reliability, Validity, and Consistency",
    "text": "7.8 Balancing Reliability, Validity, and Consistency\nWhile reliability, validity, and consistency are all crucial for high-quality research, they sometimes involve trade-offs:\n\nA highly reliable measure might lack validity if it consistently measures the wrong thing.\nStriving for perfect internal validity (e.g., in tightly controlled lab experiments) might reduce external validity.\nEnsuring high consistency across diverse contexts might require sacrificing some degree of precision or depth in specific situations.\n\nResearchers must carefully balance these aspects based on their research questions and the nature of their study. A comprehensive understanding of reliability, validity, and consistency helps in designing robust studies, interpreting results accurately, and contributing meaningfully to the body of scientific knowledge.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#bias-variance-tradeoff",
    "href": "chapter3b.html#bias-variance-tradeoff",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.9 Bias-Variance Tradeoff",
    "text": "7.9 Bias-Variance Tradeoff\nThe concepts of reliability and validity are closely related to the statistical notion of the bias-variance tradeoff. This tradeoff is fundamental in machine learning and statistical modeling.\n\nBias refers to the error introduced by approximating a real-world problem with a simplified model. High bias can lead to underfitting.\nVariance refers to the error introduced by the model’s sensitivity to small fluctuations in the training set. High variance can lead to overfitting.\n\nLet’s visualize this concept with a simplified plot:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_true &lt;- sin(x)\ny_low_bias_high_var &lt;- y_true + rnorm(100, 0, 0.3)\ny_high_bias_low_var &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_true, y_low_bias_high_var, y_high_bias_low_var),\n                 type = rep(c(\"True Function\", \"Low Bias, High Variance\", \"High Bias, Low Variance\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = type)) +\n  geom_line() +\n  geom_point(data = subset(df, type != \"True Function\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Bias-Variance Tradeoff\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Model Type\") +\n  theme_minimal()\n\n\n\n\nVisualization of Bias-Variance Tradeoff\n\n\n\n\nIn this plot: - The black line represents the true underlying function. - The blue points represent a model with low bias but high variance. It follows the true function closely on average but has a lot of noise. - The red line represents a model with high bias but low variance. It consistently underestimates the true function but has less noise.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#accuracy-and-precision",
    "href": "chapter3b.html#accuracy-and-precision",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.10 Accuracy and Precision",
    "text": "7.10 Accuracy and Precision\nThe concepts of accuracy and precision are closely related to validity and reliability:\n\nAccuracy refers to how close a measurement is to the true value (similar to validity).\nPrecision refers to how consistent or reproducible the measurements are (similar to reliability).\n\nWe can visualize these concepts using a simplified target analogy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"High Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Low Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"High Accuracy\\nLow Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Low Accuracy\\nLow Precision\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Accuracy vs Precision\")\n\n\n\n\nVisualization of Accuracy vs Precision\n\n\n\n\nIn this visualization: - High accuracy means the points are close to the center (bullseye). - High precision means the points are tightly clustered. - Each panel represents a different combination of accuracy and precision.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#conclusion",
    "href": "chapter3b.html#conclusion",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.11 Conclusion",
    "text": "7.11 Conclusion\nUnderstanding reliability and validity is crucial for conducting robust research. These concepts help us ensure that our measurements are both consistent and accurate. By relating them to ideas like the bias-variance tradeoff and accuracy-precision, we gain a deeper appreciation of the challenges involved in measurement and modeling in scientific research. As researchers, we must strive to develop measures and models that are both reliable and valid, balancing the tradeoffs between bias and variance, and between accuracy and precision. This requires careful design of research methodologies, rigorous testing of our measurement instruments, and thoughtful interpretation of our results.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#understanding-bias-vs.-variance-in-statistical-measurement",
    "href": "chapter3b.html#understanding-bias-vs.-variance-in-statistical-measurement",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.12 Understanding Bias vs. Variance in Statistical Measurement",
    "text": "7.12 Understanding Bias vs. Variance in Statistical Measurement\n\n7.12.1 Introduction\nIn statistics and machine learning, two important concepts that affect the performance of our models are bias and variance. Understanding these concepts is crucial for building effective predictive models and avoiding common pitfalls like overfitting and underfitting.\n\nBias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can lead to underfitting.\n\nThink of bias as how far off our predictions are from the true values on average.\nIn terms of validity, high bias means our model isn’t capturing the true relationship in the data.\n\nVariance refers to the amount by which our model would change if we estimated it using a different training dataset. High variance can lead to overfitting.\n\nThink of variance as how much our predictions would fluctuate if we used different datasets.\nIn terms of reliability, high variance means our model is too sensitive to the specific data it was trained on.\n\n\nWe’ll explore four scenarios to illustrate different combinations of bias and variance using synthetic data and regression models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#data-generation-and-model-fitting-function",
    "href": "chapter3b.html#data-generation-and-model-fitting-function",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.13 Data Generation and Model Fitting Function",
    "text": "7.13 Data Generation and Model Fitting Function\nFirst, let’s create a function that will help us generate data and fit models for each scenario:\n\ngenerate_and_fit &lt;- function(n, intercept, slope, noise_sd, model_degree) {\n  # Generate synthetic data\n  x &lt;- runif(n, 0, 10)\n  y &lt;- intercept + slope * x + rnorm(n, 0, noise_sd)\n  \n  # Fit model\n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", model_degree, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  # Generate predictions\n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  # Plot\n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    geom_abline(intercept = intercept, slope = slope, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Model Degree:\", model_degree),\n         subtitle = paste(\"Noise SD:\", noise_sd),\n         x = \"X (Input Variable)\",\n         y = \"Y (Target Variable)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nThis function does the following: 1. Generates synthetic data based on our parameters 2. Fits a polynomial regression model 3. Creates a plot showing the true relationship (blue dashed line), our model’s predictions (red solid line), and the data points\nNow, let’s explore our four scenarios!\n\n7.13.1 Scenario 1: Low Bias, Low Variance\nIn this ideal scenario, we use a linear model to fit linear data with low noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 1, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The model (red line) closely follows the true relationship (blue dashed line). - Data points are clustered tightly around the line, indicating low noise. - This scenario represents a good fit: the model captures the underlying trend without being overly complex.\n\n\n7.13.2 Scenario 2: Low Bias, High Variance\nHere, we use a linear model to fit linear data, but with high noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 5, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The model still captures the general trend, but data points are more scattered. - This high variance means our model’s predictions would be less reliable. - In real-world terms, this might represent a situation where our measurements are correct on average but have a lot of random error.\n\n\n7.13.3 Scenario 3: High Bias, Low Variance\nIn this case, we use a linear model to fit quadratic (curved) data with low noise.\n\nquadratic_data &lt;- function(n, intercept, slope, noise_sd, model_degree) {\n  x &lt;- runif(n, 0, 10)\n  y &lt;- intercept + slope * x^2 + rnorm(n, 0, noise_sd)\n  \n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", model_degree, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    stat_function(fun = function(x) intercept + slope * x^2, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Model Degree:\", model_degree),\n         subtitle = paste(\"Noise SD:\", noise_sd),\n         x = \"X (Input Variable)\",\n         y = \"Y (Target Variable)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nquadratic_data(n = 100, intercept = 1, slope = 0.2, noise_sd = 1, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The linear model (red line) fails to capture the curvature of the true relationship (blue dashed line). - This high bias means our model is consistently off in its predictions. - In real-world terms, this might represent using an overly simplistic model for a complex phenomenon.\n\n\n7.13.4 Scenario 4: High Bias, High Variance\nFinally, we use a high-degree polynomial to fit linear data with high noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 5, model_degree = 5)\n\n\n\n\n\n\n\n\nExplanation: - The model (red line) is overly complex, trying to fit the noise rather than the underlying trend. - This combination of high bias and high variance leads to poor generalization. - In real-world terms, this might represent overcomplicating our analysis and drawing false conclusions from random fluctuations in our data.\n\n\n7.13.5 Conclusion\nUnderstanding the bias-variance trade-off is crucial in statistical modeling:\n\nLow Bias, Low Variance: The ideal scenario, where our model accurately captures the underlying relationship without being overly sensitive to noise.\nLow Bias, High Variance: Our model is correct on average but unreliable due to high sensitivity to individual data points.\nHigh Bias, Low Variance: Our model is consistently wrong due to oversimplification but gives stable predictions.\nHigh Bias, High Variance: The worst-case scenario, where our model is both inaccurate and unreliable.\n\nIn practice, we often need to balance bias and variance. Techniques like cross-validation, regularization, and ensemble methods can help find this balance.\nRemember: - A model with high bias is too simple and misses important patterns in the data. - A model with high variance is too complex and fits noise in the training data. - The goal is to find a sweet spot that captures true patterns without overfitting to noise.\nBy understanding these concepts, you’ll be better equipped to choose appropriate models, avoid overfitting and underfitting, and build more effective predictive models in your future statistical analyses!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html",
    "href": "rozdzial3b.html",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "",
    "text": "8.1 Definiowanie Rzetelności i Trafności\nRzetelność odnosi się do spójności pomiaru. Rzetelny pomiar lub badanie daje podobne wyniki w spójnych warunkach.\nTrafność odnosi się do dokładności pomiaru. Trafny pomiar lub badanie dokładnie reprezentuje to, co twierdzi, że mierzy.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#cztery-kombinacje-rzetelności-i-trafności",
    "href": "rozdzial3b.html#cztery-kombinacje-rzetelności-i-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.2 Cztery Kombinacje Rzetelności i Trafności",
    "text": "8.2 Cztery Kombinacje Rzetelności i Trafności\nIstnieją cztery możliwe kombinacje rzetelności i trafności:\n\nWysoka Rzetelność, Wysoka Trafność\nWysoka Rzetelność, Niska Trafność\nNiska Rzetelność, Wysoka Trafność\nNiska Rzetelność, Niska Trafność\n\nPrzyjrzyjmy się każdej z tych kombinacji z przykładami i wizualizacjami.\n\n8.2.1 1. Wysoka Rzetelność, Wysoka Trafność\nTo idealny scenariusz w badaniach. Pomiary są zarówno spójne, jak i dokładne.\nPrzykład: Dobrze skalibrowana waga cyfrowa używana do pomiaru wagi. Konsekwentnie daje ten sam odczyt dla tego samego obiektu i dokładnie reprezentuje prawdziwą wagę.\n\n\n8.2.2 2. Wysoka Rzetelność, Niska Trafność\nW tym przypadku pomiary są spójne, ale niedokładne.\nPrzykład: Źle skalibrowana waga, która zawsze mierzy 5 kg za ciężko. Daje spójne wyniki (wysoka rzetelność), ale nie reprezentuje prawdziwej wagi (niska trafność).\n\n\n8.2.3 3. Niska Rzetelność, Wysoka Trafność\nTutaj pomiary są dokładne średnio, ale niespójne.\nPrzykład: Waga, która waha się wokół prawdziwej wagi. Czasami pokazuje trochę więcej, czasami trochę mniej, ale średnio jest poprawna.\n\n\n8.2.4 4. Niska Rzetelność, Niska Trafność\nTo najgorszy scenariusz, gdzie pomiary nie są ani spójne, ani dokładne.\nPrzykład: Zepsuta waga, która daje losowe odczyty niezwiązane z prawdziwą wagą.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#wizualizacja-rzetelności-i-trafności",
    "href": "rozdzial3b.html#wizualizacja-rzetelności-i-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.3 Wizualizacja Rzetelności i Trafności",
    "text": "8.3 Wizualizacja Rzetelności i Trafności\nAby lepiej zrozumieć te pojęcia, stwórzmy wizualizacje przy użyciu ggplot2 w R. Zasymulujemy dane pomiarowe dla każdego scenariusza i narysujemy je.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generowanie danych dla każdego scenariusza\nn &lt;- 100\nprawdziwa_wartosc &lt;- 50\n\ndane &lt;- tibble(\n  wysoka_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 1),\n  wysoka_rz_niska_tr = rnorm(n, mean = prawdziwa_wartosc + 5, sd = 1),\n  niska_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 5),\n  niska_rz_niska_tr = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenariusz\", values_to = \"pomiar\")\n\n# Tworzenie wykresu punktowego\nwykres_punktowy &lt;- ggplot(dane, aes(x = id, y = pomiar, color = scenariusz)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = prawdziwa_wartosc, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelność, Wysoka Trafność\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelność, Niska Trafność\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelność, Wysoka Trafność\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelność, Niska Trafność\"\n             ))) +\n  labs(title = \"Wykresy punktowe pomiarów\",\n       subtitle = \"Przerywana linia reprezentuje prawdziwą wartość\",\n       x = \"ID pomiaru\",\n       y = \"Zmierzona wartość\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Tworzenie histogramu\nwykres_hist &lt;- ggplot(dane, aes(x = pomiar, fill = scenariusz)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = prawdziwa_wartosc, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelność, Wysoka Trafność\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelność, Niska Trafność\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelność, Wysoka Trafność\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelność, Niska Trafność\"\n             ))) +\n  labs(title = \"Histogramy pomiarów\",\n       subtitle = \"Czerwona przerywana linia reprezentuje prawdziwą wartość\",\n       x = \"Zmierzona wartość\",\n       y = \"Liczba\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Łączenie wykresów\nwykres_polaczony &lt;- wykres_punktowy / wykres_hist +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Rzetelność i Trafność w Pomiarach\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Wyświetlanie połączonego wykresu\nwykres_polaczony\n\n\n\n\n\n\n\n\n\n8.3.1 Interpretacja Wizualizacji\n\nWysoka Rzetelność, Wysoka Trafność: Punkty grupują się ciasno wokół prawdziwej wartości (przerywana linia).\nWysoka Rzetelność, Niska Trafność: Punkty grupują się ciasno, ale konsekwentnie powyżej prawdziwej wartości.\nNiska Rzetelność, Wysoka Trafność: Punkty rozpraszają się szeroko, ale centrują się wokół prawdziwej wartości.\nNiska Rzetelność, Niska Trafność: Punkty rozpraszają się losowo bez wyraźnego wzoru lub relacji do prawdziwej wartości.\n\nZrozumienie rzetelności i trafności jest kluczowe w naukach o danych i badaniach. Wysoka rzetelność zapewnia spójne pomiary, podczas gdy wysoka trafność zapewnia dokładne reprezentacje tego, co zamierzamy zmierzyć. Biorąc pod uwagę oba aspekty, badacze mogą projektować bardziej solidne badania i wyciągać bardziej znaczące wnioski ze swoich danych.\nProwadząc własne badania lub analizując pracę innych, zawsze należy rozważyć: - Jak rzetelne są pomiary? - Jak trafne jest podejście do pomiaru zamierzonego pojęcia? - Czy stosowane metody wspierają zarówno rzetelność, jak i trafność?\nMając na uwadze te pytania, będziesz lepiej przygotowany do prowadzenia i interpretowania wysokiej jakości badań w naukach o danych.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#rodzaje-rzetelności",
    "href": "rozdzial3b.html#rodzaje-rzetelności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.4 Rodzaje Rzetelności",
    "text": "8.4 Rodzaje Rzetelności\nRzetelność można oceniać na kilka sposobów, każdy skupiający się na innym aspekcie spójności:\n\nRzetelność test-retest: Mierzy spójność testu w czasie. Polega na przeprowadzeniu tego samego testu na tej samej grupie uczestników w różnych momentach i porównaniu wyników.\nRzetelność między oceniającymi: Ocenia stopień zgodności między różnymi oceniającymi lub obserwatorami. Jest kluczowa, gdy w zbieraniu danych biorą udział subiektywne osądy.\nSpójność wewnętrzna: Ocenia, jak dobrze różne elementy testu lub skali mierzą ten sam konstrukt. Alfa Cronbacha jest powszechną miarą spójności wewnętrznej.\nRzetelność form równoległych: Polega na stworzeniu dwóch równoważnych form testu i przeprowadzeniu ich na tej samej grupie. Korelacja między dwoma zestawami wyników wskazuje na rzetelność.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#rodzaje-trafności",
    "href": "rozdzial3b.html#rodzaje-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.5 Rodzaje Trafności",
    "text": "8.5 Rodzaje Trafności\nTrafność jest pojęciem wieloaspektowym, z kilkoma rodzajami, które badacze muszą wziąć pod uwagę:\n\nTrafność treściowa: Zapewnia, że pomiar obejmuje wszystkie aspekty konstruktu, który ma mierzyć. Często jest oceniana przez osąd ekspertów.\nTrafność konstrukcyjna: Ocenia, czy test mierzy zamierzony konstrukt teoretyczny. Obejmuje:\n\nTrafność zbieżną: Stopień, w jakim pomiar koreluje z innymi pomiarami tego samego konstruktu.\nTrafność różnicową: Zakres, w jakim pomiar nie koreluje z pomiarami różnych konstruktów.\n\nTrafność kryterialną: Ocenia, jak dobrze pomiar przewiduje wynik. Obejmuje:\n\nTrafność współbieżną: Jak dobrze pomiar koreluje z innymi pomiarami tego samego konstruktu w tym samym czasie.\nTrafność predykcyjną: Jak dobrze pomiar przewiduje przyszłe wyniki.\n\nTrafność fasadowa: Trafność fasadowa odnosi się do tego, jak osoby badane postrzegają test i czy uważają go za odpowiedni do celu, któremu ma służyć. Brak trafności fasadowej może mieć negatywne konsekwencje, nawet jeśli test jest faktycznie trafny (czyli mierzy to, co powinien mierzyć) z punktu widzenia jego zamierzonego celu. Choć nie jest to naukowa miara, może być ważna dla zaangażowania uczestników.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#trafność-wewnętrzna-vs-zewnętrzna",
    "href": "rozdzial3b.html#trafność-wewnętrzna-vs-zewnętrzna",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.6 Trafność Wewnętrzna vs Zewnętrzna",
    "text": "8.6 Trafność Wewnętrzna vs Zewnętrzna\nTe pojęcia są kluczowe w projektowaniu eksperymentów i możliwości uogólniania wyników badań:\n\n8.6.1 Trafność Wewnętrzna\nTrafność wewnętrzna odnosi się do zakresu, w jakim badanie ustanawia związek przyczynowy między zmiennymi niezależnymi a zależnymi. Odpowiada na pytanie: “Czy eksperymentalne traktowanie rzeczywiście spowodowało zaobserwowane efekty?”\nCzynniki, które mogą zagrażać trafności wewnętrznej, obejmują: - Historia: Zewnętrzne wydarzenia występujące między pre-testem a post-testem - Dojrzewanie: Naturalne zmiany u uczestników w czasie - Efekty testowania: Zmiany wynikające z przeprowadzenia pre-testu - Instrumentacja: Zmiany w narzędziu pomiarowym lub obserwatorach - Błąd selekcji: Nielosowy przydział do grup - Utrata: Utrata uczestników podczas badania\n\n\n8.6.2 Trafność Zewnętrzna\nTrafność zewnętrzna odnosi się do zakresu, w jakim wyniki badania mogą być uogólnione na inne sytuacje, populacje lub ustawienia. Odpowiada na pytanie: “W jakim stopniu wyniki mogą być zastosowane poza konkretnym kontekstem badania?”\nCzynniki, które mogą wpływać na trafność zewnętrzną, obejmują: - Trafność populacyjna: Jak dobrze próba reprezentuje szerszą populację - Trafność ekologiczna: Jak dobrze ustawienie badania reprezentuje warunki świata rzeczywistego - Trafność czasowa: Czy wyniki pozostają prawdziwe w czasie",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#spójność-w-badaniach",
    "href": "rozdzial3b.html#spójność-w-badaniach",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.7 Spójność w Badaniach",
    "text": "8.7 Spójność w Badaniach\nSpójność jest ściśle związana z rzetelnością, ale wykracza poza sam pomiar. W badaniach spójność odnosi się do ogólnej koherencji i stabilności wyników w różnych kontekstach, metodach lub badaniach.\nKluczowe aspekty spójności w badaniach obejmują:\n\nReplikowalność: Zdolność do odtworzenia wyników badania przy użyciu tych samych metod i danych.\nOdporność: Stabilność wyników w różnych podejściach analitycznych lub niewielkich zmianach w metodologii.\nKonwergencja: Zbieżność wyników z różnych badań lub metod badających to samo zjawisko.\nSpójność długoterminowa: Stabilność wyników w czasie, szczególnie ważna w badaniach długoterminowych.\n\nZapewnienie spójności w badaniach obejmuje: - Stosowanie standaryzowanych procedur i miar - Dokładne dokumentowanie metod i decyzji analitycznych - Przeprowadzanie badań replikacyjnych - Meta-analizy w celu syntezy wyników z wielu badań",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#równoważenie-rzetelności-trafności-i-spójności",
    "href": "rozdzial3b.html#równoważenie-rzetelności-trafności-i-spójności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.8 Równoważenie Rzetelności, Trafności i Spójności",
    "text": "8.8 Równoważenie Rzetelności, Trafności i Spójności\nChociaż rzetelność, trafność i spójność są kluczowe dla wysokiej jakości badań, czasami wiążą się z kompromisami:\n\nWysoce rzetelna miara może nie mieć trafności, jeśli konsekwentnie mierzy niewłaściwą rzecz.\nDążenie do idealnej trafności wewnętrznej (np. w ściśle kontrolowanych eksperymentach laboratoryjnych) może zmniejszyć trafność zewnętrzną.\nZapewnienie wysokiej spójności w różnych kontekstach może wymagać poświęcenia pewnego stopnia precyzji lub głębi w konkretnych sytuacjach.\n\nBadacze muszą starannie równoważyć te aspekty w oparciu o swoje pytania badawcze i charakter badania. Kompleksowe zrozumienie rzetelności, trafności i spójności pomaga w projektowaniu solidnych badań, dokładnej interpretacji wyników i znaczącym wkładzie do korpusu wiedzy naukowej.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#kompromis-między-obciążeniem-a-wariancją",
    "href": "rozdzial3b.html#kompromis-między-obciążeniem-a-wariancją",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.9 Kompromis między Obciążeniem a Wariancją",
    "text": "8.9 Kompromis między Obciążeniem a Wariancją\nPojęcia rzetelności i trafności są ściśle związane ze statystycznym pojęciem kompromisu między obciążeniem a wariancją. Ten kompromis jest fundamentalny w uczeniu maszynowym i modelowaniu statystycznym.\n\nObciążenie odnosi się do błędu wprowadzonego przez przybliżenie problemu ze świata rzeczywistego uproszczonym modelem. Wysokie obciążenie może prowadzić do niedopasowania.\nWariancja odnosi się do błędu wprowadzonego przez wrażliwość modelu na małe fluktuacje w zbiorze treningowym. Wysoka wariancja może prowadzić do przeuczenia.\n\nZobrazujmy to pojęcie za pomocą uproszczonego wykresu:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_prawdziwa &lt;- sin(x)\ny_niskie_obciazenie_wysoka_wariancja &lt;- y_prawdziwa + rnorm(100, 0, 0.3)\ny_wysokie_obciazenie_niska_wariancja &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_prawdziwa, y_niskie_obciazenie_wysoka_wariancja, y_wysokie_obciazenie_niska_wariancja),\n                 typ = rep(c(\"Prawdziwa Funkcja\", \"Niskie Obciążenie, Wysoka Wariancja\", \"Wysokie Obciążenie, Niska Wariancja\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = typ)) +\n  geom_line() +\n  geom_point(data = subset(df, typ != \"Prawdziwa Funkcja\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Kompromis między Obciążeniem a Wariancją\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Typ Modelu\") +\n  theme_minimal()\n\n\n\n\nWizualizacja kompromisu między obciążeniem a wariancją\n\n\n\n\nNa tym wykresie: - Czarna linia reprezentuje prawdziwą funkcję bazową. - Niebieskie punkty reprezentują model z niskim obciążeniem, ale wysoką wariancją. Średnio podąża blisko prawdziwej funkcji, ale ma dużo szumu. - Czerwona linia reprezentuje model z wysokim obciążeniem, ale niską wariancją. Konsekwentnie niedoszacowuje prawdziwej funkcji, ale ma mniej szumu.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#dokładność-i-precyzja",
    "href": "rozdzial3b.html#dokładność-i-precyzja",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.10 Dokładność i Precyzja",
    "text": "8.10 Dokładność i Precyzja\nPojęcia dokładności i precyzji są ściśle związane z trafnością i rzetelnością:\n\nDokładność odnosi się do tego, jak blisko pomiar jest prawdziwej wartości (podobnie do trafności).\nPrecyzja odnosi się do tego, jak spójne lub powtarzalne są pomiary (podobnie do rzetelności).\n\nMożemy zobrazować te pojęcia za pomocą uproszczonej analogii do tarczy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"Wysoka Dokładność\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Niska Dokładność\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"Wysoka Dokładność\\nNiska Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Niska Dokładność\\nNiska Precyzja\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Dokładność vs Precyzja\")\n\n\n\n\nWizualizacja Dokładności vs Precyzji\n\n\n\n\nW tej wizualizacji: - Wysoka dokładność oznacza, że punkty są blisko środka (dziesiątki). - Wysoka precyzja oznacza, że punkty są ściśle zgrupowane. - Każdy panel reprezentuje inną kombinację dokładności i precyzji.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#podsumowanie",
    "href": "rozdzial3b.html#podsumowanie",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.11 Podsumowanie",
    "text": "8.11 Podsumowanie\nZrozumienie rzetelności i trafności jest kluczowe dla prowadzenia solidnych badań. Pojęcia te pomagają nam zapewnić, że nasze pomiary są zarówno spójne, jak i dokładne. Łącząc je z ideami takimi jak kompromis między obciążeniem a wariancją oraz dokładnością i precyzją, zyskujemy głębsze zrozumienie wyzwań związanych z pomiarem i modelowaniem w badaniach naukowych. Jako badacze musimy dążyć do opracowania miar i modeli, które są zarówno rzetelne, jak i trafne, równoważąc kompromisy między obciążeniem a wariancją oraz między dokładnością a precyzją. Wymaga to starannego projektowania metodologii badań, rygorystycznego testowania naszych instrumentów pomiarowych i przemyślanej interpretacji naszych wyników.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#zrozumienie-obciążenia-vs.-wariancji-w-pomiarach-statystycznych",
    "href": "rozdzial3b.html#zrozumienie-obciążenia-vs.-wariancji-w-pomiarach-statystycznych",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.12 Zrozumienie Obciążenia vs. Wariancji w Pomiarach Statystycznych",
    "text": "8.12 Zrozumienie Obciążenia vs. Wariancji w Pomiarach Statystycznych\n\n8.12.1 Wprowadzenie\nW statystyce i uczeniu maszynowym dwa ważne pojęcia, które wpływają na wydajność naszych modeli, to obciążenie (bias) i wariancja (variance). Zrozumienie tych pojęć jest kluczowe dla budowania efektywnych modeli predykcyjnych i unikania typowych pułapek, takich jak przeuczenie i niedouczenie.\n\nObciążenie odnosi się do błędu wprowadzonego przez przybliżenie rzeczywistego problemu, który może być złożony, za pomocą uproszczonego modelu. Wysokie obciążenie może prowadzić do niedouczenia.\n\nWyobraź sobie obciążenie jako średnią odległość naszych przewidywań od prawdziwych wartości.\nW kontekście trafności, wysokie obciążenie oznacza, że nasz model nie uchwycił prawdziwej zależności w danych.\n\nWariancja odnosi się do tego, jak bardzo nasz model zmieniłby się, gdybyśmy oszacowali go przy użyciu innego zbioru treningowego. Wysoka wariancja może prowadzić do przeuczenia.\n\nWyobraź sobie wariancję jako to, jak bardzo nasze przewidywania wahałyby się, gdybyśmy użyli różnych zbiorów danych.\nW kontekście rzetelności, wysoka wariancja oznacza, że nasz model jest zbyt wrażliwy na konkretne dane, na których został wytrenowany.\n\n\nZbadamy cztery scenariusze, aby zilustrować różne kombinacje obciążenia i wariancji przy użyciu syntetycznych danych i modeli regresji.\n\n\n8.12.2 Funkcja Generowania Danych i Dopasowywania Modelu\nNajpierw stwórzmy funkcję, która pomoże nam generować dane i dopasowywać modele dla każdego scenariusza:\n\ngeneruj_i_dopasuj &lt;- function(n, wyraz_wolny, nachylenie, odch_szumu, stopien_modelu) {\n  # Generowanie syntetycznych danych\n  x &lt;- runif(n, 0, 10)\n  y &lt;- wyraz_wolny + nachylenie * x + rnorm(n, 0, odch_szumu)\n  \n  # Dopasowanie modelu\n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", stopien_modelu, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  # Generowanie przewidywań\n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  # Wykres\n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    geom_abline(intercept = wyraz_wolny, slope = nachylenie, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Stopień Modelu:\", stopien_modelu),\n         subtitle = paste(\"Odchylenie Standardowe Szumu:\", odch_szumu),\n         x = \"X (Zmienna Wejściowa)\",\n         y = \"Y (Zmienna Docelowa)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nTa funkcja wykonuje następujące czynności: 1. Generuje syntetyczne dane na podstawie naszych parametrów 2. Dopasowuje model regresji wielomianowej 3. Tworzy wykres pokazujący prawdziwą zależność (niebieska przerywana linia), przewidywania naszego modelu (czerwona ciągła linia) i punkty danych\nTeraz zbadajmy nasze cztery scenariusze!\n\n\n8.12.3 Scenariusz 1: Niskie Obciążenie, Niska Wariancja\nW tym idealnym scenariuszu używamy modelu liniowego do dopasowania danych liniowych z niskim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 1, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model (czerwona linia) ściśle podąża za prawdziwą zależnością (niebieska przerywana linia). - Punkty danych są skupione blisko linii, co wskazuje na niski szum. - Ten scenariusz reprezentuje dobre dopasowanie: model uchwycił podstawowy trend bez nadmiernej złożoności.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#scenariusz-2-niskie-obciążenie-wysoka-wariancja",
    "href": "rozdzial3b.html#scenariusz-2-niskie-obciążenie-wysoka-wariancja",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.13 Scenariusz 2: Niskie Obciążenie, Wysoka Wariancja",
    "text": "8.13 Scenariusz 2: Niskie Obciążenie, Wysoka Wariancja\nTutaj używamy modelu liniowego do dopasowania danych liniowych, ale z wysokim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 5, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model nadal uchwycił ogólny trend, ale punkty danych są bardziej rozproszone. - Ta wysoka wariancja oznacza, że przewidywania naszego modelu byłyby mniej wiarygodne. - W rzeczywistych warunkach mogłoby to reprezentować sytuację, w której nasze pomiary są średnio poprawne, ale mają dużo losowego błędu.\n\n8.13.1 Scenariusz 3: Wysokie Obciążenie, Niska Wariancja\nW tym przypadku używamy modelu liniowego do dopasowania danych kwadratowych (zakrzywionych) z niskim szumem.\n\ndane_kwadratowe &lt;- function(n, wyraz_wolny, nachylenie, odch_szumu, stopien_modelu) {\n  x &lt;- runif(n, 0, 10)\n  y &lt;- wyraz_wolny + nachylenie * x^2 + rnorm(n, 0, odch_szumu)\n  \n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", stopien_modelu, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    stat_function(fun = function(x) wyraz_wolny + nachylenie * x^2, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Stopień Modelu:\", stopien_modelu),\n         subtitle = paste(\"Odchylenie Standardowe Szumu:\", odch_szumu),\n         x = \"X (Zmienna Wejściowa)\",\n         y = \"Y (Zmienna Docelowa)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\ndane_kwadratowe(n = 100, wyraz_wolny = 1, nachylenie = 0.2, odch_szumu = 1, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model liniowy (czerwona linia) nie uchwycił krzywizny prawdziwej zależności (niebieska przerywana linia). - To wysokie obciążenie oznacza, że nasz model konsekwentnie myli się w swoich przewidywaniach. - W rzeczywistych warunkach mogłoby to reprezentować użycie zbyt uproszczonego modelu dla złożonego zjawiska.\n\n\n8.13.2 Scenariusz 4: Wysokie Obciążenie, Wysoka Wariancja\nNa koniec używamy wielomianu wysokiego stopnia do dopasowania danych liniowych z wysokim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 5, stopien_modelu = 5)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model (czerwona linia) jest zbyt złożony, próbując dopasować się do szumu zamiast do podstawowego trendu. - Ta kombinacja wysokiego obciążenia i wysokiej wariancji prowadzi do słabej generalizacji. - W rzeczywistych warunkach mogłoby to reprezentować nadmierne skomplikowanie naszej analizy i wyciąganie fałszywych wniosków z losowych fluktuacji w naszych danych.\n\n\n8.13.3 Podsumowanie\nZrozumienie kompromisu między obciążeniem a wariancją jest kluczowe w modelowaniu statystycznym:\n\nNiskie Obciążenie, Niska Wariancja: Idealny scenariusz, w którym nasz model dokładnie uchwycił podstawową zależność bez nadmiernej wrażliwości na szum.\nNiskie Obciążenie, Wysoka Wariancja: Nasz model jest średnio poprawny, ale niewiarygodny ze względu na wysoką wrażliwość na pojedyncze punkty danych.\nWysokie Obciążenie, Niska Wariancja: Nasz model jest konsekwentnie błędny z powodu nadmiernego uproszczenia, ale daje stabilne przewidywania.\nWysokie Obciążenie, Wysoka Wariancja: Najgorszy scenariusz, w którym nasz model jest zarówno niedokładny, jak i niewiarygodny.\n\nW praktyce często musimy zrównoważyć obciążenie i wariancję. Techniki takie jak walidacja krzyżowa, regularyzacja i metody zespołowe mogą pomóc w znalezieniu tej równowagi.\nPamiętaj: - Model z wysokim obciążeniem jest zbyt prosty i pomija ważne wzorce w danych. - Model z wysoką wariancją jest zbyt złożony i dopasowuje się do szumu w danych treningowych. - Celem jest znalezienie złotego środka, który uchwyci prawdziwe wzorce bez nadmiernego dopasowania do szumu.\nZrozumienie tych pojęć pomoże ci lepiej wybierać odpowiednie modele, unikać przeuczenia i niedouczenia oraz budować bardziej efektywne modele predykcyjne w przyszłych analizach statystycznych!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "",
    "text": "9.1 Introduction\nResearch designs are fundamental to the scientific process, providing structured approaches to investigate hypotheses and answer research questions. This chapter explores two main categories of research designs: experimental and non-experimental, with a focus on the Neyman-Rubin potential outcome framework. We’ll delve into various design types, their characteristics, and provide practical examples using R for data analysis and visualization.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#experimental-designs",
    "href": "chapter4.html#experimental-designs",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.2 Experimental Designs",
    "text": "9.2 Experimental Designs\nExperimental designs are characterized by the researcher’s control over the independent variable(s) and random assignment of subjects to different conditions. These designs are considered the gold standard for establishing causal relationships.\n\n9.2.1 Randomized Controlled Trials (RCTs)\nRCTs are the most rigorous form of experimental design. They involve:\n\nRandom assignment of subjects to treatment and control groups\nManipulation of the independent variable\nMeasurement of the dependent variable\n\nLet’s visualize a simple RCT design:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Create sample data\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  group = factor(rep(c(\"Control\", \"Treatment\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Simulate treatment effect\ndata$post_test &lt;- ifelse(data$group == \"Treatment\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Reshape data for plotting\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"time\", values_to = \"score\")\n\n# Create plot\nggplot(data_long, aes(x = time, y = score, color = group, group = interaction(id, group))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = group), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Pre-test and Post-test Scores in RCT\",\n       x = \"Time\", y = \"Score\", color = \"Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nRandomized Controlled Trial Design\n\n\n\n\nThis plot shows individual trajectories and group means for pre-test and post-test scores in a hypothetical RCT. The treatment group shows a clear increase in scores compared to the control group.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "href": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.3 A/B Testing: An Example and Comparison with RCTs",
    "text": "9.3 A/B Testing: An Example and Comparison with RCTs\nA/B testing is a widely used experimental method in digital marketing, user experience design, and product development. This chapter will present an example of A/B testing, explain its methodology, and discuss how it differs from Randomized Controlled Trials (RCTs).\n\n9.3.1 Example: Website Landing Page Conversion Rate\nLet’s consider an example where an e-commerce company wants to improve the conversion rate of their landing page. They decide to test two different layouts: the current layout (A) and a new layout (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Simulate data\nn_visitors &lt;- 10000\ndata &lt;- data.frame(\n  Version = sample(c(\"A\", \"B\"), n_visitors, replace = TRUE),\n  Converted = rbinom(n_visitors, 1, ifelse(sample(c(\"A\", \"B\"), n_visitors, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Calculate conversion rates\nconversion_rates &lt;- data %&gt;%\n  group_by(Version) %&gt;%\n  summarise(\n    Visitors = n(),\n    Conversions = sum(Converted),\n    ConversionRate = mean(Converted)\n  )\n\n# Visualize results\nggplot(conversion_rates, aes(x = Version, y = ConversionRate, fill = Version)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", ConversionRate * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"A/B Test: Landing Page Conversion Rates\",\n       x = \"Page Version\", y = \"Conversion Rate\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure 9.1: A/B Test Results: Landing Page Conversion Rates\n\n\n\n\n\nIn this example, we simulated data for 10,000 visitors randomly assigned to either version A or B of the landing page. The results show that version B has a slightly higher conversion rate (11.44%) compared to version A (10.94%).\n\n\n9.3.2 A/B Testing Methodology\nA/B testing typically follows these steps:\n\nIdentify the element to be tested (e.g., landing page layout).\nCreate two versions: the control (A) and the variant (B).\nRandomly assign visitors to either version.\nCollect data on the metric of interest (e.g., conversion rate).\nAnalyze the results using statistical methods.\nMake a decision based on the results.\n\n\n\n9.3.3 Differences between A/B Testing and RCTs\nWhile A/B testing and Randomized Controlled Trials (RCTs) share some similarities, they have several key differences:\n\nScope and Context:\n\nA/B Testing: Typically used in digital environments for quick, iterative improvements.\nRCTs: Used in various fields, including medicine, psychology, and social sciences, often for more complex interventions.\n\nDuration:\n\nA/B Testing: Usually shorter, often running for days or weeks.\nRCTs: Can last months or years, especially in medical research.\n\nSample Size:\n\nA/B Testing: Can involve very large sample sizes due to ease of implementation in digital platforms.\nRCTs: Sample sizes are often smaller due to practical and cost constraints.\n\nBlinding:\n\nA/B Testing: Participants are usually unaware they’re part of a test.\nRCTs: May involve single, double, or triple blinding to reduce bias.\n\nEthical Considerations:\n\nA/B Testing: Generally involves low-risk changes with minimal ethical concerns.\nRCTs: Often require extensive ethical review, especially in medical contexts.\n\nOutcome Measures:\n\nA/B Testing: Typically focuses on a single, easily measurable outcome (e.g., click-through rate).\nRCTs: Often measure multiple outcomes, including potential side effects or long-term impacts.\n\nGeneralizability:\n\nA/B Testing: Results are often specific to the platform or context tested.\nRCTs: Aim for broader generalizability, though this can vary.\n\nAnalysis Complexity:\n\nA/B Testing: Often uses simpler statistical analyses.\nRCTs: May involve more complex statistical methods to account for various factors.\n\n\nA/B testing is a powerful tool for making data-driven decisions in digital environments. While it shares the fundamental principle of randomization with RCTs, it is typically simpler, faster, and more focused on specific, measurable outcomes in digital contexts. Understanding these differences helps researchers and practitioners choose the most appropriate method for their specific needs and constraints.\n\n\n9.3.4 Example 1: Effect of Sleep Duration on Cognitive Performance\nResearch Question: Does increasing sleep duration improve cognitive performance in college students?\n\n# Generating sample data\nset.seed(456)\nn &lt;- 100\npre_experimental &lt;- rnorm(n, mean = 70, sd = 10)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = 8, sd = 5)\npre_control &lt;- rnorm(n, mean = 70, sd = 10)\npost_control &lt;- pre_control + rnorm(n, mean = 1, sd = 5)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Experimental\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  Score = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = Score, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Effect of Increased Sleep Duration on Cognitive Performance\") +\n  xlab(\"Time\") +\n  ylab(\"Cognitive Performance Score\")\n\n\n\n\n\n\n\nFigure 9.2: Effect of Sleep Duration on Cognitive Performance\n\n\n\n\n\n\n9.3.4.1 Interpretation\nThis plot demonstrates the effect of increased sleep duration on cognitive performance. The experimental group, which increased their sleep duration, shows a more substantial improvement in cognitive performance compared to the control group. This suggests that increasing sleep duration may positively impact cognitive abilities in college students.\n\n\n\n9.3.5 Example 2: Impact of Mindfulness Training on Stress Levels\nResearch Question: Can a short-term mindfulness training program reduce stress levels in healthcare workers?\n\n# Generating sample data\nset.seed(789)\nn &lt;- 120\npre_experimental &lt;- rnorm(n, mean = 60, sd = 15)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = -12, sd = 8)\npre_control &lt;- rnorm(n, mean = 60, sd = 15)\npost_control &lt;- pre_control + rnorm(n, mean = -2, sd = 6)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Mindfulness\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  StressScore = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = StressScore, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Impact of Mindfulness Training on Stress Levels\") +\n  xlab(\"Time\") +\n  ylab(\"Stress Score\")\n\n\n\n\n\n\n\nFigure 9.3: Impact of Mindfulness Training on Stress Levels\n\n\n\n\n\n\n9.3.5.1 Interpretation\nThis visualization illustrates the impact of a mindfulness training program on stress levels in healthcare workers. The mindfulness group shows a more significant decrease in stress scores compared to the control group. This suggests that the mindfulness training program may be effective in reducing stress levels among healthcare workers.\nWhen interpreting such results, it’s important to consider:\n\nThe magnitude of the change in each group\nThe difference in change between the experimental and control groups\nThe variability within each group\nAny potential confounding factors not accounted for in the experimental design\n\nThese examples provide a template for visualizing and interpreting similar experimental designs across different research contexts.\n\n\n\n9.3.6 Factorial Designs\nFactorial designs allow researchers to study the effects of multiple independent variables simultaneously. They are efficient and can reveal interaction effects between variables.\nExample of a 2x2 factorial design:\n\n# Create sample data for 2x2 factorial design\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  factor_a = rep(rep(c(\"Low\", \"High\"), each = n_per_group), 2),\n  factor_b = rep(c(\"Control\", \"Treatment\"), each = n_per_group * 2),\n  outcome = NA\n)\n\n# Generate outcomes\nfactorial_data$outcome &lt;- ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Control\",\n                                 rnorm(n_per_group, 40, 5),\n                                 ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Treatment\",\n                                        rnorm(n_per_group, 45, 5),\n                                        ifelse(factorial_data$factor_a == \"High\" & factorial_data$factor_b == \"Control\",\n                                               rnorm(n_per_group, 50, 5),\n                                               rnorm(n_per_group, 60, 5))))\n\n# Create plot\nggplot(factorial_data, aes(x = factor_b, y = outcome, fill = factor_a)) +\n  geom_boxplot() +\n  facet_wrap(~factor_a, scales = \"free_x\") +\n  labs(title = \"2x2 Factorial Design\",\n       x = \"Factor B\", y = \"Outcome\", fill = \"Factor A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n2x2 Factorial Design\n\n\n\n\nThis plot illustrates a 2x2 factorial design, showing the effects of two factors (A and B) on the outcome variable. We can observe main effects for both factors and a potential interaction effect.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#non-experimental-designs",
    "href": "chapter4.html#non-experimental-designs",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.4 Non-Experimental Designs",
    "text": "9.4 Non-Experimental Designs\nNon-experimental designs are used when randomization or manipulation of variables is not possible or ethical. They include observational/descriptive studies and quasi-experimental designs.\n\n9.4.1 Observational Studies\nObservational studies involve collecting data without manipulating variables. They are useful for exploring relationships and generating hypotheses.\nExample: Correlation study\n\nset.seed(789)\nn &lt;- 100\nstudy_time &lt;- runif(n, 0, 10)\nexam_score &lt;- 50 + 5 * study_time + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(study_time, exam_score)\n\nggplot(correlation_data, aes(x = study_time, y = exam_score)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Correlation between Study Time and Exam Score\",\n       x = \"Study Time (hours)\", y = \"Exam Score\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCorrelation between Study Time and Exam Score\n\n\n\n\nThis scatter plot shows the relationship between study time and exam scores, illustrating a positive correlation typical in observational studies.\n\n\n9.4.2 Quasi-Experimental Designs\nQuasi-experimental designs lack random assignment but attempt to establish causal relationships. Common types include:\n\nDifference-in-Differences (DiD)\nRegression Discontinuity Design (RDD)\n\n\n9.4.2.1 Difference-in-Differences (DiD)\nDiD is used to estimate treatment effects by comparing the average change over time in the outcome variable for the treatment group to the average change over time for the control group.\nLet’s simulate a DiD analysis using the plm package:\n\nlibrary(plm)\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\nDifference-in-Differences Analysis\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nThe plot shows the average outcomes for treatment and control groups over time. The vertical dashed line indicates the intervention point. The DiD estimate is the difference between the two groups’ changes from pre- to post-intervention periods.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt’s important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\n9.4.2.2 Regression Discontinuity Design (RDD)\nRDD is used when treatment assignment is determined by a cutoff value on a continuous variable. It compares observations just above and below the cutoff to estimate the treatment effect.\nLet’s implement an RDD analysis using the rdrobust package:\n\nlibrary(rdrobust)\n\n# Generate synthetic RDD data\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# RDD analysis\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     4.092     0.231    17.723     0.000     [3.640 , 4.545]     \n        Robust         -         -    15.013     0.000     [3.600 , 4.680]     \n=============================================================================\n\n# Visualize RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regression Discontinuity Design\",\n       x = \"Running Variable\", y = \"Outcome\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nRegression Discontinuity Design Analysis\n\n\n\n\nThe plot shows the discontinuity at the cutoff point (x = 0), with separate regression lines fitted on either side. The treatment effect is estimated by the gap between these lines at the cutoff.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "href": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.5 The Neyman-Rubin Potential Outcome Framework",
    "text": "9.5 The Neyman-Rubin Potential Outcome Framework\nThe Neyman-Rubin potential outcome framework provides a formal approach to causal inference. It introduces the concept of potential outcomes: for each unit, we consider the outcome under treatment and the outcome under control, even though we can only observe one in reality.\nKey concepts:\n\nPotential Outcomes: \\(Y_i(1)\\) and \\(Y_i(0)\\) for treatment and control, respectively.\nObserved Outcome: \\(Y_i = Y_i(1)T_i + Y_i(0)(1-T_i)\\), where \\(T_i\\) is the treatment indicator.\nIndividual Treatment Effect: \\(\\tau_i = Y_i(1) - Y_i(0)\\)\nAverage Treatment Effect (ATE): \\(E[\\tau_i] = E[Y_i(1) - Y_i(0)]\\)\n\nThe framework emphasizes the “fundamental problem of causal inference”: we can never observe both potential outcomes for a single unit simultaneously.\n\n9.5.1 Example: Estimating ATE in an RCT\nIn an RCT, random assignment ensures that treatment is independent of potential outcomes, allowing unbiased estimation of the ATE:\n\\[\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\\]\nWhere \\(n_1\\) and \\(n_0\\) are the numbers of treated and control units, respectively.\n\n# Using the RCT data from earlier\nate_estimate &lt;- mean(data$post_test[data$group == \"Treatment\"]) - \n                mean(data$post_test[data$group == \"Control\"])\n\nWarning in mean.default(data$post_test[data$group == \"Treatment\"]): argument is\nnot numeric or logical: returning NA\n\n\nWarning in mean.default(data$post_test[data$group == \"Control\"]): argument is\nnot numeric or logical: returning NA\n\ncat(\"Estimated Average Treatment Effect:\", round(ate_estimate, 2))\n\nEstimated Average Treatment Effect: NA\n\n\nThis estimate represents the causal effect of the treatment under the assumptions of the potential outcome framework.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#conclusion",
    "href": "chapter4.html#conclusion",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.6 Conclusion",
    "text": "9.6 Conclusion\nThis chapter has explored various research designs, from experimental approaches like RCTs and factorial designs to non-experimental methods such as observational studies and quasi-experimental designs. We’ve demonstrated how to implement and visualize these designs using R, and introduced the Neyman-Rubin potential outcome framework for causal inference.\nUnderstanding these designs and their appropriate use is crucial for conducting rigorous research and drawing valid causal conclusions. Each design has its strengths and limitations, and the choice of design should be guided by the research question, ethical considerations, and practical constraints.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#references",
    "href": "chapter4.html#references",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.7 References",
    "text": "9.7 References\n\nImbens, G. W., & Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press.\nAngrist, J. D., & Pischke, J. S. (2008). Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton University Press.\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and Quasi-Experimental Designs for Generalized Causal Inference. Houghton Mifflin.\nCunningham, S. (2021). Causal Inference: The Mixtape. Yale University Press.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html",
    "href": "rozdzial4.html",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "",
    "text": "10.1 Wstęp\nProjekty badawcze stanowią fundament procesu naukowego, zapewniając ustrukturyzowane podejście do badania hipotez i odpowiadania na pytania badawcze. Ten rozdział analizuje dwie główne kategorie projektów badawczych: eksperymentalne i nieeksperymentalne, ze szczególnym uwzględnieniem modelu potencjalnych wyników Neymana-Rubina. Zagłębimy się w różne typy projektów, ich charakterystykę i przedstawimy praktyczne przykłady wykorzystania R do analizy danych i wizualizacji.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-eksperymentalne",
    "href": "rozdzial4.html#projekty-eksperymentalne",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.2 Projekty Eksperymentalne",
    "text": "10.2 Projekty Eksperymentalne\nProjekty eksperymentalne charakteryzują się kontrolą badacza nad zmienną(ymi) niezależną(ymi) oraz losowym przydziałem uczestników do różnych warunków. Te projekty są uważane za złoty standard w ustalaniu związków przyczynowych.\n\n10.2.1 Randomizowane Badania Kontrolowane (RCT)\nRCT są najbardziej rygorystyczną formą projektu eksperymentalnego. Obejmują one:\n\nLosowy przydział uczestników do grup eksperymentalnej i kontrolnej\nManipulację zmienną niezależną\nPomiar zmiennej zależnej\n\nZobaczmy wizualizację prostego projektu RCT:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Tworzenie przykładowych danych\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  grupa = factor(rep(c(\"Kontrolna\", \"Eksperymentalna\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Symulacja efektu leczenia\ndata$post_test &lt;- ifelse(data$grupa == \"Eksperymentalna\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Przekształcenie danych do formatu długiego\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"czas\", values_to = \"wynik\")\n\n# Tworzenie wykresu\nggplot(data_long, aes(x = czas, y = wynik, color = grupa, group = interaction(id, grupa))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = grupa), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Wyniki Pre-test i Post-test w RCT\",\n       x = \"Czas\", y = \"Wynik\", color = \"Grupa\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_x_discrete(labels = c(\"pre_test\" = \"Pre-test\", \"post_test\" = \"Post-test\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nProjekt Randomizowanego Badania Kontrolowanego\n\n\n\n\nTen wykres pokazuje indywidualne trajektorie i średnie grupowe dla wyników pre-test i post-test w hipotetycznym RCT. Grupa eksperymentalna wykazuje wyraźny wzrost wyników w porównaniu do grupy kontrolnej.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#testy-ab-przykład-i-porównanie-z-rct",
    "href": "rozdzial4.html#testy-ab-przykład-i-porównanie-z-rct",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.3 Testy A/B: Przykład i Porównanie z RCT",
    "text": "10.3 Testy A/B: Przykład i Porównanie z RCT\nTesty A/B to szeroko stosowana metoda eksperymentalna w marketingu cyfrowym, projektowaniu doświadczeń użytkownika i rozwoju produktów. Ten rozdział przedstawi przykład testu A/B, wyjaśni jego metodologię i omówi, czym różni się od Randomizowanych Badań Kontrolowanych (RCT).\n\n10.3.1 Przykład: Współczynnik Konwersji Strony Docelowej\nRozważmy przykład, w którym firma e-commerce chce poprawić współczynnik konwersji swojej strony docelowej. Decydują się przetestować dwa różne układy: obecny układ (A) i nowy układ (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Symulacja danych\nn_odwiedzajacych &lt;- 10000\ndane &lt;- data.frame(\n  Wersja = sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE),\n  Konwersja = rbinom(n_odwiedzajacych, 1, ifelse(sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Obliczenie współczynników konwersji\nwspolczynniki_konwersji &lt;- dane %&gt;%\n  group_by(Wersja) %&gt;%\n  summarise(\n    Odwiedzajacy = n(),\n    Konwersje = sum(Konwersja),\n    WspolczynnikKonwersji = mean(Konwersja)\n  )\n\n# Wizualizacja wyników\nggplot(wspolczynniki_konwersji, aes(x = Wersja, y = WspolczynnikKonwersji, fill = Wersja)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", WspolczynnikKonwersji * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"Test A/B: Współczynniki Konwersji Strony Docelowej\",\n       x = \"Wersja Strony\", y = \"Współczynnik Konwersji\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure 10.1: Wyniki Testu A/B: Współczynniki Konwersji Strony Docelowej\n\n\n\n\n\nW tym przykładzie zasymulowaliśmy dane dla 10 000 odwiedzających losowo przypisanych do wersji A lub B strony docelowej. Wyniki pokazują, że wersja B ma nieco wyższy współczynnik konwersji (11,44%) w porównaniu do wersji A (10,94%).\n\n\n10.3.2 Metodologia Testów A/B\nTesty A/B zazwyczaj przebiegają według następujących kroków:\n\nZidentyfikowanie elementu do przetestowania (np. układ strony docelowej).\nStworzenie dwóch wersji: kontrolnej (A) i wariantu (B).\nLosowe przypisanie odwiedzających do jednej z wersji.\nZbieranie danych o interesującej nas metryce (np. współczynniku konwersji).\nAnaliza wyników przy użyciu metod statystycznych.\nPodjęcie decyzji na podstawie wyników.\n\n\n\n10.3.3 Różnice między Testami A/B a RCT\nChoć testy A/B i Randomizowane Badania Kontrolowane (RCT) mają pewne podobieństwa, istnieje kilka kluczowych różnic:\n\nZakres i Kontekst:\n\nTesty A/B: Zazwyczaj stosowane w środowiskach cyfrowych do szybkich, iteracyjnych ulepszeń.\nRCT: Stosowane w różnych dziedzinach, w tym medycynie, psychologii i naukach społecznych, często dla bardziej złożonych interwencji.\n\nCzas Trwania:\n\nTesty A/B: Zwykle krótsze, często trwające dni lub tygodnie.\nRCT: Mogą trwać miesiące lub lata, szczególnie w badaniach medycznych.\n\nWielkość Próby:\n\nTesty A/B: Mogą obejmować bardzo duże próby ze względu na łatwość implementacji na platformach cyfrowych.\nRCT: Wielkości prób są często mniejsze ze względu na praktyczne i kosztowe ograniczenia.\n\nZaślepienie:\n\nTesty A/B: Uczestnicy zazwyczaj nie są świadomi, że biorą udział w teście.\nRCT: Mogą obejmować pojedyncze, podwójne lub potrójne zaślepienie w celu zmniejszenia błędu systematycznego.\n\nWzględy Etyczne:\n\nTesty A/B: Generalnie obejmują zmiany niskiego ryzyka z minimalnymi obawami etycznymi.\nRCT: Często wymagają obszernej oceny etycznej, szczególnie w kontekście medycznym.\n\nMiary Wyników:\n\nTesty A/B: Zazwyczaj skupiają się na pojedynczym, łatwo mierzalnym wyniku (np. współczynnik klikalności).\nRCT: Często mierzą wiele wyników, w tym potencjalne skutki uboczne lub długoterminowe efekty.\n\nMożliwość Uogólnienia:\n\nTesty A/B: Wyniki są często specyficzne dla testowanej platformy lub kontekstu.\nRCT: Dążą do szerszej możliwości uogólnienia, choć może to się różnić.\n\nZłożoność Analizy:\n\nTesty A/B: Często wykorzystują prostsze analizy statystyczne.\nRCT: Mogą obejmować bardziej złożone metody statystyczne, aby uwzględnić różne czynniki.\n\n\nTesty A/B są potężnym narzędziem do podejmowania decyzji opartych na danych w środowiskach cyfrowych. Choć dzielą podstawową zasadę randomizacji z RCT, są zazwyczaj prostsze, szybsze i bardziej skoncentrowane na konkretnych, mierzalnych wynikach w kontekstach cyfrowych. Zrozumienie tych różnic pomaga badaczom i praktykom wybrać najbardziej odpowiednią metodę do ich konkretnych potrzeb i ograniczeń.\nTesty A/B są szczególnie przydatne w optymalizacji stron internetowych, aplikacji mobilnych i kampanii marketingowych, gdzie szybkie iteracje i ciągłe ulepszenia są kluczowe. Z kolei RCT pozostają złotym standardem w badaniach naukowych, szczególnie w dziedzinach takich jak medycyna, gdzie rygorystyczna kontrola i długoterminowa obserwacja są niezbędne.\nNiezależnie od wybranej metody, kluczowe jest staranne planowanie, precyzyjne wykonanie i ostrożna interpretacja wyników. Zarówno testy A/B, jak i RCT, gdy są odpowiednio stosowane, mogą dostarczyć cennych informacji i przyczynić się do podejmowania lepszych decyzji opartych na danych.\n\n\n10.3.4 Przykład 1: Wpływ Długości Snu na Wydajność Poznawczą\nPytanie Badawcze: Czy zwiększenie długości snu poprawia wydajność poznawczą u studentów?\n\n# Generowanie przykładowych danych\nset.seed(456)\nn &lt;- 100\npre_eksperymentalna &lt;- rnorm(n, mean = 70, sd = 10)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = 8, sd = 5)\npre_kontrolna &lt;- rnorm(n, mean = 70, sd = 10)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = 1, sd = 5)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Eksperymentalna\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  Wynik = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = Wynik, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wpływ Zwiększonej Długości Snu na Wydajność Poznawczą\") +\n  xlab(\"Czas\") +\n  ylab(\"Wynik Wydajności Poznawczej\")\n\n\n\n\n\n\n\nFigure 10.2: Wpływ Długości Snu na Wydajność Poznawczą\n\n\n\n\n\n\n10.3.4.1 Interpretacja\nTen wykres pokazuje wpływ zwiększonej długości snu na wydajność poznawczą. Grupa eksperymentalna, która zwiększyła długość snu, wykazuje znacznie większą poprawę w wydajności poznawczej w porównaniu do grupy kontrolnej. Sugeruje to, że zwiększenie długości snu może pozytywnie wpływać na zdolności poznawcze studentów.\n\n\n\n10.3.5 Przykład 2: Wpływ Treningu Uważności na Poziom Stresu\nPytanie Badawcze: Czy krótkoterminowy program treningu uważności może obniżyć poziom stresu u pracowników służby zdrowia?\n\n# Generowanie przykładowych danych\nset.seed(789)\nn &lt;- 120\npre_eksperymentalna &lt;- rnorm(n, mean = 60, sd = 15)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = -12, sd = 8)\npre_kontrolna &lt;- rnorm(n, mean = 60, sd = 15)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = -2, sd = 6)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Uważność\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  PoziomStresu = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = PoziomStresu, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wpływ Treningu Uważności na Poziom Stresu\") +\n  xlab(\"Czas\") +\n  ylab(\"Poziom Stresu\")\n\n\n\n\n\n\n\nFigure 10.3: Wpływ Treningu Uważności na Poziom Stresu\n\n\n\n\n\n\n10.3.5.1 Interpretacja\nTa wizualizacja ilustruje wpływ programu treningu uważności na poziom stresu u pracowników służby zdrowia. Grupa uważności wykazuje znacznie większy spadek poziomu stresu w porównaniu do grupy kontrolnej. Sugeruje to, że program treningu uważności może być skuteczny w redukcji poziomu stresu wśród pracowników służby zdrowia.\n\n\n\n10.3.6 Projekty Czynnikowe\nProjekty czynnikowe pozwalają badaczom na jednoczesne badanie efektów wielu zmiennych niezależnych. Są one efektywne i mogą ujawniać efekty interakcji między zmiennymi.\nPrzykład projektu czynnikowego 2x2:\n\n# Tworzenie przykładowych danych dla projektu czynnikowego 2x2\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  czynnik_a = rep(rep(c(\"Niski\", \"Wysoki\"), each = n_per_group), 2),\n  czynnik_b = rep(c(\"Kontrola\", \"Interwencja\"), each = n_per_group * 2),\n  wynik = NA\n)\n\n# Generowanie wyników\nfactorial_data$wynik &lt;- ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Kontrola\",\n                               rnorm(n_per_group, 40, 5),\n                               ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Interwencja\",\n                                      rnorm(n_per_group, 45, 5),\n                                      ifelse(factorial_data$czynnik_a == \"Wysoki\" & factorial_data$czynnik_b == \"Kontrola\",\n                                             rnorm(n_per_group, 50, 5),\n                                             rnorm(n_per_group, 60, 5))))\n\n# Tworzenie wykresu\nggplot(factorial_data, aes(x = czynnik_b, y = wynik, fill = czynnik_a)) +\n  geom_boxplot() +\n  facet_wrap(~czynnik_a, scales = \"free_x\") +\n  labs(title = \"Projekt Czynnikowy 2x2\",\n       x = \"Czynnik B\", y = \"Wynik\", fill = \"Czynnik A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nProjekt Czynnikowy 2x2\n\n\n\n\nTen wykres ilustruje projekt czynnikowy 2x2, pokazując efekty dwóch czynników (A i B) na zmienną wynikową. Możemy zaobserwować główne efekty dla obu czynników oraz potencjalny efekt interakcji.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-nieeksperymentalne",
    "href": "rozdzial4.html#projekty-nieeksperymentalne",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.4 Projekty Nieeksperymentalne",
    "text": "10.4 Projekty Nieeksperymentalne\nProjekty nieeksperymentalne są stosowane, gdy randomizacja lub manipulacja zmiennymi nie jest możliwa lub etyczna. Obejmują one badania obserwacyjne/opisowe i quasi-eksperymentalne.\n\n10.4.1 Badania Obserwacyjne\nBadania obserwacyjne polegają na zbieraniu danych bez manipulowania zmiennymi. Są one przydatne do eksploracji relacji i generowania hipotez.\nPrzykład: Badanie korelacyjne\n\nset.seed(789)\nn &lt;- 100\nczas_nauki &lt;- runif(n, 0, 10)\nwynik_egzaminu &lt;- 50 + 5 * czas_nauki + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(czas_nauki, wynik_egzaminu)\n\nggplot(correlation_data, aes(x = czas_nauki, y = wynik_egzaminu)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Korelacja między Czasem Nauki a Wynikiem Egzaminu\",\n       x = \"Czas Nauki (godziny)\", y = \"Wynik Egzaminu\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nKorelacja między Czasem Nauki a Wynikiem Egzaminu\n\n\n\n\nTen wykres punktowy pokazuje relację między czasem nauki a wynikami egzaminu, ilustrując pozytywną korelację typową dla badań obserwacyjnych.\n\n\n10.4.2 Projekty Quasi-Eksperymentalne\nProjekty quasi-eksperymentalne nie mają losowego przydziału, ale próbują ustalić związki przyczynowe. Popularne typy to:\n\nRóżnica w Różnicach (DiD)\nRegresja Nieciągła (RDD)\n\n\n10.4.2.1 Różnica w Różnicach (DiD)\nDiD jest używana do oszacowania efektów interwencji poprzez porównanie średniej zmiany w czasie w zmiennej wynikowej dla grupy eksperymentalnej ze średnią zmianą w czasie dla grupy kontrolnej.\nPrzeprowadźmy symulację analizy DiD przy użyciu pakietu plm:\n\nlibrary(plm)\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nWykres pokazuje średnie wyniki dla grup interwencji i kontrolnej w czasie. Pionowa przerywana linia wskazuje punkt interwencji. Oszacowanie DiD to różnica między zmianami obu grup od okresu przed do po interwencji.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt’s important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\n10.4.2.2 Regresja Nieciągła (RDD)\nRDD jest stosowana, gdy przydział do interwencji jest określony przez wartość graniczną na ciągłej zmiennej. Porównuje obserwacje tuż powyżej i poniżej punktu granicznego, aby oszacować efekt interwencji.\nPrzeprowadźmy analizę RDD przy użyciu pakietu rdrobust:\n\nlibrary(rdrobust)\n\n# Generowanie syntetycznych danych RDD\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# Analiza RDD\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     4.092     0.231    17.723     0.000     [3.640 , 4.545]     \n        Robust         -         -    15.013     0.000     [3.600 , 4.680]     \n=============================================================================\n\n# Wizualizacja RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regresja Nieciągła\",\n       x = \"Zmienna Bieżąca\", y = \"Wynik\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAnaliza Regresji Nieciągłej\n\n\n\n\nWykres pokazuje nieciągłość w punkcie granicznym (x = 0), z oddzielnymi liniami regresji dopasowanymi po obu stronach. Efekt interwencji jest szacowany przez różnicę między tymi liniami w punkcie granicznym.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#model-potencjalnych-wyników-neymana-rubina",
    "href": "rozdzial4.html#model-potencjalnych-wyników-neymana-rubina",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.5 Model Potencjalnych Wyników Neymana-Rubina",
    "text": "10.5 Model Potencjalnych Wyników Neymana-Rubina\nModel potencjalnych wyników Neymana-Rubina zapewnia formalne podejście do wnioskowania przyczynowego. Wprowadza on koncepcję potencjalnych wyników: dla każdej jednostki rozważamy wynik w warunkach interwencji i w warunkach kontrolnych, mimo że w rzeczywistości możemy zaobserwować tylko jeden z nich.\nKluczowe pojęcia:\n\nPotencjalne Wyniki: \\(Y_i(1)\\) i \\(Y_i(0)\\) odpowiednio dla interwencji i kontroli.\nObserwowany Wynik: \\(Y_i = Y_i(1)T_i + Y_i(0)(1-T_i)\\), gdzie \\(T_i\\) to wskaźnik interwencji.\nIndywidualny Efekt Interwencji: \\(\\tau_i = Y_i(1) - Y_i(0)\\)\nPrzeciętny Efekt Interwencji (ATE): \\(E[\\tau_i] = E[Y_i(1) - Y_i(0)]\\)\n\nModel podkreśla “fundamentalny problem wnioskowania przyczynowego”: nigdy nie możemy zaobserwować obu potencjalnych wyników dla pojedynczej jednostki jednocześnie.\n\n10.5.1 Przykład: Szacowanie ATE w RCT\nW RCT, losowy przydział zapewnia, że interwencja jest niezależna od potencjalnych wyników, umożliwiając nieobciążone oszacowanie ATE:\n\\[\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\\]\nGdzie \\(n_1\\) i \\(n_0\\) to odpowiednio liczby jednostek w grupie interwencji i kontrolnej.\n\n# Używając danych RCT z wcześniejszego przykładu\nate_estimate &lt;- mean(data$post_test[data$grupa == \"Eksperymentalna\"]) - \n                mean(data$post_test[data$grupa == \"Kontrolna\"])\n\ncat(\"Oszacowany Przeciętny Efekt Interwencji:\", round(ate_estimate, 2))\n\nOszacowany Przeciętny Efekt Interwencji: 9.66",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "11.1 Introduction\nDescriptive statistics are fundamental tools in social science research, providing a concise summary of data characteristics. They serve several crucial functions:",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction",
    "href": "chapter5.html#introduction",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "Summarizing large datasets into manageable information\nIdentifying patterns and trends in data\nDetecting potential anomalies or outliers\nProviding a foundation for further statistical analysis",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#types-of-data-distributions",
    "href": "chapter5.html#types-of-data-distributions",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.2 Types of Data Distributions",
    "text": "11.2 Types of Data Distributions\n\n\n\n\n\n\nImportant\n\n\n\nData distribution informs what values a variable takes and how often.\n\n\nUnderstanding data distributions is crucial for data analysis and visualization. In this document, we’ll explore various types of distributions and how to visualize them using ggplot2 in R.\n\n11.2.1 Normal Distribution\nThe normal distribution, also known as the Gaussian distribution, is symmetric and bell-shaped.\n\n# Generate normal distribution data\nnormal_data &lt;- data.frame(x = rnorm(1000))\n\n# Plot\nggplot(normal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Normal Distribution\", x = \"Value\", y = \"Density\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\n11.2.2 Uniform Distribution\nIn a uniform distribution, all values have an equal probability of occurrence.\n\n# Generate uniform distribution data\nuniform_data &lt;- data.frame(x = runif(1000))\n\n# Plot\nggplot(uniform_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Uniform Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n11.2.3 Skewed Distributions\nSkewed distributions are asymmetric, with one tail longer than the other.\n\n# Generate right-skewed data\nright_skewed &lt;- data.frame(x = rlnorm(1000))\n\n# Plot\nggplot(right_skewed, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Right-Skewed Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n11.2.4 Bimodal Distribution\nA bimodal distribution has two peaks, indicating two distinct subgroups in the data.\n\n# Generate bimodal data\nbimodal_data &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Plot\nggplot(bimodal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Bimodal Distribution\", x = \"Value\", y = \"Density\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#visualizing-real-world-data-distributions",
    "href": "chapter5.html#visualizing-real-world-data-distributions",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.3 Visualizing Real-World Data Distributions",
    "text": "11.3 Visualizing Real-World Data Distributions\nLet’s use the palmerpenguins dataset to explore real-world data distributions.\n\n11.3.1 Histogram and Density Plot\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Distribution of Penguin Flipper Lengths\", \n       x = \"Flipper Length (mm)\", \n       y = \"Density\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\n11.3.2 Box Plot\nBox plots are useful for comparing distributions across categories.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n11.3.3 Violin Plot\nViolin plots combine box plot and density plot features.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n11.3.4 Ridgeline Plot\nRidgeline plots are useful for comparing multiple distributions.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Distribution of Flipper Length by Penguin Species\",\n       x = \"Flipper Length (mm)\",\n       y = \"Species\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#conclusion",
    "href": "chapter5.html#conclusion",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.4 Conclusion",
    "text": "11.4 Conclusion\nUnderstanding and visualizing data distributions is crucial in data analysis. ggplot2 provides a flexible and powerful toolkit for creating various types of distribution plots. By exploring different visualization techniques, we can gain insights into the underlying patterns and characteristics of our data.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-outliers",
    "href": "chapter5.html#understanding-outliers",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.5 Understanding Outliers",
    "text": "11.5 Understanding Outliers\nBefore diving into specific measures, it’s crucial to understand the concept of outliers, as they can significantly impact many descriptive statistics.\nOutliers are data points that differ significantly from other observations in the dataset. They can occur due to:\n\nMeasurement or recording errors\nGenuine extreme values in the population\nSampling from a different population\n\nOutliers can have a substantial effect on many statistical measures, especially those based on means or sums of squared deviations. Therefore, it’s essential to:\n\nIdentify outliers through both statistical methods and domain knowledge\nInvestigate the cause of outliers\nMake informed decisions about whether to include or exclude them in analyses\n\nThroughout this guide, we’ll discuss how different descriptive measures are affected by outliers.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-central-tendency",
    "href": "chapter5.html#measures-of-central-tendency",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.6 Measures of Central Tendency",
    "text": "11.6 Measures of Central Tendency\nMeasures of central tendency aim to identify the “typical” or “central” value in a dataset. The three primary measures are mean, median, and mode.\n\n11.6.1 Arithmetic Mean\nThe arithmetic mean is the sum of all values divided by the number of values.\nFormula: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\)\nImportant Property: The mean is a balancing point in the data. The sum of deviations from the mean is always zero:\n\\(\\sum_{i=1}^n (x_i - \\bar{x}) = 0\\)\nThis property makes the mean useful in many statistical calculations.\nManual Calculation Example:\nLet’s calculate the mean for the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nSum all values\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nCount the number of values\nn = 7\n\n\n3\nDivide the sum by n\n36 / 7 = 5.14\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(data)\n\n[1] 5.142857\n\n\nPros:\n\nEasy to calculate and understand\nUses all data points\nUseful for further statistical calculations\n\nCons:\n\nSensitive to outliers\nNot ideal for skewed distributions\n\nExample with outlier:\n\ndata_with_outlier &lt;- c(2, 4, 4, 5, 5, 7, 100)\nmean(data_with_outlier)\n\n[1] 18.14286\n\n\nAs we can see, the outlier (100) drastically affects the mean.\n\n\n11.6.2 Median\nThe median is the middle value when the data is ordered.\nManual Calculation Example:\nUsing the same dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nResult\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind the middle value\n5\n\n\n\nFor even number of values, take the average of the two middle values.\nR calculation:\n\nmedian(data)\n\n[1] 5\n\nmedian(data_with_outlier)\n\n[1] 5\n\n\nPros:\n\nNot affected by extreme outliers\nBetter for skewed distributions\n\nCons:\n\nDoesn’t use all data points\nLess useful for further statistical calculations\n\n\n\n11.6.3 Mode\nThe mode is the most frequently occurring value.\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nValue\nFrequency\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nThe mode is 4 and 5 (bimodal).\nR calculation:\n\nlibrary(modeest)\nmfv(data)  # Most frequent value\n\n[1] 4 5\n\n\nPros:\n\nOnly measure of central tendency for nominal data\nCan identify multiple peaks in the data\n\nCons:\n\nNot always uniquely defined\nNot useful for continuous data\n\n\n\n11.6.4 Weighted Mean\nThe weighted mean is used when some data points are more important than others. There are two types of weighted means: with not normalized weights and with normalized weights.\n\n11.6.4.1 Weighted Mean with Not Normalized Weights\nThis is the standard form of the weighted mean, where weights can be any positive numbers representing the importance of each data point.\nFormula: \\(\\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\\)\nManual Calculation Example:\nLet’s calculate the weighted mean for the dataset: 2, 4, 5, 7 with weights 1, 2, 3, 1\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 1) + (4 * 2) + (5 * 3) + (7 * 1) = 2 + 8 + 15 + 7 = 32\n\n\n2\nSum the weights\n1 + 2 + 3 + 1 = 7\n\n\n3\nDivide the result from step 1 by the result from step 2\n32 / 7 = 4.57\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\n11.6.4.2 Weighted Mean with Normalized Weights (Fractions)\nIn this case, the weights are fractions that sum to 1, representing the proportion of importance for each data point.\nFormula: \\(\\bar{x}_w = \\sum_{i=1}^n w_i x_i\\), where \\(\\sum_{i=1}^n w_i = 1\\)\nManual Calculation Example:\nLet’s calculate the weighted mean for the dataset: 2, 4, 5, 7 with normalized weights 0.1, 0.3, 0.4, 0.2\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 0.1) + (4 * 0.3) + (5 * 0.4) + (7 * 0.2)\n\n\n2\nSum the results\n0.2 + 1.2 + 2.0 + 1.4 = 4.8\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Note: these sum to 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nPros of Weighted Means:\n\nAccount for varying importance of data points\nUseful in survey analysis with different sample sizes or importance levels\nCan adjust for unequal probabilities in sampling designs\n\nCons of Weighted Means:\n\nRequire justification for weights\nCan be misused to manipulate results\nMay be less intuitive to interpret than simple arithmetic mean",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-variability",
    "href": "chapter5.html#measures-of-variability",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.7 Measures of Variability",
    "text": "11.7 Measures of Variability\nThese measures describe how spread out the data is. They are crucial for understanding the dispersion of data points around the central tendency.\n\n11.7.1 Range\nThe range is the difference between the maximum and minimum values.\nFormula: \\(R = x_{max} - x_{min}\\)\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nFind the maximum value\n9\n\n\n2\nFind the minimum value\n2\n\n\n3\nSubtract minimum from maximum\n9 - 2 = 7\n\n\n\nR calculation:\n\nrange(data)\n\n[1] 2 9\n\nmax(data) - min(data)\n\n[1] 7\n\n\nPros:\n\nSimple to calculate and understand\nGives an immediate sense of data spread\n\nCons:\n\nExtremely sensitive to outliers\nDoesn’t provide information about the distribution between extremes\n\n\n\n11.7.2 Interquartile Range (IQR)\nThe IQR is the difference between the 75th and 25th percentiles.\nFormula: \\(IQR = Q_3 - Q_1\\)\nTo find quartiles manually:\n\nFor odd number of values:\n\nQ2 (median) is the middle value\nQ1 is the median of the lower half\nQ3 is the median of the upper half\n\nFor even number of values:\n\nQ2 is the average of the two middle values\nQ1 is the median of the lower half (including the lower middle value)\nQ3 is the median of the upper half (including the upper middle value)\n\n\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind Q2 (median)\n5\n\n\n3\nFind Q1 (median of lower half)\n4\n\n\n4\nFind Q3 (median of upper half)\n7\n\n\n5\nCalculate IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nR calculation:\n\nIQR(data)\n\n[1] 2\n\n\nPros:\n\nRobust to outliers\nProvides information about the spread of the middle 50% of the data\n\nCons:\n\nIgnores the tails of the distribution\nLess efficient than standard deviation for normal distributions\n\n\n\n11.7.3 Variance\nVariance measures the average squared deviation from the mean.\nFormula: \\(s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\\)\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\(\\bar{x} = 5.14\\)\n\n\n2\nSubtract the mean from each value and square the result\n\\((2 - 5.14)^2 = 9.86\\)\n\n\n\n\n\\((4 - 5.14)^2 = 1.30\\)\n\n\n\n\n\\((4 - 5.14)^2 = 1.30\\)\n\n\n\n\n\\((5 - 5.14)^2 = 0.02\\)\n\n\n\n\n\\((5 - 5.14)^2 = 0.02\\)\n\n\n\n\n\\((7 - 5.14)^2 = 3.46\\)\n\n\n\n\n\\((9 - 5.14)^2 = 14.90\\)\n\n\n3\nSum the squared differences\n30.86\n\n\n4\nDivide by (n-1)\n30.86 / 6 = 5.14\n\n\n\nR calculation:\n\nvar(data)\n\n[1] 5.142857\n\n\nPros:\n\nUses all data points\nFoundation for many statistical tests\n\nCons:\n\nUnits are squared, making interpretation less intuitive\nSensitive to outliers\n\n\n\n11.7.4 Standard Deviation\nThe standard deviation is the square root of the variance.\nFormula: \\(s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\\)\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the variance\n\\(s^2 = 5.14\\) (from previous calculation)\n\n\n2\nTake the square root\n\\(s = \\sqrt{5.14} = 2.27\\)\n\n\n\nR calculation:\n\nsd(data)\n\n[1] 2.267787\n\n\nPros:\n\nIn same units as original data\nWidely used and understood\n\nCons:\n\nStill sensitive to outliers\nAssumes data is roughly normally distributed\n\n\n\n11.7.5 Coefficient of Variation\nThe coefficient of variation is the standard deviation divided by the mean, often expressed as a percentage.\nFormula: \\(CV = \\frac{s}{\\bar{x}} \\times 100\\%\\)\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\(\\bar{x} = 5.14\\)\n\n\n2\nCalculate the standard deviation\n\\(s = 2.27\\)\n\n\n3\nDivide s by the mean and multiply by 100\n\\((2.27 / 5.14) * 100 = 44.16\\%\\)\n\n\n\nR calculation:\n\n(sd(data) / mean(data)) * 100\n\n[1] 44.09586\n\n\nPros:\n\nAllows comparison of variability between datasets with different units or means\nUseful in fields like finance for risk assessment\n\nCons:\n\nNot meaningful for data with both positive and negative values\nCan be misleading when mean is close to zero",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-relative-position",
    "href": "chapter5.html#measures-of-relative-position",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.8 Measures of Relative Position",
    "text": "11.8 Measures of Relative Position\nThese measures help us understand where a particular value falls in relation to the entire dataset.\n\n11.8.1 Percentiles\nPercentiles divide the data into 100 equal parts.\nFormula: For the kth percentile: \\(P_k = L + \\frac{k(n+1)}{100}\\), where L is the lower limit of the interval\nManual Calculation Example (simplified method):\nTo find the 25th percentile (Q1) for the dataset: 2, 4, 4, 5, 5, 7, 9\n\nOrder the data: 2, 4, 4, 5, 5, 7, 9\nCalculate the position: \\((25/100) * (7+1) = 2\\)\nThe 25th percentile is the 2nd value: 4\n\nR calculation:\n\nquantile(data, probs = seq(0, 1, 0.25))\n\n  0%  25%  50%  75% 100% \n   2    4    5    6    9 \n\n\n\n\n11.8.2 Quartiles\nQuartiles divide the data into four equal parts.\n\nQ1: 25th percentile\nQ2: Median (50th percentile)\nQ3: 75th percentile\n\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\nManual Calculation:\nSee the IQR section for a step-by-step process to find quartiles manually.\nR calculation:\n\nsummary(data)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   4.000   5.000   5.143   6.000   9.000 \n\n\nPros:\n\nRobust to outliers\nProvide information about data spread and skewness\n\nCons:\n\nLess precise than using all data points\nMultiple methods of calculation can lead to slightly different results",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-shape",
    "href": "chapter5.html#measures-of-shape",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.9 Measures of Shape",
    "text": "11.9 Measures of Shape\n\n11.9.1 Skewness\n\n11.9.1.1 Definition\nSkewness quantifies the asymmetry of a probability distribution. It indicates whether the data clusters more on one side of the mean than the other.\n\n\n11.9.1.2 Mathematical Expression\n\\(SK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3\\)\nwhere:\n\n\\(n\\) is the sample size\n\\(x_i\\) is the i-th observation\n\\(\\bar{x}\\) is the sample mean\n\\(s\\) is the sample standard deviation\n\n\n\n11.9.1.3 Example: Voter Turnout Analysis\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n# Generate example precinct-level voter turnout data\nset.seed(123)\nturnout_data &lt;- c(\n  # Urban precincts\n  rnorm(300, mean = 65, sd = 12),\n  # Suburban precincts\n  rnorm(400, mean = 70, sd = 10),\n  # Rural precincts\n  rnorm(300, mean = 68, sd = 15)\n) |&gt; \n  # Ensure turnout stays between 0-100%\n  pmax(0) |&gt; \n  pmin(100)\n\n# Calculate and visualize\nskew_value &lt;- skewness(turnout_data)\nskew_value\n\n[1] 0.02558143\n\nggplot(data.frame(x = turnout_data), aes(x = x)) +\n  geom_histogram(bins = 50, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = mean(turnout_data), color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = median(turnout_data), color = \"blue\", linetype = \"dashed\") +\n  labs(\n    title = str_glue(\"Precinct-Level Voter Turnout Distribution (Skewness = {round(skew_value, 4)})\"),\n    subtitle = \"Red: Mean, Blue: Median\",\n    x = \"Voter Turnout (%)\",\n    y = \"Number of Precincts\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n11.9.1.4 Interpretation Guide\n\nPositive Skewness (&gt; 0): Distribution has a longer right tail\nNegative Skewness (&lt; 0): Distribution has a longer left tail\nZero Skewness: Approximately symmetric distribution\n\n\n\n\n11.9.2 Kurtosis\n\n\n11.9.3 Definition\nKurtosis measures the “tailedness” of a distribution, indicating the presence of extreme values relative to a normal distribution.\n\n11.9.3.1 Mathematical Expression\n\\(K = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\\)\n\n\n11.9.3.2 Example: Legislative Voting Analysis\n\n# Generate example legislative voting agreement scores\nset.seed(456)\nvoting_agreement &lt;- c(\n  # Regular voting patterns\n  rnorm(400, mean = 75, sd = 10),\n  # Cross-party cooperation instances\n  rnorm(80, mean = 50, sd = 15),\n  # Party-line votes\n  rnorm(20, mean = 95, sd = 5)\n) |&gt; \n  pmax(0) |&gt; \n  pmin(100)\n\nkurt_value &lt;- kurtosis(voting_agreement)\nkurt_value\n\n[1] 3.849939\n\n# Visualization with normal distribution comparison\nx_range &lt;- seq(min(voting_agreement)-1, max(voting_agreement)+1, length.out = 100)\nnormal_dist &lt;- dnorm(x_range, mean = mean(voting_agreement), sd = sd(voting_agreement))\n\nggplot() +\n  geom_density(\n    data = data.frame(x = voting_agreement), \n    aes(x = x), \n    fill = \"skyblue\", \n    alpha = 0.5\n  ) +\n  geom_line(\n    data = data.frame(x = x_range, y = normal_dist),\n    aes(x = x, y = y),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) +\n  labs(\n    title = str_glue(\"Legislative Voting Agreement Distribution (Kurtosis = {round(kurt_value, 4)})\"),\n    subtitle = \"Observed distribution (blue) vs. Normal distribution (red)\",\n    x = \"Voting Agreement Score (%)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n11.9.3.3 Interpretation Guide\n\nExcess Kurtosis: Difference from normal distribution’s kurtosis (3)\nLeptokurtic (&gt; 3): More extreme values than normal distribution\nPlatykurtic (&lt; 3): Fewer extreme values than normal distribution\nMesokurtic (= 3): Similar to normal distribution",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#bivariate-statistics",
    "href": "chapter5.html#bivariate-statistics",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.10 Bivariate Statistics",
    "text": "11.10 Bivariate Statistics\nBivariate statistics describe the relationship between two variables. We’ll explore several measures, starting with covariance and progressing to more advanced correlation measures.\n\n11.10.1 Covariance\nCovariance measures how two variables vary together.\nFormula: \\(cov(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\\)\nManual Calculation Example:\nLet’s calculate the covariance for two variables: x: 1, 2, 3, 4, 5 y: 2, 4, 5, 4, 5\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate means\n\\(\\bar{x} = 3, \\bar{y} = 4\\)\n\n\n2\nCalculate \\((x_i - \\bar{x})(y_i - \\bar{y})\\) for each pair\n\\((-2)(-2) = 4\\)\n\n\n\n\n\\((-1)(0) = 0\\)\n\n\n\n\n\\((0)(1) = 0\\)\n\n\n\n\n\\((1)(0) = 0\\)\n\n\n\n\n\\((2)(1) = 2\\)\n\n\n3\nSum the results\n\\(4 + 0 + 0 + 0 + 2 = 6\\)\n\n\n4\nDivide by (n-1)\n\\(6 / 4 = 1.5\\)\n\n\n\nR calculation:\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 5, 4, 5)\ncov(x, y)\n\n[1] 1.5\n\n\nInterpretation: - The positive covariance (1.5) indicates that x and y tend to increase together.\nPros:\n\nProvides direction of relationship (positive or negative)\nUseful in calculating other measures like correlation\n\nCons:\n\nScale-dependent, making it difficult to compare across different variable pairs\nDoesn’t provide information about the strength of the relationship\n\n\n\n11.10.2 Pearson Correlation\nPearson correlation measures the strength and direction of the linear relationship between two continuous variables.\nFormula: \\(r = \\frac{cov(X,Y)}{s_X s_Y} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\\)\nManual Calculation Example:\nUsing the same data as above:\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate covariance\n(From previous calculation) 1.5\n\n\n2\nCalculate standard deviations\n\\(s_X = \\sqrt{\\frac{10}{4}} = 1.58, s_Y = \\sqrt{\\frac{6}{4}} = 1.22\\)\n\n\n3\nDivide covariance by product of standard deviations\n\\(1.5 / (1.58 * 1.22) = 0.7746\\)\n\n\n\nR calculation:\n\ncor(x, y, method = \"pearson\")\n\n[1] 0.7745967\n\n\nInterpretation: - The correlation coefficient of 0.7746 indicates a strong positive linear relationship between x and y.\nPros:\n\nScale-independent, always between -1 and 1\nWidely understood and used\nTests for linear relationships\n\nCons:\n\nSensitive to outliers\nOnly measures linear relationships\nAssumes normally distributed variables\n\n\n\n11.10.3 Spearman Correlation\nSpearman correlation measures the strength and direction of the monotonic relationship between two variables, which can be continuous or ordinal.\nFormula: \\(r_s = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\\), where \\(d_i\\) is the difference between ranks.\nManual Calculation Example:\nLet’s use slightly different data: x: 1, 2, 3, 4, 5 y: 1, 3, 2, 5, 4\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nRank both variables\nx_rank: 1, 2, 3, 4, 5\n\n\n\n\ny_rank: 1, 3, 2, 5, 4\n\n\n2\nCalculate differences in ranks (d)\n0, -1, 1, -1, 1\n\n\n3\nSquare the differences\n0, 1, 1, 1, 1\n\n\n4\nSum the squared differences\n\\(\\sum d_i^2 = 4\\)\n\n\n5\nApply the formula\n\\(r_s = 1 - \\frac{6(4)}{5(5^2 - 1)} = 0.8\\)\n\n\n\nR calculation:\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(1, 3, 2, 5, 4)\ncor(x, y, method = \"spearman\")\n\n[1] 0.8\n\n\nInterpretation: - The Spearman correlation of 0.8 indicates a strong positive monotonic relationship between x and y.\nPros:\n\nRobust to outliers\nCan detect non-linear monotonic relationships\nSuitable for ordinal data\n\nCons:\n\nLess powerful than Pearson for detecting linear relationships in normally distributed data\nDoesn’t provide information about the shape of the relationship beyond monotonicity\n\n\n\n11.10.4 Cross-tabulation\nCross-tabulation (contingency table) shows the relationship between two categorical variables.\nExample:\nLet’s create a cross-tabulation of two variables: - Education level: High School, College, Graduate - Employment status: Employed, Unemployed\n\neducation &lt;- factor(c(\"High School\", \"College\", \"Graduate\", \"High School\", \"College\", \"Graduate\", \"High School\", \"College\", \"Graduate\"))\nemployment &lt;- factor(c(\"Employed\", \"Employed\", \"Employed\", \"Unemployed\", \"Employed\", \"Employed\", \"Unemployed\", \"Unemployed\", \"Employed\"))\n\ntable(education, employment)\n\n             employment\neducation     Employed Unemployed\n  College            2          1\n  Graduate           3          0\n  High School        1          2\n\n\nInterpretation:\n\nThis table shows the count of individuals in each combination of education level and employment status.\nFor example, we can see how many high school graduates are employed versus unemployed.\n\nPros:\n\nProvides a clear visual representation of the relationship between categorical variables\nEasy to understand and interpret\nBasis for many statistical tests (e.g., chi-square test of independence)\n\nCons:\n\nLimited to categorical data\nCan become unwieldy with many categories\nDoesn’t provide a single summary statistic of association strength",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#choosing-the-appropriate-measure",
    "href": "chapter5.html#choosing-the-appropriate-measure",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.11 Choosing the Appropriate Measure",
    "text": "11.11 Choosing the Appropriate Measure\nWhen deciding which bivariate statistic to use, consider:\n\nData type:\n\nContinuous data: Covariance, Pearson correlation\nOrdinal data: Spearman correlation\nCategorical data: Cross-tabulation\n\nRelationship type:\n\nLinear: Pearson correlation\nMonotonic but potentially non-linear: Spearman correlation\n\nPresence of outliers:\n\nIf outliers are a concern, Spearman correlation is more robust\n\nDistribution:\n\nFor normally distributed data, Pearson correlation is most powerful\nFor non-normal distributions, consider Spearman correlation\n\nSample size:\n\nFor very small samples, non-parametric methods like Spearman correlation might be preferred\n\n\nRemember, it’s often valuable to use multiple measures and visualizations (like scatter plots) to get a comprehensive understanding of the relationship between variables.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-elementary-multivariate-statistics",
    "href": "chapter5.html#introduction-to-elementary-multivariate-statistics",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.12 Introduction to Elementary Multivariate Statistics",
    "text": "11.12 Introduction to Elementary Multivariate Statistics\nMultivariate statistics involve the analysis of relationships among three or more variables simultaneously. This section will introduce some basic concepts and techniques in multivariate analysis, with a focus on correlation-based methods.\n\n11.12.1 Correlation Matrix\nA correlation matrix is a table showing the pairwise correlations of several variables. It’s a fundamental tool in multivariate analysis.\nExample: Let’s create a correlation matrix for four variables: height, weight, age, and income.\n\nset.seed(123)  # For reproducibility\nheight &lt;- rnorm(100, 170, 10)\nweight &lt;- height * 0.5 + rnorm(100, 0, 5)\nage &lt;- rnorm(100, 40, 10)\nincome &lt;- age * 1000 + rnorm(100, 0, 10000)\n\ndata &lt;- data.frame(height, weight, age, income)\n\ncor_matrix &lt;- cor(data)\nprint(cor_matrix)\n\n           height      weight         age      income\nheight  1.0000000  0.66712996 -0.12917601 -0.12246786\nweight  0.6671300  1.00000000 -0.06814187 -0.04579492\nage    -0.1291760 -0.06814187  1.00000000  0.65654902\nincome -0.1224679 -0.04579492  0.65654902  1.00000000\n\n\nInterpretation:\n\nEach cell shows the correlation between two variables.\nThe diagonal is always 1 (correlation of a variable with itself).\nLook for strong correlations (close to 1 or -1) to identify potential relationships.\n\n\n\n11.12.2 Visualizing Multivariate Relationships\n\n11.12.2.1 Scatterplot Matrix\nA scatterplot matrix shows pairwise relationships between multiple variables.\n\npairs(data)\n\n\n\n\n\n\n\n\nInterpretation:\n\nEach plot shows the relationship between two variables.\nDiagonal elements show the distribution of each variable.\nLook for patterns, clusters, or trends in the plots.\n\n\n\n11.12.2.2 Correlation Plot\nA correlation plot provides a visual representation of the correlation matrix.\n\nlibrary(corrplot)\n\ncorrplot 0.94 loaded\n\ncorrplot(cor_matrix, method = \"color\")\n\n\n\n\n\n\n\n\nInterpretation:\n\nColor intensity and size of the circles indicate the strength of correlation.\nBlue colors typically indicate positive correlations, red colors indicate negative correlations.\n\n\n\n\n11.12.3 Partial Correlation\nPartial correlation measures the relationship between two variables while controlling for one or more other variables.\nExample: Let’s calculate the partial correlation between height and weight, controlling for age.\n\nlibrary(ppcor)\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\npcor.test(data$height, data$weight, data$age)\n\n   estimate      p.value statistic   n gp  Method\n1 0.6654367 5.758157e-14  8.779896 100  1 pearson\n\n\nInterpretation:\n\nCompare this to the simple correlation between height and weight.\nA significant change might indicate that age plays a role in the height-weight relationship.\n\n\n\n11.12.4 Multiple Correlation\nMultiple correlation measures the strength of the relationship between a dependent variable and multiple independent variables.\nExample: Let’s predict weight using height and age.\n\nmodel &lt;- lm(weight ~ height + age, data = data)\nR &lt;- sqrt(summary(model)$r.squared)\nprint(paste(\"Multiple correlation coefficient:\", R))\n\n[1] \"Multiple correlation coefficient: 0.667377840470434\"\n\n\nInterpretation:\n\nR ranges from 0 to 1, with higher values indicating stronger relationships.\nR² (R-squared) represents the proportion of variance in the dependent variable explained by the independent variables.\n\n\n\n11.12.5 Factor Analysis\nFactor analysis is a technique used to reduce many variables to a smaller number of underlying factors.\nExample: Let’s perform a simple factor analysis on our dataset.\n\nlibrary(psych)\n\nRegistered S3 method overwritten by 'psych':\n  method         from  \n  plot.residuals rmutil\n\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nfa_result &lt;- fa(data, nfactors = 2, rotate = \"varimax\")\nprint(fa_result$loadings, cutoff = 0.3)\n\n\nLoadings:\n       MR2    MR1   \nheight  0.798       \nweight  0.836       \nage            0.729\nincome         0.895\n\n                 MR2   MR1\nSS loadings    1.344 1.341\nProportion Var 0.336 0.335\nCumulative Var 0.336 0.671\n\n\nInterpretation:\n\nLook at which variables load highly on each factor.\nTry to interpret what each factor might represent based on the variables that load on it.\n\n\n\n11.12.6 Considerations in Multivariate Analysis\n\nSample Size: Multivariate techniques often require larger sample sizes for stable results.\nMulticollinearity: High correlations among independent variables can cause issues in some analyses.\nOutliers: Multivariate outliers can have a strong influence on results.\nAssumptions: Many techniques assume multivariate normality and linear relationships.\nInterpretation Complexity: As the number of variables increases, interpretation can become more challenging.\n\n\n\n11.12.7 Conclusion\nThis introduction to multivariate statistics builds upon the concept of correlation to explore relationships among multiple variables. These techniques provide powerful tools for understanding complex datasets, but they also require careful consideration of assumptions and limitations. As you progress in your statistical journey, you’ll encounter more advanced multivariate techniques such as MANOVA, discriminant analysis, and structural equation modeling.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "href": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.13 Exercise 1. Center and dispersion of data",
    "text": "11.13 Exercise 1. Center and dispersion of data\n\n11.13.1 Data\nWe have salary data (in thousands of euros) from two small European companies:\n\n\n\nIndex\nCompany X\nCompany Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\nThis table presents the data for both Company X and Company Y side by side, with an index column for easy reference.\n\n\n11.13.2 Measures of Central Tendency\n\n11.13.2.1 Mean\nThe mean is the average of all values in a dataset.\nFormula: \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\)\n\n11.13.2.1.1 Manual Calculation for Company X\n\n\n\nValue (\\(x_i\\))\nFrequency (\\(f_i\\))\n\\(x_i \\cdot f_i\\)\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nTotal\nn = 20\nSum = 119\n\n\n\n\\(\\bar{x} = \\frac{119}{20} = 5.95\\)\n\n\n11.13.2.1.2 Manual Calculation for Company Y\n\n\n\nValue (\\(x_i\\))\nFrequency (\\(f_i\\))\n\\(x_i \\cdot f_i\\)\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nTotal\nn = 20\nSum = 100\n\n\n\n\\(\\bar{y} = \\frac{100}{20} = 5\\)\n\n\n11.13.2.1.3 R Verification\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\n11.13.2.2 Median\nThe median is the middle value when the data is ordered.\n\n11.13.2.2.1 Manual Calculation for Company X\nOrdered data: 2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\(\\frac{4 + 4}{2} = 4\\)\n\n\n11.13.2.2.2 Manual Calculation for Company Y\nOrdered data: 3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\(\\frac{5 + 5}{2} = 5\\)\n\n\n11.13.2.2.3 R Verification\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\n11.13.2.3 Mode\nThe mode is the most frequent value in the dataset.\nFor Company X, the mode is 3 (appears 6 times). For Company Y, there are two modes: 4 and 5 (both appear 6 times).\n\n# Function to calculate mode\nget_mode &lt;- function(x) {\n  unique_x &lt;- unique(x)\n  unique_x[which.max(tabulate(match(x, unique_x)))]\n}\n\nget_mode(X)\n\n[1] 3\n\nget_mode(Y)\n\n[1] 4\n\n\n\n\n\n11.13.3 Measures of Dispersion\n\n11.13.3.1 Variance\nThe variance measures the average squared deviation from the mean.\nFormula: \\(s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\)\n\n11.13.3.1.1 Manual Calculation for Company X\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(f_i\\)\n\\(x_i - \\bar{x}\\)\n\\((x_i - \\bar{x})^2\\)\n\\(f_i(x_i - \\bar{x})^2\\)\n\n\n\n\n2\n3\n-3.95\n15.6025\n46.8075\n\n\n3\n6\n-2.95\n8.7025\n52.215\n\n\n4\n5\n-1.95\n3.8025\n19.0125\n\n\n5\n4\n-0.95\n0.9025\n3.61\n\n\n20\n1\n14.05\n197.4025\n197.4025\n\n\n35\n1\n29.05\n843.9025\n843.9025\n\n\nTotal\n20\n\n\n1162.95\n\n\n\n\\(s^2 = \\frac{1162.95}{19} = 61.21\\)\n\n\n11.13.3.1.2 Manual Calculation for Company Y\n\n\n\n\n\n\n\n\n\n\n\\(y_i\\)\n\\(f_i\\)\n\\(y_i - \\bar{y}\\)\n\\((y_i - \\bar{y})^2\\)\n\\(f_i(y_i - \\bar{y})^2\\)\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nTotal\n20\n\n\n34\n\n\n\n\\(s^2 = \\frac{34}{19} = 1.79\\)\n\n\n11.13.3.1.3 R Verification\n\nvar(X)\n\n[1] 61.20789\n\nvar(Y)\n\n[1] 1.789474\n\n\n\n\n\n11.13.3.2 Standard Deviation\nThe standard deviation is the square root of the variance.\nFormula: \\(s = \\sqrt{s^2}\\)\n\nFor Company X: \\(s = \\sqrt{61.21} = 7.82\\)\nFor Company Y: \\(s = \\sqrt{1.79} = 1.34\\)\n\n\n11.13.3.2.1 R Verification\n\nsd(X)\n\n[1] 7.823547\n\nsd(Y)\n\n[1] 1.337712\n\n\n\n\n\n\n11.13.4 Quartiles\nQuartiles divide the dataset into four equal parts.\n\n11.13.4.1 Manual Calculation for Company X\nOrdered data: 2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35\n\nQ1 (25th percentile): median of first 10 numbers = 3\nQ2 (50th percentile, median): 4\nQ3 (75th percentile): median of last 10 numbers = 5\n\n\n\n11.13.4.2 Manual Calculation for Company Y\nOrdered data: 3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8\n\nQ1 (25th percentile): median of first 10 numbers = 4\nQ2 (50th percentile, median): 5\nQ3 (75th percentile): median of last 10 numbers = 6\n\n\n\n11.13.4.3 R Verification\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\n\n11.13.5 Tukey Box Plot\nA Tukey box plot visually represents the distribution of data based on quartiles. We’ll use ggplot2 to create the plot.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Prepare the data\ndata &lt;- data.frame(\n  Company = rep(c(\"X\", \"Y\"), each = 20),\n  Salary = c(X, Y)\n)\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot() +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\n11.13.5.1 Interpreting the Box Plot\n\nThe box represents the interquartile range (IQR) from Q1 to Q3.\nThe line inside the box is the median (Q2).\nWhiskers extend to the smallest and largest values within 1.5 * IQR.\nPoints beyond the whiskers are considered outliers.\n\n\n\n\n11.13.6 Comparison of Results\n\n\n\nMeasure\nCompany X\nCompany Y\n\n\n\n\nMean\n5.95\n5.00\n\n\nMedian\n4\n5\n\n\nMode\n3\n4 and 5\n\n\nVariance\n61.21\n1.79\n\n\nStandard Deviation\n7.82\n1.34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\n11.13.6.1 Key Observations:\n\nCentral Tendency: Company X has a higher mean but lower median than Company Y, indicating a right-skewed distribution for Company X.\nDispersion: Company X shows much higher variance and standard deviation, suggesting greater salary disparities.\nDistribution Shape: Company Y’s salaries are more tightly clustered, while Company X has extreme values (potential outliers) that significantly affect its mean and variance.\nQuartiles: Company Y’s interquartile range (Q3 - Q1) is slightly larger, but its overall range is much smaller than Company X’s.\n\n\n\n\n11.13.7 Conclusion\nThis comparative analysis reveals significant differences in salary structures between the two companies. Company X shows greater variability and potential inequality in its pay scale, while Company Y demonstrates a more consistent and narrowly distributed salary range.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-2.-comparing-district-magnitude-variability-between-countries",
    "href": "chapter5.html#exercise-2.-comparing-district-magnitude-variability-between-countries",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.14 Exercise 2. Comparing District Magnitude Variability Between Countries",
    "text": "11.14 Exercise 2. Comparing District Magnitude Variability Between Countries\n\n11.14.1 Data\nWe have data on the size of electoral districts from two countries:\n\nCountry with high variability (X): 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nCountry with low variability (Y): 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\n\n\n\n11.14.2 Measures of Central Tendency\n\n11.14.2.1 Arithmetic Mean\nThe arithmetic mean is the sum of all values divided by their count.\nFormula: \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\)\n\n11.14.2.1.1 Calculations for Country X\n\n\n\nValues\nSum\n\n\n\n\n1 + 3 + 5 + 7 + 9 + 11 + 13 + 15 + 17 + 19\n100\n\n\n\n\\(\\bar{x} = \\frac{100}{10} = 10\\)\n\n\n11.14.2.1.2 Calculations for Country Y\n\n\n\nValues\nSum\n\n\n\n\n8 + 9 + 9 + 10 + 10 + 11 + 11 + 12 + 12 + 13\n105\n\n\n\n\\(\\bar{y} = \\frac{105}{10} = 10.5\\)\n\n\n\n11.14.2.2 Median\nThe median is the middle value in an ordered set of data.\n\n11.14.2.2.1 Calculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMedian = \\(\\frac{9 + 11}{2} = 10\\)\n\n\n11.14.2.2.2 Calculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMedian = \\(\\frac{10 + 11}{2} = 10.5\\)\n\n\n\n11.14.2.3 Mode\nThe mode is the most frequently occurring value in a dataset.\nFor Country X, there is no mode (all values occur once). For Country Y, there are four modes: 9, 10, 11, and 12 (each occurs twice).\n\n\n\n11.14.3 Measures of Dispersion\n\n11.14.3.1 Range\nThe range is the difference between the maximum and minimum values.\n\n11.14.3.1.1 Calculations for Country X\nRange = 19 - 1 = 18\n\n\n11.14.3.1.2 Calculations for Country Y\nRange = 13 - 8 = 5\n\n\n\n11.14.3.2 Variance\nVariance measures the average squared deviation from the mean.\nFormula: \\(s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\)\n\n11.14.3.2.1 Calculations for Country X\n\n\n\n\\(x_i\\)\n\\((x_i - \\bar{x})\\)\n\\((x_i - \\bar{x})^2\\)\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSum\n\n330\n\n\n\n\\(s^2_X = \\frac{330}{9} = 36.67\\)\n\n\n11.14.3.2.2 Calculations for Country Y\n\n\n\n\\(x_i\\)\n\\((y_i - \\bar{y})\\)\n\\((y_i - \\bar{y})^2\\)\n\n\n\n\n8\n-2.5\n6.25\n\n\n9\n-1.5\n2.25\n\n\n9\n-1.5\n2.25\n\n\n10\n-0.5\n0.25\n\n\n10\n-0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n12\n1.5\n2.25\n\n\n12\n1.5\n2.25\n\n\n13\n2.5\n6.25\n\n\nSum\n\n22.5\n\n\n\n\\(s^2_Y = \\frac{22.5}{9} = 2.5\\)\n\n\n\n11.14.3.3 Standard Deviation\nThe standard deviation is the square root of the variance.\nFormula: \\(s = \\sqrt{s^2}\\)\n\n11.14.3.3.1 Calculations for Country X\n\\(s_X = \\sqrt{36.67} \\approx 6.06\\)\n\n\n11.14.3.3.2 Calculations for Country Y\n\\(s_Y = \\sqrt{2.5} \\approx 1.58\\)\n\n\n\n\n11.14.4 Quartiles and Interquartile Range (IQR)\nQuartiles divide the dataset into four equal parts.\n\n11.14.4.1 Calculations for Country X\n\nQ1 (25th percentile): \\(\\frac{3 + 5}{2} = 4\\)\nQ2 (50th percentile, median): 10\nQ3 (75th percentile): \\(\\frac{15 + 17}{2} = 16\\)\nIQR = Q3 - Q1 = 16 - 4 = 12\n\n\n\n11.14.4.2 Calculations for Country Y\n\nQ1 (25th percentile): 9\nQ2 (50th percentile, median): 10.5\nQ3 (75th percentile): 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n\n\n\n11.14.5 Coefficient of Variation (CV)\nThe coefficient of variation is the ratio of the standard deviation to the mean, expressed as a percentage.\nFormula: \\(CV = \\frac{s}{\\bar{x}} \\times 100\\%\\)\n\n11.14.5.1 Calculations for Country X\n\\(CV_X = \\frac{6.06}{10} \\times 100\\% = 60.6\\%\\)\n\n\n11.14.5.2 Calculations for Country Y\n\\(CV_Y = \\frac{1.58}{10.5} \\times 100\\% = 15.0\\%\\)\n\n\n\n11.14.6 Comparison of Results\n\n\n\nMeasure\nCountry X (High var.)\nCountry Y (Low var.)\n\n\n\n\nMean\n10\n10.5\n\n\nMedian\n10\n10.5\n\n\nMode\nNone\n9, 10, 11, 12\n\n\nRange\n18\n5\n\n\nVariance\n36.67\n2.5\n\n\nStandard Dev.\n6.06\n1.58\n\n\nIQR\n12\n3\n\n\nCoef. of Var.\n60.6%\n15.0%\n\n\n\n\n\n11.14.7 Boxplot Comparison\nTo visually compare the distribution of district sizes between the two countries, we can use a Tukey-style boxplot. This type of plot provides a concise summary of the data’s distribution, including the median, quartiles, and potential outliers.\n\n# Load necessary library\nlibrary(ggplot2)\n\n# Create data frames for each country\ncountry_x &lt;- data.frame(country = \"X\", size = c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19))\ncountry_y &lt;- data.frame(country = \"Y\", size = c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13))\n\n# Combine the data\nall_data &lt;- rbind(country_x, country_y)\n\n# Create the boxplot\nggplot(all_data, aes(x = country, y = size, fill = country)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(title = \"Comparison of District Magnitude Variability\",\n       x = \"Country\",\n       y = \"District Size\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\"))\n\n\n\n\nComparison of District Magnitude Variability\n\n\n\n\n\n11.14.7.1 Interpreting the Boxplot\nThe boxplot provides several key pieces of information:\n\nThe box represents the interquartile range (IQR), with the lower edge at Q1 and the upper edge at Q3.\nThe line inside the box represents the median (Q2).\nThe whiskers extend to the smallest and largest values within 1.5 times the IQR from the edges of the box.\nAny points beyond the whiskers are considered potential outliers and are plotted individually.\n\nFrom this plot, we can observe:\n\nThe median district size for Country Y is slightly higher than for Country X, consistent with our earlier calculations.\nThe box for Country X is much larger than for Country Y, indicating a greater spread of the middle 50% of the data and thus higher variability.\nCountry X’s data spans a much wider range, as shown by the longer whiskers, further confirming its higher variability.\nCountry Y’s data is more tightly clustered, with a smaller box and shorter whiskers, indicating lower variability.\nThe individual points for Country X are more spread out, while those for Country Y are more clustered, providing a visual representation of the difference in variability.\n\nThis boxplot visualization reinforces our earlier numerical analysis, clearly showing the contrast in district size variability between the two countries.\n\n\n\n11.14.8 Key Observations\n\nCentral Tendency: Both countries have similar means and medians, indicating that their average district sizes are close.\nDispersion:\n\nCountry X shows much higher variance and standard deviation, confirming its high variability.\nThe range for Country X (18) is more than three times larger than for Country Y (5).\nThe IQR for Country X (12) is four times larger than for Country Y (3), indicating a much wider spread of the middle 50% of the data.\n\nDistribution Shape:\n\nCountry X has a uniform distribution with no clear mode.\nCountry Y has a more clustered distribution with multiple modes, indicating common district sizes.\n\nCoefficient of Variation:\n\nThe CV of Country X (60.6%) is significantly higher than Country Y (15.0%), providing a standardized measure of the difference in variability.\n\nVisual Comparison:\n\nThe boxplot clearly illustrates the stark difference in variability between the two countries, supporting our numerical findings.\n\n\n\n\n11.14.9 Conclusions\nThis analysis clearly shows the contrast in electoral district size variability between the two countries:\n\nCountry X exhibits high variability, with district sizes spread widely from 1 to 19. This may indicate a diverse electoral system with a mix of small (possibly single-member) and large multi-member districts.\nCountry Y exhibits low variability, with district sizes tightly clustered between 8 and 13. This suggests a more uniform electoral system, likely consisting of medium-sized multi-member districts.\n\nThese differences in variability can have significant implications for political representation, party strategies, and electoral outcomes in each country.\nThe addition of the boxplot provides a powerful visual tool for understanding these differences, making the contrast between the two countries immediately apparent and complementing the detailed numerical analysis.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "href": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.15 Appendix: Summary Tables for Data Types and Applicable Statistical Measures",
    "text": "11.15 Appendix: Summary Tables for Data Types and Applicable Statistical Measures\n\n11.15.1 Table 1: Discrete vs. Continuous Data\n\n\n\n\n\n\n\n\nCharacteristic\nDiscrete Data\nContinuous Data\n\n\n\n\nDefinition\nCan only take specific values\nCan take any value within a range\n\n\nExamples\nNumber of children, Shoe size\nHeight, Weight, Time\n\n\nPros\n- Easy to categorize- Straightforward to count- Often simpler to analyze\n- More precise measurements- Allows for more sophisticated statistical analyses- Can be grouped into intervals\n\n\nCons\n- Limited precision- May not capture subtle differences- Some statistical methods may not be applicable\n- Can be more complex to analyze- May require larger sample sizes for meaningful analysis- Rounding errors can occur in measurement\n\n\nVisualization\nBar charts, Pie charts\nHistograms, Scatter plots, Line graphs\n\n\n\n\n\n11.15.2 Table 2: Stevens’ Typology of Measurement Scales\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nDefinition\nCategories with no order\nOrdered categories\nEqual intervals, no true zero\nEqual intervals with true zero\n\n\nExamples\nGender, Blood type\nEducation level, Likert scales\nTemperature (°C, °F), Calendar dates\nHeight, Weight, Age\n\n\nPros\n- Easy to collect- Simple to categorize\n- Captures order- Useful for rankings\n- Allows for meaningful differences- More sophisticated analyses possible\n- Most versatile- Allows all arithmetic operations\n\n\nCons\n- Limited analytical options- No arithmetic operations\n- Differences between categories not quantifiable- Limited arithmetic operations\n- No true zero point- Ratios not meaningful\n- Can be difficult to obtain true ratio measurements in some fields\n\n\n\n\n\n11.15.3 Table 3: Pros and Cons of Various Statistical Measures\n\n11.15.3.1 Measures of Center\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nMean\n- Uses all data points- Allows for further statistical calculations- Ideal for normally distributed data\n- Sensitive to outliers- Not ideal for skewed distributions- Not meaningful for nominal data\nInterval, Ratio, some Discrete, Continuous\n\n\nMedian\n- Not affected by outliers- Good for skewed distributions- Can be used with ordinal data\n- Ignores the actual values of most data points- Less useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nMode\n- Can be used with any data type- Good for finding most common category\n- May not be unique (multimodal)- Not useful for many types of analyses- Ignores magnitude of differences between values\nAll types\n\n\n\n\n\n11.15.3.2 Measures of Variability\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nRange\n- Simple to calculate and understand- Gives quick idea of data spread\n- Very sensitive to outliers- Ignores all data between extremes- Not useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nInterquartile Range (IQR)\n- Not affected by outliers- Good for skewed distributions\n- Ignores 50% of the data- Less intuitive than range\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nVariance\n- Uses all data points- Basis for many statistical procedures\n- Sensitive to outliers- Units are squared (less intuitive)\nInterval, Ratio, some Discrete, Continuous\n\n\nStandard Deviation\n- Uses all data points- Same units as original data- Widely used and understood\n- Sensitive to outliers- Assumes roughly normal distribution for interpretation\nInterval, Ratio, some Discrete, Continuous\n\n\nCoefficient of Variation\n- Allows comparison between datasets with different units or means\n- Can be misleading when means are close to zero- Not meaningful for data with negative values\nRatio, some Interval\n\n\n\n\n\n11.15.3.3 Measures of Correlation/Association\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nPearson’s r\n- Measures linear relationship- Widely used and understood\n- Assumes normal distribution- Sensitive to outliers- Only captures linear relationships\nInterval, Ratio, Continuous\n\n\nSpearman’s rho\n- Can be used with ordinal data- Captures monotonic relationships- Less sensitive to outliers\n- Loses information by converting to ranks- May miss some types of relationships\nOrdinal, Interval, Ratio\n\n\nKendall’s tau\n- Can be used with ordinal data- More robust than Spearman’s for small samples- Has nice interpretation (probability of concordance)\n- Loses information by only considering order- Computationally more intensive\nOrdinal, Interval, Ratio\n\n\nChi-square\n- Can be used with nominal data- Tests independence of categorical variables\n- Requires large sample sizes- Sensitive to sample size- Doesn’t measure strength of association\nNominal, Ordinal\n\n\nCramér’s V\n- Can be used with nominal data- Provides measure of strength of association- Normalized to [0,1] range\n- Interpretation can be subjective- May overestimate association in small samples\nNominal, Ordinal\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓*\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\n11.15.4 Notes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html",
    "href": "rozdzial5.html",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "12.1 Wprowadzenie\nStatystyki opisowe są fundamentalnymi narzędziami w badaniach nauk społecznych, zapewniającymi zwięzłe podsumowanie charakterystyk danych. Pełnią kilka kluczowych funkcji:",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wprowadzenie",
    "href": "rozdzial5.html#wprowadzenie",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "Podsumowanie dużych zbiorów danych w przystępne informacje\nIdentyfikacja wzorców i trendów w danych\nWykrywanie potencjalnych anomalii lub wartości odstających\nZapewnienie podstawy do dalszej analizy statystycznej",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#typy-rozkładów-danych",
    "href": "rozdzial5.html#typy-rozkładów-danych",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.2 Typy rozkładów danych",
    "text": "12.2 Typy rozkładów danych\n\n\n\n\n\n\nImportant\n\n\n\nRozkład danych informuje o tym, jakie wartości przyjmuje zmienna i jak często.\n\n\nZrozumienie rozkładów danych jest kluczowe dla analizy i wizualizacji danych. W tym dokumencie przyjrzymy się różnym typom rozkładów i sposobom ich wizualizacji przy użyciu ggplot2 w R.\n\n12.2.1 Rozkład normalny\nRozkład normalny, znany również jako rozkład Gaussa, jest symetryczny i ma kształt dzwonu.\n\n# Generowanie danych o rozkładzie normalnym\ndane_normalne &lt;- data.frame(x = rnorm(1000))\n\n# Wykres\nggplot(dane_normalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład normalny\", x = \"Wartość\", y = \"Gęstość\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\n12.2.2 Rozkład jednostajny\nW rozkładzie jednostajnym wszystkie wartości mają równe prawdopodobieństwo wystąpienia.\n\n# Generowanie danych o rozkładzie jednostajnym\ndane_jednostajne &lt;- data.frame(x = runif(1000))\n\n# Wykres\nggplot(dane_jednostajne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład jednostajny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\n12.2.3 Rozkłady skośne\nRozkłady skośne są asymetryczne, z jednym ogonem dłuższym niż drugi.\n\n# Generowanie danych o rozkładzie prawoskośnym\ndane_prawoskosne &lt;- data.frame(x = rlnorm(1000))\n\n# Wykres\nggplot(dane_prawoskosne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład prawoskośny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\n12.2.4 Rozkład bimodalny\nRozkład bimodalny ma dwa szczyty, wskazujące na dwie odrębne podgrupy w danych.\n\n# Generowanie danych bimodalnych\ndane_bimodalne &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Wykres\nggplot(dane_bimodalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład bimodalny\", x = \"Wartość\", y = \"Gęstość\")",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wizualizacja-rozkładów-danych-rzeczywistych",
    "href": "rozdzial5.html#wizualizacja-rozkładów-danych-rzeczywistych",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.3 Wizualizacja rozkładów danych rzeczywistych",
    "text": "12.3 Wizualizacja rozkładów danych rzeczywistych\nUżyjemy zbioru danych palmerpenguins do zbadania rozkładów danych rzeczywistych.\n\n12.3.1 Histogram i wykres gęstości\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład długości płetw pingwinów\", \n       x = \"Długość płetwy (mm)\", \n       y = \"Gęstość\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\n12.3.2 Wykres pudełkowy\nWykresy pudełkowe są przydatne do porównywania rozkładów między kategoriami.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Rozkład masy ciała pingwinów według gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa ciała (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n12.3.3 Wykres skrzypcowy\nWykresy skrzypcowe łączą cechy wykresu pudełkowego i wykresu gęstości.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Rozkład masy ciała pingwinów według gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa ciała (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n12.3.4 Wykres grzbietowy\nWykresy grzbietowe są przydatne do porównywania wielu rozkładów.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Rozkład długości płetw według gatunku pingwina\",\n       x = \"Długość płetwy (mm)\",\n       y = \"Gatunek\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#podsumowanie",
    "href": "rozdzial5.html#podsumowanie",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.4 Podsumowanie",
    "text": "12.4 Podsumowanie\nZrozumienie i wizualizacja rozkładów danych są kluczowe w analizie danych. ggplot2 zapewnia elastyczny i potężny zestaw narzędzi do tworzenia różnych typów wykresów rozkładów. Badając różne techniki wizualizacji, możemy uzyskać wgląd w podstawowe wzorce i charakterystyki naszych danych.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#zrozumienie-wartości-odstających",
    "href": "rozdzial5.html#zrozumienie-wartości-odstających",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.5 Zrozumienie Wartości Odstających",
    "text": "12.5 Zrozumienie Wartości Odstających\nPrzed zagłębieniem się w konkretne miary, kluczowe jest zrozumienie pojęcia wartości odstających, ponieważ mogą one znacząco wpływać na wiele statystyk opisowych.\nWartości odstające to punkty danych, które znacznie różnią się od innych obserwacji w zbiorze danych. Mogą wystąpić z powodu:\n\nBłędów pomiaru lub zapisu\nPrawdziwych ekstremalnych wartości w populacji\nPróbkowania z innej populacji\n\nWartości odstające mogą mieć istotny wpływ na wiele miar statystycznych, szczególnie tych opartych na średnich lub sumach kwadratów odchyleń. Dlatego ważne jest, aby:\n\nIdentyfikować wartości odstające zarówno poprzez metody statystyczne, jak i wiedzę dziedzinową\nBadać przyczyny wartości odstających\nPodejmować świadome decyzje o tym, czy włączać je do analiz, czy nie\n\nW tym przewodniku omówimy, jak różne miary opisowe są dotknięte przez wartości odstające.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-tendencji-centralnej",
    "href": "rozdzial5.html#miary-tendencji-centralnej",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.6 Miary Tendencji Centralnej",
    "text": "12.6 Miary Tendencji Centralnej\nMiary tendencji centralnej mają na celu identyfikację “typowej” lub “centralnej” wartości w zbiorze danych. Trzy podstawowe miary to średnia, mediana i moda.\n\n12.6.1 Średnia Arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez liczbę wartości.\nWzór: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\)\nWażna Właściwość: Średnia jest punktem równowagi w danych. Suma odchyleń od średniej zawsze wynosi zero:\n\\(\\sum_{i=1}^n (x_i - \\bar{x}) = 0\\)\nTa właściwość sprawia, że średnia jest użyteczna w wielu obliczeniach statystycznych.\nPrzykład Ręcznego Obliczenia:\nObliczmy średnią dla zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nSumuj wszystkie wartości\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nPolicz liczbę wartości\nn = 7\n\n\n3\nPodziel sumę przez n\n36 / 7 = 5,14\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nŁatwa do obliczenia i zrozumienia\nWykorzystuje wszystkie punkty danych\nPrzydatna do dalszych obliczeń statystycznych\n\nWady:\n\nWrażliwa na wartości odstające\nNie idealna dla rozkładów skośnych\n\n\n\n12.6.2 Mediana\nMediana to środkowa wartość, gdy dane są uporządkowane.\nPrzykład Ręcznego Obliczenia:\nUżywając tego samego zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nWynik\n\n\n\n\n1\nUporządkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajdź środkową wartość\n5\n\n\n\nDla parzystej liczby wartości, weź średnią z dwóch środkowych wartości.\nObliczenie w R:\n\nmedian(dane)\n\n[1] 5\n\n\nZalety:\n\nNie jest dotknięta przez skrajne wartości odstające\nLepsza dla rozkładów skośnych\n\nWady:\n\nNie wykorzystuje wszystkich punktów danych\nMniej przydatna do dalszych obliczeń statystycznych\n\n\n\n12.6.3 Moda\nModa to najczęściej występująca wartość.\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nWartość\nCzęstość\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nModa to 4 i 5 (rozkład bimodalny).\nObliczenie w R:\n\nlibrary(modeest)\nmfv(dane)  # Najczęściej występująca wartość\n\n[1] 4 5\n\n\nZalety:\n\nJedyna miara tendencji centralnej dla danych nominalnych\nMoże identyfikować wiele szczytów w danych\n\nWady:\n\nNie zawsze jednoznacznie zdefiniowana\nNie przydatna dla danych ciągłych",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-zmienności",
    "href": "rozdzial5.html#miary-zmienności",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.7 Miary Zmienności",
    "text": "12.7 Miary Zmienności\nTe miary opisują, jak bardzo rozproszone są dane. Są kluczowe dla zrozumienia rozproszenia punktów danych wokół tendencji centralnej.\n\n12.7.1 Rozstęp\nRozstęp to różnica między wartością maksymalną a minimalną.\nWzór: \\(R = x_{max} - x_{min}\\)\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nZnajdź wartość maksymalną\n9\n\n\n2\nZnajdź wartość minimalną\n2\n\n\n3\nOdejmij minimum od maksimum\n9 - 2 = 7\n\n\n\nObliczenie w R:\n\nrange(dane)\n\n[1] 2 9\n\nmax(dane) - min(dane)\n\n[1] 7\n\n\nZalety:\n\nProsty do obliczenia i zrozumienia\nDaje natychmiastowe poczucie rozproszenia danych\n\nWady:\n\nBardzo wrażliwy na wartości odstające\nNie dostarcza informacji o rozkładzie między skrajnościami\n\n\n\n12.7.2 Rozstęp Międzykwartylowy (IQR)\nIQR to różnica między 75. a 25. percentylem.\nWzór: \\(IQR = Q_3 - Q_1\\)\nAby znaleźć kwartyle ręcznie:\n\nDla nieparzystej liczby wartości:\n\nQ2 (mediana) to środkowa wartość\nQ1 to mediana dolnej połowy\nQ3 to mediana górnej połowy\n\nDla parzystej liczby wartości:\n\nQ2 to średnia z dwóch środkowych wartości\nQ1 to mediana dolnej połowy (włącznie z dolną środkową wartością)\nQ3 to mediana górnej połowy (włącznie z górną środkową wartością)\n\n\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nUporządkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajdź Q2 (medianę)\n5\n\n\n3\nZnajdź Q1 (medianę dolnej połowy)\n4\n\n\n4\nZnajdź Q3 (medianę górnej połowy)\n7\n\n\n5\nOblicz IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nObliczenie w R:\n\nIQR(dane)\n\n[1] 2\n\n\nZalety:\n\nOdporny na wartości odstające\nDostarcza informacji o rozproszeniu środkowych 50% danych\n\nWady:\n\nIgnoruje ogony rozkładu\nMniej efektywny niż odchylenie standardowe dla rozkładów normalnych\n\n\n\n12.7.3 Wariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: \\(s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\\)\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz średnią\n\\(\\bar{x} = 5,14\\)\n\n\n2\nOdejmij średnią od każdej wartości i podnieś wynik do kwadratu\n\\((2 - 5,14)^2 = 9,86\\)\n\n\n\n\n\\((4 - 5,14)^2 = 1,30\\)\n\n\n\n\n\\((4 - 5,14)^2 = 1,30\\)\n\n\n\n\n\\((5 - 5,14)^2 = 0,02\\)\n\n\n\n\n\\((5 - 5,14)^2 = 0,02\\)\n\n\n\n\n\\((7 - 5,14)^2 = 3,46\\)\n\n\n\n\n\\((9 - 5,14)^2 = 14,90\\)\n\n\n3\nSumuj kwadraty różnic\n30,86\n\n\n4\nPodziel przez (n-1)\n30,86 / 6 = 5,14\n\n\n\nObliczenie w R:\n\nvar(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nWykorzystuje wszystkie punkty danych\nPodstawa dla wielu testów statystycznych\n\nWady:\n\nJednostki są podniesione do kwadratu, co utrudnia interpretację\nWrażliwa na wartości odstające\n\n\n\n12.7.4 Odchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji.\nWzór: \\(s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\\)\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz wariancję\n\\(s^2 = 5,14\\) (z poprzedniego obliczenia)\n\n\n2\nWyciągnij pierwiastek kwadratowy\n\\(s = \\sqrt{5,14} = 2,27\\)\n\n\n\nObliczenie w R:\n\nsd(dane)\n\n[1] 2.267787\n\n\nZalety:\n\nW tych samych jednostkach co oryginalne dane\nSzeroko stosowane i zrozumiałe\n\nWady:\n\nNadal wrażliwe na wartości odstające\nZakłada, że dane są w przybliżeniu normalnie rozłożone",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-pozycji-względnej",
    "href": "rozdzial5.html#miary-pozycji-względnej",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.8 Miary pozycji względnej",
    "text": "12.8 Miary pozycji względnej\nTe miary pomagają nam zrozumieć, gdzie konkretna wartość znajduje się w odniesieniu do całego zbioru danych.\n\n12.8.1 Percentyle\nPercentyle dzielą dane na 100 równych części.\nWzór: Dla k-tego percentyla: \\(P_k = L + \\frac{k(n+1)}{100}\\), gdzie L to dolna granica przedziału\nPrzykład obliczeń ręcznych (uproszczona metoda): Aby znaleźć 25. percentyl (Q1) dla zbioru danych: 2, 4, 4, 5, 5, 7, 9 1. Uporządkuj dane: 2, 4, 4, 5, 5, 7, 9 2. Oblicz pozycję: \\((25/100) * (7+1) = 2\\) 3. 25. percentyl to druga wartość: 4\nObliczenia w R:\n\nquantile(dane, probs = seq(0, 1, 0.25))\n\n  0%  25%  50%  75% 100% \n   2    4    5    6    9 \n\n\n\n\n12.8.2 Kwartyle\nKwartyle dzielą dane na cztery równe części.\n\nQ1: 25. percentyl\nQ2: Mediana (50. percentyl)\nQ3: 75. percentyl\n\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\nObliczenia ręczne: Zobacz sekcję IQR, aby zapoznać się z procesem krok po kroku, jak ręcznie znaleźć kwartyle.\nObliczenia w R:\n\nsummary(dane)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   4.000   5.000   5.143   6.000   9.000 \n\n\nZalety:\n\nOdporność na wartości odstające\nDostarczają informacji o rozproszeniu danych i skośności\n\nWady:\n\nMniej precyzyjne niż wykorzystanie wszystkich punktów danych\nWiele metod obliczania może prowadzić do nieco różnych wyników",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-kształtu",
    "href": "rozdzial5.html#miary-kształtu",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.9 Miary Kształtu",
    "text": "12.9 Miary Kształtu\n\n12.9.1 Skośność\n\n12.9.1.1 Definicja\nSkośność kwantyfikuje asymetrię rozkładu prawdopodobieństwa. Wskazuje, czy dane grupują się bardziej po jednej stronie średniej niż po drugiej.\n\n\n12.9.1.2 Wyrażenie Matematyczne\n\\(SK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3\\) gdzie:\n\n\\(n\\) to wielkość próby\n\\(x_i\\) to i-ta obserwacja\n\\(\\bar{x}\\) to średnia z próby\n\\(s\\) to odchylenie standardowe z próby\n\n\n\n12.9.1.3 Przykład: Analiza Frekwencji Wyborczej\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n# Generowanie przykładowych danych frekwencji w obwodach wyborczych\nset.seed(123)\ndane_frekwencji &lt;- c(\n  # Obwody miejskie\n  rnorm(300, mean = 65, sd = 12),\n  # Obwody podmiejskie\n  rnorm(400, mean = 70, sd = 10),\n  # Obwody wiejskie\n  rnorm(300, mean = 68, sd = 15)\n) |&gt; \n  # Zapewnienie, że frekwencja mieści się w przedziale 0-100%\n  pmax(0) |&gt; \n  pmin(100)\n\n# Obliczenie i wizualizacja\nwartosc_skosnosci &lt;- skewness(dane_frekwencji)\nwartosc_skosnosci\n\n[1] 0.02558143\n\nggplot(data.frame(x = dane_frekwencji), aes(x = x)) +\n  geom_histogram(bins = 50, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = mean(dane_frekwencji), color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = median(dane_frekwencji), color = \"blue\", linetype = \"dashed\") +\n  labs(\n    title = str_glue(\"Rozkład Frekwencji w Obwodach Wyborczych (Skośność = {round(wartosc_skosnosci, 4)})\"),\n    subtitle = \"Czerwona: Średnia, Niebieska: Mediana\",\n    x = \"Frekwencja Wyborcza (%)\",\n    y = \"Liczba Obwodów\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n12.9.1.4 Przewodnik Interpretacji\n\nSkośność Dodatnia (&gt; 0): Rozkład ma dłuższy ogon prawy\nSkośność Ujemna (&lt; 0): Rozkład ma dłuższy ogon lewy\nSkośność Zero: Rozkład w przybliżeniu symetryczny\n\n\n\n\n12.9.2 Kurtoza\n\n12.9.2.1 Definicja\nKurtoza mierzy “ogoniastość” rozkładu, wskazując na obecność wartości ekstremalnych w porównaniu z rozkładem normalnym.\n\n\n12.9.2.2 Wyrażenie Matematyczne\n\\(K = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\\)\n\n\n12.9.2.3 Przykład: Analiza Głosowań Parlamentarnych\n\n# Generowanie przykładowych wyników zgodności głosowań\nset.seed(456)\nzgodnosc_glosowan &lt;- c(\n  # Standardowe wzorce głosowania\n  rnorm(400, mean = 75, sd = 10),\n  # Przypadki współpracy międzypartyjnej\n  rnorm(80, mean = 50, sd = 15),\n  # Głosowania zgodne z linią partii\n  rnorm(20, mean = 95, sd = 5)\n) |&gt; \n  pmax(0) |&gt; \n  pmin(100)\n\nwartosc_kurtozy &lt;- kurtosis(zgodnosc_glosowan)\nwartosc_kurtozy\n\n[1] 3.849939\n\n# Wizualizacja z porównaniem do rozkładu normalnego\nzakres_x &lt;- seq(min(zgodnosc_glosowan)-1, max(zgodnosc_glosowan)+1, length.out = 100)\nrozklad_normalny &lt;- dnorm(zakres_x, mean = mean(zgodnosc_glosowan), sd = sd(zgodnosc_glosowan))\n\nggplot() +\n  geom_density(\n    data = data.frame(x = zgodnosc_glosowan), \n    aes(x = x), \n    fill = \"skyblue\", \n    alpha = 0.5\n  ) +\n  geom_line(\n    data = data.frame(x = zakres_x, y = rozklad_normalny),\n    aes(x = x, y = y),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) +\n  labs(\n    title = str_glue(\"Rozkład Zgodności Głosowań (Kurtoza = {round(wartosc_kurtozy, 4)})\"),\n    subtitle = \"Rozkład obserwowany (niebieski) vs. Rozkład normalny (czerwony)\",\n    x = \"Wskaźnik Zgodności Głosowań (%)\",\n    y = \"Gęstość\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n12.9.2.4 Przewodnik Interpretacji\n\nNadwyżka Kurtozy: Różnica względem kurtozy rozkładu normalnego (3)\nLeptokurtyczny (&gt; 3): Więcej wartości ekstremalnych niż w rozkładzie normalnym\nPlatykurtyczny (&lt; 3): Mniej wartości ekstremalnych niż w rozkładzie normalnym\nMezokurtyczny (= 3): Podobny do rozkładu normalnego",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#statystyki-dwuwymiarowe",
    "href": "rozdzial5.html#statystyki-dwuwymiarowe",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.10 Statystyki Dwuwymiarowe",
    "text": "12.10 Statystyki Dwuwymiarowe\nStatystyki dwuwymiarowe opisują związek między dwiema zmiennymi. Omówimy kilka miar, zaczynając od kowariancji i przechodząc do bardziej zaawansowanych miar korelacji.\n\n12.10.1 Kowariancja\nKowariancja mierzy, jak dwie zmienne zmieniają się razem.\nWzór: \\(cov(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\\)\nPrzykład Ręcznego Obliczenia:\nObliczmy kowariancję dla dwóch zmiennych: x: 1, 2, 3, 4, 5 y: 2, 4, 5, 4, 5\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz średnie\n\\(\\bar{x} = 3, \\bar{y} = 4\\)\n\n\n2\nOblicz \\((x_i - \\bar{x})(y_i - \\bar{y})\\) dla każdej pary\n\\((-2)(-2) = 4\\)\n\n\n\n\n\\((-1)(0) = 0\\)\n\n\n\n\n\\((0)(1) = 0\\)\n\n\n\n\n\\((1)(0) = 0\\)\n\n\n\n\n\\((2)(1) = 2\\)\n\n\n3\nZsumuj wyniki\n\\(4 + 0 + 0 + 0 + 2 = 6\\)\n\n\n4\nPodziel przez (n-1)\n\\(6 / 4 = 1,5\\)\n\n\n\nObliczenie w R:\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 5, 4, 5)\ncov(x, y)\n\n[1] 1.5\n\n\nInterpretacja: - Dodatnia kowariancja (1,5) wskazuje, że x i y mają tendencję do wzrostu razem.\nZalety:\n\nDostarcza informacji o kierunku związku (dodatni lub ujemny)\nPrzydatna w obliczaniu innych miar, takich jak korelacja\n\nWady:\n\nZależna od skali, co utrudnia porównywanie między różnymi parami zmiennych\nNie dostarcza informacji o sile związku\n\n\n\n12.10.2 Korelacja Pearsona\nKorelacja Pearsona mierzy siłę i kierunek liniowego związku między dwiema zmiennymi ciągłymi.\nWzór: \\(r = \\frac{cov(X,Y)}{s_X s_Y} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\\)\nPrzykład Ręcznego Obliczenia:\nUżywając tych samych danych co wyżej:\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz kowariancję\n(Z poprzedniego obliczenia) 1,5\n\n\n2\nOblicz odchylenia standardowe\n\\(s_X = \\sqrt{\\frac{10}{4}} = 1,58, s_Y = \\sqrt{\\frac{6}{4}} = 1,22\\)\n\n\n3\nPodziel kowariancję przez iloczyn odchyleń standardowych\n\\(1,5 / (1,58 * 1,22) = 0,7746\\)\n\n\n\nObliczenie w R:\n\ncor(x, y, method = \"pearson\")\n\n[1] 0.7745967\n\n\nInterpretacja: - Współczynnik korelacji 0,7746 wskazuje na silny dodatni związek liniowy między x i y.\nZalety:\n\nNiezależna od skali, zawsze między -1 a 1\nSzeroko rozumiana i stosowana\nTestuje związki liniowe\n\nWady:\n\nWrażliwa na wartości odstające\nMierzy tylko związki liniowe\nZakłada normalnie rozłożone zmienne\n\n\n\n12.10.3 Korelacja Spearmana\nKorelacja Spearmana mierzy siłę i kierunek monotonicznego związku między dwiema zmiennymi, które mogą być ciągłe lub porządkowe.\nWzór: \\(r_s = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\\), gdzie \\(d_i\\) to różnica między rangami.\nPrzykład Ręcznego Obliczenia:\nUżyjmy nieco innych danych: x: 1, 2, 3, 4, 5 y: 1, 3, 2, 5, 4\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPrzypisz rangi obu zmiennym\nx_ranga: 1, 2, 3, 4, 5\n\n\n\n\ny_ranga: 1, 3, 2, 5, 4\n\n\n2\nOblicz różnice w rangach (d)\n0, -1, 1, -1, 1\n\n\n3\nPodnieś różnice do kwadratu\n0, 1, 1, 1, 1\n\n\n4\nZsumuj kwadraty różnic\n\\(\\sum d_i^2 = 4\\)\n\n\n5\nZastosuj wzór\n\\(r_s = 1 - \\frac{6(4)}{5(5^2 - 1)} = 0,8\\)\n\n\n\nObliczenie w R:\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(1, 3, 2, 5, 4)\ncor(x, y, method = \"spearman\")\n\n[1] 0.8\n\n\nInterpretacja: - Korelacja Spearmana 0,8 wskazuje na silny dodatni związek monotoniczny między x i y.\nZalety:\n\nOdporna na wartości odstające\nMoże wykrywać nieliniowe związki monotoniczne\nOdpowiednia dla danych porządkowych\n\nWady:\n\nMniej odporna niż korelacja Pearsona do wykrywania związków liniowych w normalnie rozłożonych danych\nNie dostarcza informacji o kształcie związku poza monotonicznością\n\n\n\n12.10.4 Tabela Krzyżowa\nTabela krzyżowa (tabela kontyngencji) pokazuje związek między dwiema zmiennymi kategorycznymi.\nPrzykład:\nStwórzmy tabelę krzyżową dla dwóch zmiennych: - Poziom wykształcenia: Średnie, Wyższe, Podyplomowe - Status zatrudnienia: Zatrudniony, Bezrobotny\n\nwyksztalcenie &lt;- factor(c(\"Średnie\", \"Wyższe\", \"Podyplomowe\", \"Średnie\", \"Wyższe\", \"Podyplomowe\", \"Średnie\", \"Wyższe\", \"Podyplomowe\"))\nzatrudnienie &lt;- factor(c(\"Zatrudniony\", \"Zatrudniony\", \"Zatrudniony\", \"Bezrobotny\", \"Zatrudniony\", \"Zatrudniony\", \"Bezrobotny\", \"Bezrobotny\", \"Zatrudniony\"))\n\ntable(wyksztalcenie, zatrudnienie)\n\n             zatrudnienie\nwyksztalcenie Bezrobotny Zatrudniony\n  Podyplomowe          0           3\n  Średnie              2           1\n  Wyższe               1           2\n\n\nInterpretacja:\n\nTa tabela pokazuje liczbę osób w każdej kombinacji poziomu wykształcenia i statusu zatrudnienia.\nNa przykład, możemy zobaczyć, ilu absolwentów szkół średnich jest zatrudnionych, a ilu bezrobotnych.\n\nZalety:\n\nZapewnia jasną wizualną reprezentację związku między zmiennymi kategorycznymi\nŁatwa do zrozumienia i interpretacji\nPodstawa dla wielu testów statystycznych (np. test chi-kwadrat niezależności)\n\nWady:\n\nOgraniczona do danych kategorycznych\nMoże stać się nieporęczna przy wielu kategoriach\nNie dostarcza pojedynczej statystyki podsumowującej siłę związku",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wybór-odpowiedniej-miary",
    "href": "rozdzial5.html#wybór-odpowiedniej-miary",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.11 Wybór Odpowiedniej Miary",
    "text": "12.11 Wybór Odpowiedniej Miary\nPrzy wyborze statystyki dwuwymiarowej należy wziąć pod uwagę:\n\nTyp danych:\n\nDane ciągłe: Kowariancja, korelacja Pearsona\nDane porządkowe: Korelacja Spearmana\nDane kategoryczne: Tabela krzyżowa\n\nTyp związku:\n\nLiniowy: Korelacja Pearsona\nMonotoniczny, ale potencjalnie nieliniowy: Korelacja Spearmana\n\nObecność wartości odstających:\n\nJeśli wartości odstające są problemem, korelacja Spearmana jest bardziej odporna\n\nRozkład:\n\nDla normalnie rozłożonych danych korelacja Pearsona jest najbardziej odporna (robust)\nDla rozkładów “nienormalnych” rozważ korelację Spearmana\n\nWielkość próby:\n\nDla bardzo małych prób metody nieparametryczne, takie jak korelacja Spearmana, mogą być preferowane\n\n\nPamiętaj, że często wartościowe jest użycie wielu miar i wizualizacji (takich jak wykresy rozrzutu), aby uzyskać kompleksowe zrozumienie związku między zmiennymi.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wprowadzenie-do-podstawowej-statystyki-wielowymiarowej",
    "href": "rozdzial5.html#wprowadzenie-do-podstawowej-statystyki-wielowymiarowej",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.12 Wprowadzenie do Podstawowej Statystyki Wielowymiarowej",
    "text": "12.12 Wprowadzenie do Podstawowej Statystyki Wielowymiarowej\nStatystyki wielowymiarowe obejmują analizę związków między trzema lub więcej zmiennymi jednocześnie. Ta sekcja wprowadzi niektóre podstawowe koncepcje i techniki analizy wielowymiarowej, koncentrując się na metodach opartych na korelacji.\n\n12.12.1 Macierz Korelacji\nMacierz korelacji to tabela pokazująca korelacje parami dla kilku zmiennych. Jest to podstawowe narzędzie w analizie wielowymiarowej.\nPrzykład: Stwórzmy macierz korelacji dla czterech zmiennych: wzrost, waga, wiek i dochód.\n\nset.seed(123)  # Dla powtarzalności\nwzrost &lt;- rnorm(100, 170, 10)\nwaga &lt;- wzrost * 0.5 + rnorm(100, 0, 5)\nwiek &lt;- rnorm(100, 40, 10)\ndochod &lt;- wiek * 1000 + rnorm(100, 0, 10000)\n\ndane &lt;- data.frame(wzrost, waga, wiek, dochod)\n\nmacierz_kor &lt;- cor(dane)\nprint(macierz_kor)\n\n           wzrost        waga        wiek      dochod\nwzrost  1.0000000  0.66712996 -0.12917601 -0.12246786\nwaga    0.6671300  1.00000000 -0.06814187 -0.04579492\nwiek   -0.1291760 -0.06814187  1.00000000  0.65654902\ndochod -0.1224679 -0.04579492  0.65654902  1.00000000\n\n\nInterpretacja: - Każda komórka pokazuje korelację między dwiema zmiennymi. - Przekątna zawsze wynosi 1 (korelacja zmiennej z samą sobą). - Szukaj silnych korelacji (bliskich 1 lub -1), aby zidentyfikować potencjalne związki.\n\n\n12.12.2 Wizualizacja Związków Wielowymiarowych\n\n12.12.2.1 Macierz Wykresów Rozrzutu\nMacierz wykresów rozrzutu pokazuje parami związki między wieloma zmiennymi.\n\npairs(dane)\n\n\n\n\n\n\n\n\nInterpretacja:\n\nKażdy wykres pokazuje związek między dwiema zmiennymi.\nElementy na przekątnej pokazują rozkład każdej zmiennej.\nSzukaj wzorców, skupisk lub trendów na wykresach.\n\n\n\n12.12.2.2 Wykres Korelacji\nWykres korelacji zapewnia wizualną reprezentację macierzy korelacji.\n\nlibrary(corrplot)\n\ncorrplot 0.94 loaded\n\ncorrplot(macierz_kor, method = \"color\")\n\n\n\n\n\n\n\n\nInterpretacja:\n\nIntensywność koloru i rozmiar kół wskazują na siłę korelacji.\nNiebieskie kolory zazwyczaj wskazują na dodatnie korelacje, czerwone na ujemne.\n\n\n\n\n12.12.3 Korelacja Cząstkowa\nKorelacja cząstkowa mierzy związek między dwiema zmiennymi przy kontrolowaniu jednej lub więcej innych zmiennych.\nPrzykład: Obliczmy korelację cząstkową między wzrostem a wagą, kontrolując wiek.\n\nlibrary(ppcor)\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\npcor.test(dane$wzrost, dane$waga, dane$wiek)\n\n   estimate      p.value statistic   n gp  Method\n1 0.6654367 5.758157e-14  8.779896 100  1 pearson\n\n\nInterpretacja:\n\nPorównaj to z prostą korelacją między wzrostem a wagą.\nZnacząca zmiana może wskazywać, że wiek odgrywa rolę w związku między wzrostem a wagą.\n\n\n\n12.12.4 Korelacja Wielokrotna\nKorelacja wielokrotna mierzy siłę związku między zmienną zależną a wieloma zmiennymi niezależnymi.\nPrzykład: Przewidźmy wagę na podstawie wzrostu i wieku.\n\nmodel &lt;- lm(waga ~ wzrost + wiek, data = dane)\nR &lt;- sqrt(summary(model)$r.squared)\nprint(paste(\"Współczynnik korelacji wielokrotnej:\", R))\n\n[1] \"Współczynnik korelacji wielokrotnej: 0.667377840470434\"\n\n\nInterpretacja:\n\nR waha się od 0 do 1, przy czym wyższe wartości wskazują na silniejsze związki.\nR² (R-kwadrat) reprezentuje proporcję wariancji w zmiennej zależnej wyjaśnioną przez zmienne niezależne.\n\n\n\n12.12.5 Analiza Czynnikowa\nAnaliza czynnikowa to technika używana do zredukowania wielu zmiennych do mniejszej liczby czynników leżących u podstaw.\nPrzykład: Wykonajmy prostą analizę czynnikową na naszym zbiorze danych.\n\nlibrary(psych)\n\nRegistered S3 method overwritten by 'psych':\n  method         from  \n  plot.residuals rmutil\n\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nwynik_fa &lt;- fa(dane, nfactors = 2, rotate = \"varimax\")\nprint(wynik_fa$loadings, cutoff = 0.3)\n\n\nLoadings:\n       MR2    MR1   \nwzrost  0.798       \nwaga    0.836       \nwiek           0.729\ndochod         0.895\n\n                 MR2   MR1\nSS loadings    1.344 1.341\nProportion Var 0.336 0.335\nCumulative Var 0.336 0.671\n\n\nInterpretacja:\n\nSpójrz, które zmienne ładują się wysoko na każdy czynnik.\nSpróbuj zinterpretować, co każdy czynnik może reprezentować na podstawie zmiennych, które się na niego ładują.\n\n\n\n12.12.6 Uwagi dotyczące Analizy Wielowymiarowej\n\nWielkość próby: Techniki wielowymiarowe często wymagają większych prób dla stabilnych wyników.\nWspółliniowość: Wysokie korelacje między zmiennymi niezależnymi mogą powodować problemy w niektórych analizach.\nWartości odstające: Wielowymiarowe wartości odstające mogą mieć silny wpływ na wyniki.\nZałożenia: Wiele technik zakłada wielowymiarową normalność i liniowe związki.\nZłożoność interpretacji: Wraz ze wzrostem liczby zmiennych interpretacja może stać się bardziej wyzwająca.\n\n\n\n12.12.7 Podsumowanie\nTo wprowadzenie do statystyki wielowymiarowej opiera się na koncepcji korelacji, aby badać związki między wieloma zmiennymi. Techniki te zapewniają potężne narzędzia do zrozumienia złożonych zbiorów danych, ale wymagają również starannego rozważenia założeń i ograniczeń. W miarę postępu w Twojej podróży statystycznej napotkasz bardziej zaawansowane techniki wielowymiarowe, takie jak MANOVA, analiza dyskryminacyjna i modelowanie równań strukturalnych.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ćwiczenie-1.-porównanie-wynagrodzeń",
    "href": "rozdzial5.html#ćwiczenie-1.-porównanie-wynagrodzeń",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.13 Ćwiczenie 1. Porównanie wynagrodzeń",
    "text": "12.13 Ćwiczenie 1. Porównanie wynagrodzeń\n\n12.13.1 Dane\nMamy dane o wynagrodzeniach (w tysiącach euro) z dwóch małych firm europejskich:\n\n\n\nIndex\nFirma X\nFirma Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\n\n\n12.13.2 Miary tendencji centralnej\n\n12.13.2.1 Średnia arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez ich liczbę.\nWzór: \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\)\n\n12.13.2.1.1 Obliczenia ręczne dla Firmy X\n\n\n\nWartość (\\(x_i\\))\nCzęstość (\\(f_i\\))\n\\(x_i \\cdot f_i\\)\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nSuma\nn = 20\nSuma = 119\n\n\n\n\\(\\bar{x} = \\frac{119}{20} = 5,95\\)\n\n\n12.13.2.1.2 Obliczenia ręczne dla Firmy Y\n\n\n\nWartość (\\(x_i\\))\nCzęstość (\\(f_i\\))\n\\(x_i \\cdot f_i\\)\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nSuma\nn = 20\nSuma = 100\n\n\n\n\\(\\bar{y} = \\frac{100}{20} = 5\\)\n\n\n12.13.2.1.3 Weryfikacja w R\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\n12.13.2.2 Mediana\nMediana to wartość środkowa w uporządkowanym zbiorze danych.\n\n12.13.2.2.1 Obliczenia ręczne dla Firmy X\nUporządkowane dane: 2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35\nn = 20 (parzyste), więc bierzemy średnią z 10. i 11. wartości:\nMediana = \\(\\frac{4 + 4}{2} = 4\\)\n\n\n12.13.2.2.2 Obliczenia ręczne dla Firmy Y\nUporządkowane dane: 3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8\nn = 20 (parzyste), więc bierzemy średnią z 10. i 11. wartości:\nMediana = \\(\\frac{5 + 5}{2} = 5\\)\n\n\n12.13.2.2.3 Weryfikacja w R\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\n12.13.2.3 Dominanta (moda)\nDominanta to najczęściej występująca wartość w zbiorze danych.\nDla Firmy X dominanta wynosi 3 (występuje 6 razy). Dla Firmy Y są dwie dominanty: 4 i 5 (obie występują 6 razy).\n\n# Funkcja do obliczania dominanty\nznajdz_dominante &lt;- function(x) {\n  unikalne_x &lt;- unique(x)\n  unikalne_x[which.max(tabulate(match(x, unikalne_x)))]\n}\n\nznajdz_dominante(X)\n\n[1] 3\n\nznajdz_dominante(Y)\n\n[1] 4\n\n\n\n\n\n12.13.3 Miary rozproszenia\n\n12.13.3.1 Wariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: \\(s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\)\n\n12.13.3.1.1 Obliczenia ręczne dla Firmy X\n\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(f_i\\)\n\\(x_i - \\bar{x}\\)\n\\((x_i - \\bar{x})^2\\)\n\\(f_i(x_i - \\bar{x})^2\\)\n\n\n\n\n2\n3\n-3,95\n15,6025\n46,8075\n\n\n3\n6\n-2,95\n8,7025\n52,215\n\n\n4\n5\n-1,95\n3,8025\n19,0125\n\n\n5\n4\n-0,95\n0,9025\n3,61\n\n\n20\n1\n14,05\n197,4025\n197,4025\n\n\n35\n1\n29,05\n843,9025\n843,9025\n\n\nSuma\n20\n\n\n1162,95\n\n\n\n\\(s^2 = \\frac{1162,95}{19} = 61,21\\)\n\n\n12.13.3.1.2 Obliczenia ręczne dla Firmy Y\n\n\n\n\n\n\n\n\n\n\n\\(y_i\\)\n\\(f_i\\)\n\\(y_i - \\bar{x}\\)\n\\((y_i - \\bar{y})^2\\)\n\\(f_i(y_i - \\bar{y})^2\\)\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nSuma\n20\n\n\n34\n\n\n\n\\(s^2 = \\frac{34}{19} = 1,79\\)\n\n\n12.13.3.1.3 Weryfikacja w R\n\nvar(X)\n\n[1] 61.20789\n\nvar(Y)\n\n[1] 1.789474\n\n\n\n\n\n12.13.3.2 Odchylenie standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji.\nWzór: \\(s = \\sqrt{s^2}\\)\n\nDla Firmy X: \\(s = \\sqrt{61,21} = 7,82\\)\nDla Firmy Y: \\(s = \\sqrt{1,79} = 1,34\\)\n\n\n12.13.3.2.1 Weryfikacja w R\n\nsd(X)\n\n[1] 7.823547\n\nsd(Y)\n\n[1] 1.337712\n\n\n\n\n\n\n12.13.4 Kwartyle\nKwartyle dzielą zbiór danych na cztery równe części.\n\n12.13.4.1 Obliczenia ręczne dla Firmy X\nUporządkowane dane: 2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 3\nQ2 (50. percentyl, mediana): 4\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 5\n\n\n\n12.13.4.2 Obliczenia ręczne dla Firmy Y\nUporządkowane dane: 3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 4\nQ2 (50. percentyl, mediana): 5\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 6\n\n\n\n12.13.4.3 Weryfikacja w R\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\n\n12.13.5 Wykres pudełkowy Tukeya\nWykres pudełkowy Tukeya wizualnie przedstawia rozkład danych na podstawie kwartyli. Użyjemy biblioteki ggplot2 do stworzenia wykresu.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Przygotowanie danych\ndane &lt;- data.frame(\n  Firma = rep(c(\"X\", \"Y\"), each = 20),\n  Wynagrodzenie = c(X, Y)\n)\n\n# Tworzenie wykresu pudełkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot() +\n  labs(title = \"Rozkład wynagrodzeń w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiące euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\n12.13.5.1 Interpretacja wykresu pudełkowego\n\nPudełko reprezentuje rozstęp międzykwartylowy (IQR) od Q1 do Q3.\nLinia wewnątrz pudełka to mediana (Q2).\nWąsy rozciągają się do najmniejszych i największych wartości w granicach 1,5 * IQR.\nPunkty poza wąsami są uznawane za wartości odstające.\n\n\n\n\n12.13.6 Porównanie wyników\n\n\n\nMiara\nFirma X\nFirma Y\n\n\n\n\nŚrednia\n5,95\n5,00\n\n\nMediana\n4\n5\n\n\nDominanta\n3\n4 i 5\n\n\nWariancja\n61,21\n1,79\n\n\nOdchylenie standard.\n7,82\n1,34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\n12.13.6.1 Kluczowe obserwacje:\n\nTendencja centralna: Firma X ma wyższą średnią, ale niższą medianę niż Firma Y, co wskazuje na prawostronnie skośny rozkład dla Firmy X.\nRozproszenie: Firma X wykazuje znacznie wyższą wariancję i odchylenie standardowe, sugerując większe dysproporcje w wynagrodzeniach.\nKształt rozkładu: Wynagrodzenia w Firmie Y są bardziej skupione, podczas gdy Firma X ma wartości ekstremalne (potencjalne wartości odstające), które znacząco wpływają na jej średnią i wariancję.\nKwartyle: Rozstęp międzykwartylowy (Q3 - Q1) Firmy Y jest nieznacznie większy, ale jej ogólny zakres jest znacznie mniejszy niż Firmy X.\n\n\n\n\n12.13.7 Wnioski\nTa analiza porównawcza ujawnia znaczące różnice w strukturach wynagrodzeń między dwiema firmami. Firma X wykazuje większą zmienność i potencjalną nierówność w swojej skali płac, podczas gdy Firma Y demonstruje bardziej spójny i wąsko rozłożony zakres wynagrodzeń.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ćwiczenie-2.-porównanie-zmienności-wielkości-okręgów-wyborczych-między-krajami",
    "href": "rozdzial5.html#ćwiczenie-2.-porównanie-zmienności-wielkości-okręgów-wyborczych-między-krajami",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.14 Ćwiczenie 2. Porównanie Zmienności Wielkości Okręgów Wyborczych Między Krajami",
    "text": "12.14 Ćwiczenie 2. Porównanie Zmienności Wielkości Okręgów Wyborczych Między Krajami\n\n12.14.1 Dane\nMamy dane o wielkości okręgów wyborczych z dwóch krajów:\n\nKraj o wysokiej zmienności (X): 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nKraj o niskiej zmienności (Y): 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\n\n\n\n12.14.2 Miary Tendencji Centralnej\n\n12.14.2.1 Średnia Arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez ich liczbę.\nWzór: \\(\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\)\n\n12.14.2.1.1 Obliczenia dla Kraju X\n\n\n\nWartości\nSuma\n\n\n\n\n1 + 3 + 5 + 7 + 9 + 11 + 13 + 15 + 17 + 19\n100\n\n\n\n\\(\\bar{x} = \\frac{100}{10} = 10\\)\n\n\n12.14.2.1.2 Obliczenia dla Kraju Y\n\n\n\nWartości\nSuma\n\n\n\n\n8 + 9 + 9 + 10 + 10 + 11 + 11 + 12 + 12 + 13\n105\n\n\n\n\\(\\bar{y} = \\frac{105}{10} = 10,5\\)\n\n\n\n12.14.2.2 Mediana\nMediana to środkowa wartość w uporządkowanym zbiorze danych.\n\n12.14.2.2.1 Obliczenia dla Kraju X\nUporządkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMediana = \\(\\frac{9 + 11}{2} = 10\\)\n\n\n12.14.2.2.2 Obliczenia dla Kraju Y\nUporządkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMediana = \\(\\frac{10 + 11}{2} = 10,5\\)\n\n\n\n12.14.2.3 Dominanta\nDominanta to najczęściej występująca wartość w zbiorze danych.\nDla Kraju X nie ma dominanty (wszystkie wartości występują raz). Dla Kraju Y są cztery dominanty: 9, 10, 11 i 12 (każda występuje dwa razy).\n\n\n\n12.14.3 Miary Rozproszenia\n\n12.14.3.1 Rozstęp\nRozstęp to różnica między wartością maksymalną a minimalną.\n\n12.14.3.1.1 Obliczenia dla Kraju X\nRozstęp = 19 - 1 = 18\n\n\n12.14.3.1.2 Obliczenia dla Kraju Y\nRozstęp = 13 - 8 = 5\n\n\n\n12.14.3.2 Wariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: \\(s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\\)\n\n12.14.3.2.1 Obliczenia dla Kraju X\n\n\n\n\\(x_i\\)\n\\((x_i - \\bar{x})\\)\n\\((x_i - \\bar{x})^2\\)\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSuma\n\n330\n\n\n\n\\(s^2_X = \\frac{330}{9} = 36,67\\)\n\n\n12.14.3.2.2 Obliczenia dla Kraju Y\n\n\n\n\\(x_i\\)\n\\((y_i - \\bar{y})\\)\n\\((y_i - \\bar{y})^2\\)\n\n\n\n\n8\n-2,5\n6,25\n\n\n9\n-1,5\n2,25\n\n\n9\n-1,5\n2,25\n\n\n10\n-0,5\n0,25\n\n\n10\n-0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n12\n1,5\n2,25\n\n\n12\n1,5\n2,25\n\n\n13\n2,5\n6,25\n\n\nSuma\n\n22,5\n\n\n\n\\(s^2_Y = \\frac{22,5}{9} = 2,5\\)\n\n\n\n12.14.3.3 Odchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji.\nWzór: \\(s = \\sqrt{s^2}\\)\n\n12.14.3.3.1 Obliczenia dla Kraju X\n\\(s_X = \\sqrt{36,67} \\approx 6,06\\)\n\n\n12.14.3.3.2 Obliczenia dla Kraju Y\n\\(s_Y = \\sqrt{2,5} \\approx 1,58\\)\n\n\n\n\n12.14.4 Kwartyle i Rozstęp Międzykwartylowy (IQR)\nKwartyle dzielą zbiór danych na cztery równe części.\n\n12.14.4.1 Obliczenia dla Kraju X\n\nQ1 (25. percentyl): \\(\\frac{3 + 5}{2} = 4\\)\nQ2 (50. percentyl, mediana): 10\nQ3 (75. percentyl): \\(\\frac{15 + 17}{2} = 16\\)\nIQR = Q3 - Q1 = 16 - 4 = 12\n\n\n\n12.14.4.2 Obliczenia dla Kraju Y\n\nQ1 (25. percentyl): 9\nQ2 (50. percentyl, mediana): 10,5\nQ3 (75. percentyl): 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n\n\n\n12.14.5 Współczynnik Zmienności (CV)\nWspółczynnik zmienności to stosunek odchylenia standardowego do średniej, wyrażony w procentach.\nWzór: \\(CV = \\frac{s}{\\bar{x}} \\times 100\\%\\)\n\n12.14.5.1 Obliczenia dla Kraju X\n\\(CV_X = \\frac{6,06}{10} \\times 100\\% = 60,6\\%\\)\n\n\n12.14.5.2 Obliczenia dla Kraju Y\n\\(CV_Y = \\frac{1,58}{10,5} \\times 100\\% = 15,0\\%\\)\n\n\n\n12.14.6 Porównanie Wyników\n\n\n\nMiara\nKraj X (Wysoka zm.)\nKraj Y (Niska zm.)\n\n\n\n\nŚrednia\n10\n10,5\n\n\nMediana\n10\n10,5\n\n\nDominanta\nBrak\n9, 10, 11, 12\n\n\nRozstęp\n18\n5\n\n\nWariancja\n36,67\n2,5\n\n\nOdch. Stand.\n6,06\n1,58\n\n\nIQR\n12\n3\n\n\nWsp. Zmienności\n60,6%\n15,0%\n\n\n\n\n\n12.14.7 Porównanie za pomocą Wykresu Pudełkowego\nAby wizualnie porównać rozkład wielkości okręgów wyborczych między dwoma krajami, możemy użyć wykresu pudełkowego w stylu Tukeya. Ten typ wykresu zapewnia zwięzłe podsumowanie rozkładu danych, w tym medianę, kwartyle i potencjalne wartości odstające.\n\n# Wczytaj niezbędną bibliotekę\nlibrary(ggplot2)\n\n# Utwórz ramki danych dla każdego kraju\nkraj_x &lt;- data.frame(kraj = \"X\", wielkosc = c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19))\nkraj_y &lt;- data.frame(kraj = \"Y\", wielkosc = c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13))\n\n# Połącz dane\nwszystkie_dane &lt;- rbind(kraj_x, kraj_y)\n\n# Utwórz wykres pudełkowy\nggplot(wszystkie_dane, aes(x = kraj, y = wielkosc, fill = kraj)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(title = \"Porównanie Zmienności Wielkości Okręgów Wyborczych\",\n       x = \"Kraj\",\n       y = \"Wielkość Okręgu\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\"))\n\n\n\n\nPorównanie Zmienności Wielkości Okręgów Wyborczych\n\n\n\n\n\n12.14.7.1 Interpretacja Wykresu Pudełkowego\nWykres pudełkowy dostarcza kilku kluczowych informacji:\n\nPudełko reprezentuje rozstęp międzykwartylowy (IQR), z dolną krawędzią na Q1 i górną na Q3.\nLinia wewnątrz pudełka reprezentuje medianę (Q2).\nWąsy rozciągają się do najmniejszej i największej wartości w zakresie 1,5 * IQR od krawędzi pudełka.\nPunkty poza wąsami są uznawane za potencjalne wartości odstające i są wykreślane indywidualnie.\n\nNa podstawie tego wykresu możemy zaobserwować:\n\nMediana wielkości okręgów dla Kraju Y jest nieznacznie wyższa niż dla Kraju X, co jest zgodne z naszymi wcześniejszymi obliczeniami.\nPudełko dla Kraju X jest znacznie większe niż dla Kraju Y, co wskazuje na większe rozproszenie środkowych 50% danych, a tym samym wyższą zmienność.\nDane Kraju X obejmują znacznie szerszy zakres, co widać po dłuższych wąsach, co dodatkowo potwierdza jego wyższą zmienność.\nDane Kraju Y są bardziej skupione, z mniejszym pudełkiem i krótszymi wąsami, co wskazuje na niższą zmienność.\nIndywidualne punkty dla Kraju X są bardziej rozproszone, podczas gdy dla Kraju Y są bardziej skupione, co stanowi wizualną reprezentację różnicy w zmienności.\n\nTa wizualizacja za pomocą wykresu pudełkowego wzmacnia naszą wcześniejszą analizę numeryczną, wyraźnie pokazując kontrast w zmienności wielkości okręgów wyborczych między dwoma krajami.\n\n\n\n12.14.8 Kluczowe Obserwacje\n\nTendencja Centralna: Oba kraje mają podobne średnie i mediany, co wskazuje, że ich przeciętne wielkości okręgów są zbliżone.\nRozproszenie:\n\nKraj X wykazuje znacznie wyższą wariancję i odchylenie standardowe, potwierdzając jego wysoką zmienność.\nRozstęp dla Kraju X (18) jest ponad trzy razy większy niż dla Kraju Y (5).\nIQR dla Kraju X (12) jest cztery razy większy niż dla Kraju Y (3), wskazując na znacznie szersze rozproszenie środkowych 50% danych.\n\nKształt Rozkładu:\n\nKraj X ma rozkład równomierny bez wyraźnej dominanty.\nKraj Y ma bardziej skupiony rozkład z wieloma dominantami, wskazując na powszechne wielkości okręgów.\n\nWspółczynnik Zmienności:\n\nCV Kraju X (60,6%) jest znacznie wyższy niż Kraju Y (15,0%), dostarczając standaryzowanej miary różnicy w zmienności.\n\nPorównanie Wizualne:\n\nWykres pudełkowy wyraźnie ilustruje znaczącą różnicę w zmienności między dwoma krajami, potwierdzając nasze numeryczne odkrycia.\n\n\n\n\n12.14.9 Wnioski\nTa analiza wyraźnie pokazuje kontrast w zmienności wielkości okręgów wyborczych między dwoma krajami:\n\nKraj X wykazuje wysoką zmienność, z wielkościami okręgów rozproszonymi szeroko od 1 do 19. Może to wskazywać na zróżnicowany system wyborczy z mieszanką małych (prawdopodobnie jednomandatowych) i dużych okręgów wielomandatowych.\nKraj Y wykazuje niską zmienność, z wielkościami okręgów ściśle skupionymi między 8 a 13. Sugeruje to bardziej jednolity system wyborczy, prawdopodobnie składający się z okręgów wielomandatowych średniej wielkości.\n\nTe różnice w zmienności mogą mieć istotne implikacje dla reprezentacji politycznej, strategii partyjnych i wyników wyborów w każdym kraju.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#appendix-tabele-podsumowujące-typy-danych-i-odpowiednie-miary-statystyczne",
    "href": "rozdzial5.html#appendix-tabele-podsumowujące-typy-danych-i-odpowiednie-miary-statystyczne",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.15 Appendix: Tabele Podsumowujące Typy Danych i Odpowiednie Miary Statystyczne",
    "text": "12.15 Appendix: Tabele Podsumowujące Typy Danych i Odpowiednie Miary Statystyczne\n\n12.15.1 Tabela 1: Dane Dyskretne vs. Ciągłe\n\n\n\n\n\n\n\n\nCharakterystyka\nDane Dyskretne\nDane Ciągłe\n\n\n\n\nDefinicja\nMogą przyjmować tylko określone wartości\nMogą przyjmować dowolną wartość w danym zakresie\n\n\nPrzykłady\nLiczba dzieci, Rozmiar buta\nWzrost, Waga, Czas\n\n\nZalety\n- Łatwe do kategoryzacji- Proste do zliczenia- Często łatwiejsze do analizy\n- Bardziej precyzyjne pomiary- Umożliwiają bardziej zaawansowane analizy statystyczne- Mogą być grupowane w przedziały\n\n\nWady\n- Ograniczona precyzja- Mogą nie uchwycić subtelnych różnic- Niektóre metody statystyczne mogą nie mieć zastosowania\n- Mogą być bardziej złożone do analizy- Mogą wymagać większych próbek dla znaczącej analizy- Mogą wystąpić błędy zaokrągleń przy pomiarze\n\n\nWizualizacja\nWykresy słupkowe, Wykresy kołowe\nHistogramy, Wykresy punktowe, Wykresy liniowe\n\n\n\n\n\n12.15.2 Tabela 2: Typologia Skal Pomiarowych Stevensa\n\n\n\n\n\n\n\n\n\n\nCharakterystyka\nNominalna\nPorządkowa\nInterwałowa\nIlorazowa\n\n\n\n\nDefinicja\nKategorie bez ustalonego porządku\nUporządkowane kategorie\nRówne interwały, brak prawdziwego zera\nRówne interwały z prawdziwym zerem\n\n\nPrzykłady\nPłeć, Grupa krwi\nPoziom wykształcenia, Skale Likerta\nTemperatura (°C, °F), Daty kalendarzowe\nWzrost, Waga, Wiek\n\n\nZalety\n- Łatwe do zebrania- Proste do kategoryzacji\n- Uwzględnia porządek- Przydatne do rankingów\n- Pozwala na znaczące różnice- Możliwe bardziej zaawansowane analizy\n- Najbardziej wszechstronne- Pozwala na wszystkie operacje arytmetyczne\n\n\nWady\n- Ograniczone opcje analityczne- Brak operacji arytmetycznych\n- Różnice między kategoriami nie są kwantyfikowalne- Ograniczone operacje arytmetyczne\n- Brak prawdziwego punktu zerowego- Stosunki nie są znaczące\n- Może być trudno uzyskać prawdziwe pomiary ilorazowe w niektórych dziedzinach\n\n\n\n\n\n12.15.3 Tabela 3: Zalety i Wady Różnych Miar Statystycznych\n\n12.15.3.1 Miary Tendencji Centralnej\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nŚrednia\n- Wykorzystuje wszystkie punkty danych- Pozwala na dalsze obliczenia statystyczne- Idealna dla danych o rozkładzie normalnym\n- Wrażliwa na wartości odstające- Nieodpowiednia dla rozkładów skośnych- Bez znaczenia dla danych nominalnych\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nMediana\n- Niewrażliwa na wartości odstające- Dobra dla rozkładów skośnych- Może być stosowana do danych porządkowych\n- Ignoruje rzeczywiste wartości większości punktów danych- Mniej użyteczna do dalszych analiz statystycznych\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nModa\n- Może być stosowana do każdego typu danych- Dobra do znajdowania najczęstszej kategorii\n- Może nie być unikalna (rozkłady multimodalne)- Nieprzydatna do wielu typów analiz- Ignoruje wielkość różnic między wartościami\nWszystkie typy\n\n\n\n\n\n12.15.3.2 Miary Zmienności\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nZakres\n- Prosty do obliczenia i zrozumienia- Daje szybki obraz rozproszenia danych\n- Bardzo wrażliwy na wartości odstające- Ignoruje wszystkie dane między ekstremami- Nieprzydatny do dalszych analiz statystycznych\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nRozstęp międzykwartylowy (IQR)\n- Niewrażliwy na wartości odstające- Dobry dla rozkładów skośnych\n- Ignoruje 50% danych- Mniej intuicyjny niż zakres\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nWariancja\n- Wykorzystuje wszystkie punkty danych- Podstawa wielu procedur statystycznych\n- Wrażliwa na wartości odstające- Jednostki są podniesione do kwadratu (mniej intuicyjne)\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nOdchylenie standardowe\n- Wykorzystuje wszystkie punkty danych- Te same jednostki co oryginalne dane- Szeroko stosowane i zrozumiałe\n- Wrażliwe na wartości odstające- Zakłada w przybliżeniu rozkład normalny dla interpretacji\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nWspółczynnik zmienności\n- Pozwala na porównanie między zbiorami danych o różnych jednostkach lub średnich\n- Może być mylący, gdy średnie są bliskie zeru- Bez znaczenia dla danych z wartościami ujemnymi\nIlorazowe, niektóre Interwałowe\n\n\n\n\n\n12.15.3.3 Miary Korelacji/Asocjacji\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nr Pearsona\n- Mierzy zależność liniową- Szeroko stosowany i zrozumiały\n- Zakłada rozkład normalny- Wrażliwy na wartości odstające- Uchwytuje tylko zależności liniowe\nInterwałowe, Ilorazowe, Ciągłe\n\n\nRho Spearmana\n- Może być stosowany do danych porządkowych- Uchwytuje zależności monotoniczne- Mniej wrażliwy na wartości odstające\n- Traci informacje przez konwersję na rangi- Może pominąć niektóre typy zależności\nPorządkowe, Interwałowe, Ilorazowe\n\n\nTau Kendalla\n- Może być stosowany do danych porządkowych- Bardziej odporny niż Spearman dla małych próbek- Ma ładną interpretację (prawdopodobieństwo zgodności)\n- Traci informacje, biorąc pod uwagę tylko porządek- Bardziej intensywny obliczeniowo\nPorządkowe, Interwałowe, Ilorazowe\n\n\nChi-kwadrat\n- Może być stosowany do danych nominalnych- Testuje niezależność zmiennych kategorycznych\n- Wymaga dużych rozmiarów próbek- Wrażliwy na rozmiar próbki- Nie mierzy siły asocjacji\nNominalne, Porządkowe\n\n\nV Craméra\n- Może być stosowany do danych nominalnych- Dostarcza miarę siły asocjacji- Znormalizowany do zakresu [0,1]\n- Interpretacja może być subiektywna- Może przeszacować asocjację w małych próbkach\nNominalne, Porządkowe\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓*\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\n12.15.4 Notes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "13  Data Visualization: with examples in R",
    "section": "",
    "text": "13.1 Introduction to Data Types and Visualization\nBefore diving into specific visualization techniques, it’s crucial to understand the different types of data you might encounter and how they influence your choice of visualization method. We’ll explore these concepts with practical examples using the ggplot2 library in R.\nFirst, let’s load the necessary libraries:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#bar-plots",
    "href": "chapter6.html#bar-plots",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.2 Bar Plots",
    "text": "13.2 Bar Plots\nBar plots are excellent for displaying categorical data or summarizing continuous data by groups.\n\n13.2.1 Understanding Bar Plots\nA bar plot represents data using rectangular bars with heights proportional to the values they represent. They are used to compare different categories or groups.\nKey components of a bar plot: 1. X-axis: Represents categories 2. Y-axis: Represents values (can be counts, percentages, or any numerical value) 3. Bars: Rectangle for each category, height corresponds to its value\n\n13.2.1.1 Example Data\nLet’s use a simple dataset of fruit sales:\n\nfruits &lt;- c(\"Apple\", \"Banana\", \"Orange\", \"Grape\")\nsales &lt;- c(120, 85, 70, 100)\n\n# Create a data frame\ndf &lt;- data.frame(fruit = fruits, sales = sales)\n\n\n\n\n13.2.2 Hand-Drawn Bar Plot\nTo create a bar plot by hand:\n\nDraw a horizontal line (x-axis) and a vertical line (y-axis) perpendicular to each other.\nLabel the x-axis with your categories (fruits), evenly spaced.\nLabel the y-axis with a suitable scale for your values (sales, 0 to 120 in increments of 20).\nFor each category, draw a rectangle (bar) whose height corresponds to its value on the y-axis scale.\nColor or shade each bar if desired.\nAdd a title and labels for both axes.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen drawing by hand, use graph paper for more precise measurements and straighter lines. Choose a scale that allows all your data to fit while maximizing the use of space.\n\n\n\n\n13.2.3 Bar Plot in Base R\n\n# Create bar plot\nbarplot(sales, names.arg = fruits, \n        main = \"Fruit Sales\",\n        xlab = \"Fruit Types\", ylab = \"Sales\")\n\n\n\n\n\n\n\n\n\n\n13.2.4 Bar Plot with ggplot2\n\n# Create bar plot with ggplot2\nggplot(df, aes(x = fruit, y = sales)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Fruit Sales\",\n       x = \"Fruit Types\", y = \"Sales\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n13.2.5 Interpreting Bar Plots\nWhen interpreting a bar plot, consider the following:\n\nRelative Heights: Compare the heights of the bars to understand which categories have higher or lower values.\nOrdering: Sometimes, bars are ordered by height to make comparisons easier.\nPatterns: Look for any patterns or trends across categories.\nOutliers: Identify any bars that are much taller or shorter than the others.\n\n\n13.2.5.1 Example Interpretation\nFor our fruit sales data:\n\nApples have the highest sales (120), followed by Grapes (100).\nOranges have the lowest sales (70).\nThere’s a considerable difference between the highest (Apples) and lowest (Oranges) sales.\nBananas and Grapes have similar sales figures, in the middle range.\n\nThis information could be useful for inventory management or marketing strategies in a fruit shop.\n\n\n\n\n\n\nNote\n\n\n\nBar plots are great for comparing categories, but they don’t show the distribution within each category. For that, you might need other plot types like box plots.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#histograms",
    "href": "chapter6.html#histograms",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.3 Histograms",
    "text": "13.3 Histograms\nHistograms visualize the distribution of a continuous variable by dividing it into intervals (bins) and showing the frequency or density of data points in each bin.\n\n13.3.1 Understanding Histograms\nKey components of a histogram: 1. X-axis: Represents the variable’s values, divided into bins 2. Y-axis: Represents frequency, relative frequency, or density 3. Bars: Rectangle for each bin, height corresponds to the y-axis measure\nThere are three main types of histograms:\n\nFrequency Histogram: The y-axis shows the count of data points in each bin.\nRelative Frequency Histogram: The y-axis shows the proportion of data points in each bin (frequency divided by total number of data points).\nDensity Histogram: The y-axis shows the density, which is the relative frequency divided by the bin width. The total area of all bars sums to 1.\n\n\n13.3.1.1 Example Data\nLet’s use a dataset of 50 student exam scores (out of 100):\n\nset.seed(123)  # for reproducibility\nscores &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\n13.3.2 Hand-Drawn Histogram\nTo create a frequency histogram by hand:\n\nFind the range of your data.\nChoose a number of bins (let’s use 7 bins).\nCreate a frequency table.\nDraw x and y axes.\nLabel x-axis with bin ranges and y-axis with frequency.\nDraw a rectangle for each bin, with height corresponding to its frequency.\nAdd a title and labels for both axes.\n\nFor a relative frequency histogram, divide each frequency by the total number of data points before drawing the bars.\nFor a density histogram, divide the relative frequency by the bin width before drawing the bars.\n\n\n\n\n\n\nTip\n\n\n\nThe number of bins can affect the interpretation. Too few bins may obscure important features, while too many may introduce noise. A common rule of thumb is to use the square root of the number of data points as the number of bins.\n\n\n\n\n13.3.3 Histograms in Base R\n\n# Frequency Histogram\nhist(scores, breaks = 7, \n     main = \"Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Relative Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Relative Frequency\")\n\n\n\n\n\n\n\n# Density Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Density Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Density\")\nlines(density(scores), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n13.3.4 Histograms with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(score = scores)\n\n# Frequency Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nggplot(df, aes(x = score, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Relative Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Relative Frequency\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Density Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Density Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Density\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n13.3.5 Interpreting Histograms\nWhen interpreting a histogram, consider:\n\nCentral Tendency: Where is the peak of the distribution?\nSpread: How wide is the distribution?\nShape: Is it symmetric, skewed, or multi-modal?\nOutliers: Are there any unusual values far from the main distribution?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#box-plots-and-tukey-box-plots",
    "href": "chapter6.html#box-plots-and-tukey-box-plots",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.4 Box Plots and Tukey Box Plots",
    "text": "13.4 Box Plots and Tukey Box Plots\nBox plots, also known as box-and-whisker plots, provide a concise summary of a distribution. We’ll focus on the Tukey-style box plot, named after the statistician John Tukey who popularized this type of plot.\n\n13.4.1 Understanding Box Plots\nA box plot represents five key statistics:\n\nMinimum value (excluding outliers)\nFirst quartile (Q1)\nMedian\nThird quartile (Q3)\nMaximum value (excluding outliers)\n\nAdditionally, box plots show:\n\nWhiskers: Lines extending from the box to the minimum and maximum values (excluding outliers)\nOutliers: Individual points beyond the whiskers\n\n\n13.4.1.1 Calculating Quartiles and Outliers\nTo create a box plot, follow these steps:\n\nOrder your data from smallest to largest.\nFind the median (middle value if odd number of data points, average of two middle values if even).\nFind Q1 (median of lower half of data) and Q3 (median of upper half of data).\nCalculate the Interquartile Range (IQR) = Q3 - Q1\nDetermine outliers using Tukey’s rule:\n\nLower outliers: &lt; Q1 - 1.5 * IQR\nUpper outliers: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe factor 1.5 in Tukey’s outlier rule is based on the properties of the normal distribution. For normally distributed data, this rule identifies about 0.7% of the data as potential outliers.\n\n\n\n\n13.4.1.2 Example Data\nLet’s use a small dataset to illustrate:\n\ndata &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\n13.4.2 Hand-Drawn Tukey Box Plot\nTo create a Tukey box plot by hand:\n\nDraw a vertical line representing the range from minimum to maximum (2 to 15 in our example, excluding the outlier).\nDraw a box from Q1 to Q3.\nDraw a horizontal line through the box at the median.\nDraw whiskers from the box to the minimum and maximum values (excluding outliers).\nRepresent the outlier (50) as an individual point beyond the whisker.\nAdd a scale to the vertical axis and label it.\n\n\n\n13.4.3 Box Plot in Base R\n\n# Create box plot\nboxplot(data, main = \"Box Plot of Sample Data\",\n        ylab = \"Values\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\n13.4.4 Tukey Box Plot with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(value = data)\n\n# Create Tukey box plot with ggplot2\nggplot(df, aes(x = \"\", y = value)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Tukey Box Plot of Sample Data\",\n       x = \"\", y = \"Values\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n13.4.5 Interpreting Box Plots\nWhen interpreting a box plot, consider the following:\n\nCentral Tendency: The median shows the center of the distribution.\nSpread: The box (IQR) represents the middle 50% of the data.\nSkewness: If the median line is closer to one end of the box, the distribution is skewed.\nOutliers: Points beyond the whiskers are potential outliers.\nComparisons: When comparing multiple box plots, look at relative positions of medians, box sizes, and presence of outliers.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#conclusion",
    "href": "chapter6.html#conclusion",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.5 Conclusion",
    "text": "13.5 Conclusion\nIn this chapter, we explored three fundamental types of data visualizations: bar plots, histograms, and box plots. We demonstrated how to create these plots by hand, using R’s base plotting system, and using the ggplot2 library.\nEach type of plot serves a different purpose: - Bar plots are excellent for comparing categories. - Histograms show the distribution of a continuous variable. - Box plots provide a concise summary of a distribution, highlighting central tendency, spread, and outliers.\nRemember, the choice of visualization depends on your data type and the insights you want to convey. Always consider your audience and the story you want to tell with your data when selecting and designing your visualizations.\nPractice creating these plots by hand to deepen your understanding of their construction and interpretation. Then, leverage the power of R and ggplot2 to quickly create and customize these visualizations for larger datasets and more complex analyses.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html",
    "href": "rozdzial6.html",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "",
    "text": "14.1 Wprowadzenie do Typów Danych i Wizualizacji\nPrzed zagłębieniem się w konkretne techniki wizualizacji, ważne jest zrozumienie różnych typów danych i ich wpływu na wybór metody wizualizacji. Przeanalizujemy te koncepcje na praktycznych przykładach z użyciem biblioteki ggplot2 w R.\nNajpierw załadujmy niezbędne biblioteki:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-słupkowe",
    "href": "rozdzial6.html#wykresy-słupkowe",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.2 Wykresy Słupkowe",
    "text": "14.2 Wykresy Słupkowe\nWykresy słupkowe doskonale nadają się do prezentacji danych kategorycznych lub podsumowania danych ciągłych w grupach.\n\n14.2.1 Zrozumienie Wykresów Słupkowych\nWykres słupkowy przedstawia dane za pomocą prostokątnych słupków, których wysokość jest proporcjonalna do reprezentowanych przez nie wartości. Służą do porównywania różnych kategorii lub grup.\nGłówne elementy wykresu słupkowego: 1. Oś X: Reprezentuje kategorie 2. Oś Y: Reprezentuje wartości (mogą to być liczebności, procenty lub dowolne wartości numeryczne) 3. Słupki: Prostokąt dla każdej kategorii, wysokość odpowiada jej wartości\n\n14.2.1.1 Przykładowe Dane\nUżyjmy prostego zestawu danych dotyczącego sprzedaży owoców:\n\nowoce &lt;- c(\"Jabłko\", \"Banan\", \"Pomarańcza\", \"Winogrono\")\nsprzedaz &lt;- c(120, 85, 70, 100)\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(owoc = owoce, sprzedaz = sprzedaz)\n\n\n\n\n14.2.2 Ręcznie Rysowany Wykres Słupkowy\nAby stworzyć wykres słupkowy ręcznie:\n\nNarysuj linię poziomą (oś X) i pionową (oś Y) prostopadłe do siebie.\nOznacz oś X swoimi kategoriami (owocami), równomiernie rozmieszczonymi.\nOznacz oś Y odpowiednią skalą dla Twoich wartości (sprzedaż, od 0 do 120 z przyrostami co 20).\nDla każdej kategorii narysuj prostokąt (słupek), którego wysokość odpowiada jej wartości na skali osi Y.\nJeśli chcesz, pokoloruj lub zacienuj każdy słupek.\nDodaj tytuł i etykiety dla obu osi.\n\n\n\n\n\n\n\nTip\n\n\n\nPrzy rysowaniu ręcznym użyj papieru milimetrowego dla dokładniejszych pomiarów i prostszych linii. Wybierz skalę, która pozwoli zmieścić wszystkie dane, maksymalnie wykorzystując dostępną przestrzeń.\n\n\n\n\n14.2.3 Wykres Słupkowy w Podstawowym R\n\n# Tworzenie wykresu słupkowego\nbarplot(sprzedaz, names.arg = owoce, \n        main = \"Sprzedaż Owoców\",\n        xlab = \"Rodzaje Owoców\", ylab = \"Sprzedaż\")\n\n\n\n\n\n\n\n\n\n\n14.2.4 Wykres Słupkowy z ggplot2\n\n# Tworzenie wykresu słupkowego z ggplot2\nggplot(df, aes(x = owoc, y = sprzedaz)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Sprzedaż Owoców\",\n       x = \"Rodzaje Owoców\", y = \"Sprzedaż\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n14.2.5 Interpretacja Wykresów Słupkowych\nPodczas interpretacji wykresu słupkowego zwróć uwagę na:\n\nWzględne Wysokości: Porównaj wysokości słupków, aby zrozumieć, które kategorie mają wyższe lub niższe wartości.\nKolejność: Czasami słupki są uporządkowane według wysokości, aby ułatwić porównania.\nWzorce: Poszukaj wzorców lub trendów między kategoriami.\nWartości Odstające: Zidentyfikuj słupki, które są znacznie wyższe lub niższe od pozostałych.\n\n\n14.2.5.1 Przykładowa Interpretacja\nDla naszych danych o sprzedaży owoców:\n\nJabłka mają najwyższą sprzedaż (120), następnie Winogrona (100).\nPomarańcze mają najniższą sprzedaż (70).\nIstnieje znaczna różnica między najwyższą (Jabłka) a najniższą (Pomarańcze) sprzedażą.\nBanany i Winogrona mają podobne wartości sprzedaży, w średnim zakresie.\n\nTa informacja może być przydatna dla zarządzania zapasami lub strategii marketingowych w sklepie owocowym.\n\n\n\n\n\n\nNote\n\n\n\nWykresy słupkowe są świetne do porównywania kategorii, ale nie pokazują rozkładu wewnątrz każdej kategorii. Do tego mogą być potrzebne inne typy wykresów, jak wykresy pudełkowe.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#histogramy",
    "href": "rozdzial6.html#histogramy",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.3 Histogramy",
    "text": "14.3 Histogramy\nHistogramy wizualizują rozkład zmiennej ciągłej poprzez podzielenie jej na przedziały (bins) i pokazanie częstości lub gęstości punktów danych w każdym przedziale.\n\n14.3.1 Zrozumienie Histogramów\nGłówne elementy histogramu: 1. Oś X: Reprezentuje wartości zmiennej, podzielone na przedziały 2. Oś Y: Reprezentuje częstość, względną częstość lub gęstość 3. Słupki: Prostokąt dla każdego przedziału, wysokość odpowiada mierze na osi Y\nIstnieją trzy główne typy histogramów:\n\nHistogram Częstości: Oś Y pokazuje liczbę punktów danych w każdym przedziale.\nHistogram Częstości Względnej: Oś Y pokazuje proporcję punktów danych w każdym przedziale (częstość podzielona przez całkowitą liczbę punktów danych).\nHistogram Gęstości: Oś Y pokazuje gęstość, która jest częstością względną podzieloną przez szerokość przedziału. Całkowita powierzchnia wszystkich słupków sumuje się do 1.\n\n\n14.3.1.1 Przykładowe Dane\nUżyjmy zbioru 50 wyników egzaminów studentów (na 100 punktów):\n\nset.seed(123)  # dla powtarzalności\nwyniki &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\n14.3.2 Ręcznie Rysowany Histogram\nAby stworzyć histogram częstości ręcznie:\n\nZnajdź zakres danych.\nWybierz liczbę przedziałów (użyjmy 7 przedziałów).\nUtwórz tabelę częstości.\nNarysuj osie X i Y.\nOznacz oś X zakresami przedziałów, a oś Y częstością.\nNarysuj prostokąt dla każdego przedziału, z wysokością odpowiadającą jego częstości.\nDodaj tytuł i etykiety dla obu osi.\n\nDla histogramu częstości względnej, podziel każdą częstość przez całkowitą liczbę punktów danych przed narysowaniem słupków.\nDla histogramu gęstości, podziel częstość względną przez szerokość przedziału przed narysowaniem słupków.\n\n\n\n\n\n\nTip\n\n\n\nLiczba przedziałów może wpłynąć na interpretację. Zbyt mało przedziałów może ukryć ważne cechy, podczas gdy zbyt wiele może wprowadzić szum. Powszechną regułą jest użycie pierwiastka kwadratowego z liczby punktów danych jako liczby przedziałów.\n\n\n\n\n14.3.3 Histogramy w Podstawowym R\n\n# Histogram Częstości\nhist(wyniki, breaks = 7, \n     main = \"Histogram Częstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość\")\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Częstości Względnej Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość Względna\")\n\n\n\n\n\n\n\n# Histogram Gęstości\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Gęstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Gęstość\")\nlines(density(wyniki), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n14.3.4 Histogramy z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wynik = wyniki)\n\n# Histogram Częstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nggplot(df, aes(x = wynik, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Względnej Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość Względna\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Histogram Gęstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Histogram Gęstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Gęstość\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n14.3.5 Interpretacja Histogramów\nPodczas interpretacji histogramu zwróć uwagę na:\n\nTendencję Centralną: Gdzie znajduje się szczyt rozkładu?\nRozrzut: Jak szeroki jest rozkład?\nKształt: Czy jest symetryczny, skośny, czy wielomodalny?\nWartości Odstające: Czy są nietypowe wartości daleko od głównego rozkładu?",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "href": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya",
    "text": "14.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya\nWykresy pudełkowe, znane również jako wykresy skrzynkowe, dostarczają zwięzłego podsumowania rozkładu. Skupimy się na wykresie pudełkowym w stylu Tukeya, nazwanym na cześć statystyka Johna Tukeya, który spopularyzował ten typ wykresu.\n\n14.4.1 Zrozumienie Wykresów Pudełkowych\nWykres pudełkowy przedstawia pięć kluczowych statystyk:\n\nWartość minimalna (z wyłączeniem wartości odstających)\nPierwszy kwartyl (Q1)\nMediana\nTrzeci kwartyl (Q3)\nWartość maksymalna (z wyłączeniem wartości odstających)\n\nDodatkowo wykresy pudełkowe pokazują:\n\nWąsy: Linie rozciągające się od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających)\nWartości odstające: Indywidualne punkty poza wąsami\n\n\n14.4.1.1 Obliczanie Kwartyli i Wartości Odstających\nAby stworzyć wykres pudełkowy, postępuj zgodnie z tymi krokami:\n\nUporządkuj dane od najmniejszej do największej wartości.\nZnajdź medianę (środkowa wartość dla nieparzystej liczby punktów danych, średnia z dwóch środkowych wartości dla parzystej).\nZnajdź Q1 (mediana dolnej połowy danych) i Q3 (mediana górnej połowy danych).\nOblicz Rozstęp Międzykwartylowy (IQR) = Q3 - Q1\nOkreśl wartości odstające używając reguły Tukeya:\n\nDolne wartości odstające: &lt; Q1 - 1.5 * IQR\nGórne wartości odstające: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nWspółczynnik 1.5 w regule Tukeya dla wartości odstających opiera się na właściwościach rozkładu normalnego. Dla danych o rozkładzie normalnym, ta reguła identyfikuje około 0.7% danych jako potencjalne wartości odstające.\n\n\n\n\n14.4.1.2 Przykładowe Dane\nUżyjmy małego zbioru danych do ilustracji:\n\ndane &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\n14.4.2 Ręcznie Rysowany Wykres Pudełkowy Tukeya\nAby stworzyć wykres pudełkowy Tukeya ręcznie:\n\nNarysuj linię pionową reprezentującą zakres od minimum do maksimum (2 do 15 w naszym przykładzie, z wyłączeniem wartości odstającej).\nNarysuj pudełko od Q1 do Q3.\nNarysuj poziomą linię przez pudełko na poziomie mediany.\nNarysuj wąsy od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających).\nPrzedstaw wartość odstającą (50) jako indywidualny punkt poza wąsem.\nDodaj skalę do osi pionowej i oznacz ją.\n\n\n\n14.4.3 Wykres Pudełkowy w Podstawowym R\n\n# Tworzenie wykresu pudełkowego\nboxplot(dane, main = \"Wykres Pudełkowy Przykładowych Danych\",\n        ylab = \"Wartości\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\n14.4.4 Wykres Pudełkowy Tukeya z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wartosc = dane)\n\n# Tworzenie wykresu pudełkowego Tukeya z ggplot2\nggplot(df, aes(x = \"\", y = wartosc)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Wykres Pudełkowy Tukeya Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n14.4.5 Interpretacja Wykresów Pudełkowych\nPodczas interpretacji wykresu pudełkowego zwróć uwagę na następujące elementy:\n\nTendencja Centralna: Mediana pokazuje środek rozkładu.\nRozrzut: Pudełko (IQR) reprezentuje środkowe 50% danych.\nSkośność: Jeśli linia mediany jest bliżej jednego końca pudełka, rozkład jest skośny.\nWartości Odstające: Punkty poza wąsami są potencjalnymi wartościami odstającymi.\nPorównania: Przy porównywaniu wielu wykresów pudełkowych, zwróć uwagę na względne położenie median, rozmiary pudełek i obecność wartości odstających.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "href": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.5 Zaawansowane Techniki Wizualizacji",
    "text": "14.5 Zaawansowane Techniki Wizualizacji\nOprócz podstawowych typów wykresów, warto poznać kilka bardziej zaawansowanych technik wizualizacji, które mogą być przydatne w analizie danych.\n\n14.5.1 Wykresy Skrzypcowe\nWykresy skrzypcowe łączą cechy wykresów pudełkowych i wykresów gęstości, dając bardziej kompletny obraz rozkładu danych.\n\n# Tworzenie wykresu skrzypcowego\nggplot(df, aes(x = \"\", y = wartosc)) +\n  geom_violin(fill = \"lightblue\") +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Wykres Skrzypcowy Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n14.5.2 Wykresy Rozrzutu z Marginesami\nŁączenie wykresów rozrzutu z histogramami na marginesach może dostarczyć więcej informacji o rozkładzie danych w dwóch wymiarach.\n\n# Generowanie danych do wykresu rozrzutu\nset.seed(123)\ndf_scatter &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100)\n)\n\n# Tworzenie wykresu rozrzutu z marginesami\nlibrary(ggExtra)\np &lt;- ggplot(df_scatter, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal()\nggMarginal(p, type = \"histogram\", fill = \"lightblue\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wnioski",
    "href": "rozdzial6.html#wnioski",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.6 Wnioski",
    "text": "14.6 Wnioski\nW tym rozdziale poznaliśmy trzy podstawowe typy wizualizacji danych: wykresy słupkowe, histogramy i wykresy pudełkowe. Pokazaliśmy, jak tworzyć te wykresy ręcznie, używając podstawowego systemu wykresów R oraz biblioteki ggplot2.\nKażdy typ wykresu służy innemu celowi: - Wykresy słupkowe doskonale nadają się do porównywania kategorii. - Histogramy pokazują rozkład zmiennej ciągłej. - Wykresy pudełkowe dostarczają zwięzłego podsumowania rozkładu, podkreślając tendencję centralną, rozrzut i wartości odstające.\nPamiętaj, że wybór wizualizacji zależy od typu danych i wniosków, które chcesz przekazać. Zawsze bierz pod uwagę swoją docelową grupę odbiorców i historię, którą chcesz opowiedzieć za pomocą swoich danych, wybierając i projektując wizualizacje.\nĆwicz tworzenie tych wykresów ręcznie, aby pogłębić zrozumienie ich konstrukcji i interpretacji. Następnie wykorzystaj moc R i ggplot2, aby szybko tworzyć i dostosowywać te wizualizacje dla większych zbiorów danych i bardziej złożonych analiz.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#ćwiczenia-praktyczne",
    "href": "rozdzial6.html#ćwiczenia-praktyczne",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.7 Ćwiczenia Praktyczne",
    "text": "14.7 Ćwiczenia Praktyczne\n\nZbierz dane o popularności różnych gatunków muzycznych wśród Twoich znajomych. Stwórz wykres słupkowy przedstawiający te dane.\nZmierz czas reakcji 30 osób na bodziec dźwiękowy (w milisekundach). Utwórz histogram tych danych.\nZbierz dane o wzroście 50 osób w Twojej społeczności. Stwórz wykres pudełkowy dla tych danych, osobno dla mężczyzn i kobiet.\nZnajdź zestaw danych online (np. na Kaggle) i stwórz trzy różne wizualizacje dla tych danych. Opisz, jakie wnioski można wyciągnąć z każdej wizualizacji.\nStwórz wykres skrzypcowy dla danych o cenach domów w różnych dzielnicach miasta. Porównaj go z wykresem pudełkowym tych samych danych. Jakie dodatkowe informacje dostarcza wykres skrzypcowy?\n\nPamiętaj, że praktyka jest kluczem do opanowania sztuki wizualizacji danych. Eksperymentuj z różnymi typami wykresów i parametrami, aby znaleźć najlepszy sposób przedstawienia swoich danych.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\n\n\n\nBlair, G., Coppock, A., & Humphreys, M. (2023). Research design in the social sciences: declaration, diagnosis, and redesign. Princeton University Press. https://book.declaredesign.org/\nBryman, A., 2016. Social research methods. Oxford University Press.\nBueno de Mesquita, Ethan and Anthony Fowler. 2021. Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis. Princeton University Press.\nCausality for Machine Learning. https://ff13.fastforwardlabs.com/\nCetinkaya-Rundel, M., Diez, D.M. and Barr, C.D., 2019 (4th ed.). OpenIntro Statistics: an Open-source Textbook: https://www.openintro.org/book/os/\nClaude [Large language model], 2024. https://www.anthropic.com\nConcepts and Computation: An Introduction to Political Methodology. https://pos3713.github.io/notes/\nHannay, K. (2019). Introduction to statistics and data science. http://khannay.com/StatsBook/\nIsmay, C. and Kim, A.Y., 2019. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. https://moderndive.com/index.html\nNavarro, D.J. and Foxcroft, D.R. (2019). Learning statistics with Jamovi: a tutorial for psychology students and other beginners. (Version 0.70). DOI: 10.24384/hgc3-7p15\nRemler, D.K. and Van Ryzin, G.G., 2014. Research methods in practice: Strategies for description and causation. Sage Publications.\nSanchez, G., Marzban, E. (2020) All Models Are Wrong: Concepts of Statistical Learning. https://allmodelsarewrong.github.io\nSchneider, W. J. (2023). Psycheval: A psychological evaluation toolkit. https://github.com/wjschne/psycheval\nTimbers, T., Campbell, T., & Lee, M. (2022). Data science: A first introduction. Chapman and Hall/CRC. https://datasciencebook.ca/",
    "crumbs": [
      "References"
    ]
  }
]