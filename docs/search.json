[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Data Analysis: An Introduction (Wprowadzenie do analizy danych społecznych) [working copy]",
    "section": "",
    "text": "Preface\nThis is a Quarto book.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "",
    "text": "1.1 What is Data Science?\nData science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#what-is-data-science",
    "href": "chapter1.html#what-is-data-science",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "",
    "text": "Key Point\n\n\n\nIn social sciences, data science combines statistical methods, computational tools, and domain expertise to analyze complex social phenomena and human behavior.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-relationship-between-statistics-and-data-science",
    "href": "chapter1.html#the-relationship-between-statistics-and-data-science",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.2 The Relationship Between Statistics and Data Science",
    "text": "1.2 The Relationship Between Statistics and Data Science\nWhile statistics and data science are closely related, they have some distinctions:\n\nStatisticsData Science\n\n\n\nFocuses on mathematical theories and methods for collecting, analyzing, interpreting, and presenting data\nEmphasizes statistical inference, hypothesis testing, and probability theory\nHas a long history in social sciences for analyzing survey data, experimental results, and observational studies\n\n\n\n\nIncorporates statistical methods along with computer science and domain expertise\nEmphasizes big data, machine learning, and predictive modeling\nIn social sciences, often deals with large-scale digital traces, social media data, and complex behavioral datasets\n\n\n\n\nData science can be seen as an evolution and expansion of traditional statistics, incorporating new technologies and methodologies to handle larger and more complex social science datasets.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#essential-concepts-in-data-science-and-statistics",
    "href": "chapter1.html#essential-concepts-in-data-science-and-statistics",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.3 Essential Concepts in Data Science and Statistics",
    "text": "1.3 Essential Concepts in Data Science and Statistics\n\n1.3.1 Statistical Population, Sample, and Data Generating Process\nUnderstanding the relationships between population, sample, and the data generating process (DGP) is crucial in social science research.\n\n\n\n\n\n\nDefinitions\n\n\n\n\nPopulation: The entire group of individuals or objects about which information is sought.\nSample: A subset of the population that is selected for study.\nData Generating Process (DGP): The underlying mechanism or system that produces the observed data.\n\n\n\n\n1.3.1.1 Comparing Population and DGP:\n\nThe population represents the group we want to study, while the DGP is the mechanism producing the characteristics we observe in that population.\nIn social sciences, the DGP often involves complex social, psychological, and economic factors that shape the population’s characteristics.\nUnderstanding the DGP helps us interpret why the population has certain characteristics and how these might change over time or in different contexts.\n\n\n\n\n\n\n\nExample: Voter Behavior Study\n\n\n\n\nPopulation: All eligible voters in a country\nSample: 1000 randomly selected eligible voters\nDGP: The complex interplay of factors influencing voting decisions, such as political beliefs, economic conditions, media exposure, and social networks.\n\nUnderstanding the DGP helps researchers interpret voter behavior and potentially predict future trends.\n\n\nLet’s visualize this concept using R:\n\n\nClick to show/hide R code\nlibrary(ggplot2)\n\n# Simulate a population based on a simple DGP\nset.seed(123)\nage &lt;- rnorm(10000, mean = 45, sd = 15)\nincome &lt;- exp(rnorm(10000, mean = 10, sd = 0.5))\nvoting_prob &lt;- plogis(-5 + 0.05 * age + 0.00003 * income + rnorm(10000, sd = 0.5))\npopulation &lt;- data.frame(age = age, income = income, voting_prob = voting_prob)\n\n# Take a random sample\nsample_indices &lt;- sample(1:nrow(population), 1000)\nsample_data &lt;- population[sample_indices, ]\n\n# Visualize\nggplot(sample_data, aes(x = age, y = income, color = voting_prob)) +\n  geom_point(alpha = 0.6) +\n  scale_color_viridis_c() +\n  labs(title = \"Sample: Age, Income, and Voting Probability\",\n       x = \"Age\", y = \"Income\", color = \"Voting Probability\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis visualization demonstrates how age and income (part of the DGP) influence voting probability in our simulated population, based on the sample we’ve drawn.\n\n\n\n1.3.2 Types of Data in Social Sciences\nSocial science research deals with various types of data:\n\nQuantitative Data: Numerical data (e.g., survey responses, economic indicators)\nQualitative Data: Non-numerical data (e.g., interview transcripts, open-ended survey responses)\nBig Data: Large-scale digital traces (e.g., social media posts, online behavior logs)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#core-components-of-data-science-in-social-research",
    "href": "chapter1.html#core-components-of-data-science-in-social-research",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.4 Core Components of Data Science in Social Research",
    "text": "1.4 Core Components of Data Science in Social Research\n\nData CollectionData ProcessingExploratory Data Analysis (EDA)Statistical InferenceMachine Learning ApplicationsData Visualization and Communication\n\n\n\nSurveys and questionnaires\nInterviews and focus groups\nDigital data collection (e.g., web scraping, API access)\nEthical considerations in data collection\n\n\n\n\nCleaning and preprocessing data\nHandling missing values and outliers\nCoding qualitative data\nData transformation and normalization\n\n\n\n\nDescriptive statistics\nData visualization\nIdentifying patterns and trends in social phenomena\n\n\n\n\nHypothesis testing in social research\nRegression analysis\nCausal inference techniques\n\n\n\n\nPredictive modeling of social behaviors\nText analysis and sentiment analysis\nNetwork analysis in social contexts\n\n\n\n\nCreating effective visualizations for social science data\nCommunicating research findings to diverse audiences\nData-driven storytelling",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#tools-for-data-science-in-social-sciences",
    "href": "chapter1.html#tools-for-data-science-in-social-sciences",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.5 Tools for Data Science in Social Sciences",
    "text": "1.5 Tools for Data Science in Social Sciences\nIn this course, we’ll primarily use R for our data analysis, as it’s widely used in social science research.\n\n1.5.1 R for Social Science Data Analysis\nR offers powerful capabilities for social science research, from data manipulation to advanced statistical modeling.\n\n\nKliknij, aby pokazać/ukryć kod R\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nKliknij, aby pokazać/ukryć kod R\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate example data with a stronger Simpson's Paradox\nn &lt;- 1000\ndata &lt;- tibble(\n  age_group = sample(c(\"Young\", \"Middle\", \"Old\"), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),\n  education_years = case_when(\n    age_group == \"Young\" ~ rnorm(n, mean = 10, sd = 1),\n    age_group == \"Middle\" ~ rnorm(n, mean = 13, sd = 1),\n    age_group == \"Old\" ~ rnorm(n, mean = 16, sd = 1)\n  ),\n  income = case_when(\n    age_group == \"Young\" ~ 70000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Middle\" ~ 50000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Old\" ~ 30000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000)\n  )\n)\n\n# Basic data summary\nsummary(data)\n\n\n  age_group         education_years      income     \n Length:1000        Min.   : 6.628   Min.   :34068  \n Class :character   1st Qu.:10.913   1st Qu.:51508  \n Mode  :character   Median :13.004   Median :63376  \n                    Mean   :12.986   Mean   :63307  \n                    3rd Qu.:14.934   3rd Qu.:75023  \n                    Max.   :18.861   Max.   :96620  \n\n\nKliknij, aby pokazać/ukryć kod R\n# Correlation analysis\ncor(data %&gt;% select(education_years, income))\n\n\n                education_years     income\neducation_years       1.0000000 -0.8152477\nincome               -0.8152477  1.0000000\n\n\nKliknij, aby pokazać/ukryć kod R\n# Overall trend (Simpson's Paradox)\noverall_plot &lt;- ggplot(data, aes(x = education_years, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Overall Relationship between Education and Income\",\n       subtitle = \"Simpson's Paradox: Appears negative\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Trend by age group (Resolving Simpson's Paradox)\ngrouped_plot &lt;- ggplot(data, aes(x = education_years, y = income, color = age_group)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Relationship between Education and Income by Age Group\",\n       subtitle = \"Resolving Simpson's Paradox: Positive relationship within groups\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Statistical analysis\nmodel_overall &lt;- lm(income ~ education_years, data = data)\nmodel_by_age &lt;- lm(income ~ education_years + age_group, data = data)\n\n# Print results\nprint(overall_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(grouped_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_overall))\n\n\n\nCall:\nlm(formula = income ~ education_years, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24451  -5439    235   5262  34328 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     121814.7     1339.5   90.94   &lt;2e-16 ***\neducation_years  -4505.4      101.3  -44.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7976 on 998 degrees of freedom\nMultiple R-squared:  0.6646,    Adjusted R-squared:  0.6643 \nF-statistic:  1978 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_by_age))\n\n\n\nCall:\nlm(formula = income ~ education_years + age_group, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-14827  -3369    118   3356  16388 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      48270.8     2028.4  23.797  &lt; 2e-16 ***\neducation_years   1135.5      154.6   7.345 4.26e-13 ***\nage_groupOld    -19942.8      593.2 -33.619  &lt; 2e-16 ***\nage_groupYoung   20461.1      600.7  34.064  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4950 on 996 degrees of freedom\nMultiple R-squared:  0.8711,    Adjusted R-squared:  0.8707 \nF-statistic:  2244 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\n# Calculate and print correlations\noverall_cor &lt;- cor(data$education_years, data$income)\ngroup_cors &lt;- data %&gt;%\n  group_by(age_group) %&gt;%\n  summarize(correlation = cor(education_years, income))\n\nprint(\"Overall correlation:\")\n\n\n[1] \"Overall correlation:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(overall_cor)\n\n\n[1] -0.8152477\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(\"Correlations by age group:\")\n\n\n[1] \"Correlations by age group:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(group_cors)\n\n\n# A tibble: 3 × 2\n  age_group correlation\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Middle          0.185\n2 Old             0.291\n3 Young           0.223\n\n\nThis example demonstrates basic data manipulation, summary statistics, and visualization using R, which are common tasks in social science research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#ethical-considerations-in-social-science-data-analysis",
    "href": "chapter1.html#ethical-considerations-in-social-science-data-analysis",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.17 Ethical Considerations in Social Science Data Analysis",
    "text": "1.17 Ethical Considerations in Social Science Data Analysis\nEthics play a crucial role in social science research:\n\nPrivacy and Consent: Ensuring participant privacy and informed consent\nData Protection: Securely storing and managing sensitive personal data\nBias and Representation: Addressing sampling bias and ensuring diverse representation\nTransparency: Clearly communicating research methods and limitations\nSocial Impact: Considering the potential societal implications of research findings\n\n\n\n\n\n\n\nImportant\n\n\n\nSocial scientists must carefully consider the ethical implications of their data collection, analysis, and dissemination practices.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#key-takeaways",
    "href": "chapter1.html#key-takeaways",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.18 Key Takeaways",
    "text": "1.18 Key Takeaways\n\nData science in social sciences builds upon traditional statistical methods, incorporating new technologies to analyze complex social phenomena.\nUnderstanding concepts like population, sample, and data generating processes is crucial for valid social science research.\nThe data science process in social research involves multiple steps from ethical data collection to the communication of insights.\nR is a powerful tool for social science data analysis, offering a wide range of capabilities.\nEthical considerations should be at the forefront of any social science data project.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#next-steps",
    "href": "chapter1.html#next-steps",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.19 Next Steps",
    "text": "1.19 Next Steps\nIn the following chapters, we’ll dive deeper into each component of the data science process, exploring statistical concepts, R programming techniques, and real-world applications in social science research.\n\n\n\n\n\n\nPractice Exercise\n\n\n\nUsing the concepts learned in this chapter, design a small-scale study on a social science topic of your choice. Identify the population, consider how you would draw a sample, and reflect on the potential data generating process. What ethical considerations would you need to address?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html",
    "href": "rozdzial1.html",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "",
    "text": "2.1 Czym jest Nauka o Danych?\nNauka o danych to interdyscyplinarna dziedzina, która wykorzystuje metody naukowe, procesy, algorytmy i systemy do wydobywania wiedzy i wglądów ze strukturalnych i niestrukturalnych danych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#czym-jest-nauka-o-danych",
    "href": "rozdzial1.html#czym-jest-nauka-o-danych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "",
    "text": "Kluczowy Punkt\n\n\n\nW naukach społecznych nauka o danych łączy metody statystyczne, narzędzia obliczeniowe i wiedzę dziedzinową do analizy złożonych zjawisk społecznych i zachowań ludzkich.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#związek-między-statystyką-a-nauką-o-danych",
    "href": "rozdzial1.html#związek-między-statystyką-a-nauką-o-danych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.2 Związek Między Statystyką a Nauką o Danych",
    "text": "2.2 Związek Między Statystyką a Nauką o Danych\nChociaż statystyka i nauka o danych są ściśle powiązane, mają pewne różnice:\n\nStatystykaNauka o Danych\n\n\n\nKoncentruje się na teoriach matematycznych i metodach zbierania, analizowania, interpretowania i prezentowania danych\nKładzie nacisk na wnioskowanie statystyczne, testowanie hipotez i teorię prawdopodobieństwa\nMa długą historię w naukach społecznych w analizie danych ankietowych, wyników eksperymentalnych i badań obserwacyjnych\n\n\n\n\nŁączy metody statystyczne z informatyką i wiedzą dziedzinową\nKładzie nacisk na big data, uczenie maszynowe i modelowanie predykcyjne\nW naukach społecznych często zajmuje się danymi cyfrowymi na dużą skalę, danymi z mediów społecznościowych i złożonymi zbiorami danych behawioralnych\n\n\n\n\nNauka o danych może być postrzegana jako ewolucja i rozszerzenie tradycyjnej statystyki, włączając nowe technologie i metodologie do obsługi większych i bardziej złożonych zbiorów danych w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#podstawowe-koncepcje-w-nauce-o-danych-i-statystyce",
    "href": "rozdzial1.html#podstawowe-koncepcje-w-nauce-o-danych-i-statystyce",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.3 Podstawowe Koncepcje w Nauce o Danych i Statystyce",
    "text": "2.3 Podstawowe Koncepcje w Nauce o Danych i Statystyce\n\n2.3.1 Populacja Statystyczna, Próba i Proces Generowania Danych\nZrozumienie relacji między populacją, próbą i procesem generowania danych (PGD) jest kluczowe w badaniach nauk społecznych.\n\n\n\n\n\n\nDefinicje\n\n\n\n\nPopulacja: Cała grupa osób lub obiektów, o których poszukuje się informacji.\nPróba: Podzbiór populacji, który jest wybierany do badania.\nProces Generowania Danych (PGD): Podstawowy mechanizm lub system, który produkuje obserwowane dane.\n\n\n\n\n2.3.1.1 Porównanie Populacji i PGD:\n\nPopulacja reprezentuje grupę, którą chcemy badać, podczas gdy PGD jest mechanizmem produkującym cechy, które obserwujemy w tej populacji.\nW naukach społecznych PGD często obejmuje złożone czynniki społeczne, psychologiczne i ekonomiczne, które kształtują cechy populacji.\nZrozumienie PGD pomaga nam interpretować, dlaczego populacja ma pewne cechy i jak mogą się one zmieniać w czasie lub w różnych kontekstach.\n\n\n\n\n\n\n\nPrzykład: Badanie Zachowań Wyborczych\n\n\n\n\nPopulacja: Wszyscy uprawnieni wyborcy w kraju\nPróba: 1000 losowo wybranych uprawnionych wyborców\nPGD: Złożone współdziałanie czynników wpływających na decyzje wyborcze, takich jak przekonania polityczne, warunki ekonomiczne, ekspozycja na media i sieci społeczne.\n\nZrozumienie PGD pomaga badaczom interpretować zachowania wyborcze i potencjalnie przewidywać przyszłe trendy.\n\n\nZobrazujmy tę koncepcję za pomocą R:\n\n\nKliknij, aby pokazać/ukryć kod R\nlibrary(ggplot2)\n\n# Symulacja populacji na podstawie prostego PGD\nset.seed(123)\nwiek &lt;- rnorm(10000, mean = 45, sd = 15)\ndochod &lt;- exp(rnorm(10000, mean = 10, sd = 0.5))\nprawdopodobienstwo_glosowania &lt;- plogis(-5 + 0.05 * wiek + 0.00003 * dochod + rnorm(10000, sd = 0.5))\npopulacja &lt;- data.frame(wiek = wiek, dochod = dochod, prawdopodobienstwo_glosowania = prawdopodobienstwo_glosowania)\n\n# Wybór losowej próby\nindeksy_proby &lt;- sample(1:nrow(populacja), 1000)\ndane_proby &lt;- populacja[indeksy_proby, ]\n\n# Wizualizacja\nggplot(dane_proby, aes(x = wiek, y = dochod, color = prawdopodobienstwo_glosowania)) +\n  geom_point(alpha = 0.6) +\n  scale_color_viridis_c() +\n  labs(title = \"Próba: Wiek, Dochód i Prawdopodobieństwo Głosowania\",\n       x = \"Wiek\", y = \"Dochód\", color = \"Prawdopodobieństwo Głosowania\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nTa wizualizacja pokazuje, jak wiek i dochód (część PGD) wpływają na prawdopodobieństwo głosowania w naszej symulowanej populacji, na podstawie wybranej próby.\n\n\n\n2.3.2 Rodzaje Danych w Naukach Społecznych\nBadania w naukach społecznych zajmują się różnymi rodzajami danych:\n\nDane Ilościowe: Dane liczbowe (np. odpowiedzi z ankiet, wskaźniki ekonomiczne)\nDane Jakościowe: Dane nieliczbowe (np. transkrypcje wywiadów, odpowiedzi na pytania otwarte w ankietach)\nBig Data: Dane cyfrowe na dużą skalę (np. posty w mediach społecznościowych, logi zachowań online)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#główne-komponenty-nauki-o-danych-w-badaniach-społecznych",
    "href": "rozdzial1.html#główne-komponenty-nauki-o-danych-w-badaniach-społecznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.4 Główne Komponenty Nauki o Danych w Badaniach Społecznych",
    "text": "2.4 Główne Komponenty Nauki o Danych w Badaniach Społecznych\n\nZbieranie DanychPrzetwarzanie DanychEksploracyjna Analiza Danych (EDA)Wnioskowanie StatystyczneZastosowania Uczenia MaszynowegoWizualizacja Danych i Komunikacja\n\n\n\nAnkiety i kwestionariusze\nWywiady i grupy fokusowe\nZbieranie danych cyfrowych (np. web scraping, dostęp do API)\nEtyczne aspekty zbierania danych\n\n\n\n\nCzyszczenie i wstępne przetwarzanie danych\nObsługa brakujących wartości i wartości odstających\nKodowanie danych jakościowych\nTransformacja i normalizacja danych\n\n\n\n\nStatystyki opisowe\nWizualizacja danych\nIdentyfikacja wzorców i trendów w zjawiskach społecznych\n\n\n\n\nTestowanie hipotez w badaniach społecznych\nAnaliza regresji\nTechniki wnioskowania przyczynowego\n\n\n\n\nModelowanie predykcyjne zachowań społecznych\nAnaliza tekstu i analiza sentymentu\nAnaliza sieci w kontekstach społecznych\n\n\n\n\nTworzenie efektywnych wizualizacji dla danych z nauk społecznych\nKomunikowanie wyników badań różnym odbiorcom\nStorytelling oparty na danych",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#narzędzia-do-nauki-o-danych-w-naukach-społecznych",
    "href": "rozdzial1.html#narzędzia-do-nauki-o-danych-w-naukach-społecznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.5 Narzędzia do Nauki o Danych w Naukach Społecznych",
    "text": "2.5 Narzędzia do Nauki o Danych w Naukach Społecznych\nW tym kursie będziemy głównie używać R do naszej analizy danych, ponieważ jest on szeroko stosowany w badaniach nauk społecznych.\n\n2.5.1 R w Analizie Danych Nauk Społecznych\nR oferuje potężne możliwości dla badań w naukach społecznych, od manipulacji danymi po zaawansowane modelowanie statystyczne.\n\n\nKliknij, aby pokazać/ukryć kod R\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nKliknij, aby pokazać/ukryć kod R\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate example data with a stronger Simpson's Paradox\nn &lt;- 1000\ndata &lt;- tibble(\n  age_group = sample(c(\"Young\", \"Middle\", \"Old\"), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),\n  education_years = case_when(\n    age_group == \"Young\" ~ rnorm(n, mean = 10, sd = 1),\n    age_group == \"Middle\" ~ rnorm(n, mean = 13, sd = 1),\n    age_group == \"Old\" ~ rnorm(n, mean = 16, sd = 1)\n  ),\n  income = case_when(\n    age_group == \"Young\" ~ 70000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Middle\" ~ 50000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Old\" ~ 30000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000)\n  )\n)\n\n# Basic data summary\nsummary(data)\n\n\n  age_group         education_years      income     \n Length:1000        Min.   : 6.628   Min.   :34068  \n Class :character   1st Qu.:10.913   1st Qu.:51508  \n Mode  :character   Median :13.004   Median :63376  \n                    Mean   :12.986   Mean   :63307  \n                    3rd Qu.:14.934   3rd Qu.:75023  \n                    Max.   :18.861   Max.   :96620  \n\n\nKliknij, aby pokazać/ukryć kod R\n# Correlation analysis\ncor(data %&gt;% select(education_years, income))\n\n\n                education_years     income\neducation_years       1.0000000 -0.8152477\nincome               -0.8152477  1.0000000\n\n\nKliknij, aby pokazać/ukryć kod R\n# Overall trend (Simpson's Paradox)\noverall_plot &lt;- ggplot(data, aes(x = education_years, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Overall Relationship between Education and Income\",\n       subtitle = \"Simpson's Paradox: Appears negative\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Trend by age group (Resolving Simpson's Paradox)\ngrouped_plot &lt;- ggplot(data, aes(x = education_years, y = income, color = age_group)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Relationship between Education and Income by Age Group\",\n       subtitle = \"Resolving Simpson's Paradox: Positive relationship within groups\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Statistical analysis\nmodel_overall &lt;- lm(income ~ education_years, data = data)\nmodel_by_age &lt;- lm(income ~ education_years + age_group, data = data)\n\n# Print results\nprint(overall_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(grouped_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_overall))\n\n\n\nCall:\nlm(formula = income ~ education_years, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24451  -5439    235   5262  34328 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     121814.7     1339.5   90.94   &lt;2e-16 ***\neducation_years  -4505.4      101.3  -44.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7976 on 998 degrees of freedom\nMultiple R-squared:  0.6646,    Adjusted R-squared:  0.6643 \nF-statistic:  1978 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_by_age))\n\n\n\nCall:\nlm(formula = income ~ education_years + age_group, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-14827  -3369    118   3356  16388 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      48270.8     2028.4  23.797  &lt; 2e-16 ***\neducation_years   1135.5      154.6   7.345 4.26e-13 ***\nage_groupOld    -19942.8      593.2 -33.619  &lt; 2e-16 ***\nage_groupYoung   20461.1      600.7  34.064  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4950 on 996 degrees of freedom\nMultiple R-squared:  0.8711,    Adjusted R-squared:  0.8707 \nF-statistic:  2244 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\n# Calculate and print correlations\noverall_cor &lt;- cor(data$education_years, data$income)\ngroup_cors &lt;- data %&gt;%\n  group_by(age_group) %&gt;%\n  summarize(correlation = cor(education_years, income))\n\nprint(\"Overall correlation:\")\n\n\n[1] \"Overall correlation:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(overall_cor)\n\n\n[1] -0.8152477\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(\"Correlations by age group:\")\n\n\n[1] \"Correlations by age group:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(group_cors)\n\n\n# A tibble: 3 × 2\n  age_group correlation\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Middle          0.185\n2 Old             0.291\n3 Young           0.223\n\n\nTen przykład demonstruje podstawową manipulację danymi, statystyki opisowe i wizualizację przy użyciu R, które są powszechnymi zadaniami w badaniach nauk społecznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#etyczne-aspekty-w-analizie-danych-nauk-społecznych",
    "href": "rozdzial1.html#etyczne-aspekty-w-analizie-danych-nauk-społecznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.17 Etyczne Aspekty w Analizie Danych Nauk Społecznych",
    "text": "2.17 Etyczne Aspekty w Analizie Danych Nauk Społecznych\nEtyka odgrywa kluczową rolę w badaniach nauk społecznych:\n\nPrywatność i Zgoda: Zapewnienie prywatności uczestników i świadomej zgody\nOchrona Danych: Bezpieczne przechowywanie i zarządzanie wrażliwymi danymi osobowymi\nBłędy i Reprezentacja: Adresowanie błędów próbkowania i zapewnienie różnorodnej reprezentacji\nPrzejrzystość: Jasne komunikowanie metod badawczych i ograniczeń\nWpływ Społeczny: Rozważanie potencjalnych społecznych implikacji wyników badań\n\n\n\n\n\n\n\nWażne\n\n\n\nNaukowcy społeczni muszą starannie rozważyć etyczne implikacje swoich praktyk zbierania, analizy i rozpowszechniania danych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#kluczowe-wnioski",
    "href": "rozdzial1.html#kluczowe-wnioski",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.18 Kluczowe Wnioski",
    "text": "2.18 Kluczowe Wnioski\n\nNauka o danych w naukach społecznych bazuje na tradycyjnych metodach statystycznych, włączając nowe technologie do analizy złożonych zjawisk społecznych.\nZrozumienie koncepcji takich jak populacja, próba i procesy generowania danych jest kluczowe dla prawidłowych badań w naukach społecznych.\nProces nauki o danych w badaniach społecznych obejmuje wiele etapów, od etycznego zbierania danych po komunikację wniosków.\nR jest potężnym narzędziem do analizy danych w naukach społecznych, oferującym szeroki zakres możliwości.\nAspekty etyczne powinny być na pierwszym planie każdego projektu związanego z danymi w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#następne-kroki",
    "href": "rozdzial1.html#następne-kroki",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.19 Następne Kroki",
    "text": "2.19 Następne Kroki\nW kolejnych rozdziałach zagłębimy się w każdy komponent procesu nauki o danych, badając koncepcje statystyczne, techniki programowania w R i rzeczywiste zastosowania w badaniach nauk społecznych.\n\n\n\n\n\n\nĆwiczenie Praktyczne\n\n\n\nWykorzystując koncepcje poznane w tym rozdziale, zaprojektuj małe badanie na wybrany temat z nauk społecznych. Określ populację, zastanów się, jak wybrałbyś próbę i przemyśl potencjalny proces generowania danych. Jakie aspekty etyczne należałoby uwzględnić?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1 Introduction\nIn social science research, understanding the nature of our data is crucial for selecting appropriate analysis methods and drawing valid conclusions. This chapter explores fundamental concepts of data types, starting from basic mathematical set theory and progressing to practical applications in social science research.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#foundations-in-number-sets",
    "href": "chapter2.html#foundations-in-number-sets",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.2 Foundations in Number Sets",
    "text": "3.2 Foundations in Number Sets\nBefore diving into data types, it’s essential to understand the basic number sets that form the foundation of our understanding of data.\n\n3.2.1 Basic Number Sets\n\nNatural Numbers (ℕ): The counting numbers {1, 2, 3, …}\nIntegers (ℤ): Includes natural numbers, their negatives, and zero {…, -2, -1, 0, 1, 2, …}\nRational Numbers (ℚ): Numbers that can be expressed as a fraction of two integers\nReal Numbers (ℝ): All numbers on the number line, including rationals and irrationals\n\n\n\n3.2.2 Properties of Sets\n\nCountable Sets: Sets whose elements can be put in a one-to-one correspondence with the natural numbers. For example, the set of integers is countable.\nUncountable Sets: Sets that are not countable. The set of real numbers is uncountable.\nDiscrete Sets: Sets where each element is separated from other elements by a finite gap. The integers form a discrete set.\nDense Sets: Sets where between any two elements, there is always another element of the set. The rational numbers and real numbers are dense sets.\n\n\n\n\n\n\n\nNote\n\n\n\nUnderstanding these set properties is crucial for grasping the nature of different data types in social sciences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#discrete-vs.-continuous-data",
    "href": "chapter2.html#discrete-vs.-continuous-data",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.3 Discrete vs. Continuous Data",
    "text": "3.3 Discrete vs. Continuous Data\nNow that we have a foundation in number sets, we can better understand the distinction between discrete and continuous data in social science research.\n\n3.3.1 Discrete Data\nDiscrete data corresponds to discrete sets in mathematics. It can only take on specific, separate values, often from a countable set.\nProperties of discrete data:\n\nValues are distinct and separate\nOften (but not always) represented by integers\nTypically counted rather than measured\nCan be finite or infinite\n\n\nExamples of discrete data in social sciences:\n\nNumber of children in a family (ℕ)\nEducational level (e.g., 1 = high school, 2 = bachelor’s, 3 = master’s)\nVoting choices in an election (e.g., 1 = Party A, 2 = Party B, 3 = Party C)\n\n\n\n\n3.3.2 Continuous Data\nContinuous data corresponds to dense sets in mathematics, typically represented by real numbers. It can take any value within a range.\nProperties of continuous data:\n\nValues can be any real number within a range\nRepresented by real numbers (ℝ)\nTypically measured rather than counted\nAlways infinite (in theory, though limited by measurement precision in practice)\n\n\nExamples of continuous data in social sciences:\n\nAge (can be any real number ≥ 0)\nIncome (can be any non-negative real number)\nTime spent on a task (can be any non-negative real number)\n\n\n\n\n3.3.3 Visualization of Discrete vs. Continuous Data\nLet’s visualize the difference using R:\n\n\nClick to show/hide R code\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# Generate sample data\nset.seed(123)\ndiscrete_data &lt;- sample(1:5, 1000, replace = TRUE)\ncontinuous_data &lt;- rnorm(1000, mean = 3, sd = 1)\n\n# Create data frames\ndiscrete_df &lt;- data.frame(value = discrete_data, type = \"Discrete\")\ncontinuous_df &lt;- data.frame(value = continuous_data, type = \"Continuous\")\n\n# Discrete plot\np1 &lt;- ggplot(discrete_df, aes(x = value)) +\n  geom_bar() +\n  scale_x_continuous(breaks = 1:5) +\n  labs(title = \"Discrete Data\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\n# Continuous plot\np2 &lt;- ggplot(continuous_df, aes(x = value)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Continuous Data\", x = \"Value\", y = \"Count\") +\n  theme_minimal()\n\n# Display plots side by side\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n3.3.4 The Continuum Between Discrete and Continuous\nIn practice, the distinction between discrete and continuous data can sometimes blur:\n\nDiscretized Continuous Data: Continuous variables that are rounded or grouped into categories (e.g., age groups, income brackets).\nHigh-Cardinality Discrete Data: Discrete variables with many possible values can approximate continuous data (e.g., ZIP codes, detailed occupation codes).\nLimited Precision Measurements: Continuous variables measured with limited precision appear discrete (e.g., temperature measured to the nearest degree).\n\n\n\n\n\n\n\nImportant\n\n\n\nThe choice to treat data as discrete or continuous often depends on the research context, measurement precision, and analytical goals.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#implications-for-social-science-research",
    "href": "chapter2.html#implications-for-social-science-research",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.4 Implications for Social Science Research",
    "text": "3.4 Implications for Social Science Research\nUnderstanding the nature of data as discrete or continuous, and their relationship to mathematical sets, has important implications for social science research:\n\nMeasurement and Operationalization: How we define and measure variables can influence whether they are treated as discrete or continuous.\nStatistical Analysis: Different statistical techniques are appropriate for discrete vs. continuous data. For example:\n\nDiscrete: Chi-square tests, logistic regression\nContinuous: t-tests, linear regression\n\nData Visualization: The choice of visualization technique depends on whether data is discrete or continuous (e.g., bar plots vs. histograms).\nInterpretation of Results: Understanding the discrete or continuous nature of data is crucial for correctly interpreting research findings and their implications.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#practical-exercise",
    "href": "chapter2.html#practical-exercise",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.5 Practical Exercise",
    "text": "3.5 Practical Exercise\nLet’s create a dataset that includes both discrete and continuous variables, and explore how their properties influence our analysis:\n\n\nClick to show/hide R code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nClick to show/hide R code\n# Create sample dataset\nset.seed(456)\nn &lt;- 200\n\ndata &lt;- tibble(\n  id = 1:n,\n  age = round(runif(n, 18, 65), 1),  # Continuous\n  income = round(rlnorm(n, meanlog = 10.5, sdlog = 0.5), 2),  # Continuous\n  education_years = sample(10:22, n, replace = TRUE),  # Discrete\n  job_satisfaction = sample(1:5, n, replace = TRUE)  # Discrete (Likert-type)\n)\n\n# Summary statistics\nsummary(data)\n\n\n       id              age            income       education_years\n Min.   :  1.00   Min.   :18.10   Min.   : 11039   Min.   :10.00  \n 1st Qu.: 50.75   1st Qu.:32.67   1st Qu.: 24964   1st Qu.:13.00  \n Median :100.50   Median :43.75   Median : 36294   Median :17.00  \n Mean   :100.50   Mean   :43.52   Mean   : 40536   Mean   :16.02  \n 3rd Qu.:150.25   3rd Qu.:55.42   3rd Qu.: 53754   3rd Qu.:19.00  \n Max.   :200.00   Max.   :64.70   Max.   :140046   Max.   :22.00  \n job_satisfaction\n Min.   :1.000   \n 1st Qu.:2.000   \n Median :3.000   \n Mean   :3.305   \n 3rd Qu.:4.000   \n Max.   :5.000   \n\n\nClick to show/hide R code\n# Visualizations\np1 &lt;- ggplot(data, aes(x = age)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Age Distribution (Continuous)\", x = \"Age\", y = \"Count\")\n\np2 &lt;- ggplot(data, aes(x = education_years)) +\n  geom_bar(fill = \"lightgreen\", color = \"black\") +\n  labs(title = \"Education Years (Discrete)\", x = \"Years of Education\", y = \"Count\")\n\np3 &lt;- ggplot(data, aes(x = income)) +\n  geom_histogram(bins = 30, fill = \"pink\", color = \"black\") +\n  labs(title = \"Income Distribution (Continuous)\", x = \"Income\", y = \"Count\")\n\np4 &lt;- ggplot(data, aes(x = factor(job_satisfaction))) +\n  geom_bar(fill = \"lightyellow\", color = \"black\") +\n  labs(title = \"Job Satisfaction (Discrete)\", x = \"Satisfaction Level\", y = \"Count\")\n\n# Arrange plots\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\nClick to show/hide R code\n# Correlation analysis (appropriate for continuous variables)\ncor(data$age, data$income)\n\n\n[1] 0.03168301\n\n\nClick to show/hide R code\n# Chi-square test (appropriate for discrete variables)\nchisq.test(table(data$education_years, data$job_satisfaction))\n\n\nWarning in chisq.test(table(data$education_years, data$job_satisfaction)):\nChi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table(data$education_years, data$job_satisfaction)\nX-squared = 51.754, df = 48, p-value = 0.3295",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#conclusion",
    "href": "chapter2.html#conclusion",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.6 Conclusion",
    "text": "3.6 Conclusion\nUnderstanding the fundamental properties of number sets and how they relate to discrete and continuous data is crucial in social science research. This knowledge informs our choices in measurement, analysis, and interpretation of data. As we’ve seen, while the mathematical distinction between discrete and continuous is clear, real-world data often exists on a continuum between these two types. Researchers must carefully consider the nature of their data when designing studies, choosing analytical methods, and drawing conclusions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#references",
    "href": "chapter2.html#references",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.7 References",
    "text": "3.7 References\n\nGentle, J. E. (2009). Computational statistics (Vol. 308). New York: Springer.\nStevens, S. S. (1946). On the theory of scales of measurement. Science, 103(2684), 677-680.\nAgresti, A. (2018). Statistical methods for the social sciences. Pearson.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html",
    "href": "rozdzial2.html",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "",
    "text": "4.1 Wprowadzenie\nW badaniach nauk społecznych zrozumienie natury naszych danych jest kluczowe dla wyboru odpowiednich metod analizy i wyciągania prawidłowych wniosków. Ten rozdział bada fundamentalne koncepcje typów danych, zaczynając od podstawowej teorii zbiorów matematycznych i przechodząc do praktycznych zastosowań w badaniach nauk społecznych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "href": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.2 Podstawy Zbiorów Liczbowych",
    "text": "4.2 Podstawy Zbiorów Liczbowych\nZanim zagłębimy się w typy danych, istotne jest zrozumienie podstawowych zbiorów liczbowych, które tworzą fundament naszego rozumienia danych.\n\n4.2.1 Podstawowe Zbiory Liczbowe\n\nLiczby Naturalne (ℕ): Liczby do liczenia {1, 2, 3, …}\nLiczby Całkowite (ℤ): Obejmują liczby naturalne, ich przeciwne i zero {…, -2, -1, 0, 1, 2, …}\nLiczby Wymierne (ℚ): Liczby, które można wyrazić jako iloraz dwóch liczb całkowitych\nLiczby Rzeczywiste (ℝ): Wszystkie liczby na osi liczbowej, włączając wymierne i niewymierne\n\n\n\n4.2.2 Właściwości Zbiorów\n\nZbiory Przeliczalne: Zbiory, których elementy można ustawić w odpowiedniości jeden-do-jednego z liczbami naturalnymi. Na przykład, zbiór liczb całkowitych jest przeliczalny.\nZbiory Nieprzeliczalne: Zbiory, które nie są przeliczalne. Zbiór liczb rzeczywistych jest nieprzeliczalny.\nZbiory Dyskretne: Zbiory, w których każdy element jest oddzielony od innych elementów skończoną przerwą. Liczby całkowite tworzą zbiór dyskretny.\nZbiory Gęste: Zbiory, w których między dowolnymi dwoma elementami zawsze znajduje się inny element tego zbioru. Liczby wymierne i rzeczywiste są zbiorami gęstymi.\n\n\n\n\n\n\n\nNote\n\n\n\nZrozumienie tych właściwości zbiorów jest kluczowe dla uchwycenia natury różnych typów danych w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#dane-dyskretne-vs.-dane-ciągłe",
    "href": "rozdzial2.html#dane-dyskretne-vs.-dane-ciągłe",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.3 Dane Dyskretne vs. Dane Ciągłe",
    "text": "4.3 Dane Dyskretne vs. Dane Ciągłe\nTeraz, gdy mamy podstawy w zbiorach liczbowych, możemy lepiej zrozumieć rozróżnienie między danymi dyskretnymi a ciągłymi w badaniach nauk społecznych.\n\n4.3.1 Dane Dyskretne\nDane dyskretne odpowiadają zbiorom dyskretnym w matematyce. Mogą przyjmować tylko określone, oddzielne wartości, często z przeliczalnego zbioru.\nWłaściwości danych dyskretnych:\n\nWartości są odrębne i oddzielne\nCzęsto (ale nie zawsze) reprezentowane przez liczby całkowite\nZazwyczaj liczone, a nie mierzone\nMogą być skończone lub nieskończone\n\n\nPrzykłady danych dyskretnych w naukach społecznych:\n\nLiczba dzieci w rodzinie (ℕ)\nPoziom wykształcenia (np. 1 = szkoła średnia, 2 = licencjat, 3 = magisterium)\nWybory wyborcze (np. 1 = Partia A, 2 = Partia B, 3 = Partia C)\n\n\n\n\n4.3.2 Dane Ciągłe\nDane ciągłe odpowiadają zbiorom gęstym w matematyce, typowo reprezentowanym przez liczby rzeczywiste. Mogą przyjmować dowolną wartość w danym zakresie.\nWłaściwości danych ciągłych:\n\nWartości mogą być dowolną liczbą rzeczywistą w danym zakresie\nReprezentowane przez liczby rzeczywiste (ℝ)\nZazwyczaj mierzone, a nie liczone\nZawsze nieskończone (w teorii, choć ograniczone precyzją pomiaru w praktyce)\n\n\nPrzykłady danych ciągłych w naukach społecznych:\n\nWiek (może być dowolną liczbą rzeczywistą ≥ 0)\nDochód (może być dowolną nieujemną liczbą rzeczywistą)\nCzas spędzony na zadaniu (może być dowolną nieujemną liczbą rzeczywistą)\n\n\n\n\n4.3.3 Wizualizacja Danych Dyskretnych vs. Ciągłych\nZobrazujmy różnicę przy użyciu R:\n\n\nKliknij, aby pokazać/ukryć kod R\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# Generowanie przykładowych danych\nset.seed(123)\ndane_dyskretne &lt;- sample(1:5, 1000, replace = TRUE)\ndane_ciągłe &lt;- rnorm(1000, mean = 3, sd = 1)\n\n# Tworzenie ramek danych\ndf_dyskretne &lt;- data.frame(wartość = dane_dyskretne, typ = \"Dyskretne\")\ndf_ciągłe &lt;- data.frame(wartość = dane_ciągłe, typ = \"Ciągłe\")\n\n# Wykres dla danych dyskretnych\np1 &lt;- ggplot(df_dyskretne, aes(x = wartość)) +\n  geom_bar() +\n  scale_x_continuous(breaks = 1:5) +\n  labs(title = \"Dane Dyskretne\", x = \"Wartość\", y = \"Liczba\") +\n  theme_minimal()\n\n# Wykres dla danych ciągłych\np2 &lt;- ggplot(df_ciągłe, aes(x = wartość)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Dane Ciągłe\", x = \"Wartość\", y = \"Liczba\") +\n  theme_minimal()\n\n# Wyświetlanie wykresów obok siebie\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n4.3.4 Kontinuum Między Danymi Dyskretnymi a Ciągłymi\nW praktyce rozróżnienie między danymi dyskretnymi a ciągłymi może się czasami zacierać:\n\nDane Ciągłe Zdyskretyzowane: Zmienne ciągłe, które są zaokrąglone lub pogrupowane w kategorie (np. grupy wiekowe, przedziały dochodów).\nDane Dyskretne o Wysokiej Liczności: Zmienne dyskretne z wieloma możliwymi wartościami mogą przybliżać dane ciągłe (np. kody pocztowe, szczegółowe kody zawodów).\nPomiary o Ograniczonej Precyzji: Zmienne ciągłe mierzone z ograniczoną precyzją wydają się dyskretne (np. temperatura mierzona do najbliższego stopnia).\n\n\n\n\n\n\n\nImportant\n\n\n\nWybór traktowania danych jako dyskretnych lub ciągłych często zależy od kontekstu badawczego, precyzji pomiaru i celów analitycznych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#implikacje-dla-badań-w-naukach-społecznych",
    "href": "rozdzial2.html#implikacje-dla-badań-w-naukach-społecznych",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.4 Implikacje dla Badań w Naukach Społecznych",
    "text": "4.4 Implikacje dla Badań w Naukach Społecznych\nZrozumienie natury danych jako dyskretnych lub ciągłych oraz ich związku ze zbiorami matematycznymi ma ważne implikacje dla badań w naukach społecznych:\n\nPomiar i Operacjonalizacja: Sposób, w jaki definiujemy i mierzymy zmienne, może wpływać na to, czy są one traktowane jako dyskretne czy ciągłe.\nAnaliza Statystyczna: Różne techniki statystyczne są odpowiednie dla danych dyskretnych vs. ciągłych. Na przykład:\n\nDyskretne: Testy chi-kwadrat, regresja logistyczna\nCiągłe: Testy t, regresja liniowa\n\nWizualizacja Danych: Wybór techniki wizualizacji zależy od tego, czy dane są dyskretne czy ciągłe (np. wykresy słupkowe vs. histogramy).\nInterpretacja Wyników: Zrozumienie dyskretnej lub ciągłej natury danych jest kluczowe dla prawidłowej interpretacji wyników badań i ich implikacji.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#ćwiczenie-praktyczne",
    "href": "rozdzial2.html#ćwiczenie-praktyczne",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.5 Ćwiczenie Praktyczne",
    "text": "4.5 Ćwiczenie Praktyczne\nStwórzmy zbiór danych, który zawiera zarówno zmienne dyskretne, jak i ciągłe, i zbadajmy, jak ich właściwości wpływają na naszą analizę:\n\n\nKliknij, aby pokazać/ukryć kod R\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nKliknij, aby pokazać/ukryć kod R\n# Tworzenie przykładowego zbioru danych\nset.seed(456)\nn &lt;- 200\n\ndane &lt;- tibble(\n  id = 1:n,\n  wiek = round(runif(n, 18, 65), 1),  # Ciągłe\n  dochód = round(rlnorm(n, meanlog = 10.5, sdlog = 0.5), 2),  # Ciągłe\n  lata_edukacji = sample(10:22, n, replace = TRUE),  # Dyskretne\n  satysfakcja_z_pracy = sample(1:5, n, replace = TRUE)  # Dyskretne (typu Likerta)\n)\n\n# Statystyki opisowe\nsummary(dane)\n\n\n       id              wiek           dochód       lata_edukacji  \n Min.   :  1.00   Min.   :18.10   Min.   : 11039   Min.   :10.00  \n 1st Qu.: 50.75   1st Qu.:32.67   1st Qu.: 24964   1st Qu.:13.00  \n Median :100.50   Median :43.75   Median : 36294   Median :17.00  \n Mean   :100.50   Mean   :43.52   Mean   : 40536   Mean   :16.02  \n 3rd Qu.:150.25   3rd Qu.:55.42   3rd Qu.: 53754   3rd Qu.:19.00  \n Max.   :200.00   Max.   :64.70   Max.   :140046   Max.   :22.00  \n satysfakcja_z_pracy\n Min.   :1.000      \n 1st Qu.:2.000      \n Median :3.000      \n Mean   :3.305      \n 3rd Qu.:4.000      \n Max.   :5.000      \n\n\nKliknij, aby pokazać/ukryć kod R\n# Wizualizacje\np1 &lt;- ggplot(dane, aes(x = wiek)) +\n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Rozkład Wieku (Ciągłe)\", x = \"Wiek\", y = \"Liczba\")\n\np2 &lt;- ggplot(dane, aes(x = lata_edukacji)) +\n  geom_bar(fill = \"lightgreen\", color = \"black\") +\n  labs(title = \"Lata Edukacji (Dyskretne)\", x = \"Lata Edukacji\", y = \"Liczba\")\n\np3 &lt;- ggplot(dane, aes(x = dochód)) +\n  geom_histogram(bins = 30, fill = \"pink\", color = \"black\") +\n  labs(title = \"Rozkład Dochodów (Ciągłe)\", x = \"Dochód\", y = \"Liczba\")\n\np4 &lt;- ggplot(dane, aes(x = factor(satysfakcja_z_pracy))) +\n  geom_bar(fill = \"lightyellow\", color = \"black\") +\n  labs(title = \"Satysfakcja z Pracy (Dyskretne)\", x = \"Poziom Satysfakcji\", y = \"Liczba\")\n\n# Układanie wykresów\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\n# Analiza korelacji (odpowiednia dla zmiennych ciągłych)\ncor(dane$wiek, dane$dochód)\n\n\n[1] 0.03168301\n\n\nKliknij, aby pokazać/ukryć kod R\n# Test chi-kwadrat (odpowiedni dla zmiennych dyskretnych)\nchisq.test(table(dane$lata_edukacji, dane$satysfakcja_z_pracy))\n\n\nWarning in chisq.test(table(dane$lata_edukacji, dane$satysfakcja_z_pracy)):\nChi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table(dane$lata_edukacji, dane$satysfakcja_z_pracy)\nX-squared = 51.754, df = 48, p-value = 0.3295",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#podsumowanie",
    "href": "rozdzial2.html#podsumowanie",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.6 Podsumowanie",
    "text": "4.6 Podsumowanie\nZrozumienie fundamentalnych właściwości zbiorów liczbowych i ich związku z danymi dyskretnymi i ciągłymi jest kluczowe w badaniach nauk społecznych. Ta wiedza wpływa na nasze wybory w zakresie pomiaru, analizy i interpretacji danych. Jak widzieliśmy, podczas gdy matematyczne rozróżnienie między dyskretnym a ciągłym jest jasne, dane ze świata rzeczywistego często istnieją na kontinuum między tymi dwoma typami. Badacze muszą starannie rozważyć naturę swoich danych podczas projektowania badań, wyboru metod analitycznych i wyciągania wniosków.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#bibliografia",
    "href": "rozdzial2.html#bibliografia",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.7 Bibliografia",
    "text": "4.7 Bibliografia\n\nGentle, J. E. (2009). Computational statistics (Vol. 308). New York: Springer.\nStevens, S. S. (1946). On the theory of scales of measurement. Science, 103(2684), 677-680.\nAgresti, A. (2018). Statistical methods for the social sciences. Pearson.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "",
    "text": "5.1 Introduction to Randomness\nRandomness is a cornerstone concept in statistics and scientific research. It refers to the unpredictability of individual outcomes, even when the overall pattern may be predictable. In the social sciences, understanding randomness is crucial for designing studies, collecting data, and interpreting results.\nConsider flipping a fair coin. While we know that the probability of getting heads is 50%, we can’t predict with certainty the outcome of any single flip. This unpredictability is the essence of randomness.\nExamples of random phenomena in social sciences include:\nUnderstanding randomness helps researchers distinguish between genuine effects and chance occurrences. For instance, if we observe a slight difference in test scores between two groups, randomness helps us determine whether this difference is likely due to a real effect or just chance variation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#introduction-to-randomness",
    "href": "chapter3.html#introduction-to-randomness",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "",
    "text": "Participant Selection: In a psychology experiment studying reaction times, the order in which participants arrive at the lab may be random.\nEconomic Behavior: The daily fluctuations in stock prices often exhibit random patterns, influenced by countless unpredictable factors.\nSocial Interactions: The occurrence of chance encounters between individuals in a community can be considered random events.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-bridging-sample-and-population",
    "href": "chapter3.html#sampling-bridging-sample-and-population",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.2 Sampling: Bridging Sample and Population",
    "text": "5.2 Sampling: Bridging Sample and Population\nSampling is the process of selecting a subset (sample) from a larger group (population) to make inferences about the population. It’s a critical skill in social science research, as studying entire populations is often impractical, too expensive, or sometimes impossible.\nKey Terms:\n\nPopulation: The entire group about which we want to draw conclusions.\nSample: A subset of the population that we actually study.\nSampling Frame: The list or procedure used to identify all members of the population.\n\nExample: Suppose we want to study the job satisfaction of all teachers in the United States (the population). Instead of surveying millions of teachers, we might select a sample of 5,000 teachers from various states, school districts, and grade levels.\nRandomness in sampling helps ensure that the sample is representative of the population, reducing bias and allowing for more accurate inferences. This is why probability sampling methods, which we’ll discuss next, are often preferred in scientific research.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-methods",
    "href": "chapter3.html#sampling-methods",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.3 Sampling Methods",
    "text": "5.3 Sampling Methods\n\n5.3.1 Probability Sampling\nProbability sampling methods involve random selection, giving each member of the population a known, non-zero chance of being selected.\n\nSimple Random Sampling: Each member of the population has an equal chance of being selected.\nExample: To select 100 students from a university with 10,000 students, you could assign each student a number from 1 to 10,000, then use a random number generator to select 100 numbers.\nStratified Random Sampling: The population is divided into subgroups (strata) based on shared characteristics, then samples are randomly selected from each stratum.\nExample: In a national political survey, you might divide the population into strata based on geographic regions (Northeast, Midwest, South, West) and then randomly sample from each region. This ensures representation from all areas of the country.\nCluster Sampling: The population is divided into clusters (usually geographic), some clusters are randomly selected, and all members within those clusters are studied.\nExample: To study high school students’ study habits, you might randomly select 20 high schools from across the country and then survey all students in those schools.\nSystematic Sampling: Selecting every kth item from a list after a random start.\nExample: At a busy shopping mall, you might survey every 20th person who enters the mall, starting with a randomly chosen number between 1 and 20.\n\n\n\n5.3.2 Non-probability Sampling\nNon-probability sampling doesn’t involve random selection. While it can introduce bias, it may be necessary in certain situations, especially when dealing with hard-to-reach populations or when resources are limited.\n\nConvenience Sampling: Selecting easily accessible subjects.\nExample: A researcher studying college students’ sleep patterns might survey students in their own classes or around campus.\nPurposive Sampling: Selecting subjects based on specific characteristics.\nExample: For a study on the experiences of CEOs in the tech industry, a researcher might intentionally seek out and interview CEOs from various tech companies.\nSnowball Sampling: Participants recruit other participants.\nExample: In a study of undocumented immigrants’ access to healthcare, researchers might ask initial participants to refer other potential participants from their community.\nQuota Sampling: Selecting participants to meet specific quotas for certain characteristics.\nExample: In a market research study, researchers might ensure they interview a specific number of people from different age groups, genders, and income levels to match the demographics of the target market.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#making-inferences-from-samples",
    "href": "chapter3.html#making-inferences-from-samples",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.4 Making Inferences from Samples",
    "text": "5.4 Making Inferences from Samples\nStatistical inference is the process of drawing conclusions about a population based on a sample. This allows researchers to estimate characteristics of the entire population (parameters) using characteristics of the sample (statistics).\n\n\n\n\n\n\nNote\n\n\n\nThe Soup Analogy: A Taste of Statistics\n\n\nWhen you taste a spoonful of soup and decide it isn’t salty enough, that’s exploratory/descriptive analysis.\nIf you generalize and conclude that your entire pot of soup needs salt, that’s an inference.\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population).\nIf the soup is not well stirred (heterogeneous population), it doesn’t matter how large a spoon you have (sample size), it will still not accurately represent the whole. If the soup is well stirred (homogeneous population), even a small spoon will suffice to test the soup.\n\n\n\n\n\n\n\n\ngraph TD\n    DGP[Data Generating Process] --&gt;|Generates| A[Population]\n    A --&gt;|Random Selection| B[Sample]\n    B --&gt;|Statistical Inference| C[Estimates & Conclusions]\n    C --&gt;|Generalize back to| A\n    C -.-&gt;|Attempt to understand| DGP\n\n    style DGP fill:#1E90FF,stroke:#000,stroke-width:4px,color:#FFF\n    style A fill:#DC143C,stroke:#000,stroke-width:4px,color:#FFF\n    style B fill:#228B22,stroke:#000,stroke-width:2px,color:#FFF\n    style C fill:#8B4513,stroke:#000,stroke-width:2px,color:#FFF\n    \n    classDef note fill:#F0F0F0,stroke:#000,stroke-width:1px;\n    D[[\"DGP:\n    Underlying process\n    that generates data\"]]\n    E[[\"Population:\n    Entire group of interest\"]]\n    F[[\"Sample:\n    Subset of population\"]]\n    G[[\"Inference:\n    Drawing conclusions\n    about population\n    and DGP\"]]\n    \n    class D,E,F,G note\n    \n    D --&gt; DGP\n    E --&gt; A\n    F --&gt; B\n    G --&gt; C\n\n\n\n\n\n\nKey Concepts:\n\nPoint Estimates: A single value used to estimate a population parameter.\nExample: The mean income of a sample of 1000 workers might be used to estimate the mean income of all workers in a country.\nConfidence Intervals: A range of values likely to contain the true population parameter.\nExample: We might say, “We are 95% confident that the true population mean income falls between $45,000 and $55,000.”\nMargin of Error: The range of values above and below the sample statistic in a confidence interval.\nExample: In political polling, you might see a statement like “Candidate A is preferred by 52% of voters, with a margin of error of ±3%.”\nHypothesis Testing: A method for making decisions about population parameters based on sample data.\nExample: A researcher might test whether there’s a significant difference in test scores between students who study with music and those who study in silence.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-and-non-sampling-errors",
    "href": "chapter3.html#sampling-and-non-sampling-errors",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.5 Sampling and Non-sampling Errors",
    "text": "5.5 Sampling and Non-sampling Errors\nUnderstanding potential errors in research is crucial for interpreting results accurately.\nSampling Error: The difference between a sample statistic and the true population parameter, occurring due to chance variations in the selection of sample members.\nExample: If we estimate the average height of all adult males in a country based on a sample, our estimate will likely differ somewhat from the true average due to sampling error.\nNon-sampling Errors: Errors not due to chance, which can occur in both sample surveys and censuses.\n\nCoverage Error: When the sampling frame doesn’t accurately represent the population.\nExample: A telephone survey that only calls landlines would miss people who only have cell phones, potentially biasing the results.\nNon-response Error: When selected participants fail to respond, potentially introducing bias.\nExample: In a survey about job satisfaction, highly satisfied or highly dissatisfied employees might be more likely to respond, skewing the results.\nMeasurement Error: Inaccuracies in the data collected.\nExample: A poorly worded survey question might be interpreted differently by different respondents, leading to inconsistent data.\nProcessing Error: Mistakes made during data entry, coding, or analysis.\nExample: Accidentally entering “99” instead of “9” for a participant’s response could significantly skew the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sample-size-and-power",
    "href": "chapter3.html#sample-size-and-power",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.6 Sample Size and Power",
    "text": "5.6 Sample Size and Power\nDetermining the appropriate sample size involves balancing the need for precision with available resources.\nSample Size Considerations: - Larger samples generally provide more precise estimates but are more costly and time-consuming to obtain. - The required sample size depends on factors such as the desired level of precision, the variability in the population, and the type of analysis planned.\nExample: To estimate the proportion of voters who support a particular policy with a margin of error of ±3% at a 95% confidence level, you would need a sample size of about 1067 voters (assuming maximum variability).\nStatistical Power: The probability that a study will detect an effect when there is an effect to be detected.\nFactors affecting power: 1. Sample size 2. Effect size (the magnitude of the difference or relationship you’re trying to detect) 3. Chosen significance level (usually 0.05)\nExample: In a study comparing two teaching methods, having a larger sample size would increase the likelihood of detecting a significant difference between the methods, if such a difference exists.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-in-the-digital-age",
    "href": "chapter3.html#sampling-in-the-digital-age",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.7 Sampling in the Digital Age",
    "text": "5.7 Sampling in the Digital Age\nThe advent of big data and digital technologies has transformed sampling practices in many fields.\nBig Data Opportunities and Challenges: - Unprecedented volumes of information available - Potential lack of representativeness - Data quality concerns - Privacy and ethical issues\nExample: Social media data can provide real-time insights into public opinion, but users of a particular platform may not be representative of the general population.\nWeb-based Surveys: - Offer new opportunities for data collection - Face challenges such as coverage bias (not everyone has internet access) and self-selection bias\nExample: An online survey about internet usage habits would inherently exclude people without internet access, potentially biasing the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#ethical-considerations-in-sampling",
    "href": "chapter3.html#ethical-considerations-in-sampling",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.8 Ethical Considerations in Sampling",
    "text": "5.8 Ethical Considerations in Sampling\nEthical sampling practices are crucial in social science research:\n\nInformed Consent: Participants should understand the study’s purpose and agree to participate.\nExample: Before conducting interviews about sensitive topics like mental health, researchers must clearly explain the study’s aims and potential risks to participants.\nPrivacy and Confidentiality: Researchers must protect participants’ personal information.\nExample: In a study on workplace harassment, researchers might use code numbers instead of names to protect participants’ identities.\nRepresentativeness and Inclusivity: Samples should fairly represent diverse populations, including marginalized groups.\nExample: A study on urban housing should make efforts to include participants from various socioeconomic backgrounds, ethnicities, and housing situations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#case-studies",
    "href": "chapter3.html#case-studies",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.9 Case Studies",
    "text": "5.9 Case Studies\n[This section would include detailed examples of sampling in practice, such as: 1. How polling organizations predict election outcomes 2. How companies use market research to understand consumer preferences 3. How public health researchers study disease prevalence in populations]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#conclusion",
    "href": "chapter3.html#conclusion",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.10 Conclusion",
    "text": "5.10 Conclusion\nSampling remains a cornerstone of social science research, even in the era of big data. Understanding sampling principles helps researchers design studies, interpret results, and make valid inferences about populations. As we’ve seen, the journey from sample to population involves careful consideration of sampling methods, potential errors, ethical issues, and the ever-evolving landscape of data collection in the digital age.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#review-questions-and-exercises",
    "href": "chapter3.html#review-questions-and-exercises",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.11 Review Questions and Exercises",
    "text": "5.11 Review Questions and Exercises\n[This section would include a mix of multiple-choice questions, short answer prompts, and practical exercises to reinforce key concepts]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#further-reading",
    "href": "chapter3.html#further-reading",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.12 Further Reading",
    "text": "5.12 Further Reading\n[This section would list additional resources for students interested in deepening their understanding of sampling and related topics]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html",
    "href": "rozdzial3.html",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "",
    "text": "6.1 Wprowadzenie do Losowości\nLosowość jest fundamentalnym pojęciem w statystyce i badaniach naukowych. Odnosi się do nieprzewidywalności indywidualnych wyników, nawet gdy ogólny wzorzec może być przewidywalny. W naukach społecznych zrozumienie losowości jest kluczowe dla projektowania badań, zbierania danych i interpretacji wyników.\nRozważmy rzut uczciwą monetą. Chociaż wiemy, że prawdopodobieństwo wypadnięcia orła wynosi 50%, nie możemy z pewnością przewidzieć wyniku pojedynczego rzutu. Ta nieprzewidywalność jest istotą losowości.\nPrzykłady losowych zjawisk w naukach społecznych obejmują:\nZrozumienie losowości pomaga badaczom odróżnić rzeczywiste efekty od przypadkowych zdarzeń. Na przykład, jeśli zaobserwujemy niewielką różnicę w wynikach testów między dwiema grupami, losowość pomaga nam określić, czy ta różnica jest prawdopodobnie spowodowana rzeczywistym efektem, czy tylko przypadkową zmiennością.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wprowadzenie-do-losowości",
    "href": "rozdzial3.html#wprowadzenie-do-losowości",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "",
    "text": "Wybór uczestników: W eksperymencie psychologicznym badającym czasy reakcji, kolejność, w jakiej uczestnicy przybywają do laboratorium, może być losowa.\nZachowania ekonomiczne: Codzienne wahania cen akcji często wykazują losowe wzorce, na które wpływa niezliczona ilość nieprzewidywalnych czynników.\nInterakcje społeczne: Występowanie przypadkowych spotkań między osobami w społeczności można uznać za zdarzenia losowe.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#próbkowanie-łączenie-próby-i-populacji",
    "href": "rozdzial3.html#próbkowanie-łączenie-próby-i-populacji",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.2 Próbkowanie: Łączenie Próby i Populacji",
    "text": "6.2 Próbkowanie: Łączenie Próby i Populacji\nPróbkowanie to proces wybierania podzbioru (próby) z większej grupy (populacji) w celu wyciągnięcia wniosków o populacji. Jest to kluczowa umiejętność w badaniach nauk społecznych, ponieważ badanie całych populacji jest często niepraktyczne, zbyt kosztowne lub czasami niemożliwe.\nKluczowe pojęcia:\n\nPopulacja: Cała grupa, o której chcemy wyciągnąć wnioski.\nPróba: Podzbiór populacji, który faktycznie badamy.\nOperat losowania: Lista lub procedura używana do identyfikacji wszystkich członków populacji.\n\nPrzykład: Załóżmy, że chcemy zbadać satysfakcję z pracy wszystkich nauczycieli w Polsce (populacja). Zamiast ankietować setki tysięcy nauczycieli, możemy wybrać próbę 5000 nauczycieli z różnych województw, powiatów i poziomów nauczania.\nLosowość w próbkowaniu pomaga zapewnić, że próba jest reprezentatywna dla populacji, zmniejszając błędy systematyczne i umożliwiając dokładniejsze wnioskowanie. Dlatego metody próbkowania probabilistycznego, które omówimy dalej, są często preferowane w badaniach naukowych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#metody-próbkowania",
    "href": "rozdzial3.html#metody-próbkowania",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.3 Metody Próbkowania",
    "text": "6.3 Metody Próbkowania\n\n6.3.1 Próbkowanie Probabilistyczne\nMetody próbkowania probabilistycznego obejmują losowy wybór, dając każdemu członkowi populacji znaną, niezerową szansę na wybór.\n\nProsty Dobór Losowy: Każdy członek populacji ma równą szansę na wybór.\nPrzykład: Aby wybrać 100 studentów z uniwersytetu liczącego 10 000 studentów, można przypisać każdemu studentowi numer od 1 do 10 000, a następnie użyć generatora liczb losowych do wybrania 100 numerów.\nDobór Losowy Warstwowy: Populacja jest podzielona na podgrupy (warstwy) na podstawie wspólnych cech, a następnie próbki są losowo wybierane z każdej warstwy.\nPrzykład: W ogólnopolskim badaniu politycznym można podzielić populację na warstwy na podstawie regionów geograficznych (np. Polska Zachodnia, Centralna, Wschodnia) i losowo pobierać próbki z każdego regionu. Zapewnia to reprezentację ze wszystkich obszarów kraju.\nDobór Losowy Grupowy: Populacja jest podzielona na skupiska (zwykle geograficzne), niektóre skupiska są losowo wybierane, a wszyscy członkowie w tych skupiskach są badani.\nPrzykład: Aby zbadać nawyki uczenia się uczniów szkół średnich, można losowo wybrać 20 szkół z całego kraju, a następnie przeprowadzić ankietę wśród wszystkich uczniów w tych szkołach.\nDobór Systematyczny: Wybieranie co k-tego elementu z listy po losowym starcie.\nPrzykład: W ruchliwym centrum handlowym można ankietować co 20. osobę wchodzącą do centrum, zaczynając od losowo wybranej liczby między 1 a 20.\n\n\n\n6.3.2 Próbkowanie Nieprobabilistyczne\nPróbkowanie nieprobabilistyczne nie obejmuje losowego wyboru. Chociaż może wprowadzać błędy systematyczne, może być konieczne w niektórych sytuacjach, zwłaszcza w przypadku trudno dostępnych populacji lub gdy zasoby są ograniczone.\n\nDobór Wygodny: Wybieranie łatwo dostępnych podmiotów.\nPrzykład: Badacz studiujący wzorce snu studentów może przeprowadzić ankietę wśród studentów na własnych zajęciach lub na terenie kampusu.\nDobór Celowy: Wybieranie podmiotów na podstawie określonych cech.\nPrzykład: W badaniu doświadczeń prezesów w branży technologicznej badacz może celowo szukać i przeprowadzać wywiady z prezesami różnych firm technologicznych.\nDobór Metodą Kuli Śnieżnej: Uczestnicy rekrutują innych uczestników.\nPrzykład: W badaniu dostępu imigrantów bez dokumentów do opieki zdrowotnej, badacze mogą poprosić początkowych uczestników o polecenie innych potencjalnych uczestników z ich społeczności.\nDobór Kwotowy: Wybieranie uczestników w celu spełnienia określonych kwot dla pewnych cech.\nPrzykład: W badaniu rynku badacze mogą zapewnić, że przeprowadzają wywiady z określoną liczbą osób z różnych grup wiekowych, płci i poziomów dochodów, aby dopasować się do demografii rynku docelowego.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wnioskowanie-z-prób",
    "href": "rozdzial3.html#wnioskowanie-z-prób",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.4 Wnioskowanie z Prób",
    "text": "6.4 Wnioskowanie z Prób\nWnioskowanie statystyczne to proces wyciągania wniosków o populacji na podstawie próby. Pozwala to badaczom oszacować charakterystyki całej populacji (parametry) przy użyciu charakterystyk próby (statystyk).\n\n\n\n\n\n\nNote\n\n\n\nThe Soup Analogy: A Taste of Statistics\n\n\nWhen you taste a spoonful of soup and decide it isn’t salty enough, that’s exploratory/descriptive analysis.\nIf you generalize and conclude that your entire pot of soup needs salt, that’s an inference.\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population).\nIf the soup is not well stirred (heterogeneous population), it doesn’t matter how large a spoon you have (sample size), it will still not accurately represent the whole. If the soup is well stirred (homogeneous population), even a small spoon will suffice to test the soup.\n\n\n\nKluczowe pojęcia:\n\nEstymatory punktowe: Pojedyncza wartość używana do oszacowania parametru populacji.\nPrzykład: Średni dochód z próby 1000 pracowników może być użyty do oszacowania średniego dochodu wszystkich pracowników w kraju.\nPrzedziały ufności: Zakres wartości, który prawdopodobnie zawiera prawdziwy parametr populacji.\nPrzykład: Możemy powiedzieć: “Jesteśmy w 95% pewni, że prawdziwy średni dochód populacji mieści się między 4500 a 5500 złotych”.\nMargines błędu: Zakres wartości powyżej i poniżej statystyki próby w przedziale ufności.\nPrzykład: W sondażach politycznych można zobaczyć stwierdzenie: “Kandydat A jest preferowany przez 52% wyborców, z marginesem błędu ±3%”.\nTestowanie hipotez: Metoda podejmowania decyzji o parametrach populacji na podstawie danych z próby.\nPrzykład: Badacz może testować, czy istnieje istotna różnica w wynikach testów między uczniami, którzy uczą się przy muzyce, a tymi, którzy uczą się w ciszy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#błędy-próbkowania-i-błędy-niepróbkowe",
    "href": "rozdzial3.html#błędy-próbkowania-i-błędy-niepróbkowe",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.5 Błędy Próbkowania i Błędy Niepróbkowe",
    "text": "6.5 Błędy Próbkowania i Błędy Niepróbkowe\nZrozumienie potencjalnych błędów w badaniach jest kluczowe dla dokładnej interpretacji wyników.\nBłąd próbkowania: Różnica między statystyką próby a prawdziwym parametrem populacji, występująca z powodu przypadkowych wahań w wyborze członków próby.\nPrzykład: Jeśli oszacujemy średni wzrost wszystkich dorosłych mężczyzn w kraju na podstawie próby, nasze oszacowanie prawdopodobnie będzie się nieco różnić od prawdziwej średniej z powodu błędu próbkowania.\nBłędy niepróbkowe: Błędy nie wynikające z przypadku, które mogą wystąpić zarówno w badaniach próbkowych, jak i spisach.\n\nBłąd pokrycia: Gdy operat losowania nie reprezentuje dokładnie populacji.\nPrzykład: Badanie telefoniczne, które dzwoni tylko na telefony stacjonarne, pominęłoby osoby posiadające tylko telefony komórkowe, potencjalnie wypaczając wyniki.\nBłąd braku odpowiedzi: Gdy wybrani uczestnicy nie odpowiadają, potencjalnie wprowadzając błąd systematyczny.\nPrzykład: W badaniu satysfakcji z pracy, bardzo zadowoleni lub bardzo niezadowoleni pracownicy mogą być bardziej skłonni do odpowiedzi, wypaczając wyniki.\nBłąd pomiaru: Niedokładności w zebranych danych.\nPrzykład: Źle sformułowane pytanie ankietowe może być różnie interpretowane przez różnych respondentów, prowadząc do niespójnych danych.\nBłąd przetwarzania: Błędy popełnione podczas wprowadzania danych, kodowania lub analizy.\nPrzykład: Przypadkowe wprowadzenie “99” zamiast “9” dla odpowiedzi uczestnika mogłoby znacząco wypaczyć wyniki.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wielkość-próby-i-moc-statystyczna",
    "href": "rozdzial3.html#wielkość-próby-i-moc-statystyczna",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.6 Wielkość Próby i Moc Statystyczna",
    "text": "6.6 Wielkość Próby i Moc Statystyczna\nOkreślenie odpowiedniej wielkości próby wymaga zrównoważenia potrzeby precyzji z dostępnymi zasobami.\nRozważania dotyczące wielkości próby: - Większe próby generalnie zapewniają bardziej precyzyjne oszacowania, ale są bardziej kosztowne i czasochłonne do uzyskania. - Wymagana wielkość próby zależy od czynników takich jak pożądany poziom precyzji, zmienność w populacji i rodzaj planowanej analizy.\nPrzykład: Aby oszacować proporcję wyborców popierających konkretną politykę z marginesem błędu ±3% na poziomie ufności 95%, potrzebna byłaby próba około 1067 wyborców (zakładając maksymalną zmienność).\nMoc statystyczna: Prawdopodobieństwo, że badanie wykryje efekt, gdy taki efekt istnieje.\nCzynniki wpływające na moc: 1. Wielkość próby 2. Wielkość efektu (wielkość różnicy lub związku, który próbujemy wykryć) 3. Wybrany poziom istotności (zwykle 0,05)\nPrzykład: W badaniu porównującym dwie metody nauczania, większa wielkość próby zwiększyłaby prawdopodobieństwo wykrycia istotnej różnicy między metodami, jeśli taka różnica istnieje.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#próbkowanie-w-erze-cyfrowej",
    "href": "rozdzial3.html#próbkowanie-w-erze-cyfrowej",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.7 Próbkowanie w Erze Cyfrowej",
    "text": "6.7 Próbkowanie w Erze Cyfrowej\nPojawienie się big data i technologii cyfrowych zmieniło praktyki próbkowania w wielu dziedzinach.\nMożliwości i wyzwania Big Data: - Bezprecedensowe ilości dostępnych informacji - Potencjalny brak reprezentatywności - Problemy z jakością danych - Kwestie prywatności i etyki\nPrzykład: Dane z mediów społecznościowych mogą dostarczyć wglądu w opinię publiczną w czasie rzeczywistym, ale użytkownicy konkretnej platformy mogą nie być reprezentatywni dla ogólnej populacji.\nBadania internetowe: - Oferują nowe możliwości zbierania danych - Stają przed wyzwaniami takimi jak błąd pokrycia (nie każdy ma dostęp do internetu) i błąd samoselekcji\nPrzykład: Ankieta online na temat nawyków korzystania z internetu z natury wykluczałaby osoby bez dostępu do internetu, potencjalnie wypaczając wyniki.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#etyczne-aspekty-próbkowania",
    "href": "rozdzial3.html#etyczne-aspekty-próbkowania",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.8 Etyczne Aspekty Próbkowania",
    "text": "6.8 Etyczne Aspekty Próbkowania\nEtyczne praktyki próbkowania są kluczowe w badaniach nauk społecznych:\n\nŚwiadoma zgoda: Uczestnicy powinni rozumieć cel badania i zgodzić się na udział.\nPrzykład: Przed przeprowadzeniem wywiadów na temat wrażliwych tematów, takich jak zdrowie psychiczne, badacze muszą jasno wyjaśnić cele badania i potencjalne ryzyko uczestnikom.\nPrywatność i poufność: Badacze muszą chronić dane osobowe uczestników.\nPrzykład: W badaniu dotyczącym mobbingu w miejscu pracy, badacze mogą używać kodów numerycznych zamiast nazwisk, aby chronić tożsamość uczestników.\nReprezentatywność i inkluzywność: Próby powinny sprawiedliwie reprezentować zróżnicowane populacje, w tym grupy marginalizowane.\n\nPrzykład: Badanie dotyczące mieszkalnictwa miejskiego powinno dołożyć starań, aby uwzględnić uczestników z różnych środowisk społeczno-ekonomicznych, grup etnicznych i sytuacji mieszkaniowych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#studia-przypadków",
    "href": "rozdzial3.html#studia-przypadków",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.9 Studia Przypadków",
    "text": "6.9 Studia Przypadków\n[Ta sekcja zawierałaby szczegółowe przykłady zastosowania próbkowania w praktyce, takie jak:]\n\nJak organizacje badania opinii publicznej przewidują wyniki wyborów\nPrzykład: Wyobraźmy sobie, że chcemy przewidzieć wynik wyborów prezydenckich w Polsce. Ośrodek badania opinii publicznej mógłby zastosować następującą metodologię:\n\nZastosowanie próbkowania warstwowego, dzieląc populację na warstwy według województw.\nLosowy wybór respondentów z każdej warstwy, proporcjonalnie do liczby mieszkańców.\nPrzeprowadzenie wywiadów telefonicznych z wybranymi osobami, pytając o ich preferencje wyborcze.\nWażenie wyników, aby skorygować ewentualne nadreprezentacje lub niedoreprezentacje pewnych grup demograficznych.\nObliczenie marginesu błędu i przedziałów ufności dla otrzymanych wyników.\n\nJak firmy wykorzystują badania rynku do zrozumienia preferencji konsumentów\nPrzykład: Firma produkująca napoje chce wprowadzić nowy smak na rynek polski. Mogłaby przeprowadzić badanie w następujący sposób:\n\nWykorzystanie próbkowania kwotowego, aby zapewnić reprezentację różnych grup wiekowych i regionów.\nOrganizacja degustacji w centrach handlowych w różnych miastach.\nPrzeprowadzenie ankiet online wśród członków panelu konsumenckiego.\nAnaliza danych z uwzględnieniem różnic demograficznych i geograficznych.\nWykorzystanie wyników do dostosowania produktu i strategii marketingowej.\n\nJak badacze zdrowia publicznego badają występowanie chorób w populacjach\nPrzykład: Badanie częstości występowania cukrzycy typu 2 w Polsce:\n\nZastosowanie próbkowania dwustopniowego: najpierw losowy wybór gmin, potem losowy wybór mieszkańców w tych gminach.\nPrzeprowadzenie badań przesiewowych wśród wybranych osób (pomiary poziomu cukru we krwi, BMI, itp.).\nZbieranie dodatkowych danych o stylu życia i historii medycznej.\nAnaliza danych z uwzględnieniem czynników demograficznych i środowiskowych.\nOszacowanie częstości występowania cukrzycy w całej populacji na podstawie wyników próby.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#podsumowanie",
    "href": "rozdzial3.html#podsumowanie",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.10 Podsumowanie",
    "text": "6.10 Podsumowanie\nPróbkowanie pozostaje fundamentem badań w naukach społecznych, nawet w erze big data. Zrozumienie zasad próbkowania pomaga badaczom projektować badania, interpretować wyniki i wyciągać trafne wnioski o populacjach. Jak widzieliśmy, droga od próby do populacji wymaga starannego rozważenia metod próbkowania, potencjalnych błędów, kwestii etycznych i stale ewoluującego krajobrazu gromadzenia danych w erze cyfrowej.\nKluczowe punkty do zapamiętania:\n\nLosowość jest podstawą wielu metod próbkowania i pomaga zapewnić reprezentatywność próby.\nIstnieją różne metody próbkowania, zarówno probabilistyczne, jak i nieprobabilistyczne, każda z własnymi zaletami i ograniczeniami.\nWnioskowanie statystyczne pozwala nam wyciągać wnioski o populacji na podstawie danych z próby.\nBłędy próbkowania i niepróbkowe mogą wpływać na jakość naszych wniosków, dlatego ważne jest ich zrozumienie i minimalizowanie.\nWielkość próby i moc statystyczna są kluczowe dla zapewnienia wiarygodności wyników badań.\nEra cyfrowa przynosi nowe możliwości i wyzwania w zakresie próbkowania i gromadzenia danych.\nEtyczne aspekty próbkowania, w tym świadoma zgoda, prywatność i reprezentatywność, są nieodłączną częścią procesu badawczego.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#pytania-kontrolne-i-ćwiczenia",
    "href": "rozdzial3.html#pytania-kontrolne-i-ćwiczenia",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.11 Pytania Kontrolne i Ćwiczenia",
    "text": "6.11 Pytania Kontrolne i Ćwiczenia\n\nJakie są główne różnice między próbkowaniem probabilistycznym a nieprobabilistycznym? Podaj przykłady sytuacji, w których każde z nich mogłoby być odpowiednie.\nWyobraź sobie, że prowadzisz badanie dotyczące opinii studentów na temat nauczania zdalnego. Zaproponuj odpowiednią metodę próbkowania i uzasadnij swój wybór.\nJakie potencjalne błędy próbkowania i niepróbkowe mogłyby wystąpić w badaniu opisanym w pytaniu 2? Jak można by je zminimalizować?\nOblicz wymaganą wielkość próby dla badania opinii publicznej, zakładając poziom ufności 95%, margines błędu 3% i maksymalną zmienność (p = 0,5).\nOmów etyczne aspekty próbkowania w kontekście badania wrażliwych grup społecznych, takich jak osoby bezdomne lub nieletni.\nJak era big data wpływa na tradycyjne metody próbkowania? Omów zarówno możliwości, jak i wyzwania.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#dalsza-lektura",
    "href": "rozdzial3.html#dalsza-lektura",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.12 Dalsza Lektura",
    "text": "6.12 Dalsza Lektura\n\nBabbie, E. (2020). “Badania społeczne w praktyce”. Warszawa: Wydawnictwo Naukowe PWN.\nFrankfort-Nachmias, C., & Nachmias, D. (2001). “Metody badawcze w naukach społecznych”. Poznań: Zysk i S-ka.\nGruszczyński, L. A. (2003). “Kwestionariusze w socjologii: budowa narzędzi do badań surveyowych”. Katowice: Wydawnictwo Uniwersytetu Śląskiego.\nSzreder, M. (2010). “Metody i techniki sondażowych badań opinii”. Warszawa: Polskie Wydawnictwo Ekonomiczne.\nBrzeziński, J. (2019). “Metodologia badań psychologicznych”. Warszawa: Wydawnictwo Naukowe PWN.\n\nTe źródła zapewniają dogłębne omówienie tematów związanych z próbkowaniem i metodologią badań w naukach społecznych w kontekście polskim.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "chapter3b.html",
    "href": "chapter3b.html",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "",
    "text": "7.1 Defining Reliability and Validity\nReliability refers to the consistency of a measure. A reliable measurement or study produces similar results under consistent conditions.\nValidity refers to the accuracy of a measure. A valid measurement or study accurately represents what it claims to measure.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#the-four-combinations-of-reliability-and-validity",
    "href": "chapter3b.html#the-four-combinations-of-reliability-and-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.2 The Four Combinations of Reliability and Validity",
    "text": "7.2 The Four Combinations of Reliability and Validity\nThere are four possible combinations of reliability and validity:\n\nHigh Reliability, High Validity\nHigh Reliability, Low Validity\nLow Reliability, High Validity\nLow Reliability, Low Validity\n\nLet’s explore each of these combinations with examples and visualizations.\n\n7.2.1 1. High Reliability, High Validity\nThis is the ideal scenario in research. Measurements are both consistent and accurate.\nExample: A well-calibrated digital scale used to measure weight. It consistently gives the same reading for the same object and accurately represents the true weight.\n\n\n7.2.2 2. High Reliability, Low Validity\nIn this case, measurements are consistent but not accurate.\nExample: A miscalibrated scale that always measures 5 kg too heavy. It gives consistent results (high reliability) but doesn’t represent the true weight (low validity).\n\n\n7.2.3 3. Low Reliability, High Validity\nHere, measurements are accurate on average but inconsistent.\nExample: A scale that fluctuates around the true weight. Sometimes it’s a bit over, sometimes a bit under, but on average, it’s correct.\n\n\n7.2.4 4. Low Reliability, Low Validity\nThis is the worst-case scenario, where measurements are neither consistent nor accurate.\nExample: A broken scale that gives random readings unrelated to the true weight.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#visualizing-reliability-and-validity",
    "href": "chapter3b.html#visualizing-reliability-and-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.3 Visualizing Reliability and Validity",
    "text": "7.3 Visualizing Reliability and Validity\nTo better understand these concepts, let’s create visualizations using ggplot2 in R. We’ll simulate measurement data for each scenario and plot them.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generate data for each scenario\nn &lt;- 100\ntrue_value &lt;- 50\n\ndata &lt;- tibble(\n  high_rel_high_val = rnorm(n, mean = true_value, sd = 1),\n  high_rel_low_val = rnorm(n, mean = true_value + 5, sd = 1),\n  low_rel_high_val = rnorm(n, mean = true_value, sd = 5),\n  low_rel_low_val = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenario\", values_to = \"measurement\")\n\n# Create the scatterplot\nscatter_plot &lt;- ggplot(data, aes(x = id, y = measurement, color = scenario)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = true_value, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Scatterplots of Measurements\",\n       subtitle = \"Dashed line represents the true value\",\n       x = \"Measurement ID\",\n       y = \"Measured Value\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Create the histogram\nhist_plot &lt;- ggplot(data, aes(x = measurement, fill = scenario)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = true_value, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Histograms of Measurements\",\n       subtitle = \"Red dashed line represents the true value\",\n       x = \"Measured Value\",\n       y = \"Count\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Combine the plots\ncombined_plot &lt;- scatter_plot / hist_plot +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Reliability and Validity in Measurements\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Display the combined plot\ncombined_plot\n\n\n\n\n\n\n\n\n\n7.3.1 Interpreting the Visualizations\n\nHigh Reliability, High Validity: Points cluster tightly around the true value (dashed line).\nHigh Reliability, Low Validity: Points cluster tightly, but consistently above the true value.\nLow Reliability, High Validity: Points scatter widely but center around the true value.\nLow Reliability, Low Validity: Points scatter randomly with no clear pattern or relation to the true value.\n\nUnderstanding reliability and validity is crucial in data science and research. High reliability ensures consistent measurements, while high validity ensures accurate representations of what we intend to measure. By considering both aspects, researchers can design more robust studies and draw more meaningful conclusions from their data.\nWhen conducting your own research or analyzing others’ work, always consider: - How reliable are the measurements? - How valid is the approach for measuring the intended concept? - Do the methods used support both reliability and validity?\nBy keeping these questions in mind, you’ll be better equipped to produce and interpret high-quality research in data science.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#types-of-reliability",
    "href": "chapter3b.html#types-of-reliability",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.4 Types of Reliability",
    "text": "7.4 Types of Reliability\nReliability can be assessed in several ways, each focusing on a different aspect of consistency:\n\nTest-Retest Reliability: This measures the consistency of a test over time. It involves administering the same test to the same group of participants at different times and comparing the results.\nInter-Rater Reliability: This assesses the degree of agreement among different raters or observers. It’s crucial when subjective judgments are involved in data collection.\nInternal Consistency: This evaluates how well different items on a test or scale measure the same construct. Cronbach’s alpha is a common measure of internal consistency.\nParallel Forms Reliability: This involves creating two equivalent forms of a test and administering them to the same group. The correlation between the two sets of scores indicates reliability.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#types-of-validity",
    "href": "chapter3b.html#types-of-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.5 Types of Validity",
    "text": "7.5 Types of Validity\nValidity is a multifaceted concept, with several types that researchers need to consider:\n\nContent Validity: This ensures that a measure covers all aspects of the construct it aims to measure. It’s often assessed by expert judgment.\nConstruct Validity: This evaluates whether a test measures the intended theoretical construct. It includes:\n\nConvergent Validity: The degree to which the measure correlates with other measures of the same construct.\nDiscriminant Validity: The extent to which the measure does not correlate with measures of different constructs.\n\nCriterion Validity: This assesses how well a measure predicts an outcome. It includes:\n\nConcurrent Validity: How well the measure correlates with other measures of the same construct at the same time.\nPredictive Validity: How well the measure predicts future outcomes.\n\nFace Validity: This refers to whether a test appears to measure what it claims to measure. While not a scientific measure, it can be important for participant buy-in.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#internal-vs.-external-validity",
    "href": "chapter3b.html#internal-vs.-external-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.6 Internal vs. External Validity",
    "text": "7.6 Internal vs. External Validity\nThese concepts are crucial in experimental design and the generalizability of research findings:\n\n7.6.1 Internal Validity\nInternal validity refers to the extent to which a study establishes a causal relationship between the independent and dependent variables. It answers the question: “Did the experimental treatment actually cause the observed effects?”\nFactors that can threaten internal validity include: - History: External events occurring between pre-test and post-test - Maturation: Natural changes in participants over time - Testing effects: Changes due to taking a pre-test - Instrumentation: Changes in the measurement tool or observers - Selection bias: Non-random assignment to groups - Attrition: Loss of participants during the study\n\n\n7.6.2 External Validity\nExternal validity refers to the extent to which the results of a study can be generalized to other situations, populations, or settings. It addresses the question: “To what extent can the findings be applied beyond the specific context of the study?”\nFactors that can affect external validity include: - Population validity: How well the sample represents the larger population - Ecological validity: How well the study setting represents real-world conditions - Temporal validity: Whether the results hold true across time",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#consistency-in-research",
    "href": "chapter3b.html#consistency-in-research",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.7 Consistency in Research",
    "text": "7.7 Consistency in Research\nConsistency is closely related to reliability but extends beyond just measurement. In research, consistency refers to the overall coherence and stability of results across different contexts, methods, or studies.\nKey aspects of consistency in research include:\n\nReplicability: The ability to reproduce study results using the same methods and data.\nRobustness: The stability of findings across different analytical approaches or slight variations in methodology.\nConvergence: The alignment of results from different studies or methods investigating the same phenomenon.\nLongitudinal Consistency: The stability of findings over time, especially important in longitudinal studies.\n\nEnsuring consistency in research involves: - Using standardized procedures and measures - Thoroughly documenting methods and analytical decisions - Conducting replication studies - Meta-analyses to synthesize findings across multiple studies",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#balancing-reliability-validity-and-consistency",
    "href": "chapter3b.html#balancing-reliability-validity-and-consistency",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.8 Balancing Reliability, Validity, and Consistency",
    "text": "7.8 Balancing Reliability, Validity, and Consistency\nWhile reliability, validity, and consistency are all crucial for high-quality research, they sometimes involve trade-offs:\n\nA highly reliable measure might lack validity if it consistently measures the wrong thing.\nStriving for perfect internal validity (e.g., in tightly controlled lab experiments) might reduce external validity.\nEnsuring high consistency across diverse contexts might require sacrificing some degree of precision or depth in specific situations.\n\nResearchers must carefully balance these aspects based on their research questions and the nature of their study. A comprehensive understanding of reliability, validity, and consistency helps in designing robust studies, interpreting results accurately, and contributing meaningfully to the body of scientific knowledge.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#bias-variance-tradeoff",
    "href": "chapter3b.html#bias-variance-tradeoff",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.9 Bias-Variance Tradeoff",
    "text": "7.9 Bias-Variance Tradeoff\nThe concepts of reliability and validity are closely related to the statistical notion of the bias-variance tradeoff. This tradeoff is fundamental in machine learning and statistical modeling.\n\nBias refers to the error introduced by approximating a real-world problem with a simplified model. High bias can lead to underfitting.\nVariance refers to the error introduced by the model’s sensitivity to small fluctuations in the training set. High variance can lead to overfitting.\n\nLet’s visualize this concept with a simplified plot:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_true &lt;- sin(x)\ny_low_bias_high_var &lt;- y_true + rnorm(100, 0, 0.3)\ny_high_bias_low_var &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_true, y_low_bias_high_var, y_high_bias_low_var),\n                 type = rep(c(\"True Function\", \"Low Bias, High Variance\", \"High Bias, Low Variance\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = type)) +\n  geom_line() +\n  geom_point(data = subset(df, type != \"True Function\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Bias-Variance Tradeoff\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Model Type\") +\n  theme_minimal()\n\n\n\n\nVisualization of Bias-Variance Tradeoff\n\n\n\n\nIn this plot: - The black line represents the true underlying function. - The blue points represent a model with low bias but high variance. It follows the true function closely on average but has a lot of noise. - The red line represents a model with high bias but low variance. It consistently underestimates the true function but has less noise.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#accuracy-and-precision",
    "href": "chapter3b.html#accuracy-and-precision",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.10 Accuracy and Precision",
    "text": "7.10 Accuracy and Precision\nThe concepts of accuracy and precision are closely related to validity and reliability:\n\nAccuracy refers to how close a measurement is to the true value (similar to validity).\nPrecision refers to how consistent or reproducible the measurements are (similar to reliability).\n\nWe can visualize these concepts using a simplified target analogy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"High Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Low Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"High Accuracy\\nLow Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Low Accuracy\\nLow Precision\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Accuracy vs Precision\")\n\n\n\n\nVisualization of Accuracy vs Precision\n\n\n\n\nIn this visualization: - High accuracy means the points are close to the center (bullseye). - High precision means the points are tightly clustered. - Each panel represents a different combination of accuracy and precision.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html",
    "href": "rozdzial3b.html",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "",
    "text": "8.1 Definiowanie Rzetelności i Trafności\nRzetelność odnosi się do spójności pomiaru. Rzetelny pomiar lub badanie daje podobne wyniki w spójnych warunkach.\nTrafność odnosi się do dokładności pomiaru. Trafny pomiar lub badanie dokładnie reprezentuje to, co twierdzi, że mierzy.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#cztery-kombinacje-rzetelności-i-trafności",
    "href": "rozdzial3b.html#cztery-kombinacje-rzetelności-i-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.2 Cztery Kombinacje Rzetelności i Trafności",
    "text": "8.2 Cztery Kombinacje Rzetelności i Trafności\nIstnieją cztery możliwe kombinacje rzetelności i trafności:\n\nWysoka Rzetelność, Wysoka Trafność\nWysoka Rzetelność, Niska Trafność\nNiska Rzetelność, Wysoka Trafność\nNiska Rzetelność, Niska Trafność\n\nPrzyjrzyjmy się każdej z tych kombinacji z przykładami i wizualizacjami.\n\n8.2.1 1. Wysoka Rzetelność, Wysoka Trafność\nTo idealny scenariusz w badaniach. Pomiary są zarówno spójne, jak i dokładne.\nPrzykład: Dobrze skalibrowana waga cyfrowa używana do pomiaru wagi. Konsekwentnie daje ten sam odczyt dla tego samego obiektu i dokładnie reprezentuje prawdziwą wagę.\n\n\n8.2.2 2. Wysoka Rzetelność, Niska Trafność\nW tym przypadku pomiary są spójne, ale niedokładne.\nPrzykład: Źle skalibrowana waga, która zawsze mierzy 5 kg za ciężko. Daje spójne wyniki (wysoka rzetelność), ale nie reprezentuje prawdziwej wagi (niska trafność).\n\n\n8.2.3 3. Niska Rzetelność, Wysoka Trafność\nTutaj pomiary są dokładne średnio, ale niespójne.\nPrzykład: Waga, która waha się wokół prawdziwej wagi. Czasami pokazuje trochę więcej, czasami trochę mniej, ale średnio jest poprawna.\n\n\n8.2.4 4. Niska Rzetelność, Niska Trafność\nTo najgorszy scenariusz, gdzie pomiary nie są ani spójne, ani dokładne.\nPrzykład: Zepsuta waga, która daje losowe odczyty niezwiązane z prawdziwą wagą.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#wizualizacja-rzetelności-i-trafności",
    "href": "rozdzial3b.html#wizualizacja-rzetelności-i-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.3 Wizualizacja Rzetelności i Trafności",
    "text": "8.3 Wizualizacja Rzetelności i Trafności\nAby lepiej zrozumieć te pojęcia, stwórzmy wizualizacje przy użyciu ggplot2 w R. Zasymulujemy dane pomiarowe dla każdego scenariusza i narysujemy je.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generowanie danych dla każdego scenariusza\nn &lt;- 100\nprawdziwa_wartosc &lt;- 50\n\ndane &lt;- tibble(\n  wysoka_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 1),\n  wysoka_rz_niska_tr = rnorm(n, mean = prawdziwa_wartosc + 5, sd = 1),\n  niska_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 5),\n  niska_rz_niska_tr = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenariusz\", values_to = \"pomiar\")\n\n# Tworzenie wykresu punktowego\nwykres_punktowy &lt;- ggplot(dane, aes(x = id, y = pomiar, color = scenariusz)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = prawdziwa_wartosc, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelność, Wysoka Trafność\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelność, Niska Trafność\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelność, Wysoka Trafność\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelność, Niska Trafność\"\n             ))) +\n  labs(title = \"Wykresy punktowe pomiarów\",\n       subtitle = \"Przerywana linia reprezentuje prawdziwą wartość\",\n       x = \"ID pomiaru\",\n       y = \"Zmierzona wartość\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Tworzenie histogramu\nwykres_hist &lt;- ggplot(dane, aes(x = pomiar, fill = scenariusz)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = prawdziwa_wartosc, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelność, Wysoka Trafność\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelność, Niska Trafność\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelność, Wysoka Trafność\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelność, Niska Trafność\"\n             ))) +\n  labs(title = \"Histogramy pomiarów\",\n       subtitle = \"Czerwona przerywana linia reprezentuje prawdziwą wartość\",\n       x = \"Zmierzona wartość\",\n       y = \"Liczba\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Łączenie wykresów\nwykres_polaczony &lt;- wykres_punktowy / wykres_hist +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Rzetelność i Trafność w Pomiarach\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Wyświetlanie połączonego wykresu\nwykres_polaczony\n\n\n\n\n\n\n\n\n\n8.3.1 Interpretacja Wizualizacji\n\nWysoka Rzetelność, Wysoka Trafność: Punkty grupują się ciasno wokół prawdziwej wartości (przerywana linia).\nWysoka Rzetelność, Niska Trafność: Punkty grupują się ciasno, ale konsekwentnie powyżej prawdziwej wartości.\nNiska Rzetelność, Wysoka Trafność: Punkty rozpraszają się szeroko, ale centrują się wokół prawdziwej wartości.\nNiska Rzetelność, Niska Trafność: Punkty rozpraszają się losowo bez wyraźnego wzoru lub relacji do prawdziwej wartości.\n\nZrozumienie rzetelności i trafności jest kluczowe w naukach o danych i badaniach. Wysoka rzetelność zapewnia spójne pomiary, podczas gdy wysoka trafność zapewnia dokładne reprezentacje tego, co zamierzamy zmierzyć. Biorąc pod uwagę oba aspekty, badacze mogą projektować bardziej solidne badania i wyciągać bardziej znaczące wnioski ze swoich danych.\nProwadząc własne badania lub analizując pracę innych, zawsze należy rozważyć: - Jak rzetelne są pomiary? - Jak trafne jest podejście do pomiaru zamierzonego pojęcia? - Czy stosowane metody wspierają zarówno rzetelność, jak i trafność?\nMając na uwadze te pytania, będziesz lepiej przygotowany do prowadzenia i interpretowania wysokiej jakości badań w naukach o danych.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#rodzaje-rzetelności",
    "href": "rozdzial3b.html#rodzaje-rzetelności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.4 Rodzaje Rzetelności",
    "text": "8.4 Rodzaje Rzetelności\nRzetelność można oceniać na kilka sposobów, każdy skupiający się na innym aspekcie spójności:\n\nRzetelność test-retest: Mierzy spójność testu w czasie. Polega na przeprowadzeniu tego samego testu na tej samej grupie uczestników w różnych momentach i porównaniu wyników.\nRzetelność między oceniającymi: Ocenia stopień zgodności między różnymi oceniającymi lub obserwatorami. Jest kluczowa, gdy w zbieraniu danych biorą udział subiektywne osądy.\nSpójność wewnętrzna: Ocenia, jak dobrze różne elementy testu lub skali mierzą ten sam konstrukt. Alfa Cronbacha jest powszechną miarą spójności wewnętrznej.\nRzetelność form równoległych: Polega na stworzeniu dwóch równoważnych form testu i przeprowadzeniu ich na tej samej grupie. Korelacja między dwoma zestawami wyników wskazuje na rzetelność.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#rodzaje-trafności",
    "href": "rozdzial3b.html#rodzaje-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.5 Rodzaje Trafności",
    "text": "8.5 Rodzaje Trafności\nTrafność jest pojęciem wieloaspektowym, z kilkoma rodzajami, które badacze muszą wziąć pod uwagę:\n\nTrafność treściowa: Zapewnia, że pomiar obejmuje wszystkie aspekty konstruktu, który ma mierzyć. Często jest oceniana przez osąd ekspertów.\nTrafność konstrukcyjna: Ocenia, czy test mierzy zamierzony konstrukt teoretyczny. Obejmuje:\n\nTrafność zbieżną: Stopień, w jakim pomiar koreluje z innymi pomiarami tego samego konstruktu.\nTrafność różnicową: Zakres, w jakim pomiar nie koreluje z pomiarami różnych konstruktów.\n\nTrafność kryterialną: Ocenia, jak dobrze pomiar przewiduje wynik. Obejmuje:\n\nTrafność współbieżną: Jak dobrze pomiar koreluje z innymi pomiarami tego samego konstruktu w tym samym czasie.\nTrafność predykcyjną: Jak dobrze pomiar przewiduje przyszłe wyniki.\n\nTrafność fasadowa: Odnosi się do tego, czy test wydaje się mierzyć to, co twierdzi, że mierzy. Choć nie jest to naukowa miara, może być ważna dla zaangażowania uczestników.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#trafność-wewnętrzna-vs-zewnętrzna",
    "href": "rozdzial3b.html#trafność-wewnętrzna-vs-zewnętrzna",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.6 Trafność Wewnętrzna vs Zewnętrzna",
    "text": "8.6 Trafność Wewnętrzna vs Zewnętrzna\nTe pojęcia są kluczowe w projektowaniu eksperymentów i możliwości uogólniania wyników badań:\n\n8.6.1 Trafność Wewnętrzna\nTrafność wewnętrzna odnosi się do zakresu, w jakim badanie ustanawia związek przyczynowy między zmiennymi niezależnymi a zależnymi. Odpowiada na pytanie: “Czy eksperymentalne traktowanie rzeczywiście spowodowało zaobserwowane efekty?”\nCzynniki, które mogą zagrażać trafności wewnętrznej, obejmują: - Historia: Zewnętrzne wydarzenia występujące między pre-testem a post-testem - Dojrzewanie: Naturalne zmiany u uczestników w czasie - Efekty testowania: Zmiany wynikające z przeprowadzenia pre-testu - Instrumentacja: Zmiany w narzędziu pomiarowym lub obserwatorach - Błąd selekcji: Nielosowy przydział do grup - Utrata: Utrata uczestników podczas badania\n\n\n8.6.2 Trafność Zewnętrzna\nTrafność zewnętrzna odnosi się do zakresu, w jakim wyniki badania mogą być uogólnione na inne sytuacje, populacje lub ustawienia. Odpowiada na pytanie: “W jakim stopniu wyniki mogą być zastosowane poza konkretnym kontekstem badania?”\nCzynniki, które mogą wpływać na trafność zewnętrzną, obejmują: - Trafność populacyjna: Jak dobrze próba reprezentuje szerszą populację - Trafność ekologiczna: Jak dobrze ustawienie badania reprezentuje warunki świata rzeczywistego - Trafność czasowa: Czy wyniki pozostają prawdziwe w czasie",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#spójność-w-badaniach",
    "href": "rozdzial3b.html#spójność-w-badaniach",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.7 Spójność w Badaniach",
    "text": "8.7 Spójność w Badaniach\nSpójność jest ściśle związana z rzetelnością, ale wykracza poza sam pomiar. W badaniach spójność odnosi się do ogólnej koherencji i stabilności wyników w różnych kontekstach, metodach lub badaniach.\nKluczowe aspekty spójności w badaniach obejmują:\n\nReplikowalność: Zdolność do odtworzenia wyników badania przy użyciu tych samych metod i danych.\nOdporność: Stabilność wyników w różnych podejściach analitycznych lub niewielkich zmianach w metodologii.\nKonwergencja: Zbieżność wyników z różnych badań lub metod badających to samo zjawisko.\nSpójność długoterminowa: Stabilność wyników w czasie, szczególnie ważna w badaniach długoterminowych.\n\n[Continuation of the previous content…]\nZapewnienie spójności w badaniach obejmuje: - Stosowanie standaryzowanych procedur i miar - Dokładne dokumentowanie metod i decyzji analitycznych - Przeprowadzanie badań replikacyjnych - Meta-analizy w celu syntezy wyników z wielu badań",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#równoważenie-rzetelności-trafności-i-spójności",
    "href": "rozdzial3b.html#równoważenie-rzetelności-trafności-i-spójności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.8 Równoważenie Rzetelności, Trafności i Spójności",
    "text": "8.8 Równoważenie Rzetelności, Trafności i Spójności\nChociaż rzetelność, trafność i spójność są kluczowe dla wysokiej jakości badań, czasami wiążą się z kompromisami:\n\nWysoce rzetelna miara może nie mieć trafności, jeśli konsekwentnie mierzy niewłaściwą rzecz.\nDążenie do idealnej trafności wewnętrznej (np. w ściśle kontrolowanych eksperymentach laboratoryjnych) może zmniejszyć trafność zewnętrzną.\nZapewnienie wysokiej spójności w różnych kontekstach może wymagać poświęcenia pewnego stopnia precyzji lub głębi w konkretnych sytuacjach.\n\nBadacze muszą starannie równoważyć te aspekty w oparciu o swoje pytania badawcze i charakter badania. Kompleksowe zrozumienie rzetelności, trafności i spójności pomaga w projektowaniu solidnych badań, dokładnej interpretacji wyników i znaczącym wkładzie do korpusu wiedzy naukowej.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#kompromis-między-obciążeniem-a-wariancją",
    "href": "rozdzial3b.html#kompromis-między-obciążeniem-a-wariancją",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.9 Kompromis między Obciążeniem a Wariancją",
    "text": "8.9 Kompromis między Obciążeniem a Wariancją\nPojęcia rzetelności i trafności są ściśle związane ze statystycznym pojęciem kompromisu między obciążeniem a wariancją. Ten kompromis jest fundamentalny w uczeniu maszynowym i modelowaniu statystycznym.\n\nObciążenie odnosi się do błędu wprowadzonego przez przybliżenie problemu ze świata rzeczywistego uproszczonym modelem. Wysokie obciążenie może prowadzić do niedopasowania.\nWariancja odnosi się do błędu wprowadzonego przez wrażliwość modelu na małe fluktuacje w zbiorze treningowym. Wysoka wariancja może prowadzić do przeuczenia.\n\nZobrazujmy to pojęcie za pomocą uproszczonego wykresu:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_prawdziwa &lt;- sin(x)\ny_niskie_obciazenie_wysoka_wariancja &lt;- y_prawdziwa + rnorm(100, 0, 0.3)\ny_wysokie_obciazenie_niska_wariancja &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_prawdziwa, y_niskie_obciazenie_wysoka_wariancja, y_wysokie_obciazenie_niska_wariancja),\n                 typ = rep(c(\"Prawdziwa Funkcja\", \"Niskie Obciążenie, Wysoka Wariancja\", \"Wysokie Obciążenie, Niska Wariancja\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = typ)) +\n  geom_line() +\n  geom_point(data = subset(df, typ != \"Prawdziwa Funkcja\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Kompromis między Obciążeniem a Wariancją\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Typ Modelu\") +\n  theme_minimal()\n\n\n\n\nWizualizacja kompromisu między obciążeniem a wariancją\n\n\n\n\nNa tym wykresie: - Czarna linia reprezentuje prawdziwą funkcję bazową. - Niebieskie punkty reprezentują model z niskim obciążeniem, ale wysoką wariancją. Średnio podąża blisko prawdziwej funkcji, ale ma dużo szumu. - Czerwona linia reprezentuje model z wysokim obciążeniem, ale niską wariancją. Konsekwentnie niedoszacowuje prawdziwej funkcji, ale ma mniej szumu.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#dokładność-i-precyzja",
    "href": "rozdzial3b.html#dokładność-i-precyzja",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.10 Dokładność i Precyzja",
    "text": "8.10 Dokładność i Precyzja\nPojęcia dokładności i precyzji są ściśle związane z trafnością i rzetelnością:\n\nDokładność odnosi się do tego, jak blisko pomiar jest prawdziwej wartości (podobnie do trafności).\nPrecyzja odnosi się do tego, jak spójne lub powtarzalne są pomiary (podobnie do rzetelności).\n\nMożemy zobrazować te pojęcia za pomocą uproszczonej analogii do tarczy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"Wysoka Dokładność\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Niska Dokładność\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"Wysoka Dokładność\\nNiska Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Niska Dokładność\\nNiska Precyzja\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Dokładność vs Precyzja\")\n\n\n\n\nWizualizacja Dokładności vs Precyzji\n\n\n\n\nW tej wizualizacji: - Wysoka dokładność oznacza, że punkty są blisko środka (dziesiątki). - Wysoka precyzja oznacza, że punkty są ściśle zgrupowane. - Każdy panel reprezentuje inną kombinację dokładności i precyzji.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "",
    "text": "9.1 Introduction\nResearch designs are fundamental to the scientific process, providing structured approaches to investigate hypotheses and answer research questions. This chapter explores two main categories of research designs: experimental and non-experimental, with a focus on the Neyman-Rubin potential outcome framework. We’ll delve into various design types, their characteristics, and provide practical examples using R for data analysis and visualization.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#experimental-designs",
    "href": "chapter4.html#experimental-designs",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.2 Experimental Designs",
    "text": "9.2 Experimental Designs\nExperimental designs are characterized by the researcher’s control over the independent variable(s) and random assignment of subjects to different conditions. These designs are considered the gold standard for establishing causal relationships.\n\n9.2.1 Randomized Controlled Trials (RCTs)\nRCTs are the most rigorous form of experimental design. They involve:\n\nRandom assignment of subjects to treatment and control groups\nManipulation of the independent variable\nMeasurement of the dependent variable\n\nLet’s visualize a simple RCT design:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Create sample data\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  group = factor(rep(c(\"Control\", \"Treatment\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Simulate treatment effect\ndata$post_test &lt;- ifelse(data$group == \"Treatment\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Reshape data for plotting\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"time\", values_to = \"score\")\n\n# Create plot\nggplot(data_long, aes(x = time, y = score, color = group, group = interaction(id, group))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = group), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Pre-test and Post-test Scores in RCT\",\n       x = \"Time\", y = \"Score\", color = \"Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nRandomized Controlled Trial Design\n\n\n\n\nThis plot shows individual trajectories and group means for pre-test and post-test scores in a hypothetical RCT. The treatment group shows a clear increase in scores compared to the control group.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "href": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.3 A/B Testing: An Example and Comparison with RCTs",
    "text": "9.3 A/B Testing: An Example and Comparison with RCTs\nA/B testing is a widely used experimental method in digital marketing, user experience design, and product development. This chapter will present an example of A/B testing, explain its methodology, and discuss how it differs from Randomized Controlled Trials (RCTs).\n\n9.3.1 Example: Website Landing Page Conversion Rate\nLet’s consider an example where an e-commerce company wants to improve the conversion rate of their landing page. They decide to test two different layouts: the current layout (A) and a new layout (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Simulate data\nn_visitors &lt;- 10000\ndata &lt;- data.frame(\n  Version = sample(c(\"A\", \"B\"), n_visitors, replace = TRUE),\n  Converted = rbinom(n_visitors, 1, ifelse(sample(c(\"A\", \"B\"), n_visitors, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Calculate conversion rates\nconversion_rates &lt;- data %&gt;%\n  group_by(Version) %&gt;%\n  summarise(\n    Visitors = n(),\n    Conversions = sum(Converted),\n    ConversionRate = mean(Converted)\n  )\n\n# Visualize results\nggplot(conversion_rates, aes(x = Version, y = ConversionRate, fill = Version)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", ConversionRate * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"A/B Test: Landing Page Conversion Rates\",\n       x = \"Page Version\", y = \"Conversion Rate\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure 9.1: A/B Test Results: Landing Page Conversion Rates\n\n\n\n\n\nIn this example, we simulated data for 10,000 visitors randomly assigned to either version A or B of the landing page. The results show that version B has a slightly higher conversion rate (11.44%) compared to version A (10.94%).\n\n\n9.3.2 A/B Testing Methodology\nA/B testing typically follows these steps:\n\nIdentify the element to be tested (e.g., landing page layout).\nCreate two versions: the control (A) and the variant (B).\nRandomly assign visitors to either version.\nCollect data on the metric of interest (e.g., conversion rate).\nAnalyze the results using statistical methods.\nMake a decision based on the results.\n\n\n\n9.3.3 Differences between A/B Testing and RCTs\nWhile A/B testing and Randomized Controlled Trials (RCTs) share some similarities, they have several key differences:\n\nScope and Context:\n\nA/B Testing: Typically used in digital environments for quick, iterative improvements.\nRCTs: Used in various fields, including medicine, psychology, and social sciences, often for more complex interventions.\n\nDuration:\n\nA/B Testing: Usually shorter, often running for days or weeks.\nRCTs: Can last months or years, especially in medical research.\n\nSample Size:\n\nA/B Testing: Can involve very large sample sizes due to ease of implementation in digital platforms.\nRCTs: Sample sizes are often smaller due to practical and cost constraints.\n\nBlinding:\n\nA/B Testing: Participants are usually unaware they’re part of a test.\nRCTs: May involve single, double, or triple blinding to reduce bias.\n\nEthical Considerations:\n\nA/B Testing: Generally involves low-risk changes with minimal ethical concerns.\nRCTs: Often require extensive ethical review, especially in medical contexts.\n\nOutcome Measures:\n\nA/B Testing: Typically focuses on a single, easily measurable outcome (e.g., click-through rate).\nRCTs: Often measure multiple outcomes, including potential side effects or long-term impacts.\n\nGeneralizability:\n\nA/B Testing: Results are often specific to the platform or context tested.\nRCTs: Aim for broader generalizability, though this can vary.\n\nAnalysis Complexity:\n\nA/B Testing: Often uses simpler statistical analyses.\nRCTs: May involve more complex statistical methods to account for various factors.\n\n\nA/B testing is a powerful tool for making data-driven decisions in digital environments. While it shares the fundamental principle of randomization with RCTs, it is typically simpler, faster, and more focused on specific, measurable outcomes in digital contexts. Understanding these differences helps researchers and practitioners choose the most appropriate method for their specific needs and constraints.\n\n\n9.3.4 Example 1: Effect of Sleep Duration on Cognitive Performance\nResearch Question: Does increasing sleep duration improve cognitive performance in college students?\n\n# Generating sample data\nset.seed(456)\nn &lt;- 100\npre_experimental &lt;- rnorm(n, mean = 70, sd = 10)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = 8, sd = 5)\npre_control &lt;- rnorm(n, mean = 70, sd = 10)\npost_control &lt;- pre_control + rnorm(n, mean = 1, sd = 5)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Experimental\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  Score = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = Score, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Effect of Increased Sleep Duration on Cognitive Performance\") +\n  xlab(\"Time\") +\n  ylab(\"Cognitive Performance Score\")\n\n\n\n\n\n\n\nFigure 9.2: Effect of Sleep Duration on Cognitive Performance\n\n\n\n\n\n\n9.3.4.1 Interpretation\nThis plot demonstrates the effect of increased sleep duration on cognitive performance. The experimental group, which increased their sleep duration, shows a more substantial improvement in cognitive performance compared to the control group. This suggests that increasing sleep duration may positively impact cognitive abilities in college students.\n\n\n\n9.3.5 Example 2: Impact of Mindfulness Training on Stress Levels\nResearch Question: Can a short-term mindfulness training program reduce stress levels in healthcare workers?\n\n# Generating sample data\nset.seed(789)\nn &lt;- 120\npre_experimental &lt;- rnorm(n, mean = 60, sd = 15)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = -12, sd = 8)\npre_control &lt;- rnorm(n, mean = 60, sd = 15)\npost_control &lt;- pre_control + rnorm(n, mean = -2, sd = 6)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Mindfulness\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  StressScore = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = StressScore, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Impact of Mindfulness Training on Stress Levels\") +\n  xlab(\"Time\") +\n  ylab(\"Stress Score\")\n\n\n\n\n\n\n\nFigure 9.3: Impact of Mindfulness Training on Stress Levels\n\n\n\n\n\n\n9.3.5.1 Interpretation\nThis visualization illustrates the impact of a mindfulness training program on stress levels in healthcare workers. The mindfulness group shows a more significant decrease in stress scores compared to the control group. This suggests that the mindfulness training program may be effective in reducing stress levels among healthcare workers.\nWhen interpreting such results, it’s important to consider:\n\nThe magnitude of the change in each group\nThe difference in change between the experimental and control groups\nThe variability within each group\nAny potential confounding factors not accounted for in the experimental design\n\nThese examples provide a template for visualizing and interpreting similar experimental designs across different research contexts.\n\n\n\n9.3.6 Factorial Designs\nFactorial designs allow researchers to study the effects of multiple independent variables simultaneously. They are efficient and can reveal interaction effects between variables.\nExample of a 2x2 factorial design:\n\n# Create sample data for 2x2 factorial design\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  factor_a = rep(rep(c(\"Low\", \"High\"), each = n_per_group), 2),\n  factor_b = rep(c(\"Control\", \"Treatment\"), each = n_per_group * 2),\n  outcome = NA\n)\n\n# Generate outcomes\nfactorial_data$outcome &lt;- ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Control\",\n                                 rnorm(n_per_group, 40, 5),\n                                 ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Treatment\",\n                                        rnorm(n_per_group, 45, 5),\n                                        ifelse(factorial_data$factor_a == \"High\" & factorial_data$factor_b == \"Control\",\n                                               rnorm(n_per_group, 50, 5),\n                                               rnorm(n_per_group, 60, 5))))\n\n# Create plot\nggplot(factorial_data, aes(x = factor_b, y = outcome, fill = factor_a)) +\n  geom_boxplot() +\n  facet_wrap(~factor_a, scales = \"free_x\") +\n  labs(title = \"2x2 Factorial Design\",\n       x = \"Factor B\", y = \"Outcome\", fill = \"Factor A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n2x2 Factorial Design\n\n\n\n\nThis plot illustrates a 2x2 factorial design, showing the effects of two factors (A and B) on the outcome variable. We can observe main effects for both factors and a potential interaction effect.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#non-experimental-designs",
    "href": "chapter4.html#non-experimental-designs",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.4 Non-Experimental Designs",
    "text": "9.4 Non-Experimental Designs\nNon-experimental designs are used when randomization or manipulation of variables is not possible or ethical. They include observational/descriptive studies and quasi-experimental designs.\n\n9.4.1 Observational Studies\nObservational studies involve collecting data without manipulating variables. They are useful for exploring relationships and generating hypotheses.\nExample: Correlation study\n\nset.seed(789)\nn &lt;- 100\nstudy_time &lt;- runif(n, 0, 10)\nexam_score &lt;- 50 + 5 * study_time + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(study_time, exam_score)\n\nggplot(correlation_data, aes(x = study_time, y = exam_score)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Correlation between Study Time and Exam Score\",\n       x = \"Study Time (hours)\", y = \"Exam Score\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCorrelation between Study Time and Exam Score\n\n\n\n\nThis scatter plot shows the relationship between study time and exam scores, illustrating a positive correlation typical in observational studies.\n\n\n9.4.2 Quasi-Experimental Designs\nQuasi-experimental designs lack random assignment but attempt to establish causal relationships. Common types include:\n\nDifference-in-Differences (DiD)\nRegression Discontinuity Design (RDD)\n\n\n9.4.2.1 Difference-in-Differences (DiD)\nDiD is used to estimate treatment effects by comparing the average change over time in the outcome variable for the treatment group to the average change over time for the control group.\nLet’s simulate a DiD analysis using the plm package:\n\nlibrary(plm)\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\nDifference-in-Differences Analysis\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nThe plot shows the average outcomes for treatment and control groups over time. The vertical dashed line indicates the intervention point. The DiD estimate is the difference between the two groups’ changes from pre- to post-intervention periods.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt’s important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\n9.4.2.2 Regression Discontinuity Design (RDD)\nRDD is used when treatment assignment is determined by a cutoff value on a continuous variable. It compares observations just above and below the cutoff to estimate the treatment effect.\nLet’s implement an RDD analysis using the rdrobust package:\n\nlibrary(rdrobust)\n\n# Generate synthetic RDD data\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# RDD analysis\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     4.092     0.231    17.723     0.000     [3.640 , 4.545]     \n        Robust         -         -    15.013     0.000     [3.600 , 4.680]     \n=============================================================================\n\n# Visualize RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regression Discontinuity Design\",\n       x = \"Running Variable\", y = \"Outcome\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nRegression Discontinuity Design Analysis\n\n\n\n\nThe plot shows the discontinuity at the cutoff point (x = 0), with separate regression lines fitted on either side. The treatment effect is estimated by the gap between these lines at the cutoff.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "href": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.5 The Neyman-Rubin Potential Outcome Framework",
    "text": "9.5 The Neyman-Rubin Potential Outcome Framework\nThe Neyman-Rubin potential outcome framework provides a formal approach to causal inference. It introduces the concept of potential outcomes: for each unit, we consider the outcome under treatment and the outcome under control, even though we can only observe one in reality.\nKey concepts:\n\nPotential Outcomes: \\(Y_i(1)\\) and \\(Y_i(0)\\) for treatment and control, respectively.\nObserved Outcome: \\(Y_i = Y_i(1)T_i + Y_i(0)(1-T_i)\\), where \\(T_i\\) is the treatment indicator.\nIndividual Treatment Effect: \\(\\tau_i = Y_i(1) - Y_i(0)\\)\nAverage Treatment Effect (ATE): \\(E[\\tau_i] = E[Y_i(1) - Y_i(0)]\\)\n\nThe framework emphasizes the “fundamental problem of causal inference”: we can never observe both potential outcomes for a single unit simultaneously.\n\n9.5.1 Example: Estimating ATE in an RCT\nIn an RCT, random assignment ensures that treatment is independent of potential outcomes, allowing unbiased estimation of the ATE:\n\\[\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\\]\nWhere \\(n_1\\) and \\(n_0\\) are the numbers of treated and control units, respectively.\n\n# Using the RCT data from earlier\nate_estimate &lt;- mean(data$post_test[data$group == \"Treatment\"]) - \n                mean(data$post_test[data$group == \"Control\"])\n\nWarning in mean.default(data$post_test[data$group == \"Treatment\"]): argument is\nnot numeric or logical: returning NA\n\n\nWarning in mean.default(data$post_test[data$group == \"Control\"]): argument is\nnot numeric or logical: returning NA\n\ncat(\"Estimated Average Treatment Effect:\", round(ate_estimate, 2))\n\nEstimated Average Treatment Effect: NA\n\n\nThis estimate represents the causal effect of the treatment under the assumptions of the potential outcome framework.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#conclusion",
    "href": "chapter4.html#conclusion",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.6 Conclusion",
    "text": "9.6 Conclusion\nThis chapter has explored various research designs, from experimental approaches like RCTs and factorial designs to non-experimental methods such as observational studies and quasi-experimental designs. We’ve demonstrated how to implement and visualize these designs using R, and introduced the Neyman-Rubin potential outcome framework for causal inference.\nUnderstanding these designs and their appropriate use is crucial for conducting rigorous research and drawing valid causal conclusions. Each design has its strengths and limitations, and the choice of design should be guided by the research question, ethical considerations, and practical constraints.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#references",
    "href": "chapter4.html#references",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.7 References",
    "text": "9.7 References\n\nImbens, G. W., & Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press.\nAngrist, J. D., & Pischke, J. S. (2008). Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton University Press.\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and Quasi-Experimental Designs for Generalized Causal Inference. Houghton Mifflin.\nCunningham, S. (2021). Causal Inference: The Mixtape. Yale University Press.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html",
    "href": "rozdzial4.html",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "",
    "text": "10.1 Wstęp\nProjekty badawcze stanowią fundament procesu naukowego, zapewniając ustrukturyzowane podejście do badania hipotez i odpowiadania na pytania badawcze. Ten rozdział analizuje dwie główne kategorie projektów badawczych: eksperymentalne i nieeksperymentalne, ze szczególnym uwzględnieniem modelu potencjalnych wyników Neymana-Rubina. Zagłębimy się w różne typy projektów, ich charakterystykę i przedstawimy praktyczne przykłady wykorzystania R do analizy danych i wizualizacji.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-eksperymentalne",
    "href": "rozdzial4.html#projekty-eksperymentalne",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.2 Projekty Eksperymentalne",
    "text": "10.2 Projekty Eksperymentalne\nProjekty eksperymentalne charakteryzują się kontrolą badacza nad zmienną(ymi) niezależną(ymi) oraz losowym przydziałem uczestników do różnych warunków. Te projekty są uważane za złoty standard w ustalaniu związków przyczynowych.\n\n10.2.1 Randomizowane Badania Kontrolowane (RCT)\nRCT są najbardziej rygorystyczną formą projektu eksperymentalnego. Obejmują one:\n\nLosowy przydział uczestników do grup eksperymentalnej i kontrolnej\nManipulację zmienną niezależną\nPomiar zmiennej zależnej\n\nZobaczmy wizualizację prostego projektu RCT:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Tworzenie przykładowych danych\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  grupa = factor(rep(c(\"Kontrolna\", \"Eksperymentalna\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Symulacja efektu leczenia\ndata$post_test &lt;- ifelse(data$grupa == \"Eksperymentalna\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Przekształcenie danych do formatu długiego\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"czas\", values_to = \"wynik\")\n\n# Tworzenie wykresu\nggplot(data_long, aes(x = czas, y = wynik, color = grupa, group = interaction(id, grupa))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = grupa), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Wyniki Pre-test i Post-test w RCT\",\n       x = \"Czas\", y = \"Wynik\", color = \"Grupa\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_x_discrete(labels = c(\"pre_test\" = \"Pre-test\", \"post_test\" = \"Post-test\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nProjekt Randomizowanego Badania Kontrolowanego\n\n\n\n\nTen wykres pokazuje indywidualne trajektorie i średnie grupowe dla wyników pre-test i post-test w hipotetycznym RCT. Grupa eksperymentalna wykazuje wyraźny wzrost wyników w porównaniu do grupy kontrolnej.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#testy-ab-przykład-i-porównanie-z-rct",
    "href": "rozdzial4.html#testy-ab-przykład-i-porównanie-z-rct",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.3 Testy A/B: Przykład i Porównanie z RCT",
    "text": "10.3 Testy A/B: Przykład i Porównanie z RCT\nTesty A/B to szeroko stosowana metoda eksperymentalna w marketingu cyfrowym, projektowaniu doświadczeń użytkownika i rozwoju produktów. Ten rozdział przedstawi przykład testu A/B, wyjaśni jego metodologię i omówi, czym różni się od Randomizowanych Badań Kontrolowanych (RCT).\n\n10.3.1 Przykład: Współczynnik Konwersji Strony Docelowej\nRozważmy przykład, w którym firma e-commerce chce poprawić współczynnik konwersji swojej strony docelowej. Decydują się przetestować dwa różne układy: obecny układ (A) i nowy układ (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Symulacja danych\nn_odwiedzajacych &lt;- 10000\ndane &lt;- data.frame(\n  Wersja = sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE),\n  Konwersja = rbinom(n_odwiedzajacych, 1, ifelse(sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Obliczenie współczynników konwersji\nwspolczynniki_konwersji &lt;- dane %&gt;%\n  group_by(Wersja) %&gt;%\n  summarise(\n    Odwiedzajacy = n(),\n    Konwersje = sum(Konwersja),\n    WspolczynnikKonwersji = mean(Konwersja)\n  )\n\n# Wizualizacja wyników\nggplot(wspolczynniki_konwersji, aes(x = Wersja, y = WspolczynnikKonwersji, fill = Wersja)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", WspolczynnikKonwersji * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"Test A/B: Współczynniki Konwersji Strony Docelowej\",\n       x = \"Wersja Strony\", y = \"Współczynnik Konwersji\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure 10.1: Wyniki Testu A/B: Współczynniki Konwersji Strony Docelowej\n\n\n\n\n\nW tym przykładzie zasymulowaliśmy dane dla 10 000 odwiedzających losowo przypisanych do wersji A lub B strony docelowej. Wyniki pokazują, że wersja B ma nieco wyższy współczynnik konwersji (11,44%) w porównaniu do wersji A (10,94%).\n\n\n10.3.2 Metodologia Testów A/B\nTesty A/B zazwyczaj przebiegają według następujących kroków:\n\nZidentyfikowanie elementu do przetestowania (np. układ strony docelowej).\nStworzenie dwóch wersji: kontrolnej (A) i wariantu (B).\nLosowe przypisanie odwiedzających do jednej z wersji.\nZbieranie danych o interesującej nas metryce (np. współczynniku konwersji).\nAnaliza wyników przy użyciu metod statystycznych.\nPodjęcie decyzji na podstawie wyników.\n\n\n\n10.3.3 Różnice między Testami A/B a RCT\nChoć testy A/B i Randomizowane Badania Kontrolowane (RCT) mają pewne podobieństwa, istnieje kilka kluczowych różnic:\n\nZakres i Kontekst:\n\nTesty A/B: Zazwyczaj stosowane w środowiskach cyfrowych do szybkich, iteracyjnych ulepszeń.\nRCT: Stosowane w różnych dziedzinach, w tym medycynie, psychologii i naukach społecznych, często dla bardziej złożonych interwencji.\n\nCzas Trwania:\n\nTesty A/B: Zwykle krótsze, często trwające dni lub tygodnie.\nRCT: Mogą trwać miesiące lub lata, szczególnie w badaniach medycznych.\n\nWielkość Próby:\n\nTesty A/B: Mogą obejmować bardzo duże próby ze względu na łatwość implementacji na platformach cyfrowych.\nRCT: Wielkości prób są często mniejsze ze względu na praktyczne i kosztowe ograniczenia.\n\nZaślepienie:\n\nTesty A/B: Uczestnicy zazwyczaj nie są świadomi, że biorą udział w teście.\nRCT: Mogą obejmować pojedyncze, podwójne lub potrójne zaślepienie w celu zmniejszenia błędu systematycznego.\n\nWzględy Etyczne:\n\nTesty A/B: Generalnie obejmują zmiany niskiego ryzyka z minimalnymi obawami etycznymi.\nRCT: Często wymagają obszernej oceny etycznej, szczególnie w kontekście medycznym.\n\nMiary Wyników:\n\nTesty A/B: Zazwyczaj skupiają się na pojedynczym, łatwo mierzalnym wyniku (np. współczynnik klikalności).\nRCT: Często mierzą wiele wyników, w tym potencjalne skutki uboczne lub długoterminowe efekty.\n\nMożliwość Uogólnienia:\n\nTesty A/B: Wyniki są często specyficzne dla testowanej platformy lub kontekstu.\nRCT: Dążą do szerszej możliwości uogólnienia, choć może to się różnić.\n\nZłożoność Analizy:\n\nTesty A/B: Często wykorzystują prostsze analizy statystyczne.\nRCT: Mogą obejmować bardziej złożone metody statystyczne, aby uwzględnić różne czynniki.\n\n\nTesty A/B są potężnym narzędziem do podejmowania decyzji opartych na danych w środowiskach cyfrowych. Choć dzielą podstawową zasadę randomizacji z RCT, są zazwyczaj prostsze, szybsze i bardziej skoncentrowane na konkretnych, mierzalnych wynikach w kontekstach cyfrowych. Zrozumienie tych różnic pomaga badaczom i praktykom wybrać najbardziej odpowiednią metodę do ich konkretnych potrzeb i ograniczeń.\nTesty A/B są szczególnie przydatne w optymalizacji stron internetowych, aplikacji mobilnych i kampanii marketingowych, gdzie szybkie iteracje i ciągłe ulepszenia są kluczowe. Z kolei RCT pozostają złotym standardem w badaniach naukowych, szczególnie w dziedzinach takich jak medycyna, gdzie rygorystyczna kontrola i długoterminowa obserwacja są niezbędne.\nNiezależnie od wybranej metody, kluczowe jest staranne planowanie, precyzyjne wykonanie i ostrożna interpretacja wyników. Zarówno testy A/B, jak i RCT, gdy są odpowiednio stosowane, mogą dostarczyć cennych informacji i przyczynić się do podejmowania lepszych decyzji opartych na danych.\n\n\n10.3.4 Przykład 1: Wpływ Długości Snu na Wydajność Poznawczą\nPytanie Badawcze: Czy zwiększenie długości snu poprawia wydajność poznawczą u studentów?\n\n# Generowanie przykładowych danych\nset.seed(456)\nn &lt;- 100\npre_eksperymentalna &lt;- rnorm(n, mean = 70, sd = 10)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = 8, sd = 5)\npre_kontrolna &lt;- rnorm(n, mean = 70, sd = 10)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = 1, sd = 5)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Eksperymentalna\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  Wynik = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = Wynik, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wpływ Zwiększonej Długości Snu na Wydajność Poznawczą\") +\n  xlab(\"Czas\") +\n  ylab(\"Wynik Wydajności Poznawczej\")\n\n\n\n\n\n\n\nFigure 10.2: Wpływ Długości Snu na Wydajność Poznawczą\n\n\n\n\n\n\n10.3.4.1 Interpretacja\nTen wykres pokazuje wpływ zwiększonej długości snu na wydajność poznawczą. Grupa eksperymentalna, która zwiększyła długość snu, wykazuje znacznie większą poprawę w wydajności poznawczej w porównaniu do grupy kontrolnej. Sugeruje to, że zwiększenie długości snu może pozytywnie wpływać na zdolności poznawcze studentów.\n\n\n\n10.3.5 Przykład 2: Wpływ Treningu Uważności na Poziom Stresu\nPytanie Badawcze: Czy krótkoterminowy program treningu uważności może obniżyć poziom stresu u pracowników służby zdrowia?\n\n# Generowanie przykładowych danych\nset.seed(789)\nn &lt;- 120\npre_eksperymentalna &lt;- rnorm(n, mean = 60, sd = 15)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = -12, sd = 8)\npre_kontrolna &lt;- rnorm(n, mean = 60, sd = 15)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = -2, sd = 6)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Uważność\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  PoziomStresu = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = PoziomStresu, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wpływ Treningu Uważności na Poziom Stresu\") +\n  xlab(\"Czas\") +\n  ylab(\"Poziom Stresu\")\n\n\n\n\n\n\n\nFigure 10.3: Wpływ Treningu Uważności na Poziom Stresu\n\n\n\n\n\n\n10.3.5.1 Interpretacja\nTa wizualizacja ilustruje wpływ programu treningu uważności na poziom stresu u pracowników służby zdrowia. Grupa uważności wykazuje znacznie większy spadek poziomu stresu w porównaniu do grupy kontrolnej. Sugeruje to, że program treningu uważności może być skuteczny w redukcji poziomu stresu wśród pracowników służby zdrowia.\n\n\n\n10.3.6 Projekty Czynnikowe\nProjekty czynnikowe pozwalają badaczom na jednoczesne badanie efektów wielu zmiennych niezależnych. Są one efektywne i mogą ujawniać efekty interakcji między zmiennymi.\nPrzykład projektu czynnikowego 2x2:\n\n# Tworzenie przykładowych danych dla projektu czynnikowego 2x2\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  czynnik_a = rep(rep(c(\"Niski\", \"Wysoki\"), each = n_per_group), 2),\n  czynnik_b = rep(c(\"Kontrola\", \"Interwencja\"), each = n_per_group * 2),\n  wynik = NA\n)\n\n# Generowanie wyników\nfactorial_data$wynik &lt;- ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Kontrola\",\n                               rnorm(n_per_group, 40, 5),\n                               ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Interwencja\",\n                                      rnorm(n_per_group, 45, 5),\n                                      ifelse(factorial_data$czynnik_a == \"Wysoki\" & factorial_data$czynnik_b == \"Kontrola\",\n                                             rnorm(n_per_group, 50, 5),\n                                             rnorm(n_per_group, 60, 5))))\n\n# Tworzenie wykresu\nggplot(factorial_data, aes(x = czynnik_b, y = wynik, fill = czynnik_a)) +\n  geom_boxplot() +\n  facet_wrap(~czynnik_a, scales = \"free_x\") +\n  labs(title = \"Projekt Czynnikowy 2x2\",\n       x = \"Czynnik B\", y = \"Wynik\", fill = \"Czynnik A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nProjekt Czynnikowy 2x2\n\n\n\n\nTen wykres ilustruje projekt czynnikowy 2x2, pokazując efekty dwóch czynników (A i B) na zmienną wynikową. Możemy zaobserwować główne efekty dla obu czynników oraz potencjalny efekt interakcji.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-nieeksperymentalne",
    "href": "rozdzial4.html#projekty-nieeksperymentalne",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.4 Projekty Nieeksperymentalne",
    "text": "10.4 Projekty Nieeksperymentalne\nProjekty nieeksperymentalne są stosowane, gdy randomizacja lub manipulacja zmiennymi nie jest możliwa lub etyczna. Obejmują one badania obserwacyjne/opisowe i quasi-eksperymentalne.\n\n10.4.1 Badania Obserwacyjne\nBadania obserwacyjne polegają na zbieraniu danych bez manipulowania zmiennymi. Są one przydatne do eksploracji relacji i generowania hipotez.\nPrzykład: Badanie korelacyjne\n\nset.seed(789)\nn &lt;- 100\nczas_nauki &lt;- runif(n, 0, 10)\nwynik_egzaminu &lt;- 50 + 5 * czas_nauki + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(czas_nauki, wynik_egzaminu)\n\nggplot(correlation_data, aes(x = czas_nauki, y = wynik_egzaminu)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Korelacja między Czasem Nauki a Wynikiem Egzaminu\",\n       x = \"Czas Nauki (godziny)\", y = \"Wynik Egzaminu\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nKorelacja między Czasem Nauki a Wynikiem Egzaminu\n\n\n\n\nTen wykres punktowy pokazuje relację między czasem nauki a wynikami egzaminu, ilustrując pozytywną korelację typową dla badań obserwacyjnych.\n\n\n10.4.2 Projekty Quasi-Eksperymentalne\nProjekty quasi-eksperymentalne nie mają losowego przydziału, ale próbują ustalić związki przyczynowe. Popularne typy to:\n\nRóżnica w Różnicach (DiD)\nRegresja Nieciągła (RDD)\n\n\n10.4.2.1 Różnica w Różnicach (DiD)\nDiD jest używana do oszacowania efektów interwencji poprzez porównanie średniej zmiany w czasie w zmiennej wynikowej dla grupy eksperymentalnej ze średnią zmianą w czasie dla grupy kontrolnej.\nPrzeprowadźmy symulację analizy DiD przy użyciu pakietu plm:\n\nlibrary(plm)\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nWykres pokazuje średnie wyniki dla grup interwencji i kontrolnej w czasie. Pionowa przerywana linia wskazuje punkt interwencji. Oszacowanie DiD to różnica między zmianami obu grup od okresu przed do po interwencji.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt’s important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\n10.4.2.2 Regresja Nieciągła (RDD)\nRDD jest stosowana, gdy przydział do interwencji jest określony przez wartość graniczną na ciągłej zmiennej. Porównuje obserwacje tuż powyżej i poniżej punktu granicznego, aby oszacować efekt interwencji.\nPrzeprowadźmy analizę RDD przy użyciu pakietu rdrobust:\n\nlibrary(rdrobust)\n\n# Generowanie syntetycznych danych RDD\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# Analiza RDD\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     4.092     0.231    17.723     0.000     [3.640 , 4.545]     \n        Robust         -         -    15.013     0.000     [3.600 , 4.680]     \n=============================================================================\n\n# Wizualizacja RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regresja Nieciągła\",\n       x = \"Zmienna Bieżąca\", y = \"Wynik\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAnaliza Regresji Nieciągłej\n\n\n\n\nWykres pokazuje nieciągłość w punkcie granicznym (x = 0), z oddzielnymi liniami regresji dopasowanymi po obu stronach. Efekt interwencji jest szacowany przez różnicę między tymi liniami w punkcie granicznym.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#model-potencjalnych-wyników-neymana-rubina",
    "href": "rozdzial4.html#model-potencjalnych-wyników-neymana-rubina",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.5 Model Potencjalnych Wyników Neymana-Rubina",
    "text": "10.5 Model Potencjalnych Wyników Neymana-Rubina\nModel potencjalnych wyników Neymana-Rubina zapewnia formalne podejście do wnioskowania przyczynowego. Wprowadza on koncepcję potencjalnych wyników: dla każdej jednostki rozważamy wynik w warunkach interwencji i w warunkach kontrolnych, mimo że w rzeczywistości możemy zaobserwować tylko jeden z nich.\nKluczowe pojęcia:\n\nPotencjalne Wyniki: \\(Y_i(1)\\) i \\(Y_i(0)\\) odpowiednio dla interwencji i kontroli.\nObserwowany Wynik: \\(Y_i = Y_i(1)T_i + Y_i(0)(1-T_i)\\), gdzie \\(T_i\\) to wskaźnik interwencji.\nIndywidualny Efekt Interwencji: \\(\\tau_i = Y_i(1) - Y_i(0)\\)\nPrzeciętny Efekt Interwencji (ATE): \\(E[\\tau_i] = E[Y_i(1) - Y_i(0)]\\)\n\nModel podkreśla “fundamentalny problem wnioskowania przyczynowego”: nigdy nie możemy zaobserwować obu potencjalnych wyników dla pojedynczej jednostki jednocześnie.\n\n10.5.1 Przykład: Szacowanie ATE w RCT\nW RCT, losowy przydział zapewnia, że interwencja jest niezależna od potencjalnych wyników, umożliwiając nieobciążone oszacowanie ATE:\n\\[\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\\]\nGdzie \\(n_1\\) i \\(n_0\\) to odpowiednio liczby jednostek w grupie interwencji i kontrolnej.\n\n# Używając danych RCT z wcześniejszego przykładu\nate_estimate &lt;- mean(data$post_test[data$grupa == \"Eksperymentalna\"]) - \n                mean(data$post_test[data$grupa == \"Kontrolna\"])\n\ncat(\"Oszacowany Przeciętny Efekt Interwencji:\", round(ate_estimate, 2))\n\nOszacowany Przeciętny Efekt Interwencji: 9.66",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "",
    "text": "11.1 Understanding Outliers\nBefore diving into specific measures, it’s crucial to understand the concept of outliers, as they can significantly impact many descriptive statistics.\nOutliers are data points that differ significantly from other observations in the dataset. They can occur due to:\nOutliers can have a substantial effect on many statistical measures, especially those based on means or sums of squared deviations. Therefore, it’s essential to:\nThroughout this chapter, we’ll discuss how different descriptive measures are affected by outliers.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-outliers",
    "href": "chapter5.html#understanding-outliers",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "",
    "text": "Measurement or recording errors\nGenuine extreme values in the population\nSampling from a different population\n\n\n\nIdentify outliers through both statistical methods and domain knowledge\nInvestigate the cause of outliers\nMake informed decisions about whether to include or exclude them in analyses",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#arithmetic-mean",
    "href": "chapter5.html#arithmetic-mean",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "12.1 Arithmetic Mean",
    "text": "12.1 Arithmetic Mean\nThe arithmetic mean is the sum of all values divided by the number of values.\nFormula: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\)\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(data)\n\n[1] 5.142857\n\n\nPros:\n\nEasy to calculate and understand\nUses all data points\nUseful for further statistical calculations\n\nCons:\n\nSensitive to outliers\nNot ideal for skewed distributions\n\nExample with outlier:\n\ndata_with_outlier &lt;- c(2, 4, 4, 5, 5, 7, 100)\nmean(data_with_outlier)\n\n[1] 18.14286\n\n\nAs we can see, the outlier (100) drastically affects the mean.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#median",
    "href": "chapter5.html#median",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "12.2 Median",
    "text": "12.2 Median\nThe median is the middle value when the data is ordered.\nR calculation:\n\nmedian(data)\n\n[1] 5\n\nmedian(data_with_outlier)\n\n[1] 5\n\n\nPros:\n\nNot affected by extreme outliers\nBetter for skewed distributions\n\nCons:\n\nDoesn’t use all data points\nLess useful for further statistical calculations",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#mode",
    "href": "chapter5.html#mode",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "12.3 Mode",
    "text": "12.3 Mode\nThe mode is the most frequently occurring value.\nR calculation:\n\nlibrary(modeest)\nmfv(data)  # Most frequent value\n\n[1] 4 5\n\n\nPros:\n\nOnly measure of central tendency for nominal data\nCan identify multiple peaks in the data\n\nCons:\n\nNot always uniquely defined\nNot useful for continuous data",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#weighted-mean",
    "href": "chapter5.html#weighted-mean",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "12.4 Weighted Mean",
    "text": "12.4 Weighted Mean\nThe weighted mean is used when some data points are more important than others. There are two types of weighted means: with not normalized weights and with normalized weights.\n\n12.4.1 Weighted Mean with Not Normalized Weights\nThis is the standard form of the weighted mean, where weights can be any positive numbers representing the importance of each data point.\nFormula: \\(\\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\\)\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\n12.4.2 Weighted Mean with Normalized Weights (Fractions)\nIn this case, the weights are fractions that sum to 1, representing the proportion of importance for each data point.\nFormula: \\(\\bar{x}_w = \\sum_{i=1}^n w_i x_i\\), where \\(\\sum_{i=1}^n w_i = 1\\)\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Note: these sum to 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nPros of Weighted Means:\n\nAccount for varying importance of data points\nUseful in survey analysis with different sample sizes or importance levels\nCan adjust for unequal probabilities in sampling designs\n\nCons of Weighted Means:\n\nRequire justification for weights\nCan be misused to manipulate results\nMay be less intuitive to interpret than simple arithmetic mean\n\nComparison:\nThe not normalized weights are often easier to assign based on real-world importance or sample sizes, but require the additional step of normalization in the calculation. Normalized weights (fractions) make the calculation simpler but may be less intuitive to assign directly.\nExample in Social Science:\nSuppose we’re calculating the average income for a region with three cities:\nCity A: Average income $50,000, population 100,000 City B: Average income $60,000, population 200,000 City C: Average income $70,000, population 300,000\nWe can use population as weights:\n\nincomes &lt;- c(50000, 60000, 70000)\npopulations &lt;- c(100000, 200000, 300000)\n\n# Not normalized weights\nweighted.mean(incomes, populations)\n\n[1] 63333.33\n\n# Normalized weights\npop_normalized &lt;- populations / sum(populations)\nsum(incomes * pop_normalized)\n\n[1] 63333.33\n\n\nThis weighted average gives a more accurate representation of the region’s average income than a simple arithmetic mean of the three city averages would.\nPros:\n\nAccounts for varying importance of data points\nUseful in survey analysis with different sample sizes\n\nCons:\n\nRequires justification for weights\nCan be misused to manipulate results",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#range",
    "href": "chapter5.html#range",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "13.1 Range",
    "text": "13.1 Range\nThe range is the difference between the maximum and minimum values.\nFormula: \\(R = x_{max} - x_{min}\\)\nR calculation:\n\nrange(data)\n\n[1] 2 9\n\nmax(data) - min(data)\n\n[1] 7\n\n\nPros:\n\nSimple to calculate and understand\nGives an immediate sense of data spread\n\nCons:\n\nExtremely sensitive to outliers\nDoesn’t provide information about the distribution between extremes",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#interquartile-range-iqr",
    "href": "chapter5.html#interquartile-range-iqr",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "13.2 Interquartile Range (IQR)",
    "text": "13.2 Interquartile Range (IQR)\nThe IQR is the difference between the 75th and 25th percentiles.\nFormula: \\(IQR = Q_3 - Q_1\\)\nR calculation:\n\nIQR(data)\n\n[1] 2\n\n\nPros:\n\nRobust to outliers\nProvides information about the spread of the middle 50% of the data\n\nCons:\n\nIgnores the tails of the distribution\nLess efficient than standard deviation for normal distributions",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#variance",
    "href": "chapter5.html#variance",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "13.3 Variance",
    "text": "13.3 Variance\nVariance measures the average squared deviation from the mean.\nFormula: \\(s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\\)\nR calculation:\n\nvar(data)\n\n[1] 5.142857\n\n\nPros:\n\nUses all data points\nFoundation for many statistical tests\n\nCons:\n\nUnits are squared, making interpretation less intuitive\nSensitive to outliers",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#standard-deviation",
    "href": "chapter5.html#standard-deviation",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "13.4 Standard Deviation",
    "text": "13.4 Standard Deviation\nThe standard deviation is the square root of the variance.\nFormula: \\(s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\\)\nR calculation:\n\nsd(data)\n\n[1] 2.267787\n\n\nPros:\n\nIn same units as original data\nWidely used and understood\n\nCons:\n\nStill sensitive to outliers\nAssumes data is roughly normally distributed",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#coefficient-of-variation",
    "href": "chapter5.html#coefficient-of-variation",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "13.5 Coefficient of Variation",
    "text": "13.5 Coefficient of Variation\nThe coefficient of variation is the standard deviation divided by the mean, often expressed as a percentage.\nFormula: \\(CV = \\frac{s}{\\bar{x}} \\times 100\\%\\)\nR calculation:\n\n(sd(data) / mean(data)) * 100\n\n[1] 44.09586\n\n\nPros:\n\nAllows comparison of variability between datasets with different units or means\nUseful in fields like finance for risk assessment\n\nCons:\n\nNot meaningful for data with both positive and negative values\nCan be misleading when mean is close to zero",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#percentiles",
    "href": "chapter5.html#percentiles",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "14.1 Percentiles",
    "text": "14.1 Percentiles\nPercentiles divide the data into 100 equal parts.\nFormula: For the kth percentile: \\(P_k = L + \\frac{k(n+1)}{100}\\), where L is the lower limit of the interval\nR calculation:\n\nquantile(data, probs = seq(0, 1, 0.25))\n\n  0%  25%  50%  75% 100% \n   2    4    5    6    9",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#quartiles",
    "href": "chapter5.html#quartiles",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "14.2 Quartiles",
    "text": "14.2 Quartiles\nQuartiles divide the data into four equal parts.\n\nQ1: 25th percentile\nQ2: Median (50th percentile)\nQ3: 75th percentile\n\nR calculation:\n\nsummary(data)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   4.000   5.000   5.143   6.000   9.000 \n\n\nPros:\n\nRobust to outliers\nProvide information about data spread and skewness\n\nCons:\n\nLess precise than using all data points\nMultiple methods of calculation can lead to slightly different results",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#skewness",
    "href": "chapter5.html#skewness",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "15.1 Skewness",
    "text": "15.1 Skewness\nSkewness measures the asymmetry of the probability distribution.\nFormula: \\(SK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3\\)\nR calculation:\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nskewness(data)\n\n[1] 0.4592793\n\n\nInterpretation:\n\nPositive skewness: right tail is longer (mean &gt; median)\nNegative skewness: left tail is longer (mean &lt; median)\nZero skewness: symmetrical distribution\n\nPros:\n\nProvides information about distribution shape\nUseful for checking assumptions of normality\n\nCons:\n\nSensitive to outliers\nCan be misleading for multimodal distributions",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#kurtosis",
    "href": "chapter5.html#kurtosis",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "15.2 Kurtosis",
    "text": "15.2 Kurtosis\nKurtosis measures the “tailedness” of the probability distribution.\nFormula: \\(K = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\\)\nR calculation:\n\nkurtosis(data)\n\n[1] 2.457047\n\n\nInterpretation:\n\nPositive kurtosis: heavy tails, peaked distribution\nNegative kurtosis: light tails, flat distribution\nZero kurtosis: normal distribution\n\nPros:\n\nProvides information about extreme values in the distribution\nUseful for financial modeling and risk assessment\n\nCons:\n\nSensitive to outliers\nCan be difficult to interpret practically",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#bivariate-statistics",
    "href": "chapter5.html#bivariate-statistics",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "16.1 Bivariate Statistics",
    "text": "16.1 Bivariate Statistics\n\n16.1.1 Correlation\nCorrelation measures the strength and direction of the linear relationship between two variables.\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 5, 4, 5)\ncor(x, y)\n\n[1] 0.7745967\n\n\n\n\n16.1.2 Covariance\nCovariance measures how two variables vary together.\n\ncov(x, y)\n\n[1] 1.5\n\n\n\n\n16.1.3 Cross-tabulation\nCross-tabulation (contingency table) shows the relationship between two categorical variables.\n\ntable(cut(x, 2), cut(y, 2))\n\n           \n            (2,3.5] (3.5,5]\n  (0.996,3]       1       2\n  (3,5]           0       2",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#multivariate-statistics",
    "href": "chapter5.html#multivariate-statistics",
    "title": "11  Comprehensive Guide to Univariate Descriptive Statistics",
    "section": "16.2 Multivariate Statistics",
    "text": "16.2 Multivariate Statistics\n\nMultiple Correlation: Correlation between a dependent variable and multiple independent variables.\nPartial Correlation: Correlation between two variables while controlling for others.\nFactor Analysis: Technique to reduce many variables to a few underlying factors.\n\nThese topics will be covered in more detail in later chapters.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Comprehensive Guide to Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html",
    "href": "rozdzial5.html",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "",
    "text": "12.1 Zrozumienie wartości odstających\nZanim zagłębimy się w konkretne miary, kluczowe jest zrozumienie koncepcji wartości odstających, ponieważ mogą one znacząco wpływać na wiele statystyk opisowych.\nWartości odstające to punkty danych, które znacznie różnią się od innych obserwacji w zbiorze danych. Mogą wystąpić z powodu:\nWartości odstające mogą mieć istotny wpływ na wiele miar statystycznych, szczególnie tych opartych na średnich lub sumach kwadratów odchyleń. Dlatego ważne jest, aby:\nW trakcie tego rozdziału omówimy, jak różne miary opisowe są dotknięte przez wartości odstające.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#zrozumienie-wartości-odstających",
    "href": "rozdzial5.html#zrozumienie-wartości-odstających",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "",
    "text": "Błędów pomiaru lub rejestracji\nPrawdziwych skrajnych wartości w populacji\nPróbkowania z innej populacji\n\n\n\nIdentyfikować wartości odstające zarówno poprzez metody statystyczne, jak i wiedzę dziedzinową\nBadać przyczyny występowania wartości odstających\nPodejmować świadome decyzje o tym, czy włączyć je do analiz, czy nie",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#średnia-arytmetyczna",
    "href": "rozdzial5.html#średnia-arytmetyczna",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "13.1 Średnia arytmetyczna",
    "text": "13.1 Średnia arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez liczbę wartości.\nWzór: \\(\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\\)\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nŁatwa do obliczenia i zrozumienia\nWykorzystuje wszystkie punkty danych\nPrzydatna do dalszych obliczeń statystycznych\n\nWady:\n\nWrażliwa na wartości odstające\nNieidealna dla rozkładów skośnych\n\nPrzykład z wartością odstającą:\n\ndane_z_odstajaca &lt;- c(2, 4, 4, 5, 5, 7, 100)\nmean(dane_z_odstajaca)\n\n[1] 18.14286\n\n\nJak widać, wartość odstająca (100) drastycznie wpływa na średnią.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#mediana",
    "href": "rozdzial5.html#mediana",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "13.2 Mediana",
    "text": "13.2 Mediana\nMediana to środkowa wartość, gdy dane są uporządkowane.\nObliczenie w R:\n\nmedian(dane)\n\n[1] 5\n\nmedian(dane_z_odstajaca)\n\n[1] 5\n\n\nZalety:\n\nNie jest dotknięta przez skrajne wartości odstające\nLepsza dla rozkładów skośnych\n\nWady:\n\nNie wykorzystuje wszystkich punktów danych\nMniej przydatna do dalszych obliczeń statystycznych",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#moda",
    "href": "rozdzial5.html#moda",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "13.3 Moda",
    "text": "13.3 Moda\nModa to najczęściej występująca wartość.\nObliczenie w R:\n\nlibrary(modeest)\nmfv(dane)  # Najczęściej występująca wartość\n\n[1] 4 5\n\n\nZalety:\n\nJedyna miara tendencji centralnej dla danych nominalnych\nMoże identyfikować wiele szczytów w danych\n\nWady:\n\nNie zawsze jednoznacznie zdefiniowana\nNieprzydatna dla danych ciągłych",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#średnia-ważona",
    "href": "rozdzial5.html#średnia-ważona",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "13.4 Średnia ważona",
    "text": "13.4 Średnia ważona\nŚrednia ważona jest używana, gdy niektóre punkty danych są ważniejsze od innych. Rozróżniamy dwa typy średnich ważonych: z wagami nienormalizowanymi i z wagami znormalizowanymi.\n\n13.4.1 Średnia ważona z wagami nienormalizowanymi\nJest to standardowa forma średniej ważonej, gdzie wagi mogą być dowolnymi liczbami dodatnimi reprezentującymi ważność każdego punktu danych.\nWzór: \\(\\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\\)\nObliczenie w R:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\n13.4.2 Średnia ważona z wagami znormalizowanymi (ułamkami)\nW tym przypadku wagi są ułamkami sumującymi się do 1, reprezentującymi proporcję ważności dla każdego punktu danych.\nWzór: \\(\\bar{x}_w = \\sum_{i=1}^n w_i x_i\\), gdzie \\(\\sum_{i=1}^n w_i = 1\\)\nObliczenie w R:\n\nx &lt;- c(2, 4, 5, 7)\nw_znormalizowane &lt;- c(0.1, 0.3, 0.4, 0.2)  # Uwaga: te sumują się do 1\nsum(x * w_znormalizowane)\n\n[1] 4.8\n\n\nZalety średnich ważonych:\n\nUwzględniają różną ważność punktów danych\nPrzydatne w analizie badań z różnymi wielkościami próby lub poziomami ważności\nMogą korygować nierówne prawdopodobieństwa w projektach próbkowania\n\nWady średnich ważonych:\n\nWymagają uzasadnienia dla wag\nMogą być niewłaściwie użyte do manipulacji wynikami\nMogą być mniej intuicyjne w interpretacji niż prosta średnia arytmetyczna\n\nPorównanie:\nWagi nienormalizowane są często łatwiejsze do przypisania na podstawie rzeczywistej ważności lub wielkości próby, ale wymagają dodatkowego kroku normalizacji w obliczeniach. Wagi znormalizowane (ułamki) upraszczają obliczenia, ale mogą być mniej intuicyjne do bezpośredniego przypisania.\nPrzykład w naukach społecznych:\nZałóżmy, że obliczamy średni dochód dla regionu z trzema miastami:\nMiasto A: Średni dochód 50 000 zł, populacja 100 000 Miasto B: Średni dochód 60 000 zł, populacja 200 000 Miasto C: Średni dochód 70 000 zł, populacja 300 000\nMożemy użyć populacji jako wag:\n\ndochody &lt;- c(50000, 60000, 70000)\npopulacje &lt;- c(100000, 200000, 300000)\n\n# Wagi nienormalizowane\nweighted.mean(dochody, populacje)\n\n[1] 63333.33\n\n# Wagi znormalizowane\npop_znormalizowane &lt;- populacje / sum(populacje)\nsum(dochody * pop_znormalizowane)\n\n[1] 63333.33\n\n\nTa średnia ważona daje dokładniejsze przedstawienie średniego dochodu regionu niż prosta średnia arytmetyczna średnich dochodów trzech miast.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#rozstęp",
    "href": "rozdzial5.html#rozstęp",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "14.1 Rozstęp",
    "text": "14.1 Rozstęp\nRozstęp to różnica między wartością maksymalną a minimalną.\nWzór: \\(R = x_{max} - x_{min}\\)\nObliczenie w R:\n\nrange(dane)\n\n[1] 2 9\n\nmax(dane) - min(dane)\n\n[1] 7\n\n\nZalety:\n\nProsty do obliczenia i zrozumienia\nDaje natychmiastowe poczucie rozpiętości danych\n\nWady:\n\nNiezwykle wrażliwy na wartości odstające\nNie dostarcza informacji o rozkładzie między skrajnościami",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#rozstęp-międzykwartylowy-iqr",
    "href": "rozdzial5.html#rozstęp-międzykwartylowy-iqr",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "14.2 Rozstęp międzykwartylowy (IQR)",
    "text": "14.2 Rozstęp międzykwartylowy (IQR)\nIQR to różnica między 75. a 25. percentylem.\nWzór: \\(IQR = Q_3 - Q_1\\)\nObliczenie w R:\n\nIQR(dane)\n\n[1] 2\n\n\nZalety:\n\nOdporny na wartości odstające\nDostarcza informacji o rozpiętości środkowych 50% danych\n\nWady:\n\nIgnoruje ogony rozkładu\nMniej efektywny niż odchylenie standardowe dla rozkładów normalnych",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wariancja",
    "href": "rozdzial5.html#wariancja",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "14.3 Wariancja",
    "text": "14.3 Wariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: \\(s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\\)\nObliczenie w R:\n\nvar(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nWykorzystuje wszystkie punkty danych\nPodstawa dla wielu testów statystycznych\n\nWady:\n\nJednostki są podniesione do kwadratu, co utrudnia interpretację\nWrażliwa na wartości odstające",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#odchylenie-standardowe",
    "href": "rozdzial5.html#odchylenie-standardowe",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "14.4 Odchylenie standardowe",
    "text": "14.4 Odchylenie standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji.\nWzór: \\(s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\\)\nObliczenie w R:\n\nsd(dane)\n\n[1] 2.267787\n\n\nZalety:\n\nW tych samych jednostkach co oryginalne dane\nSzeroko stosowane i rozumiane\n\nWady:\n\nNadal wrażliwe na wartości odstające\nZakłada, że dane są w przybliżeniu normalnie rozłożone",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#współczynnik-zmienności",
    "href": "rozdzial5.html#współczynnik-zmienności",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "14.5 Współczynnik zmienności",
    "text": "14.5 Współczynnik zmienności\nWspółczynnik zmienności to odchylenie standardowe podzielone przez średnią, często wyrażane jako procent.\nWzór: \\(CV = \\frac{s}{\\bar{x}} \\times 100\\%\\)\nObliczenie w R:\n\n(sd(dane) / mean(dane)) * 100\n\n[1] 44.09586\n\n\nZalety:\n\nPozwala na porównanie zmienności między zbiorami danych o różnych jednostkach lub średnich\nPrzydatny w dziedzinach takich jak finanse do oceny ryzyka\n\nWady:\n\nNie ma sensu dla danych z wartościami zarówno dodatnimi, jak i ujemnymi\nMoże być mylący, gdy średnia jest bliska zeru",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#percentyle",
    "href": "rozdzial5.html#percentyle",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "15.1 Percentyle",
    "text": "15.1 Percentyle\nPercentyle dzielą dane na 100 równych części.\nWzór: Dla k-tego percentyla: \\(P_k = L + \\frac{k(n+1)}{100}\\), gdzie L to dolna granica przedziału\nObliczenie w R:\n\nquantile(dane, probs = seq(0, 1, 0.25))\n\n  0%  25%  50%  75% 100% \n   2    4    5    6    9",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#kwartyle",
    "href": "rozdzial5.html#kwartyle",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "15.2 Kwartyle",
    "text": "15.2 Kwartyle\nKwartyle dzielą dane na cztery równe części.\n\nQ1: 25. percentyl\nQ2: Mediana (50. percentyl)\nQ3: 75. percentyl\n\nObliczenie w R:\n\nsummary(dane)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  2.000   4.000   5.000   5.143   6.000   9.000 \n\n\nZalety:\n\nOdporne na wartości odstające\nDostarczają informacji o rozpiętości i skośności danych\n\nWady:\n\nMniej precyzyjne niż użycie wszystkich punktów danych\nRóżne metody obliczania mogą prowadzić do nieznacznie różnych wyników",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#skośność",
    "href": "rozdzial5.html#skośność",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "16.1 Skośność",
    "text": "16.1 Skośność\nSkośność mierzy asymetrię rozkładu prawdopodobieństwa.\nWzór: \\(SK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3\\)\nObliczenie w R:\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nskewness(dane)\n\n[1] 0.4592793\n\n\nInterpretacja:\n\nSkośność dodatnia: prawy ogon jest dłuższy (średnia &gt; mediana)\nSkośność ujemna: lewy ogon jest dłuższy (średnia &lt; mediana)\nSkośność zero: rozkład symetryczny\n\nZalety:\n\nDostarcza informacji o kształcie rozkładu\nPrzydatna do sprawdzania założeń normalności\n\nWady:\n\nWrażliwa na wartości odstające\nMoże być myląca dla rozkładów wielomodalnych",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#kurtoza",
    "href": "rozdzial5.html#kurtoza",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "16.2 Kurtoza",
    "text": "16.2 Kurtoza\nKurtoza mierzy “grubość ogonów” rozkładu prawdopodobieństwa.\nWzór: \\(K = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\\)\nObliczenie w R:\n\nkurtosis(dane)\n\n[1] 2.457047\n\n\nInterpretacja:\n\nKurtoza dodatnia: ciężkie ogony, rozkład wysmukły\nKurtoza ujemna: lekkie ogony, rozkład płaski\nKurtoza zero: rozkład normalny\n\nZalety:\n\nDostarcza informacji o wartościach skrajnych w rozkładzie\nPrzydatna do modelowania finansowego i oceny ryzyka\n\nWady:\n\nWrażliwa na wartości odstające\nMoże być trudna do praktycznej interpretacji",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#statystyki-dwuwymiarowe",
    "href": "rozdzial5.html#statystyki-dwuwymiarowe",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "17.1 Statystyki dwuwymiarowe",
    "text": "17.1 Statystyki dwuwymiarowe\n\n17.1.1 Korelacja\nKorelacja mierzy siłę i kierunek liniowego związku między dwiema zmiennymi.\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 5, 4, 5)\ncor(x, y)\n\n[1] 0.7745967\n\n\n\n\n17.1.2 Kowariancja\nKowariancja mierzy, jak dwie zmienne zmieniają się razem.\n\ncov(x, y)\n\n[1] 1.5\n\n\n\n\n17.1.3 Tabela krzyżowa\nTabela krzyżowa (tabela kontyngencji) pokazuje relację między dwiema zmiennymi kategorialnymi.\n\ntable(cut(x, 2), cut(y, 2))\n\n           \n            (2,3.5] (3.5,5]\n  (0.996,3]       1       2\n  (3,5]           0       2",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#statystyki-wielowymiarowe",
    "href": "rozdzial5.html#statystyki-wielowymiarowe",
    "title": "12  Kompleksowy przewodnik po jednowymiarowych statystykach opisowych",
    "section": "17.2 Statystyki wielowymiarowe",
    "text": "17.2 Statystyki wielowymiarowe\n\nKorelacja wielokrotna: Korelacja między zmienną zależną a wieloma zmiennymi niezależnymi.\nKorelacja cząstkowa: Korelacja między dwiema zmiennymi przy kontrolowaniu innych.\nAnaliza czynnikowa: Technika redukcji wielu zmiennych do kilku podstawowych czynników.\n\nTe tematy zostaną omówione bardziej szczegółowo w kolejnych rozdziałach.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Kompleksowy przewodnik po jednowymiarowych statystykach opisowych</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "13  Data Visualization: From Pen and Paper to R",
    "section": "",
    "text": "13.1 Introduction to Data Types and Visualization\nBefore diving into specific visualization techniques, it’s crucial to understand the different types of data you might encounter and how they influence your choice of visualization method. We’ll explore these concepts with practical examples using the ggplot2 library in R.\nFirst, let’s load the necessary libraries:\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: From Pen and Paper to R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#introduction-to-data-types-and-visualization",
    "href": "chapter6.html#introduction-to-data-types-and-visualization",
    "title": "13  Data Visualization: From Pen and Paper to R",
    "section": "",
    "text": "13.1.1 Data Types and Visualization Examples\n\nCategorical (Qualitative) Data\n\nNominal Data: Categories with no inherent order. Example: Colors of cars sold\n\n\n\n   # Create sample data\n   car_colors &lt;- data.frame(\n     color = c(\"Red\", \"Blue\", \"Green\", \"Black\", \"White\"),\n     count = c(22, 18, 15, 30, 25)\n   )\n\n   # Bar plot\n   ggplot(car_colors, aes(x = color, y = count)) +\n     geom_bar(stat = \"identity\", fill = \"skyblue\") +\n     labs(title = \"Car Sales by Color (Bar Plot)\",\n          x = \"Color\", y = \"Number of Cars Sold\") +\n     theme_minimal()\n\n\n\n\n\n\n\n   # Pie chart\n   ggplot(car_colors, aes(x = \"\", y = count, fill = color)) +\n     geom_bar(stat = \"identity\", width = 1) +\n     coord_polar(\"y\", start = 0) +\n     labs(title = \"Car Sales by Color (Pie Chart)\") +\n     theme_void() +\n     theme(legend.title = element_blank())\n\n\n\n\n\n\n\n   # Treemap\n   library(treemapify)\n   ggplot(car_colors, aes(area = count, fill = color, label = color)) +\n     geom_treemap() +\n     geom_treemap_text() +\n     labs(title = \"Car Sales by Color (Treemap)\") +\n     theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nOrdinal Data: Categories with a meaningful order. Example: Education levels\n\n\n   # Create sample data\n   education_levels &lt;- data.frame(\n     level = factor(c(\"High School\", \"Bachelor's\", \"Master's\", \"PhD\"),\n                    levels = c(\"High School\", \"Bachelor's\", \"Master's\", \"PhD\")),\n     count = c(100, 80, 40, 20)\n   )\n\n   # Ordered bar plot\n   ggplot(education_levels, aes(x = level, y = count)) +\n     geom_bar(stat = \"identity\", fill = \"lightgreen\") +\n     labs(title = \"Highest Education Level Achieved (Bar Plot)\",\n          x = \"Education Level\", y = \"Number of People\") +\n     theme_minimal()\n\n\n\n\n\n\n\n   # Stacked bar plot (with arbitrary grouping)\n   education_levels_stacked &lt;- education_levels %&gt;%\n     mutate(group = c(\"Group A\", \"Group A\", \"Group B\", \"Group B\"))\n\n   ggplot(education_levels_stacked, aes(x = group, y = count, fill = level)) +\n     geom_bar(stat = \"identity\") +\n     labs(title = \"Education Levels by Group (Stacked Bar Plot)\",\n          x = \"Group\", y = \"Number of People\") +\n     theme_minimal()\n\n\n\n\n\n\n\n   # Heatmap (with arbitrary second dimension)\n   education_heatmap &lt;- education_levels %&gt;%\n     mutate(year = 2020:2023,\n            count = count * (1 + seq(0, 0.3, length.out = 4))) %&gt;%\n     pivot_wider(names_from = year, values_from = count)\n\n   education_heatmap_long &lt;- education_heatmap %&gt;%\n     pivot_longer(cols = -level, names_to = \"year\", values_to = \"count\")\n\n   ggplot(education_heatmap_long, aes(x = year, y = level, fill = count)) +\n     geom_tile() +\n     scale_fill_viridis_c() +\n     labs(title = \"Education Levels Over Time (Heatmap)\",\n          x = \"Year\", y = \"Education Level\") +\n     theme_minimal()\n\n\n\n\n\n\n\n\n\nNumerical (Quantitative) Data\n\nDiscrete Data: Countable values, often integers. Example: Number of children in families\n\n\n\n   # Create sample data\n   family_sizes &lt;- data.frame(\n     children = 0:5,\n     families = c(20, 45, 30, 20, 10, 5)\n   )\n\n   # Bar plot\n   ggplot(family_sizes, aes(x = factor(children), y = families)) +\n     geom_bar(stat = \"identity\", fill = \"coral\") +\n     labs(title = \"Distribution of Family Sizes (Bar Plot)\",\n          x = \"Number of Children\", y = \"Number of Families\") +\n     theme_minimal()\n\n\n\n\n\n\n\n   # Line plot (assuming data over time)\n   family_sizes_time &lt;- family_sizes %&gt;%\n     mutate(year = 2015:2020)\n\n   ggplot(family_sizes_time, aes(x = year, y = families, group = children, color = factor(children))) +\n     geom_line() +\n     geom_point() +\n     labs(title = \"Family Sizes Over Time (Line Plot)\",\n          x = \"Year\", y = \"Number of Families\", color = \"Number of Children\") +\n     theme_minimal()\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n   # Scatter plot (comparing two discrete variables)\n   family_data &lt;- expand.grid(\n     children = 0:5,\n     bedrooms = 1:4\n   ) %&gt;%\n   mutate(families = rpois(n(), lambda = 10))\n\n   ggplot(family_data, aes(x = children, y = bedrooms, size = families)) +\n     geom_point(alpha = 0.6) +\n     scale_size_continuous(range = c(1, 10)) +\n     labs(title = \"Families by Number of Children and Bedrooms (Scatter Plot)\",\n          x = \"Number of Children\", y = \"Number of Bedrooms\", size = \"Number of Families\") +\n     theme_minimal()\n\n\n\n\n\n\n\n\n\nContinuous Data: Measurable values that can take any value within a range. Example: Heights of individuals\n\n\n   # Create sample data\n   set.seed(123)\n   heights &lt;- data.frame(height = rnorm(1000, mean = 170, sd = 10))\n\n   # Histogram\n   ggplot(heights, aes(x = height)) +\n     geom_histogram(binwidth = 5, fill = \"purple\", color = \"black\") +\n     labs(title = \"Distribution of Heights (Histogram)\",\n          x = \"Height (cm)\", y = \"Frequency\") +\n     theme_minimal()\n\n\n\n\n\n\n\n   # Box plot\n   ggplot(heights, aes(y = height)) +\n     geom_boxplot(fill = \"lightblue\") +\n     labs(title = \"Distribution of Heights (Box Plot)\",\n          y = \"Height (cm)\") +\n     theme_minimal() +\n     theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n   # Density plot\n   ggplot(heights, aes(x = height)) +\n     geom_density(fill = \"orange\", alpha = 0.7) +\n     labs(title = \"Distribution of Heights (Density Plot)\",\n          x = \"Height (cm)\", y = \"Density\") +\n     theme_minimal()\n\n\n\n\n\n\n\n   # Q-Q plot\n   ggplot(heights, aes(sample = height)) +\n     stat_qq() +\n     stat_qq_line() +\n     labs(title = \"Q-Q Plot of Heights\",\n          x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n     theme_minimal()\n\n\n\n\n\n\n\n\n\n\n13.1.2 Combining Different Data Types\nOften, you’ll want to visualize relationships between different types of data. Here are some examples:\n\n# Create sample data\nset.seed(123)\ncar_data &lt;- data.frame(\n  make = rep(c(\"Toyota\", \"Honda\", \"Ford\", \"BMW\"), each = 50),\n  price = c(rnorm(50, 25000, 5000), rnorm(50, 27000, 5000),\n            rnorm(50, 35000, 7000), rnorm(50, 50000, 10000)),\n  mpg = c(rnorm(50, 30, 5), rnorm(50, 28, 5),\n          rnorm(50, 25, 5), rnorm(50, 22, 5)),\n  sales = round(runif(200, 1000, 5000))\n)\n\n# Box plot\nggplot(car_data, aes(x = make, y = price, fill = make)) +\n  geom_boxplot() +\n  labs(title = \"Car Prices by Make (Box Plot)\",\n       x = \"Make\", y = \"Price ($)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Scatter plot with color\nggplot(car_data, aes(x = mpg, y = price, color = make)) +\n  geom_point(alpha = 0.7) +\n  labs(title = \"Car Prices vs. MPG by Make (Scatter Plot)\",\n       x = \"Miles per Gallon\", y = \"Price ($)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Bubble plot\nggplot(car_data, aes(x = mpg, y = price, size = sales, color = make)) +\n  geom_point(alpha = 0.7) +\n  scale_size_continuous(range = c(1, 10)) +\n  labs(title = \"Car Prices vs. MPG by Make and Sales (Bubble Plot)\",\n       x = \"Miles per Gallon\", y = \"Price ($)\", size = \"Sales\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Violin plot\nggplot(car_data, aes(x = make, y = price, fill = make)) +\n  geom_violin() +\n  labs(title = \"Distribution of Car Prices by Make (Violin Plot)\",\n       x = \"Make\", y = \"Price ($)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThese examples demonstrate how to visualize relationships between categorical data (car make), continuous data (price, mpg), and discrete numerical data (sales).\n\n\n13.1.3 Interpreting Different Types of Plots\nLet’s go through each type of plot and discuss how to interpret them:\n\nBar Plot (Nominal Data: Car Sales by Color)\n\n\nggplot(car_colors, aes(x = color, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Car Sales by Color (Bar Plot)\",\n       x = \"Color\", y = \"Number of Cars Sold\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretation: - Each bar represents a category (color), and its height shows the count or frequency. - Quickly compare quantities across categories. - In this example, we can see that Black cars are the most popular, followed by White. Green cars are the least popular. - The order of bars doesn’t matter for nominal data, but you might choose to order by frequency for easier comparison.\n\nPie Chart (Nominal Data: Car Sales by Color)\n\n\nggplot(car_colors, aes(x = \"\", y = count, fill = color)) +\n  geom_bar(stat = \"identity\", width = 1) +\n  coord_polar(\"y\", start = 0) +\n  labs(title = \"Car Sales by Color (Pie Chart)\") +\n  theme_void() +\n  theme(legend.title = element_blank())\n\n\n\n\n\n\n\n\nInterpretation: - Each slice represents a category, and its size shows the proportion of the whole. - Good for showing composition when you have a small number of categories. - We can see that Black and White make up about half of all car sales. - However, precise comparisons can be difficult, especially for similar-sized slices.\n\nTreemap (Nominal Data: Car Sales by Color)\n\n\nggplot(car_colors, aes(area = count, fill = color, label = color)) +\n  geom_treemap() +\n  geom_treemap_text() +\n  labs(title = \"Car Sales by Color (Treemap)\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nInterpretation: - Similar to a pie chart, but uses rectangles instead of slices. - The area of each rectangle is proportional to the category’s value. - Useful for displaying hierarchical data, though in this simple example we only have one level. - We can quickly see that Black and White are the top sellers, while Green has the smallest market share.\n\nOrdered Bar Plot (Ordinal Data: Education Levels)\n\n\nggplot(education_levels, aes(x = level, y = count)) +\n  geom_bar(stat = \"identity\", fill = \"lightgreen\") +\n  labs(title = \"Highest Education Level Achieved (Bar Plot)\",\n       x = \"Education Level\", y = \"Number of People\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretation: - Similar to a regular bar plot, but the categories have a meaningful order. - We can see a clear trend: as education level increases, the number of people decreases. - High School graduates are the most common, while PhDs are the least common. - This plot helps visualize the educational attainment distribution in the population.\n\nStacked Bar Plot (Ordinal Data: Education Levels by Group)\n\n\nggplot(education_levels_stacked, aes(x = group, y = count, fill = level)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Education Levels by Group (Stacked Bar Plot)\",\n       x = \"Group\", y = \"Number of People\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretation: - Shows the composition of each group and allows for comparison between groups. - We can compare both the total height of each bar (total people in each group) and the proportion of education levels within each group. - In this example, we see that Group A has more people overall, and a higher proportion of lower education levels compared to Group B.\n\nHeatmap (Ordinal Data: Education Levels Over Time)\n\n\nggplot(education_heatmap_long, aes(x = year, y = level, fill = count)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  labs(title = \"Education Levels Over Time (Heatmap)\",\n       x = \"Year\", y = \"Education Level\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretation: - Colors represent values, with darker colors typically indicating higher values. - Good for showing patterns across two dimensions (in this case, education level and year). - We can see that for all education levels, the count increases over time (colors get darker from left to right). - The gradient is steeper for lower education levels, suggesting faster growth in these categories.\n\nBar Plot (Discrete Data: Family Sizes)\n\n\nggplot(family_sizes, aes(x = factor(children), y = families)) +\n  geom_bar(stat = \"identity\", fill = \"coral\") +\n  labs(title = \"Distribution of Family Sizes (Bar Plot)\",\n       x = \"Number of Children\", y = \"Number of Families\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretation: - Similar to nominal data bar plots, but the x-axis represents discrete numerical values. - We can see the distribution of family sizes. - Families with 1 child are the most common, followed by those with 2 children. - There’s a sharp decline in frequency for families with 3 or more children.\n\nLine Plot (Discrete Data: Family Sizes Over Time)\n\n\nggplot(family_sizes_time, aes(x = year, y = families, group = children, color = factor(children))) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"Family Sizes Over Time (Line Plot)\",\n       x = \"Year\", y = \"Number of Families\", color = \"Number of Children\") +\n  theme_minimal()\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\n\n\n\n\nInterpretation: - Shows trends over time for different categories (family sizes). - Each line represents a family size, allowing us to compare trends across sizes. - We can see that families with 1 child have been increasing over time, while other family sizes have remained relatively stable or decreased slightly.\n\nScatter Plot (Discrete Data: Families by Children and Bedrooms)\n\n\nggplot(family_data, aes(x = children, y = bedrooms, size = families)) +\n  geom_point(alpha = 0.6) +\n  scale_size_continuous(range = c(1, 10)) +\n  labs(title = \"Families by Number of Children and Bedrooms (Scatter Plot)\",\n       x = \"Number of Children\", y = \"Number of Bedrooms\", size = \"Number of Families\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretation: - Each point represents a combination of number of children and number of bedrooms. - The size of the point represents the number of families in that category. - We can see patterns such as larger families tend to have more bedrooms. - The largest points are in the middle of the plot, suggesting that moderate-sized families (2-3 children) in moderate-sized homes (2-3 bedrooms) are most common.\n\nHistogram (Continuous Data: Heights)\n\n\nggplot(heights, aes(x = height)) +\n  geom_histogram(binwidth = 5, fill = \"purple\", color = \"black\") +\n  labs(title = \"Distribution of Heights (Histogram)\",\n       x = \"Height (cm)\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretation: - Shows the distribution of a continuous variable. - The x-axis is divided into bins, and the height of each bar shows how many data points fall into that bin. - We can see that the heights are roughly normally distributed, with a peak around 170 cm. - The spread of the data (how wide the histogram is) gives us an idea of the variability in heights.\n\nBox Plot (Continuous Data: Heights)\n\n\nggplot(heights, aes(y = height)) +\n  geom_boxplot(fill = \"lightblue\") +\n  labs(title = \"Distribution of Heights (Box Plot)\",\n       y = \"Height (cm)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\nInterpretation: - Shows the five-number summary: minimum, first quartile, median, third quartile, and maximum. - The box represents the interquartile range (IQR), with the median as a line in the middle. - Whiskers extend to the min and max values, excluding outliers. - Points beyond the whiskers are potential outliers. - We can quickly see the center, spread, and skewness of the data. - In this case, the median is close to the center of the box, suggesting a relatively symmetric distribution.\n\nDensity Plot (Continuous Data: Heights)\n\n\nggplot(heights, aes(x = height)) +\n  geom_density(fill = \"orange\", alpha = 0.7) +\n  labs(title = \"Distribution of Heights (Density Plot)\",\n       x = \"Height (cm)\", y = \"Density\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretation: - Shows the distribution of a continuous variable as a smooth curve. - The area under the curve sums to 1, representing the entire population. - Peaks in the curve represent concentrations of data points. - We can see that the distribution is roughly symmetric and unimodal (one peak). - Compared to a histogram, a density plot can sometimes reveal subtle features of the distribution more clearly.\n\nQ-Q Plot (Continuous Data: Heights)\n\n\nggplot(heights, aes(sample = height)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(title = \"Q-Q Plot of Heights\",\n       x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretation: - Used to compare a sample distribution to a theoretical distribution (usually normal). - If the points roughly follow the diagonal line, it suggests the data is normally distributed. - Deviations from the line indicate departures from normality. - In this case, the points follow the line quite closely, suggesting that heights are approximately normally distributed. - Any curved patterns or significant deviations would suggest skewness or heavy tails.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: From Pen and Paper to R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#bar-plots",
    "href": "chapter6.html#bar-plots",
    "title": "13  Data Visualization: From Pen and Paper to R",
    "section": "13.2 Bar Plots",
    "text": "13.2 Bar Plots\nBar plots are excellent for displaying categorical data or summarizing continuous data by groups.\n\n13.2.1 Understanding Bar Plots\nA bar plot represents data using rectangular bars with heights proportional to the values they represent. They are used to compare different categories or groups.\nKey components of a bar plot: 1. X-axis: Represents categories 2. Y-axis: Represents values (can be counts, percentages, or any numerical value) 3. Bars: Rectangle for each category, height corresponds to its value\n\n13.2.1.1 Example Data\nLet’s use a simple dataset of fruit sales:\nFruit: Apple, Banana, Orange, Grape\nSales: 120,   85,     70,    100\n\n\n\n13.2.2 Hand-Drawn Bar Plot\nTo create a bar plot by hand:\n\nDraw a horizontal line (x-axis) and a vertical line (y-axis) perpendicular to each other.\nLabel the x-axis with your categories (fruits), evenly spaced.\nLabel the y-axis with a suitable scale for your values (sales, 0 to 120 in increments of 20).\nFor each category, draw a rectangle (bar) whose height corresponds to its value on the y-axis scale.\nColor or shade each bar if desired.\nAdd a title and labels for both axes.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen drawing by hand, use graph paper for more precise measurements and straighter lines. Choose a scale that allows all your data to fit while maximizing the use of space.\n\n\n\n\n13.2.3 Bar Plot in Base R\n\n# Sample data\nfruits &lt;- c(\"Apple\", \"Banana\", \"Orange\", \"Grape\")\nsales &lt;- c(120, 85, 70, 100)\n\n# Create bar plot\nbarplot(sales, names.arg = fruits, \n        main = \"Fruit Sales\",\n        xlab = \"Fruit Types\", ylab = \"Sales\")\n\n\n\n\n\n\n\n\n\n\n13.2.4 Bar Plot with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(fruit = fruits, sales = sales)\n\n# Create bar plot with ggplot2\nggplot(df, aes(x = fruit, y = sales)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Fruit Sales\",\n       x = \"Fruit Types\", y = \"Sales\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n13.2.5 Interpreting Bar Plots\nWhen interpreting a bar plot, consider the following:\n\nRelative Heights: Compare the heights of the bars to understand which categories have higher or lower values.\nOrdering: Sometimes, bars are ordered by height to make comparisons easier.\nPatterns: Look for any patterns or trends across categories.\nOutliers: Identify any bars that are much taller or shorter than the others.\n\n\n13.2.5.1 Example Interpretation\nFor our fruit sales data:\n\nApples have the highest sales (120), followed by Grapes (100).\nOranges have the lowest sales (70).\nThere’s a considerable difference between the highest (Apples) and lowest (Oranges) sales.\nBananas and Grapes have similar sales figures, in the middle range.\n\nThis information could be useful for inventory management or marketing strategies in a fruit shop.\n\n\n\n\n\n\nNote\n\n\n\nBar plots are great for comparing categories, but they don’t show the distribution within each category. For that, you might need other plot types like box plots.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: From Pen and Paper to R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#histograms",
    "href": "chapter6.html#histograms",
    "title": "13  Data Visualization: From Pen and Paper to R",
    "section": "13.3 Histograms",
    "text": "13.3 Histograms\nHistograms visualize the distribution of a continuous variable by dividing it into intervals (bins) and showing the frequency or density of data points in each bin.\n\n13.3.1 Understanding Histograms\nKey components of a histogram: 1. X-axis: Represents the variable’s values, divided into bins 2. Y-axis: Represents frequency, relative frequency, or density 3. Bars: Rectangle for each bin, height corresponds to the y-axis measure\nThere are three main types of histograms:\n\nFrequency Histogram: The y-axis shows the count of data points in each bin.\nRelative Frequency Histogram: The y-axis shows the proportion of data points in each bin (frequency divided by total number of data points).\nDensity Histogram: The y-axis shows the density, which is the relative frequency divided by the bin width. The total area of all bars sums to 1.\n\n\n13.3.1.1 Example Data\nLet’s use a dataset of 50 student exam scores (out of 100):\n\nscores &lt;- c(65, 72, 81, 58, 90, 75, 68, 82, 79, 73, \n            67, 85, 78, 71, 94, 69, 88, 76, 80, 87, \n            63, 92, 70, 83, 77, 86, 74, 89, 66, 95, \n            84, 73, 91, 69, 78, 82, 75, 88, 71, 93, \n            68, 87, 76, 80, 85, 72, 90, 79, 74, 81)\n\n\n\n\n13.3.2 Hand-Drawn Histogram\nTo create a frequency histogram by hand:\n\nFind the range of your data (95 - 58 = 37).\nChoose a number of bins (let’s use 7 bins of width 6).\nCreate a frequency table:\n\n55-60: 1\n61-66: 2\n67-72: 9\n73-78: 11\n79-84: 11\n85-90: 10\n91-96: 6\n\nDraw x and y axes.\nLabel x-axis with bin ranges and y-axis with frequency.\nDraw a rectangle for each bin, with height corresponding to its frequency.\nAdd a title and labels for both axes.\n\nFor a relative frequency histogram, divide each frequency by the total number of data points (50 in this case) before drawing the bars.\nFor a density histogram, divide the relative frequency by the bin width (6 in this case) before drawing the bars.\n\n\n\n\n\n\nTip\n\n\n\nThe number of bins can affect the interpretation. Too few bins may obscure important features, while too many may introduce noise. A common rule of thumb is to use the square root of the number of data points as the number of bins.\n\n\n\n\n13.3.3 Histograms in Base R\n\n# Frequency Histogram\nhist(scores, breaks = 7, \n     main = \"Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Relative Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Relative Frequency\")\n\n\n\n\n\n\n\n# Density Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Density Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Density\")\nlines(density(scores), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n13.3.4 Histograms with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(score = scores)\n\n# Frequency Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nggplot(df, aes(x = score, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Relative Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Relative Frequency\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Density Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Density Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Density\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n13.3.5 Interpreting Histograms\nWhen interpreting a histogram, consider:\n\nCentral Tendency: Where is the peak of the distribution?\nSpread: How wide is the distribution?\nShape: Is it symmetric, skewed, or multi-modal?\nOutliers: Are there any unusual values far from the main distribution?\n\nThe interpretation remains similar for all three types of histograms, but the y-axis values have different meanings:\n\nIn a frequency histogram, you’re looking at actual counts.\nIn a relative frequency histogram, you’re looking at proportions (which sum to 1 across all bins).\nIn a density histogram, the total area of all bars sums to 1, allowing for comparison between datasets of different sizes.\n\n\n13.3.5.1 Example Interpretation\nFor our exam scores data:\n\nCentral Tendency: The peak is around 75-80, suggesting this is the most common score range.\nSpread: Scores range from about 55 to 95, with most falling between 65 and 90.\nShape: The distribution is roughly symmetric, with a slight negative skew (tail extends more to the left).\nOutliers: There don’t appear to be any significant outliers.\n\nThis distribution suggests that the exam was well-designed, with a good spread of scores and no ceiling or floor effects.\n\n\n\n\n\n\nNote\n\n\n\nDensity histograms are particularly useful when comparing distributions of different sizes, as the area under the curve is standardized. The overlaid density curve (red line in our examples) provides a smoothed representation of the distribution.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: From Pen and Paper to R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#box-plots-and-tufte-box-plots",
    "href": "chapter6.html#box-plots-and-tufte-box-plots",
    "title": "13  Data Visualization: From Pen and Paper to R",
    "section": "13.4 Box Plots and Tufte Box Plots",
    "text": "13.4 Box Plots and Tufte Box Plots\nBox plots, also known as box-and-whisker plots, provide a concise summary of a distribution. We’ll focus on the Tufte-style box plot, which emphasizes data over chart elements.\n\n13.4.1 Understanding Box Plots\nA box plot represents five key statistics:\n\nMinimum value\nFirst quartile (Q1)\nMedian\nThird quartile (Q3)\nMaximum value\n\nAdditionally, box plots often show outliers, which are data points that fall far from the rest of the distribution.\n\n13.4.1.1 Calculating Quartiles and Outliers\nTo create a box plot, follow these steps:\n\nOrder your data from smallest to largest.\nFind the median (middle value if odd number of data points, average of two middle values if even).\nFind Q1 (median of lower half of data) and Q3 (median of upper half of data).\nCalculate the Interquartile Range (IQR) = Q3 - Q1\nDetermine outliers using this formula:\n\nLower outliers: &lt; Q1 - 1.5 * IQR\nUpper outliers: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe factor 1.5 in the outlier formula is conventional but can be adjusted based on how strictly you want to define outliers.\n\n\n\n\n13.4.1.2 Example Calculation\nLet’s use a small dataset to illustrate:\nData: 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50\n\nData is already ordered\nMedian = 7 (middle value)\nQ1 = 4 (median of 2, 3, 4, 5, 6) Q3 = 10 (median of 8, 9, 10, 15, 50)\nIQR = 10 - 4 = 6\nOutlier thresholds:\n\nLower: 4 - 1.5 * 6 = -5\nUpper: 10 + 1.5 * 6 = 19\n\n\nTherefore, 50 is an outlier in this dataset.\n\n\n\n13.4.2 Hand-Drawn Tufte Box Plot\nTo create a Tufte box plot by hand:\n\nDraw a vertical line representing the range from minimum to maximum (2 to 15 in our example, excluding the outlier).\nDraw a narrow rectangle from Q1 to Q3 (4 to 10), centered on the vertical line.\nDraw a horizontal line through the rectangle at the median (7).\nRepresent the outlier (50) as an individual point above the maximum.\nAdd a scale to the vertical line and label it.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen drawing by hand, use graph paper for more precise measurements. Choose a scale that allows you to represent your data clearly.\n\n\n\n\n13.4.3 Box Plot in Base R\n\n# Sample data\ndata &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n# Create box plot\nboxplot(data, main = \"Box Plot of Sample Data\",\n        ylab = \"Values\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\n13.4.4 Tufte Box Plot with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(value = data)\n\n# Create Tufte box plot with ggplot2\nggplot(df, aes(x = \"\", y = value)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Tufte Box Plot of Sample Data\",\n       x = \"\", y = \"Values\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n13.4.5 Interpreting Box Plots\nWhen interpreting a box plot, consider the following:\n\nCentral Tendency: The median shows the center of the distribution. If it’s not in the middle of the box, the distribution is skewed.\nSpread: The box (IQR) represents the middle 50% of the data. A larger box indicates more variability.\nSkewness: If the median line is closer to one end of the box, or if one whisker is much longer than the other, the distribution is skewed.\nOutliers: Points beyond the whiskers are potential outliers. They may be data entry errors, or they might represent important extreme values in your dataset.\nComparisons: When comparing multiple box plots, look at relative positions of medians, box sizes, and presence of outliers.\n\n\n13.4.5.1 Example Interpretation\nFor our sample data (2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50):\n\nCentral Tendency: The median (7) is close to the center of the box, suggesting a relatively symmetric distribution for the middle 50% of the data.\nSpread: The box represents values from 4 to 10, showing moderate variability in the central portion of the data.\nSkewness: The upper whisker extends further than the lower whisker, indicating a slight positive skew in the overall distribution.\nOutliers: There’s one clear outlier (50) well above the upper whisker. This value is unusually high compared to the rest of the dataset and warrants investigation.\nOverall Distribution: Most of the data is clustered between 2 and 15, with a fairly even spread, but there’s an unusual high value that stretches the overall range significantly.\n\n\n\n\n\n\n\nNote\n\n\n\nRemember that while box plots provide a good summary, they can hide certain features of the data, such as bimodality. It’s often useful to complement box plots with other visualizations like histograms or density plots for a more complete picture.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: From Pen and Paper to R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#conclusion",
    "href": "chapter6.html#conclusion",
    "title": "13  Data Visualization: From Pen and Paper to R",
    "section": "13.5 Conclusion",
    "text": "13.5 Conclusion\nIn this chapter, we explored three fundamental types of data visualizations: bar plots, histograms, and box plots. We demonstrated how to create these plots by hand, using R’s base plotting system, and using the ggplot2 library.\nEach type of plot serves a different purpose: - Bar plots are excellent for comparing categories. - Histograms show the distribution of a continuous variable. - Box plots provide a concise summary of a distribution, highlighting central tendency, spread, and outliers.\nRemember, the choice of visualization depends on your data type and the insights you want to convey. Always consider your audience and the story you want to tell with your data when selecting and designing your visualizations.\nPractice creating these plots by hand to deepen your understanding of their construction and interpretation. Then, leverage the power of R and ggplot2 to quickly create and customize these visualizations for larger datasets and more complex analyses.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: From Pen and Paper to R</span>"
    ]
  },
  {
    "objectID": "chapter1.html#understanding-spurious-correlations-confounders-and-colliders",
    "href": "chapter1.html#understanding-spurious-correlations-confounders-and-colliders",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.11 Understanding Spurious Correlations, Confounders, and Colliders",
    "text": "1.11 Understanding Spurious Correlations, Confounders, and Colliders\nIn this tutorial, we’ll explore three important concepts in statistical analysis: spurious correlations, confounders, and colliders. Understanding these concepts is crucial for avoiding misinterpretation of data and drawing incorrect conclusions from statistical analyses.\nLet’s start by loading the necessary libraries:\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nset.seed(123) # for reproducibility",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#spurious-correlations",
    "href": "chapter1.html#spurious-correlations",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.12 Spurious Correlations",
    "text": "1.12 Spurious Correlations\nSpurious correlations are relationships between variables that appear to be causal but are actually coincidental or caused by an unseen third factor.\n\n1.12.1 Example: Ice Cream Sales and Drowning Incidents\nLet’s create a dataset that shows a spurious correlation between ice cream sales and drowning incidents:\n\nn &lt;- 100\nspurious_data &lt;- tibble(\n  temperature = rnorm(n, mean = 25, sd = 5),\n  ice_cream_sales = 100 + 5 * temperature + rnorm(n, sd = 10),\n  drowning_incidents = 1 + 0.5 * temperature + rnorm(n, sd = 2)\n)\n\nggplot(spurious_data, aes(x = ice_cream_sales, y = drowning_incidents)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Spurious Correlation: Ice Cream Sales vs. Drowning Incidents\",\n       x = \"Ice Cream Sales\", y = \"Drowning Incidents\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis plot shows a positive correlation between ice cream sales and drowning incidents. However, this relationship is spurious. The real cause for both is the temperature:\n\nggplot(spurious_data, aes(x = temperature)) +\n  geom_point(aes(y = ice_cream_sales), color = \"blue\") +\n  geom_point(aes(y = drowning_incidents * 10), color = \"red\") +\n  geom_smooth(aes(y = ice_cream_sales), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(aes(y = drowning_incidents * 10), method = \"lm\", se = FALSE, color = \"red\") +\n  scale_y_continuous(\n    name = \"Ice Cream Sales\",\n    sec.axis = sec_axis(~./10, name = \"Drowning Incidents\")\n  ) +\n  labs(title = \"Temperature as the Common Cause\",\n       x = \"Temperature\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#confounders",
    "href": "chapter1.html#confounders",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.13 Confounders",
    "text": "1.13 Confounders\nA confounder is a variable that influences both the dependent variable and independent variable, causing a spurious association.\n\n1.13.1 Example: Education, Income, and Age\nLet’s create a dataset where age confounds the relationship between education and income:\n\nn &lt;- 1000\nconfounder_data &lt;- tibble(\n  age = runif(n, 25, 65),\n  education = round(10 + 0.1 * age + rnorm(n, sd = 2)),\n  income = 20000 + 1000 * education + 500 * age + rnorm(n, sd = 5000)\n)\n\n# Without controlling for age\nmodel_naive &lt;- lm(income ~ education, data = confounder_data)\n\n# Controlling for age\nmodel_adjusted &lt;- lm(income ~ education + age, data = confounder_data)\n\n# Visualize\nggplot(confounder_data, aes(x = education, y = income, color = age)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_smooth(aes(group = cut(age, breaks = 3)), method = \"lm\", se = FALSE) +\n  scale_color_viridis_c() +\n  labs(title = \"Education vs Income, Confounded by Age\",\n       x = \"Years of Education\", y = \"Income\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n\nCompare the coefficients:\n\nsummary(model_naive)$coefficients[\"education\", \"Estimate\"]\n\n[1] 2328.718\n\nsummary(model_adjusted)$coefficients[\"education\", \"Estimate\"]\n\n[1] 1101.783\n\n\nThe effect of education on income is overestimated when we don’t control for age.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#colliders",
    "href": "chapter1.html#colliders",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.14 Colliders",
    "text": "1.14 Colliders\nA collider is a variable that is influenced by both the independent variable and the dependent variable. Controlling for a collider can introduce a spurious correlation.\n\n1.14.1 Example: Job Satisfaction, Salary, and Work-Life Balance\nLet’s create a dataset where work-life balance is a collider between job satisfaction and salary:\n\nn &lt;- 1000\ncollider_data &lt;- tibble(\n  job_satisfaction = rnorm(n),\n  salary = rnorm(n),\n  work_life_balance = -0.5 * job_satisfaction - 0.5 * salary + rnorm(n, sd = 0.5)\n)\n\n# Without controlling for work-life balance\nmodel_correct &lt;- lm(salary ~ job_satisfaction, data = collider_data)\n\n# Incorrectly controlling for work-life balance\nmodel_collider &lt;- lm(salary ~ job_satisfaction + work_life_balance, data = collider_data)\n\n# Visualize\nggplot(collider_data, aes(x = job_satisfaction, y = salary, color = work_life_balance)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_color_viridis_c() +\n  labs(title = \"Job Satisfaction vs Salary, Work-Life Balance as Collider\",\n       x = \"Job Satisfaction\", y = \"Salary\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCompare the coefficients:\n\nsummary(model_correct)$coefficients[\"job_satisfaction\", \"Estimate\"]\n\n[1] 0.02063487\n\nsummary(model_collider)$coefficients[\"job_satisfaction\", \"Estimate\"]\n\n[1] -0.4794016\n\n\nControlling for the collider (work-life balance) introduces a spurious correlation between job satisfaction and salary.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#conclusion",
    "href": "chapter1.html#conclusion",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.10 Conclusion",
    "text": "1.10 Conclusion\nWe’ve explored a range of models from deterministic to highly complex stochastic ones. Each type of model has its place in science, depending on the system being studied and the level of uncertainty involved.\nRemember, the choice between deterministic and stochastic models often depends on the nature of the system you’re studying and the questions you’re trying to answer. Deterministic models are great for systems with well-understood mechanics, while stochastic models shine when dealing with inherent randomness or complex, not fully understood systems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#further-reading",
    "href": "chapter1.html#further-reading",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.16 Further Reading",
    "text": "1.16 Further Reading\n\nPearl, J. (2009). Causality: Models, Reasoning, and Inference. Cambridge University Press.\nHernán, M. A., & Robins, J. M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zrozumienie-pozornych-korelacji-zmiennych-zakłócających-i-kolizyjnych",
    "href": "rozdzial1.html#zrozumienie-pozornych-korelacji-zmiennych-zakłócających-i-kolizyjnych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.11 Zrozumienie Pozornych Korelacji, Zmiennych Zakłócających i Kolizyjnych",
    "text": "2.11 Zrozumienie Pozornych Korelacji, Zmiennych Zakłócających i Kolizyjnych\nW tej sekcji zbadamy trzy ważne pojęcia w analizie statystycznej: pozorne korelacje, zmienne zakłócające i zmienne kolizyjne. Zrozumienie tych pojęć jest kluczowe dla uniknięcia błędnej interpretacji danych i wyciągania nieprawidłowych wniosków z analiz statystycznych.\nZacznijmy od załadowania niezbędnych bibliotek:\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nset.seed(123) # dla powtarzalności",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#pozorne-korelacje",
    "href": "rozdzial1.html#pozorne-korelacje",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.12 Pozorne Korelacje",
    "text": "2.12 Pozorne Korelacje\nPozorne korelacje to związki między zmiennymi, które wydają się przyczynowe, ale w rzeczywistości są przypadkowe lub spowodowane przez niewidoczny trzeci czynnik.\n\n2.12.1 Przykład: Sprzedaż lodów a przypadki utonięć\nStwórzmy zbiór danych, który pokazuje pozorną korelację między sprzedażą lodów a przypadkami utonięć:\n\nn &lt;- 100\ndane_pozorne &lt;- tibble(\n  temperatura = rnorm(n, mean = 25, sd = 5),\n  sprzedaz_lodow = 100 + 5 * temperatura + rnorm(n, sd = 10),\n  przypadki_utoniec = 1 + 0.5 * temperatura + rnorm(n, sd = 2)\n)\n\nggplot(dane_pozorne, aes(x = sprzedaz_lodow, y = przypadki_utoniec)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Pozorna Korelacja: Sprzedaż Lodów vs Przypadki Utonięć\",\n       x = \"Sprzedaż Lodów\", y = \"Przypadki Utonięć\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTen wykres pokazuje pozytywną korelację między sprzedażą lodów a przypadkami utonięć. Jednak ta relacja jest pozorna. Prawdziwą przyczyną obu zjawisk jest temperatura:\n\nggplot(dane_pozorne, aes(x = temperatura)) +\n  geom_point(aes(y = sprzedaz_lodow), color = \"blue\") +\n  geom_point(aes(y = przypadki_utoniec * 10), color = \"red\") +\n  geom_smooth(aes(y = sprzedaz_lodow), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(aes(y = przypadki_utoniec * 10), method = \"lm\", se = FALSE, color = \"red\") +\n  scale_y_continuous(\n    name = \"Sprzedaż Lodów\",\n    sec.axis = sec_axis(~./10, name = \"Przypadki Utonięć\")\n  ) +\n  labs(title = \"Temperatura jako Wspólna Przyczyna\",\n       x = \"Temperatura\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zmienne-zakłócające",
    "href": "rozdzial1.html#zmienne-zakłócające",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.13 Zmienne Zakłócające",
    "text": "2.13 Zmienne Zakłócające\nZmienna zakłócająca to zmienna, która wpływa zarówno na zmienną zależną, jak i niezależną, powodując pozorny związek.\n\n2.13.1 Przykład: Edukacja, Dochód i Wiek\nStwórzmy zbiór danych, w którym wiek zakłóca relację między edukacją a dochodem:\n\nn &lt;- 1000\ndane_zaklocajace &lt;- tibble(\n  wiek = runif(n, 25, 65),\n  edukacja = round(10 + 0.1 * wiek + rnorm(n, sd = 2)),\n  dochod = 20000 + 1000 * edukacja + 500 * wiek + rnorm(n, sd = 5000)\n)\n\n# Bez kontrolowania wieku\nmodel_naiwny &lt;- lm(dochod ~ edukacja, data = dane_zaklocajace)\n\n# Kontrolowanie wieku\nmodel_skorygowany &lt;- lm(dochod ~ edukacja + wiek, data = dane_zaklocajace)\n\n# Wizualizacja\nggplot(dane_zaklocajace, aes(x = edukacja, y = dochod, color = wiek)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_smooth(aes(group = cut(wiek, breaks = 3)), method = \"lm\", se = FALSE) +\n  scale_color_viridis_c() +\n  labs(title = \"Edukacja vs Dochód, Zakłócone przez Wiek\",\n       x = \"Lata Edukacji\", y = \"Dochód\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\n\nPorównajmy współczynniki:\n\nsummary(model_naiwny)$coefficients[\"edukacja\", \"Estimate\"]\n\n[1] 2328.718\n\nsummary(model_skorygowany)$coefficients[\"edukacja\", \"Estimate\"]\n\n[1] 1101.783\n\n\nEfekt edukacji na dochód jest przeszacowany, gdy nie kontrolujemy wieku.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zmienne-kolizyjne",
    "href": "rozdzial1.html#zmienne-kolizyjne",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.14 Zmienne Kolizyjne",
    "text": "2.14 Zmienne Kolizyjne\nZmienna kolizyjna to zmienna, na którą wpływają zarówno zmienna niezależna, jak i zmienna zależna. Kontrolowanie zmiennej kolizyjnej może wprowadzić pozorną korelację.\n\n2.14.1 Przykład: Satysfakcja z pracy, Wynagrodzenie i Równowaga między pracą a życiem prywatnym\nStwórzmy zbiór danych, w którym równowaga między pracą a życiem prywatnym jest zmienną kolizyjną między satysfakcją z pracy a wynagrodzeniem:\n\nn &lt;- 1000\ndane_kolizyjne &lt;- tibble(\n  satysfakcja_z_pracy = rnorm(n),\n  wynagrodzenie = rnorm(n),\n  rownowaga_praca_zycie = -0.5 * satysfakcja_z_pracy - 0.5 * wynagrodzenie + rnorm(n, sd = 0.5)\n)\n\n# Bez kontrolowania równowagi praca-życie\nmodel_poprawny &lt;- lm(wynagrodzenie ~ satysfakcja_z_pracy, data = dane_kolizyjne)\n\n# Błędne kontrolowanie równowagi praca-życie\nmodel_kolizyjny &lt;- lm(wynagrodzenie ~ satysfakcja_z_pracy + rownowaga_praca_zycie, data = dane_kolizyjne)\n\n# Wizualizacja\nggplot(dane_kolizyjne, aes(x = satysfakcja_z_pracy, y = wynagrodzenie, color = rownowaga_praca_zycie)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_color_viridis_c() +\n  labs(title = \"Satysfakcja z Pracy vs Wynagrodzenie, Równowaga Praca-Życie jako Zmienna Kolizyjna\",\n       x = \"Satysfakcja z Pracy\", y = \"Wynagrodzenie\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPorównajmy współczynniki:\n\nsummary(model_poprawny)$coefficients[\"satysfakcja_z_pracy\", \"Estimate\"]\n\n[1] 0.02063487\n\nsummary(model_kolizyjny)$coefficients[\"satysfakcja_z_pracy\", \"Estimate\"]\n\n[1] -0.4794016\n\n\nKontrolowanie zmiennej kolizyjnej (równowaga praca-życie) wprowadza pozorną korelację między satysfakcją z pracy a wynagrodzeniem.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#podsumowanie",
    "href": "rozdzial1.html#podsumowanie",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.10 Podsumowanie",
    "text": "2.10 Podsumowanie\nZbadaliśmy szereg modeli, od deterministycznych po wysoce złożone modele stochastyczne. Każdy rodzaj modelu ma swoje miejsce w nauce, w zależności od badanego systemu i poziomu niepewności.\nPamiętaj, że wybór między modelami deterministycznymi a stochastycznymi często zależy od natury badanego systemu i pytań, na które próbujesz odpowiedzieć. Modele deterministyczne są świetne dla systemów o dobrze zrozumiałej mechanice, podczas gdy modele stochastyczne sprawdzają się przy radzeniu sobie z nieodłączną losowością lub złożonymi, nie w pełni zrozumiałymi systemami.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#dalsza-lektura",
    "href": "rozdzial1.html#dalsza-lektura",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.16 Dalsza Lektura",
    "text": "2.16 Dalsza Lektura\n\nPearl, J. (2009). Causality: Models, Reasoning, and Inference. Cambridge University Press.\nHernán, M. A., & Robins, J. M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter1.html#models-in-science-from-deterministic-to-stochastic",
    "href": "chapter1.html#models-in-science-from-deterministic-to-stochastic",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.6 Models in Science: From Deterministic to Stochastic",
    "text": "1.6 Models in Science: From Deterministic to Stochastic\nIn this tutorial, we’ll explore different types of models used in science, ranging from deterministic models often seen in physics to stochastic models common in social sciences and machine learning. We’ll use R to demonstrate some concepts and provide real-world examples.\nA model in science is a simplified representation of a complex system or phenomenon. It’s designed to help us understand, explain, and make predictions about the real world. Models can take various forms, including mathematical equations, computer simulations, or conceptual frameworks. They allow scientists to focus on key aspects of a system while ignoring less relevant details, making complex problems more manageable and easier to study.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#deterministic-models",
    "href": "chapter1.html#deterministic-models",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.7 1. Deterministic Models",
    "text": "1.7 1. Deterministic Models\nDeterministic models are those where the output is fully determined by the parameter values and the initial conditions. These models are often used in physics and engineering.\n\n1.7.1 Example: Simple Harmonic Motion\nA classic example of a deterministic model is simple harmonic motion, described by the equation:\n\\[x(t) = A \\cos(\\omega t + \\phi)\\]\nWhere: - \\(x(t)\\) is the position at time \\(t\\) - \\(A\\) is the amplitude - \\(\\omega\\) is the angular frequency - \\(\\phi\\) is the phase\nLet’s simulate this in R:\n\n# Simple Harmonic Motion\nsim_harmonic_motion &lt;- function(A, omega, phi, t) {\n  A * cos(omega * t + phi)\n}\n\n# Generate data\nt &lt;- seq(0, 10, by = 0.1)\nx &lt;- sim_harmonic_motion(A = 1, omega = 2*pi, phi = 0, t = t)\n\n# Plot\nplot(t, x, type = \"l\", xlab = \"Time\", ylab = \"Position\", \n     main = \"Simple Harmonic Motion\")\n\n\n\n\n\n\n\n\nThis will produce a plot of a simple harmonic oscillation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#stochastic-models-in-social-sciences",
    "href": "chapter1.html#stochastic-models-in-social-sciences",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.8 2. Stochastic Models in Social Sciences",
    "text": "1.8 2. Stochastic Models in Social Sciences\nStochastic models incorporate randomness and are often used in social sciences where there’s inherent uncertainty in the systems being studied.\n\n1.8.1 Example: Ordinary Least Squares (OLS) Regression\nOLS is a fundamental stochastic model in social sciences. It’s represented as:\n\\[Y = \\beta_0 + \\beta_1X + \\epsilon\\]\nWhere: - \\(Y\\) is the dependent variable - \\(X\\) is the independent variable - \\(\\beta_0\\) and \\(\\beta_1\\) are parameters - \\(\\epsilon\\) is the error term (stochastic component)\nLet’s demonstrate OLS in R:\n\n# Generate some sample data\nset.seed(123)\nX &lt;- rnorm(100)\nY &lt;- 2 + 3*X + rnorm(100, sd = 0.5)\n\n# Fit OLS model\nmodel &lt;- lm(Y ~ X)\n\n# Summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95367 -0.34175 -0.04375  0.29032  1.64520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.94860    0.04878   39.95   &lt;2e-16 ***\nX            2.97376    0.05344   55.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4854 on 98 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.969 \nF-statistic:  3097 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n# Plot\nplot(X, Y, main = \"OLS Regression\")\nabline(model, col = \"red\")\n\n\n\n\n\n\n\n\nThis will fit an OLS model to some simulated data and plot the results.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#advanced-stochastic-models-large-language-models",
    "href": "chapter1.html#advanced-stochastic-models-large-language-models",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.9 3. Advanced Stochastic Models: Large Language Models",
    "text": "1.9 3. Advanced Stochastic Models: Large Language Models\nLarge Language Models (LLMs) like GPT-3 are complex stochastic models used in natural language processing. While we can’t implement a full LLM in this tutorial, we can discuss its principles.\nLLMs are based on the transformer architecture and use self-attention mechanisms. They’re trained on vast amounts of text data and learn to predict the next token in a sequence.\nThe core of an LLM can be thought of as a conditional probability distribution:\n\\[P(x_t | x_{&lt;t}, \\theta)\\]\nWhere: - \\(x_t\\) is the current token - \\(x_{&lt;t}\\) represents all previous tokens - \\(\\theta\\) are the model parameters\nUnlike deterministic models, LLMs produce different outputs even for the same input due to their stochastic nature.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#conclusion-1",
    "href": "chapter1.html#conclusion-1",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.15 Conclusion",
    "text": "1.15 Conclusion\nUnderstanding spurious correlations, confounders, and colliders is crucial for proper statistical analysis and causal inference. Always consider the underlying causal structure of your data and be cautious about which variables you control for in your analyses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#modele-w-nauce-od-deterministycznych-do-stochastycznych",
    "href": "rozdzial1.html#modele-w-nauce-od-deterministycznych-do-stochastycznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.6 Modele w Nauce: Od Deterministycznych do Stochastycznych",
    "text": "2.6 Modele w Nauce: Od Deterministycznych do Stochastycznych\nW tej sekcji omówimy różne rodzaje modeli stosowanych w nauce, od modeli deterministycznych często spotykanych w fizyce po modele stochastyczne powszechne w naukach społecznych i uczeniu maszynowym. Użyjemy R do zademonstrowania niektórych koncepcji i podamy rzeczywiste przykłady.\nModel w nauce to uproszczona reprezentacja złożonego systemu lub zjawiska. Jest on zaprojektowany, aby pomóc nam zrozumieć, wyjaśnić i przewidywać zjawiska zachodzące w rzeczywistym świecie. Modele mogą przybierać różne formy, w tym równania matematyczne, symulacje komputerowe lub ramy koncepcyjne. Pozwalają naukowcom skupić się na kluczowych aspektach systemu, ignorując mniej istotne szczegóły, co sprawia, że złożone problemy stają się łatwiejsze do zarządzania i badania.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#modele-deterministyczne",
    "href": "rozdzial1.html#modele-deterministyczne",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.7 1. Modele Deterministyczne",
    "text": "2.7 1. Modele Deterministyczne\nModele deterministyczne to te, w których wynik jest w pełni określony przez wartości parametrów i warunki początkowe. Modele te są często używane w fizyce i inżynierii.\n\n2.7.1 Przykład: Ruch Harmoniczny Prosty\nKlasycznym przykładem modelu deterministycznego jest ruch harmoniczny prosty, opisany równaniem:\n\\[x(t) = A \\cos(\\omega t + \\phi)\\]\nGdzie: - \\(x(t)\\) to położenie w czasie \\(t\\) - \\(A\\) to amplituda - \\(\\omega\\) to częstość kątowa - \\(\\phi\\) to faza\nZasymulujmy to w R:\n\n# Ruch Harmoniczny Prosty\nsymuluj_ruch_harmoniczny &lt;- function(A, omega, phi, t) {\n  A * cos(omega * t + phi)\n}\n\n# Generowanie danych\nt &lt;- seq(0, 10, by = 0.1)\nx &lt;- symuluj_ruch_harmoniczny(A = 1, omega = 2*pi, phi = 0, t = t)\n\n# Wykres\nplot(t, x, type = \"l\", xlab = \"Czas\", ylab = \"Położenie\", \n     main = \"Ruch Harmoniczny Prosty\")\n\n\n\n\n\n\n\n\nTo wygeneruje wykres prostego ruchu harmonicznego.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#modele-stochastyczne-w-naukach-społecznych",
    "href": "rozdzial1.html#modele-stochastyczne-w-naukach-społecznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.8 2. Modele Stochastyczne w Naukach Społecznych",
    "text": "2.8 2. Modele Stochastyczne w Naukach Społecznych\nModele stochastyczne uwzględniają losowość i są często używane w naukach społecznych, gdzie istnieje nieodłączna niepewność w badanych systemach.\n\n2.8.1 Przykład: Regresja Metodą Najmniejszych Kwadratów (OLS)\nOLS to podstawowy model stochastyczny w naukach społecznych. Jest reprezentowany jako:\n\\[Y = \\beta_0 + \\beta_1X + \\epsilon\\]\nGdzie: - \\(Y\\) to zmienna zależna - \\(X\\) to zmienna niezależna - \\(\\beta_0\\) i \\(\\beta_1\\) to parametry - \\(\\epsilon\\) to składnik błędu (komponent stochastyczny)\nZademonstrujmy OLS w R:\n\n# Generowanie przykładowych danych\nset.seed(123)\nX &lt;- rnorm(100)\nY &lt;- 2 + 3*X + rnorm(100, sd = 0.5)\n\n# Dopasowanie modelu OLS\nmodel &lt;- lm(Y ~ X)\n\n# Podsumowanie modelu\nsummary(model)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95367 -0.34175 -0.04375  0.29032  1.64520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.94860    0.04878   39.95   &lt;2e-16 ***\nX            2.97376    0.05344   55.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4854 on 98 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.969 \nF-statistic:  3097 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n# Wykres\nplot(X, Y, main = \"Regresja OLS\")\nabline(model, col = \"red\")\n\n\n\n\n\n\n\n\nTo dopasuje model OLS do symulowanych danych i wykreśli wyniki.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zaawansowane-modele-stochastyczne-duże-modele-językowe",
    "href": "rozdzial1.html#zaawansowane-modele-stochastyczne-duże-modele-językowe",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.9 3. Zaawansowane Modele Stochastyczne: Duże Modele Językowe",
    "text": "2.9 3. Zaawansowane Modele Stochastyczne: Duże Modele Językowe\nDuże Modele Językowe (LLM), takie jak GPT-3, to złożone modele stochastyczne używane w przetwarzaniu języka naturalnego. Chociaż nie możemy zaimplementować pełnego LLM w tym tutorialu, możemy omówić jego zasady.\nLLM opierają się na architekturze transformatora i wykorzystują mechanizmy samouwagi. Są trenowane na ogromnych ilościach danych tekstowych i uczą się przewidywać następny token w sekwencji.\nRdzeń LLM można postrzegać jako warunkowy rozkład prawdopodobieństwa:\n\\[P(x_t | x_{&lt;t}, \\theta)\\]\nGdzie: - \\(x_t\\) to aktualny token - \\(x_{&lt;t}\\) reprezentuje wszystkie poprzednie tokeny - \\(\\theta\\) to parametry modelu\n\n\n\n\n\n\nTip\n\n\n\nTokeny w Dużych Modelach Językowych (LLM) to podstawowe jednostki tekstu, które model przetwarza. Można je postrzegać jako części słów lub znaki interpunkcyjne. Oto kluczowe informacje o tokenach:\nDefinicja: Tokeny to najmniejsze jednostki tekstu, które LLM przetwarza. Mogą to być całe słowa, części słów, a nawet pojedyncze znaki lub znaki interpunkcyjne. Tokenizacja: Proces dzielenia tekstu na tokeny nazywa się tokenizacją. LLM używają specyficznych algorytmów do wykonania tego zadania. Przykłady:\nSłowo “kot” może być pojedynczym tokenem. Dłuższe słowo jak “zrozumienie” może być podzielone na wiele tokenów, np. “zrozum” i “ienie”. Znaki interpunkcyjne jak “.” czy “?” są często oddzielnymi tokenami. Powszechne przedrostki lub przyrostki mogą być własnymi tokenami.\nSłownictwo: LLM mają ustalone słownictwo tokenów, które rozpoznają. To słownictwo zazwyczaj obejmuje od dziesiątek tysięcy do setek tysięcy tokenów. Znaczenie: Sposób tokenizacji tekstu może wpływać na to, jak model rozumie i generuje język. Jest to szczególnie ważne przy obsłudze różnych języków, rzadkich słów lub specjalistycznego słownictwa. Kontekst: W równaniu dla LLM: \\[P(x_t | x_{&lt;t}, \\theta)\\] Gdzie:\n\\(x_t\\) reprezentuje bieżący token \\(x_{&lt;t}\\) reprezentuje wszystkie poprzednie tokeny w sekwencji \\(\\theta\\) reprezentuje parametry modelu\n\n\nW przeciwieństwie do modeli deterministycznych, LLM produkują różne wyniki nawet dla tego samego wejścia ze względu na ich stochastyczną naturę.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#podsumowanie-1",
    "href": "rozdzial1.html#podsumowanie-1",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.15 Podsumowanie",
    "text": "2.15 Podsumowanie\nZrozumienie pozornych korelacji, zmiennych zakłócających i kolizyjnych jest kluczowe dla prawidłowej analizy statystycznej i wnioskowania przyczynowego. Zawsze rozważ podstawową strukturę przyczynową swoich danych i bądź ostrożny w kwestii tego, które zmienne kontrolujesz w swoich analizach.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  }
]