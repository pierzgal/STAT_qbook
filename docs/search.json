[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Data Analysis: An Introduction (Wprowadzenie do analizy danych społecznych)",
    "section": "",
    "text": "Preface\n\n\n\n\n\n\nImportant\n\n\n\nThis is a preliminary draft of a Quarto class notes on social data analysis. Please do not cite or reproduce its contents, as it may contain errors!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "",
    "text": "1.1 What is Data Science?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#what-is-data-science",
    "href": "chapter1.html#what-is-data-science",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "",
    "text": "Important\n\n\n\nStatistics and Data Science are The Art and Science of Learning from Data.\n\n\n\nData science and statistics are powerful tools that help us understand complex phenomena across various social sciences, including political science, economics, and sociology. These complementary fields provide researchers and practitioners with the means to analyze trends, behaviors, and outcomes in society, offering insights that can shape policy and advance our understanding of human interaction.\nStatistics provides the mathematical foundation for analyzing societal trends and outcomes, offering methods for designing studies, summarizing data, and making inferences. Data science expands on this foundation by incorporating computational methods and domain expertise to handle larger datasets and perform more complex analyses.\nTogether, these disciplines allow us to collect and process large datasets, visualize complex information, uncover patterns in social interactions, evaluate policy impacts, and support evidence-based decision-making. Their applications are vast and varied, from studying voting patterns and analyzing economic indicators to researching social inequalities and examining human behavior.\nAs our world becomes increasingly data-driven, the importance of data science and statistics in social sciences continues to grow.\n\n\n\n\n\n\n\nNote\n\n\n\nIn social sciences, data science combines statistical methods, computational tools, and domain expertise to analyze complex social phenomena and human behavior.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#the-relationship-between-statistics-and-data-science",
    "href": "chapter1.html#the-relationship-between-statistics-and-data-science",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.2 The Relationship Between Statistics and Data Science",
    "text": "1.2 The Relationship Between Statistics and Data Science\nStatistics and data science are closely interrelated fields with significant overlap, especially in social sciences. Rather than strict divisions, it’s more accurate to view them as complementary approaches on a continuum:\n\nTraditional StatisticsModern Data ScienceEvolving Landscape\n\n\n\nRooted in mathematical theories and methods for data analysis\nEmphasizes statistical inference, hypothesis testing, and probability theory\nHistorically central to social sciences for analyzing surveys, experiments, and observational studies\n\n\n\n\nIntegrates statistical methods with computer science and domain expertise\nExpands focus to include machine learning, big data processing, and predictive modeling\nIn social sciences, often tackles large-scale digital data and complex behavioral datasets\n\n\n\n\nBoundaries between statistics and data science are increasingly blurred\nMany techniques and tools are shared across both fields\nSocial scientists often combine traditional statistical approaches with newer data science methods\nThe choice of approach depends on research questions, data characteristics, and specific analytical needs\n\n\n\n\nData science can be seen as an evolution and expansion of traditional statistics, incorporating new technologies and methodologies to handle larger and more complex social science datasets.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#essential-concepts-in-data-science-and-statistics",
    "href": "chapter1.html#essential-concepts-in-data-science-and-statistics",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.3 Essential Concepts in Data Science and Statistics",
    "text": "1.3 Essential Concepts in Data Science and Statistics\n\n1.3.1 Population, Sample and related concepts\n\n\n\n\n\n\nImportant\n\n\n\n\nData: Observations or measurements collected from a sample or population.\nPopulation: The entire set of individuals or items under study at a specific time.\n\nExample: All eligible voters in a country during a specific election year.\n\nSample: A subset of the population that is actually measured. A representative sample is a subset of a larger population that accurately reflects the characteristics of that population. The sample should mirror the population in terms of important traits like age, gender, socioeconomic status, etc. Often uses random sampling methods to avoid bias. Large enough to be statistically significant, but smaller than the full population.\n\nExample: 1500 randomly selected eligible voters surveyed in a pre-election poll.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nData Generating Process (DGP) and Superpopulation: Expanding on Traditional Concepts\nIn traditional statistics, we often work with two key concepts:\n\nPopulation: The entire group we want to study.\nSample: A subset of the population that we actually observe and analyze.\n\nWhile these concepts are fundamental, modern research often requires us to think beyond this dichotomy. This is where Data Generating Process (DGP) and superpopulation come in, extending our understanding of data and populations.\nData Generating Process (DGP):\nThe DGP is the underlying mechanism that produces the data we observe in the real world, whether in our sample or the entire population.\nIntuitive explanation: Think of the DGP as a complex system that takes various inputs and produces observable outcomes. It’s the “black box” that transforms causes into effects, not just for our sample, but for the entire population and beyond.\nExample: Consider a study on voter behavior. The traditional approach might define the population as “all registered voters” and take a sample from this group. The DGP, however, would include factors like demographic characteristics, economic conditions, political events, and media influence that shape voting behavior for all voters, sampled or not.\nSuperpopulation:\nThe superpopulation is a theoretical concept that extends beyond both the sample and the observable population to include all potential outcomes that could occur under similar conditions or processes.\nExamples:\n\nTraditional approach vs. Superpopulation approach:\n\nTraditional: population (all registered voters in a state), sample (1000 surveyed voters)\nSuperpopulation: All possible voters and voting scenarios, including future elections and hypothetical political contexts\n\nWhen sample equals population:\nIn studies of all 50 U.S. states:\n\nTraditional view: No distinction between sample and population\nSuperpopulation view: Considers these 50 states as a “sample” from a theoretical set of all possible state-policy interactions\n\n\nReal-world application: Let’s say researchers are studying the impact of a new urban planning policy across several cities:\n\nTraditional approach:\n\nPopulation: All cities in the country\nSample: The cities included in the study\n\nSuperpopulation approach:\n\nObserved data: The cities in the study\nSuperpopulation: All cities (existing or potential) where similar urban planning principles could be applied\n\n\nThe DGP in this case would be the complex set of factors that determine how urban planning policies affect city outcomes, applicable not just to the sampled cities or even all existing cities, but to the broader concept of “city” itself.\nImportant considerations:\n\nScope and limitations: Researchers should clearly define what units or processes they’re trying to understand, beyond just describing their sample and population.\nGeneralizability: When making claims about a superpopulation, researchers should explicitly state the “scope conditions” - the boundaries within which their findings are expected to hold true.\nContext-specificity: While the superpopulation concept allows for broader inferences than traditional sampling, it’s important to recognize that DGPs can vary across different contexts.\n\nBy incorporating these concepts alongside traditional population-sample thinking, researchers can make more nuanced inferences and be more transparent about the extent to which their findings might apply beyond their specific observed data, while still respecting the foundational principles of statistical inference.\nSummary example: Pizza Quality in New York City\nPopulation: All pizzerias currently operating in New York City. This is a finite, countable group of establishments that exist at the present moment.\nSample: A selection of 50 pizzerias chosen randomly from different boroughs of New York City. These are the specific pizzerias where researchers will taste and rate pizzas.\nSuperpopulation: All possible pizzerias that could exist in New York City, including:\n\nCurrent pizzerias\nFuture pizzerias that haven’t opened yet\nPizzerias that have closed down\nHypothetical pizzerias that might exist under different economic or cultural conditions\n\nThe superpopulation concept allows us to think about pizza quality beyond just the current snapshot of New York pizzerias.\nData Generating Process (DGP): The DGP is the complex set of factors that contribute to the quality of pizza in any given pizzeria. This might include:\n\nIngredients: Quality and source of flour, tomatoes, cheese, etc.\nChef’s skill: Training, experience, and personal touch of the pizza maker\nEquipment: Type and condition of the oven, tools used\nRecipe: Proportions of ingredients, preparation methods\nEnvironmental factors: Humidity, water quality in New York\nCultural influences: Local pizza-making traditions, customer preferences\nEconomic factors: Cost of ingredients, rent prices affecting business decisions\n\nThe DGP is like the “pizza quality recipe” that applies not just to our sample or even the current population, but to all potential pizzerias in the superpopulation.\nIntuitive Breakdown:\n\nIf you visit all current NYC pizzerias and rate them, you’ve assessed the population.\nIf you randomly select 50 pizzerias to visit and rate, you’ve taken a sample.\nIf you consider how pizza quality might vary in all possible NYC pizzerias (past, present, future, and hypothetical), you’re thinking about the superpopulation.\nIf you’re trying to understand all the factors that go into making a quality pizza in NYC, regardless of whether a pizzeria currently exists or not, you’re exploring the Data Generating Process.\n\n\n\n\n\n\n\n\ngraph TD\n    A[Data Generating Process DGP]\n    B(Population)\n    C[Sample]\n    A --&gt;|Generates| B\n    B --&gt;|Sampled from| C\n    C -.-&gt;|Inference| B\n    C -.-&gt;|Inference| A\n    B -.-&gt;|Inference| A\n    \n    classDef dgp fill:#f9f,stroke:#333,stroke-width:4px;\n    classDef pop fill:#bbf,stroke:#333,stroke-width:2px;\n    classDef sam fill:#bfb,stroke:#333,stroke-width:2px;\n    \n    class A dgp;\n    class B pop;\n    class C sam;\n\n\n\n\n\n\n\n\n\n\n\n\nExplanation of the DGP, Population, and Sample Diagram\n\n\n\nThis diagram illustrates the relationships between the Data Generating Process (DGP), population, and sample, including paths of inference:\n\nDirect relationships (solid arrows):\n\nThe DGP generates the population\nSamples are drawn from the population\n\nInference paths (dashed arrows):\n\nFrom Sample to Population: Traditional statistical inference\nFrom Sample to DGP: Inferring about the underlying process from sample data\nFrom Population to DGP: Inferring about the DGP using complete population data\n\n\nFor example, in our study on the effect of electoral rules on voter turnout in Polish municipalities (1998-2010 municipal elections):\n\nWe have data for the entire population of municipalities, so we don’t need to infer from a sample to the population.\nOur focus is on using the complete population data (rightmost dashed arrow) to make inferences about the underlying DGP—the complex processes by which electoral rules influence voter turnout across municipalities.\nThis approach allows us to potentially understand the mechanisms behind how different electoral systems (e.g., proportional representation vs. plurality vote) affect turnout rates, and make informed predictions about how changes in electoral rules might impact future turnout or how these effects might generalize to similar contexts.\n\n\n\n\n\n\nPopulation vs. sample. Retrieved from: https://allmodelsarewrong.github.io/mse.html\n\n\nData forms the foundation of statistical analysis. It can be:\n\nPrimary data: Collected firsthand for a specific purpose\nSecondary data: Obtained from existing sources\n\nExample: In a study of university students’ heights, the population is all university students in the country, while a sample might be 1000 randomly selected students.\n\n\n1.3.2 Variables and Constants\nVariables are characteristics that can take different values across a dataset. They can be:\n\nQuantitative (numeric):\n\nContinuous: Height, weight, temperature\nDiscrete: Number of children, count of errors in a program\n\nQualitative (categorical):\n\nNominal: Blood type, eye color\nOrdinal: Education level, customer satisfaction rating\n\n\nConstants are fixed values that remain unchanged throughout an analysis.\n\n1.3.2.1 Types of Data in Social Sciences\nSocial science research deals with various types of data:\n\nQuantitative Data: Numerical data (e.g., survey responses, economic indicators)\nQualitative Data: Non-numerical data (e.g., interview transcripts, open-ended survey responses)\nBig Data: Large-scale digital traces (e.g., social media posts, online behavior logs)\n\n\n\n\n1.3.3 Population Parameters and Estimands\nPopulation parameters are numerical characteristics of a population. Key points:\n\nThey describe the entire population, not just a sample.\nThey are usually denoted by Greek letters.\nIn most cases, they cannot be directly calculated because we can’t measure the entire population.\nThey are determined by the underlying Data Generating Process (DGP).\n\nCommon population parameters include:\n\nPopulation mean (\\mu): The average value of a variable in the population.\nPopulation variance (\\sigma^2): A measure of variability in the population.\nPopulation proportion (p): The proportion of individuals in the population with a certain characteristic.\n\nAn estimand is the target of estimation - the specific population parameter or function of parameters that we aim to estimate. It defines what we want to know about the population.\n\n\n\n\n\n\nExample: Height of University Students\n\n\n\nConsider the height of all university students in a country:\n\n\\mu (estimand): The true average height of all university students (population mean)\n\\sigma^2 (estimand): The true variance of heights in the population\n\nThese parameters are unknown estimands that we aim to estimate using sample data.\n\n\n\n\n1.3.4 Statistic(s) and Estimators\nA statistic (singular) or sample statistic is any quantity computed from values in a sample, which is considered for a statistical purpose.\nWhen a statistic is used for estimating an estimand (population parameter), it is called an estimator. Estimators are functions of sample data that provide approximate values for unknown population parameters.\nExamples of statistics/estimators:\n\nSample mean: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i (estimates \\mu)\nSample variance: s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2 (estimates \\sigma^2)\nSample proportion: \\hat{p} = \\frac{x}{n} (estimates p)\n\n\n\n1.3.5 Estimates\nAn estimate is the specific value obtained by applying an estimator to a particular sample. It is a point value that approximates the true estimand (population parameter).\nExample: If we calculate a sample mean height of 68 inches from our data, then 68 inches is our estimate of the estimand \\mu (population mean height).\n\n\n1.3.6 Statistical Models\n\n\n\n\n\n\nNote\n\n\n\nA model in science is a simplified representation of a complex system or phenomenon. It’s designed to help us understand, explain, and make predictions about the real world. Models can take various forms, including mathematical equations, computer simulations, or conceptual frameworks. They allow scientists to focus on key aspects of a system while ignoring less relevant details, making complex problems more manageable and easier to study.\n\n\nStatistical models represent relationships between variables and help in making predictions or inferences about estimands (population parameters).\nExample: A linear regression model y = \\beta_0 + \\beta_1x + \\epsilon describes the relationship between an independent variable x and a dependent variable y, where:\n\ny is the dependent variable (e.g. quantity demanded)\nx is the independent variable (e.g. price, income level of the consumer)\n\\beta_0 and \\beta_1 are parameters, estimands to be estimated\n\\epsilon is the error term, representing unexplained variation\n\nI’ll help you create a callout note about causal inference and counterfactuals for Quarto.\n\n\n\n\n\n\nCausal Inference and Counterfactuals\n\n\n\nIn social sciences, we often want to understand what would have happened if we had done something differently - this hypothetical scenario is called a counterfactual. For instance:\n\nWhat would a person’s income be if they had attended college vs. if they hadn’t?\nHow would voter turnout change if voting was mandatory?\n\nSince we can’t observe both scenarios simultaneously, statistical models help us estimate these counterfactuals by: 1. Controlling for confounding variables 2. Comparing similar groups that differ only in the treatment 3. Using techniques like propensity score matching or instrumental variables\n\n\n\nFundamental problem of causal inference: We can think of causal inference as a PREDICTION problem. How could we predict the counterfactual given that we never observe it?\n\n\nRemember: Correlation ≠ Causation, but careful research design and statistical methods can help us make causal claims.\n\n\n\nConfounding bias and spurious correlation (https://www.bradyneal.com/causal-inference-course) drinking the night before is a common cause of sleeping with shoes on and waking up with a headache :-)\n\n\n\n\n\nReverse causality: https://ff13.fastforwardlabs.com/\n\n\n\n\nThis creates a note-type callout in Quarto that explains the concept succinctly while highlighting key points about counterfactuals and their estimation.\n\n\n1.3.7 Inference\nStatistical inference is the process of drawing conclusions about estimands (population parameters) based on sample data. It involves two main types:\n\nEstimation: Using sample statistics (estimators) to estimate estimands (population parameters)\nHypothesis testing: Making decisions about estimands based on sample evidence\n\n\n\n\n\n\n\nEstimation and Hypothesis Testing\n\n\n\n\nEstimation\n\nEstimation is about determining the likely value of a population parameter based on sample data. In the context of a binomial distribution, we might be interested in estimating the probability of success (p) for a certain event.\nExample: Coin Flipping\nLet’s say we’re flipping a coin 100 times and want to estimate the probability of getting heads.\n\nWe flip the coin 100 times and observe 55 heads.\nOur point estimate for p (probability of heads) would be 55/100 = 0.55\nWe might also calculate a confidence interval, e.g., a 95% confidence interval might be (0.45, 0.65).\n\nThe confidence interval tells us a range where we think the true probability might lie. In plain English, this means: “We’re 95% confident that the true probability of getting heads is between 45% and 65%.”\nThe goal here is to provide our best guess of the true probability of heads, along with a range of plausible values.\nImportant Concepts in Estimation:\n\nBias\n\nBias refers to the tendency of an estimator to systematically overestimate or underestimate the true value of a population parameter (estimand).\n\nAn unbiased estimator is one whose average value (when estimation is repeated multiple times) equals the true value of the parameter.\nBias can be understood as the difference between the average value of the estimator and the true value of the parameter.\n\n\nEfficiency\n\nEfficiency refers to the precision of an estimator. A more efficient estimator produces results closer to the true parameter value, i.e., it has less dispersion in its results.\n\nIt is most often measured by the variance of the estimator (lower variance means higher efficiency)\nFor unbiased estimators, efficiency is often compared using Mean Squared Error (MSE)\n\n\nHypothesis Testing\n\nHypothesis testing, on the other hand, is about making a decision between two competing claims about a population parameter. We typically have a null hypothesis (H0) and an alternative hypothesis (H1).\nExample: Is the Coin Fair?\nUsing the same coin-flipping scenario, let’s say we want to test if the coin is fair (p = 0.5) or biased towards heads (p &gt; 0.5).\n\nNull hypothesis (H0): p = 0.5 (the coin is fair)\nAlternative hypothesis (H1): p &gt; 0.5 (the coin is biased towards heads)\nWe observe 55 heads out of 100 flips\n\nIntroducing p-values and “Probabilistic Proof by Contradiction”\nNow, let’s dive into the concept of p-values and how hypothesis testing works as a kind of “probabilistic proof by contradiction”:\n\nWe start by assuming the null hypothesis (H0) is true. In this case, we assume the coin is fair.\nWe then ask: “If the coin were truly fair, how likely would it be to observe 55 or more heads out of 100 flips?”\nThis probability is called the p-value. It’s the probability of observing our data (or more extreme data) assuming the null hypothesis is true.\nIf this probability (the p-value) is very small, we have a contradiction: we’ve observed something that should be very rare if our assumption (H0) were true.\nWe typically set a threshold called the significance level (often 0.05 or 5%) for what we consider “very small.”\nIf the p-value is less than our chosen significance level, we reject H0. We conclude that our observation is too unlikely under H0, so we favor the alternative hypothesis instead.\nIf the p-value is greater than our significance level, we fail to reject H0. We don’t have enough evidence to conclude the coin is biased.\n\nThis process is like a “probabilistic proof by contradiction” because:\n\nWe start by assuming H0 (like assuming the opposite of what we want to prove in a proof by contradiction).\nWe see if this assumption leads to a very unlikely situation (our observed data).\nIf it does, we reject the assumption (H0) and favor the alternative.\n\nThe p-value quantifies exactly how unlikely our observation is under H0. A very small p-value (like 0.01) means: “If H0 were true, we’d only expect to see data this extreme about 1% of the time.”\nHypothesis testing and estimation are related but distinct statistical procedures; hypothesis testing can be used to make inferences about estimates and can complement estimation in several ways, e.g.:\n\nTesting Point Estimates: Hypothesis testing can be used to evaluate whether a point estimate is significantly different from a hypothesized value. For example, if we estimate that a coin has a 0.55 probability of landing heads, we could use a hypothesis test to determine if this is significantly different from 0.5 (a fair coin).\nParameter Significance: In multivariate models, hypothesis tests (like t-tests in regression) can help determine which estimated parameters are significantly different from zero, providing insight into which variables are important in the model.\n\n\n\n\n\n1.3.8 Relationships Between Concepts\n\nThe Data Generating Process (DGP) determines the actual values of population parameters (estimands).\nEstimands are estimated using statistics calculated from the sample (estimators).\nThe quality of estimators is assessed based on properties such as bias and efficiency in estimating the estimand.\nStatistical models use estimated parameters to describe relationships between variables in the population.\nStatistical inference involves drawing conclusions about estimands based on sample data, utilizing the properties of estimators.\n\n\n\n\n\n\n\nExample: Studying Voting Behavior\n\n\n\n\nPopulation: All eligible voters in a country\nEstimand: p = true proportion of voters supporting a given candidate\nSample: 1000 randomly selected eligible voters\nEstimator: \\hat{p} = proportion of voters in the sample supporting the candidate\nEstimate: Specific value of \\hat{p} calculated from the sample (e.g., 0.52)\nDGP: Complex interaction of factors influencing voting decisions, such as political beliefs, economic conditions, media exposure, and social networks.\n\nUnderstanding the DGP helps researchers interpret why the estimand p has a certain value and how it might change over time. For example, a sudden change in the economy might affect voters’ preferences, thereby changing the value of p.\nBias and efficiency in the context of the example:\n\nIf \\hat{p} is an unbiased estimator, it means that when the survey is repeated multiple times with different samples, the average value of \\hat{p} will be close to the true value of p.\nThe efficiency of \\hat{p} determines how dispersed the results of individual surveys are around this average. The less dispersion, the more efficient the estimator.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#core-components-of-data-science-in-scientific-research",
    "href": "chapter1.html#core-components-of-data-science-in-scientific-research",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.4 Core Components of Data Science in Scientific Research",
    "text": "1.4 Core Components of Data Science in Scientific Research\n\nData CollectionData ProcessingExploratory Data Analysis (EDA)Statistical InferenceMachine LearningData Visualization and CommunicationReproducibility and Open Science\n\n\n\nExperimental methods: Controlled studies where researchers manipulate variables to observe effects\nObservational studies: Gathering data by watching and recording without interfering\nSurveys and interviews: Collecting information directly from people through questions\nDigital data collection: Gathering data from online sources, sensors, or computer systems\nEthical considerations: Ensuring research respects participants’ rights and well-being\n\n\n\n\nData cleaning: Removing errors and inconsistencies from raw data\nHandling missing values: Addressing gaps in the dataset that could affect analysis\nData transformation: Converting data into formats suitable for analysis, like changing text to numbers\n\n\n\n\nDescriptive statistics: Summarizing data with measures like mean, median, and standard deviation\nData visualization: Creating graphs and charts to visually represent data patterns\nPattern identification: Discovering trends or relationships in the data\n\n\n\n\nHypothesis testing: Using data to evaluate claims about populations\nRegression analysis: Examining relationships between variables and making predictions\nCausal inference: Determining if one variable directly influences another\n\n\n\n\nSupervised learning: Training models to predict outcomes using data with known answers\nUnsupervised learning: Finding hidden patterns in data without predefined categories\nNatural Language Processing (NLP): Teaching computers to understand and analyze human language\n\n\n\n\nEffective visualizations: Creating clear, informative graphics to represent complex data\nScience communication: Explaining findings to different audiences, from experts to the public\nScientific writing: Preparing research papers and reports to share results\n\n\n\n\nVersion control: Tracking changes in data and code throughout the research process\nOpen data practices: Sharing research data and methods for verification and further study\nReproducible workflows: Documenting research steps so others can repeat the study",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#tools-for-data-science-in-social-sciences",
    "href": "chapter1.html#tools-for-data-science-in-social-sciences",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.5 Tools for Data Science in Social Sciences",
    "text": "1.5 Tools for Data Science in Social Sciences\nIn this course, we’ll use R for our data analysis, as it’s widely used in social science research.\n\n1.5.1 R for Social Science Data Analysis\nR offers powerful capabilities for social science research, from data manipulation to advanced statistical modeling.\n\n\nKliknij, aby pokazać/ukryć kod R\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nKliknij, aby pokazać/ukryć kod R\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate example data with a Simpson's Paradox\nn &lt;- 1000\ndata &lt;- tibble(\n  age_group = sample(c(\"Young\", \"Middle\", \"Old\"), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),\n  education_years = case_when(\n    age_group == \"Young\" ~ rnorm(n, mean = 10, sd = 1),\n    age_group == \"Middle\" ~ rnorm(n, mean = 13, sd = 1),\n    age_group == \"Old\" ~ rnorm(n, mean = 16, sd = 1)\n  ),\n  income = case_when(\n    age_group == \"Young\" ~ 70000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Middle\" ~ 50000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Old\" ~ 30000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000)\n  )\n)\n\n# Basic data summary\nsummary(data)\n\n\n  age_group         education_years      income     \n Length:1000        Min.   : 6.628   Min.   :34068  \n Class :character   1st Qu.:10.913   1st Qu.:51508  \n Mode  :character   Median :13.004   Median :63376  \n                    Mean   :12.986   Mean   :63307  \n                    3rd Qu.:14.934   3rd Qu.:75023  \n                    Max.   :18.861   Max.   :96620  \n\n\nKliknij, aby pokazać/ukryć kod R\n# Correlation analysis\ncor(data %&gt;% select(education_years, income))\n\n\n                education_years     income\neducation_years       1.0000000 -0.8152477\nincome               -0.8152477  1.0000000\n\n\nKliknij, aby pokazać/ukryć kod R\n# Overall trend (Simpson's Paradox)\noverall_plot &lt;- ggplot(data, aes(x = education_years, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Overall Relationship between Education and Income\",\n       subtitle = \"Simpson's Paradox: Appears negative\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Trend by age group (Resolving Simpson's Paradox)\ngrouped_plot &lt;- ggplot(data, aes(x = education_years, y = income, color = age_group)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Relationship between Education and Income by Age Group\",\n       subtitle = \"Resolving Simpson's Paradox: Positive relationship within groups\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Statistical analysis\nmodel_overall &lt;- lm(income ~ education_years, data = data)\nmodel_by_age &lt;- lm(income ~ education_years + age_group, data = data)\n\n# Print results\nprint(overall_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(grouped_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_overall))\n\n\n\nCall:\nlm(formula = income ~ education_years, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24451  -5439    235   5262  34328 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     121814.7     1339.5   90.94   &lt;2e-16 ***\neducation_years  -4505.4      101.3  -44.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7976 on 998 degrees of freedom\nMultiple R-squared:  0.6646,    Adjusted R-squared:  0.6643 \nF-statistic:  1978 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_by_age))\n\n\n\nCall:\nlm(formula = income ~ education_years + age_group, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-14827  -3369    118   3356  16388 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      48270.8     2028.4  23.797  &lt; 2e-16 ***\neducation_years   1135.5      154.6   7.345 4.26e-13 ***\nage_groupOld    -19942.8      593.2 -33.619  &lt; 2e-16 ***\nage_groupYoung   20461.1      600.7  34.064  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4950 on 996 degrees of freedom\nMultiple R-squared:  0.8711,    Adjusted R-squared:  0.8707 \nF-statistic:  2244 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\n# Calculate and print correlations\noverall_cor &lt;- cor(data$education_years, data$income)\ngroup_cors &lt;- data %&gt;%\n  group_by(age_group) %&gt;%\n  summarize(correlation = cor(education_years, income))\n\nprint(\"Overall correlation:\")\n\n\n[1] \"Overall correlation:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(overall_cor)\n\n\n[1] -0.8152477\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(\"Correlations by age group:\")\n\n\n[1] \"Correlations by age group:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(group_cors)\n\n\n# A tibble: 3 × 2\n  age_group correlation\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Middle          0.185\n2 Old             0.291\n3 Young           0.223\n\n\nThis example demonstrates basic data manipulation, summary statistics, and visualization using R, which are common tasks in social science research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#causal-inference-vs.-observational-studies",
    "href": "chapter1.html#causal-inference-vs.-observational-studies",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.6 Causal Inference vs. Observational Studies",
    "text": "1.6 Causal Inference vs. Observational Studies\nIn social sciences and beyond, understanding the relationship between variables is crucial. Two key approaches to this are causal inference and observational studies, each with its own strengths and limitations.\n\nCausal InferenceObservational StudiesKey Distinction: Correlation vs. Causation\n\n\n\nAims to establish cause-and-effect relationships\nOften involves experimental designs or advanced statistical techniques\nSeeks to answer “What if?” questions and determine the impact of interventions\nExamples: Randomized controlled trials, quasi-experimental designs, instrumental variables\n\n\n\n\nExamine relationships between variables without direct intervention\nRely on data collected from natural settings or existing datasets\nCan identify correlations and patterns but struggle to establish causation\nExamples: Cohort studies, case-control studies, cross-sectional surveys\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember: Correlation Does Not Imply Causation\n\n\n\nA fundamental principle in research is that correlation between two variables does not necessarily imply a causal relationship. This concept is crucial when interpreting results from observational studies.\n\nCorrelation: Measures the strength and direction of a relationship between variables\nCausation: Indicates that changes in one variable directly cause changes in another\n\nWhile strong correlations can suggest potential causal links, additional evidence and rigorous methods are required to establish causality.\n\n\n\nChallenges in Establishing CausalityMethods to Strengthen Causal ClaimsImportance in Social Sciences\n\n\n\nConfounding variables: Unmeasured factors that affect both the presumed cause and effect\nReverse causality: The presumed effect might actually be causing the presumed cause\nSelection bias: Non-random selection of subjects into study groups\n\n\n\n\nRandomized controlled trials (when ethical and feasible)\nNatural experiments or quasi-experimental designs\nPropensity score matching\nDifference-in-differences analysis\nInstrumental variable approaches\nDirected acyclic graphs (DAGs) for visualizing causal relationships\n\n\n\nUnderstanding the distinction between causal inference and observational studies is crucial in social sciences, where ethical considerations often limit experimental manipulation. Researchers must carefully design studies and interpret results to avoid misleading conclusions about causality.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#models-in-science-from-deterministic-to-stochastic",
    "href": "chapter1.html#models-in-science-from-deterministic-to-stochastic",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.7 Models in Science: From Deterministic to Stochastic (*)",
    "text": "1.7 Models in Science: From Deterministic to Stochastic (*)\nModels are essential tools in scientific research, helping scientists to represent, understand, and predict complex phenomena. This section explores the main types of models used in science, along with examples of their applications. It’s important to note that these categories often overlap, and many scientific models incorporate multiple aspects.\n\n1.7.1 Mathematical Models\nMathematical models use equations and mathematical concepts to describe and analyze systems or phenomena. They can be further divided into several subcategories, though it’s important to note that some complex models may incorporate elements from multiple categories:\n\n1.7.1.1 a. Deterministic Models\nDeterministic models provide precise predictions based on a set of variables, without incorporating randomness at the macroscopic level.\nExample: Newton’s laws of motion, which can precisely predict the motion of objects under known forces in classical mechanics.\n\n\n1.7.1.2 b. Stochastic Models\nStochastic models incorporate randomness and probability. However, it’s crucial to distinguish between two fundamentally different types of stochastic models:\n\n1.7.1.2.1 i. Classical Stochastic Models\nThese models deal with randomness arising from incomplete information or complex interactions in classical systems. The underlying system is deterministic, but practical limitations in measurement or computation lead to the use of probabilistic descriptions.\nExample: Regression models in statistics, where the randomness represents unexplained variation or measurement error:\ny = β_0 + β_1x + ε\nWhere:\n\ny is the dependent variable (e.g. quantity demanded)\nx is the independent variable (e.g. price, income level of the consumer)\nβ_0 and β_1 are parameters\nε is the error term, representing unexplained variation\n\n\n\n1.7.1.2.2 ii. Quantum Stochastic Models\nThese models deal with the fundamental, irreducible randomness inherent in quantum mechanical systems. This randomness is not due to lack of information, but is a core feature of quantum reality.\nExample: The Standard Model in particle physics, which describes particle interactions using quantum field theory. For instance, the decay of a particle is inherently probabilistic:\nP(t) = e^{-t/τ}\nWhere:\n\nP(t) is the probability that the particle has not decayed after time t\nτ is the mean lifetime of the particle\n\n\n\n\n1.7.1.3 c. Computer Simulation Models\nComputer simulations use algorithms and computational methods based on mathematical models to simulate complex systems and predict their behavior over time. These can be deterministic or stochastic.\nExample: Climate models that simulate the Earth’s climate system, incorporating factors such as atmospheric composition, ocean currents, and solar radiation to project future climate scenarios.\n\n\n\n1.7.2 Conceptual Models\nConceptual models are abstract representations of systems or processes, often using diagrams or flowcharts to illustrate relationships between components.\nExample: The water cycle model in Earth sciences, which illustrates the continuous movement of water within the Earth and atmosphere through processes such as evaporation, precipitation, and runoff.\n\n\n1.7.3 Physical Models\nPhysical models are tangible representations of objects or systems, often scaled down or simplified versions of the real thing.\nExample: Wind tunnel models in aerodynamics research, used to study the effects of air moving past solid objects and optimize designs for aircraft, vehicles, or buildings.\n\n\n1.7.4 Theoretical Models\nTheoretical models are abstract frameworks based on fundamental principles and hypotheses, often used to explain observed phenomena or predict new ones. These models frequently employ mathematical formulations and can be deterministic or stochastic in nature.\nExample: The theory of evolution by natural selection, which provides a framework for understanding the diversity and adaptation of life forms over time.\n\n\n1.7.5 Conclusion\nThese various forms of models play crucial roles in scientific research, each offering unique advantages for understanding and predicting natural phenomena. Scientists often use multiple types of models in conjunction to gain comprehensive insights into complex systems and processes.\nIt’s important to recognize that these categories are not mutually exclusive and often overlap:\n\nMathematical models form the foundation for many other types of models, including computer simulations and some theoretical models.\nComputer simulation models are essentially mathematical models implemented through computational methods, and can be either deterministic or stochastic.\nTheoretical models often employ mathematical formulations and may be implemented as computer simulations.\nPhysical models may be designed based on mathematical models and can be used to validate computer simulations.\n\nThe choice of model type often depends on the specific research question, the nature of the system being studied, the available data, and the computational resources at hand. As science progresses, the boundaries between these model types continue to blur, leading to increasingly sophisticated and interdisciplinary approaches to modeling complex phenomena.\nIt’s crucial to distinguish between different types of stochastic models. Classical stochastic models, such as those used in regression analysis, deal with randomness arising from incomplete information or complex interactions in otherwise deterministic systems. In contrast, quantum stochastic models, like those in particle physics, deal with fundamental, irreducible randomness inherent in quantum mechanical systems. This distinction reflects the profound differences between classical and quantum paradigms in physics and highlights the diverse ways in which probability is used in scientific modeling.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#understanding-spurious-correlations-confounders-and-colliders",
    "href": "chapter1.html#understanding-spurious-correlations-confounders-and-colliders",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.8 Understanding Spurious Correlations, Confounders, and Colliders (*)",
    "text": "1.8 Understanding Spurious Correlations, Confounders, and Colliders (*)\nIn this tutorial, we’ll explore three important concepts in statistical analysis: spurious correlations, confounders, and colliders. Understanding these concepts is crucial for avoiding misinterpretation of data and drawing incorrect conclusions from statistical analyses.\nLet’s start by loading the necessary libraries:\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nset.seed(123) # for reproducibility\n\n\n1.8.1 Spurious Correlations\nSpurious correlations are relationships between variables that appear to be causal but are actually coincidental or caused by an unseen third factor.\n\n\n1.8.2 Example: Ice Cream Sales and Drowning Incidents\nLet’s create a dataset that shows a spurious correlation between ice cream sales and drowning incidents:\n\nn &lt;- 100\nspurious_data &lt;- tibble(\n  temperature = rnorm(n, mean = 25, sd = 5),\n  ice_cream_sales = 100 + 5 * temperature + rnorm(n, sd = 10),\n  drowning_incidents = 1 + 0.5 * temperature + rnorm(n, sd = 2)\n)\n\nggplot(spurious_data, aes(x = ice_cream_sales, y = drowning_incidents)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Spurious Correlation: Ice Cream Sales vs. Drowning Incidents\",\n       x = \"Ice Cream Sales\", y = \"Drowning Incidents\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis plot shows a positive correlation between ice cream sales and drowning incidents. However, this relationship is spurious. The real cause for both is the temperature:\n\nggplot(spurious_data, aes(x = temperature)) +\n  geom_point(aes(y = ice_cream_sales), color = \"blue\") +\n  geom_point(aes(y = drowning_incidents * 10), color = \"red\") +\n  geom_smooth(aes(y = ice_cream_sales), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(aes(y = drowning_incidents * 10), method = \"lm\", se = FALSE, color = \"red\") +\n  scale_y_continuous(\n    name = \"Ice Cream Sales\",\n    sec.axis = sec_axis(~./10, name = \"Drowning Incidents\")\n  ) +\n  labs(title = \"Temperature as the Common Cause\",\n       x = \"Temperature\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n1.8.3 Confounders\nA confounder is a variable that influences both the dependent variable and independent variable, causing a spurious association.\n\n\n1.8.4 Example: Education, Income, and Age\n\nlibrary(tidyverse)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nn &lt;- 1000\nconfounder_data &lt;- tibble(\n  age = runif(n, 25, 65),\n  education = round(10 + 0.1 * age + rnorm(n, sd = 2)),\n  income = 20000 + 1000 * education + 500 * age + rnorm(n, sd = 5000)\n)\n\n# Without controlling for age\nmodel_naive &lt;- lm(income ~ education, data = confounder_data)\n# Controlling for age\nmodel_adjusted &lt;- lm(income ~ education + age, data = confounder_data)\n\n# Create age groups for visualization\nconfounder_data &lt;- confounder_data %&gt;%\n  mutate(age_group = cut(age, breaks = 3, labels = c(\"Young\", \"Middle\", \"Old\")))\n\n# Visualize\nggplot(confounder_data, aes(x = education, y = income)) +\n  geom_point(aes(color = age), alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1.2) +\n  geom_smooth(aes(group = age_group, color = as.numeric(age_group)), \n              method = \"lm\", se = FALSE, linewidth = 1) +\n  scale_color_viridis_c(name = \"Age\", \n                        breaks = c(30, 45, 60), \n                        labels = c(\"Young\", \"Middle\", \"Old\")) +\n  labs(title = \"Education vs Income, Confounded by Age\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCompare the coefficients:\n\nsummary(model_naive)$coefficients[\"education\", \"Estimate\"]\n\n[1] 2328.718\n\nsummary(model_adjusted)$coefficients[\"education\", \"Estimate\"]\n\n[1] 1101.783\n\n\nThe effect of education on income is overestimated when we don’t control for age.\n\n\n1.8.5 Colliders\nA collider is a variable that is influenced by both the independent variable and the dependent variable. Controlling for a collider can introduce a spurious correlation.\n\n\n1.8.6 Example: Job Satisfaction, Salary, and Work-Life Balance\nLet’s create a dataset where work-life balance is a collider between job satisfaction and salary:\n\nn &lt;- 1000\ncollider_data &lt;- tibble(\n  job_satisfaction = rnorm(n),\n  salary = rnorm(n),\n  work_life_balance = -0.5 * job_satisfaction - 0.5 * salary + rnorm(n, sd = 0.5)\n)\n\n# Without controlling for work-life balance\nmodel_correct &lt;- lm(salary ~ job_satisfaction, data = collider_data)\n\n# Incorrectly controlling for work-life balance\nmodel_collider &lt;- lm(salary ~ job_satisfaction + work_life_balance, data = collider_data)\n\n# Visualize\nggplot(collider_data, aes(x = job_satisfaction, y = salary, color = work_life_balance)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_color_viridis_c() +\n  labs(title = \"Job Satisfaction vs Salary, Work-Life Balance as Collider\",\n       x = \"Job Satisfaction\", y = \"Salary\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nCompare the coefficients:\n\nsummary(model_correct)$coefficients[\"job_satisfaction\", \"Estimate\"]\n\n[1] 0.02063487\n\nsummary(model_collider)$coefficients[\"job_satisfaction\", \"Estimate\"]\n\n[1] -0.4794016\n\n\nControlling for the collider (work-life balance) introduces a spurious correlation between job satisfaction and salary.\n\n\n1.8.7 Conclusion\nUnderstanding spurious correlations, confounders, and colliders is crucial for proper statistical analysis and causal inference. Always consider the underlying causal structure of your data and be cautious about which variables you control for in your analyses.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#further-reading",
    "href": "chapter1.html#further-reading",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.9 Further Reading",
    "text": "1.9 Further Reading\n\nPearl, J. (2009). Causality: Models, Reasoning, and Inference. Cambridge University Press.\nHernán, M. A., & Robins, J. M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#ethical-considerations-in-social-science-data-analysis",
    "href": "chapter1.html#ethical-considerations-in-social-science-data-analysis",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.10 Ethical Considerations in Social Science Data Analysis",
    "text": "1.10 Ethical Considerations in Social Science Data Analysis\nEthics play a crucial role in social science research:\n\nPrivacy and Consent: Ensuring participant privacy and informed consent\nData Protection: Securely storing and managing sensitive personal data\nBias and Representation: Addressing sampling bias and ensuring diverse representation\nTransparency: Clearly communicating research methods and limitations\nSocial Impact: Considering the potential societal implications of research findings\n\n\n\n\n\n\n\nImportant\n\n\n\nSocial scientists must carefully consider the ethical implications of their data collection, analysis, and dissemination practices.\n\n\n\n1.10.1 Key Takeaways\n\nData science in social sciences builds upon traditional statistical methods, incorporating new technologies to analyze complex social phenomena.\nUnderstanding concepts like population, sample, and data generating processes is crucial for valid social science research.\nThe data science process in social research involves multiple steps from ethical data collection to the communication of insights.\nR is a powerful tool for social science data analysis, offering a wide range of capabilities.\nEthical considerations should be at the forefront of any social science data project.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-a-classical-vs-quantum-randomness-understanding-the-fundamental-differences",
    "href": "chapter1.html#appendix-a-classical-vs-quantum-randomness-understanding-the-fundamental-differences",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.11 Appendix A: Classical vs Quantum Randomness: Understanding the Fundamental Differences",
    "text": "1.11 Appendix A: Classical vs Quantum Randomness: Understanding the Fundamental Differences\nTo understand how the randomness in quantum mechanics differs from the randomness represented by the error term in regression models, we need to examine their origins, nature, and implications.\n\n1.11.1 Origin of Randomness\n\n1.11.1.1 Classical Randomness (Regression Models)\n\nSource: Incomplete information or complex interactions in an otherwise deterministic system.\nNature: Epistemic uncertainty (due to lack of knowledge).\nExample: In a regression model, y = β_0 + β_1x + ε, the error term ε represents unexplained variation.\n\n\n\n1.11.1.2 Quantum Randomness\n\nSource: Fundamental property of quantum systems.\nNature: Ontic uncertainty (inherent to the system, not due to lack of knowledge).\nExample: The exact time of decay of a radioactive atom cannot be predicted, only its probability.\n\n\n\n\n1.11.2 Philosophical Implications\n\n1.11.2.1 Classical Randomness\n\nDeterminism: Underlying reality is deterministic; randomness reflects our ignorance.\nHidden Variables: In principle, if we had complete information, we could predict outcomes precisely.\n\n\n\n1.11.2.2 Quantum Randomness\n\nIndeterminism: Randomness is a fundamental feature of reality, not just our description of it.\nNo Hidden Variables: Even with complete information about a quantum system, some outcomes remain unpredictable (as suggested by Bell’s theorem).\n\n\n\n\n1.11.3 Mathematical Treatment\n\n1.11.3.1 Classical Randomness\n\nProbability Theory: Based on classical probability theory.\nDistribution: Often assumed to follow known distributions (e.g., normal distribution in many regression models).\nCentral Limit Theorem: Applies to large samples of random variables.\n\n\n\n1.11.3.2 Quantum Randomness\n\nQuantum Probability: Based on the mathematical framework of quantum mechanics.\nWave Function: Describes the quantum state and its evolution.\nBorn Rule: Gives probabilities of measurement outcomes from the wave function.\n\n\n\n\n1.11.4 Predictability and Control\n\n1.11.4.1 Classical Randomness\n\nReducible: In principle, can be reduced by gathering more data or improving measurement precision.\nControllable: Systematic errors can be identified and corrected.\n\n\n\n1.11.4.2 Quantum Randomness\n\nIrreducible: Cannot be eliminated even with perfect measurements.\nFundamentally Uncontrollable: The act of measurement itself affects the system (measurement problem).\n\n\n\n\n1.11.5 Practical Implications\n\n1.11.5.1 Classical Randomness\n\nError Reduction: Focus on improving measurement techniques and data collection.\nModel Refinement: Aim to explain more variance and reduce the error term.\n\n\n\n1.11.5.2 Quantum Randomness\n\nInherent Limitation: Accept fundamental limits on predictability.\nProbabilistic Predictions: Focus on accurate probability distributions rather than exact outcomes.\n\n\n\n\n1.11.6 Examples to Understand the Difference\n\n1.11.6.1 Classical Randomness Example\nImagine flipping a coin. Classical physics says the outcome is determined by initial conditions (force applied, air resistance, etc.). The “randomness” comes from our inability to precisely measure and account for all these factors.\n\n\n1.11.6.2 Quantum Randomness Example\nIn the double-slit experiment, individual particles show interference patterns as if they went through both slits simultaneously. The exact path of any individual particle is fundamentally undetermined until measured, and this indeterminacy cannot be resolved by more precise measurements.\n\n\n\n1.11.7 Conclusion\nWhile both types of randomness lead to probabilistic predictions, their fundamental natures are quite different:\n\nClassical randomness in regression models is a reflection of our incomplete knowledge or measurement limitations in an otherwise deterministic system.\nQuantum randomness is a fundamental property of quantum systems, representing an inherent indeterminacy in nature that persists even with perfect knowledge and measurement.\n\nUnderstanding these differences is crucial for correctly interpreting and applying statistical models in different scientific contexts, from social sciences using regression analysis to quantum physics experiments.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-b-large-language-models---understanding-their-stochastic-nature",
    "href": "chapter1.html#appendix-b-large-language-models---understanding-their-stochastic-nature",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.12 Appendix B: Large Language Models - Understanding Their Stochastic Nature",
    "text": "1.12 Appendix B: Large Language Models - Understanding Their Stochastic Nature\nLarge Language Models (LLMs) like GPT-3, BERT, and Claude have revolutionized natural language processing but can make puzzling mistakes, especially in mathematical tasks. This appendix explains LLMs’ functioning, stochastic nature, and compares them to classical statistical models.\n\n1.12.1 LLM Basics and Stochastic Nature\nLLMs are trained on vast text data to predict the probability distribution of the next token in a sequence. They use transformer architectures for processing and generating text. Key aspects of their stochastic nature include:\n\nProbabilistic token selection: LLMs choose each word based on calculated probabilities, not fixed rules.\nTemperature-controlled randomness: A “temperature” parameter adjusts the randomness of selections, balancing creativity and coherence.\nNon-deterministic outputs: The same input can produce different outputs in separate runs.\nContextual ambiguity: LLMs interpret context probabilistically, sometimes leading to misunderstandings.\n\n\n\n1.12.2 Comparison to Classical Statistical Models\nTo understand LLMs better, let’s compare them to Ordinary Least Squares (OLS) regression:\n\n\n\n\n\n\n\n\nAspect\nOLS Regression\nLarge Language Models\n\n\n\n\nBasic Function\nPredicts continuous outcomes based on input variables\nPredicts probability distribution of next token based on previous tokens\n\n\nInput-Output\nContinuous variables, linear relationships\nDiscrete tokens, non-linear relationships\n\n\nPrediction Type\nPoint predictions with confidence intervals\nProbability distributions over possible tokens\n\n\nModel Complexity\nFew parameters\nBillions of parameters\n\n\nInterpretability\nClear coefficient interpretations\nLargely opaque internal workings\n\n\nNoise Handling\nAssumes random noise in outcome variable\nDeals with natural language variability\n\n\nExtrapolation\nLess reliable outside training range\nLess reliable on unfamiliar topics\n\n\n\nBoth models aim to learn input-output mappings based on training data patterns.\n\n\n1.12.3 Implications for Mathematical Tasks\nLLMs’ stochastic nature affects mathematical operations:\n\nVariable outputs for repeated calculations: Each attempt might yield a different result due to probabilistic token selection.\nConfidence doesn’t guarantee correctness: High model confidence can occur even for incorrect answers.\nApproximation rather than exact computation: LLMs pattern-match rather than perform precise calculations.\n\nLimitations in mathematical tasks stem from:\n\nTraining objective mismatch: LLMs are trained for language prediction, not mathematical accuracy.\nLack of explicit mathematical reasoning: They don’t have built-in mathematical rules or operations.\nAbsence of working memory: LLMs can’t reliably store and manipulate intermediate results.\nLimited context window: They may lose track of relevant information in long problems.\nTraining data limitations: Underrepresentation of certain math concepts can lead to poor performance.\nLack of consistency checks: LLMs don’t verify the logical consistency of their outputs.\n\n\n\n1.12.4 Best Practices and Conclusion\nWhen using LLMs for mathematical tasks:\n\nFocus on conceptual explanations, not precise calculations: LLMs excel at explaining concepts but may falter on exact computations.\nVerify results with dedicated software: Always double-check LLM calculations with proper math tools.\nBreak down complex problems: Splitting tasks into smaller steps can improve LLM performance.\nBe aware of rephrasing effects: Different phrasings of the same problem may yield different results.\nUse as assistive tools, not replacements for expertise: LLMs should complement, not substitute, mathematical expertise.\n\nUnderstanding LLMs’ probabilistic nature helps leverage their strengths in language tasks while recognizing their limitations in domains requiring deterministic precision, like mathematics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-c-deterministic-and-stochastic-models",
    "href": "chapter1.html#appendix-c-deterministic-and-stochastic-models",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.13 Appendix C: Deterministic and Stochastic Models (*)",
    "text": "1.13 Appendix C: Deterministic and Stochastic Models (*)\n\n1.13.1 Deterministic Models\nDeterministic models are those where the output is fully determined by the parameter values and the initial conditions. These models are often used in physics and engineering.\n\n\n1.13.2 Example: Uniformly Accelerated Motion\nA classic example of a deterministic model is uniformly accelerated motion, described by the equation:\nx(t) = x_0 + v_0t + \\frac{1}{2}at^2\nWhere:\n\nx(t) is the position at time t\nx_0 is the initial position\nv_0 is the initial velocity\na is the acceleration\nt is time\n\nLet’s simulate this in R:\n\n# Uniformly accelerated motion\nsimulate_accelerated_motion &lt;- function(x0, v0, a, t) {\n  x0 + v0 * t + 0.5 * a * t^2\n}\n\n# Generating data\nt &lt;- seq(0, 10, by = 0.1)\nx &lt;- simulate_accelerated_motion(x0 = 0, v0 = 2, a = 1, t = t)\n\n# Plot\nplot(t, x, type = \"l\", xlab = \"Time\", ylab = \"Position\", \n     main = \"Uniformly Accelerated Motion\")\n\n\n\n\n\n\n\n\nThis code will generate a plot of uniformly accelerated motion, which is an intuitive example from Newtonian dynamics. In this case, an object starts moving with an initial velocity and accelerates uniformly, resulting in a parabolic trajectory on the position-time graph.\n\n\n1.13.3 Stochastic Models in Social Sciences\nStochastic models incorporate randomness and are often used in social sciences where there’s inherent uncertainty in the systems being studied.\n\n\n1.13.4 Example: Ordinary Least Squares (OLS) Regression\nOLS is a fundamental stochastic model in social sciences. It’s represented as:\nY = \\beta_0 + \\beta_1X + \\epsilon\nWhere:\n\nY is the dependent variable\nX is the independent variable\n\\beta_0 and \\beta_1 are parameters\n\\epsilon is the error term (stochastic component)\n\nLet’s demonstrate OLS in R:\n\n# Generate some sample data\nset.seed(123)\nX &lt;- rnorm(100)\nY &lt;- 2 + 3*X + rnorm(100, sd = 0.5)\n\n# Fit OLS model\nmodel &lt;- lm(Y ~ X)\n\n# Summary of the model\nsummary(model)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95367 -0.34175 -0.04375  0.29032  1.64520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.94860    0.04878   39.95   &lt;2e-16 ***\nX            2.97376    0.05344   55.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4854 on 98 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.969 \nF-statistic:  3097 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n# Plot\nplot(X, Y, main = \"OLS Regression\")\nabline(model, col = \"red\")\n\n\n\n\n\n\n\n\nThis will fit an OLS model to some simulated data and plot the results.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff\n\n\n\n\n1.13.5 Advanced Stochastic Models: Large Language Models\nLarge Language Models (LLMs) like GPT-3 are complex stochastic models used in natural language processing. While we can’t implement a full LLM in this tutorial, we can discuss its principles.\nLLMs are based on the transformer architecture and use self-attention mechanisms. They’re trained on vast amounts of text data and learn to predict the next token in a sequence.\nThe core of an LLM can be thought of as a conditional probability distribution:\nP(x_t | x_{&lt;t}, \\theta)\nWhere: - x_t is the current token - x_{&lt;t} represents all previous tokens - \\theta are the model parameters\n\n\n\n\n\n\nNote\n\n\n\nTokens in Large Language Models (LLMs) are the basic units of text that the model processes. They can be thought of as pieces of words or punctuation marks. Here are key points about tokens:\nDefinition: Tokens are the smallest units of text that an LLM processes. They can be whole words, parts of words, or even individual characters or punctuation marks. Tokenization: The process of breaking text into tokens is called tokenization. LLMs use specific algorithms to perform this task. Examples:\nThe word “cat” might be a single token. A longer word like “understanding” might be broken into multiple tokens, e.g., “under” and “standing”. Punctuation marks like “.” or “?” are often individual tokens. Common prefixes or suffixes might be their own tokens.\nVocabulary: LLMs have a fixed vocabulary of tokens they recognize. This vocabulary typically ranges from tens of thousands to hundreds of thousands of tokens. Significance: The way text is tokenized can affect how the model understands and generates language. It’s particularly important for handling different languages, rare words, or specialized vocabulary. Context: In the equation for LLMs: P(x_t | x_{&lt;t}, \\theta) Where:\nx_t represents the current token x_{&lt;t} represents all previous tokens in the sequence \\theta represents the model parameters\n\n\nUnlike deterministic models, LLMs produce different outputs even for the same input due to their stochastic nature.\n\n\n1.13.6 Conclusion\nWe’ve explored a range of models from deterministic to highly complex stochastic ones. Each type of model has its place in science, depending on the system being studied and the level of uncertainty involved.\nRemember, the choice between deterministic and stochastic models often depends on the nature of the system you’re studying and the questions you’re trying to answer. Deterministic models are great for systems with well-understood mechanics, while stochastic models shine when dealing with inherent randomness or complex, not fully understood systems.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter1.html#appendix-d-introduction-to-r-rstudio-and-tidyverse",
    "href": "chapter1.html#appendix-d-introduction-to-r-rstudio-and-tidyverse",
    "title": "1  Introduction to Data Science and Statistics for Social Sciences",
    "section": "1.14 Appendix D: Introduction to R, RStudio, and tidyverse",
    "text": "1.14 Appendix D: Introduction to R, RStudio, and tidyverse\nR is a powerful programming language and environment for statistical computing and graphics. It’s widely used in academia, especially in fields like social sciences, for data analysis and visualization.\n\n1.14.0.1 Key features of R:\n\nOpen-source and free\nExtensive package ecosystem\nStrong community support\nExcellent for statistical analysis and data visualization\n\n\n\n1.14.1 Getting Started with RStudio\nRStudio is an Integrated Development Environment (IDE) for R that makes it easier to work with R.\n\n1.14.1.1 Installing R and RStudio\n\nDownload and install R from CRAN\nDownload and install RStudio from RStudio’s website\n\n\n\n1.14.1.2 RStudio Interface\nRStudio has four main panes:\n\nSource Editor: Where you write and edit your R scripts\nConsole: Where you can type R commands and see output\nEnvironment/History: Shows all objects in your workspace and command history\nFiles/Plots/Packages/Help: Multipurpose pane for file management, viewing plots, managing packages, and accessing help\n\n\n\n1.14.1.3 Basic RStudio Features\n\nCreating a new R script: File &gt; New File &gt; R Script\nRunning code: Select code and press Ctrl+Enter (Cmd+Enter on Mac)\nInstalling packages: Tools &gt; Install Packages\nGetting help: Type ?function_name in the console\n\n\n\n\n1.14.2 R Basics\n\n1.14.2.1 Data Types in R\n\n# Numeric\nx &lt;- 10.5\nclass(x)\n\n[1] \"numeric\"\n\n# Integer\ny &lt;- 1L\nclass(y)\n\n[1] \"integer\"\n\n# Character\nname &lt;- \"Alice\"\nclass(name)\n\n[1] \"character\"\n\n# Logical\nis_student &lt;- TRUE\nclass(is_student)\n\n[1] \"logical\"\n\n\n\n\n1.14.2.2 Data Structures\n\n1.14.2.2.1 Vectors\n\n# Create a vector\nnumbers &lt;- c(1, 2, 3, 4, 5)\nfruits &lt;- c(\"apple\", \"banana\", \"cherry\")\n\n# Vector operations\nnumbers + 2\n\n[1] 3 4 5 6 7\n\nnumbers * 2\n\n[1]  2  4  6  8 10\n\nmean(numbers)\n\n[1] 3\n\nlength(fruits)\n\n[1] 3\n\n\n\n\n1.14.2.2.2 Matrices\n\n# Create a matrix\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\nprint(m)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n# Matrix operations\nt(m)  # transpose\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nm * 2  # scalar multiplication\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    4    8   12\n\n\n\n\n1.14.2.2.3 Data Frames\n\n# Create a data frame\ndf &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  age = c(25, 30, 35),\n  student = c(TRUE, FALSE, TRUE)\n)\nprint(df)\n\n     name age student\n1   Alice  25    TRUE\n2     Bob  30   FALSE\n3 Charlie  35    TRUE\n\n# Accessing data frame elements\ndf$name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\"\n\ndf[1, 2]\n\n[1] 25\n\ndf[df$age &gt; 25, ]\n\n     name age student\n2     Bob  30   FALSE\n3 Charlie  35    TRUE\n\n\n\n\n\n1.14.2.3 Functions\n\n# Define a function\ngreet &lt;- function(name) {\n  paste(\"Hello,\", name, \"!\")\n}\n\n# Use the function\ngreet(\"Alice\")\n\n[1] \"Hello, Alice !\"\n\n# Function with multiple arguments\ncalculate_bmi &lt;- function(weight, height) {\n  bmi &lt;- weight / (height^2)\n  return(bmi)\n}\n\ncalculate_bmi(70, 1.75)\n\n[1] 22.85714\n\n\n\n\n1.14.2.4 Control Structures\n\n# If-else statement\nx &lt;- 10\nif (x &gt; 5) {\n  print(\"x is greater than 5\")\n} else {\n  print(\"x is not greater than 5\")\n}\n\n[1] \"x is greater than 5\"\n\n# For loop\nfor (i in 1:5) {\n  print(paste(\"Iteration\", i))\n}\n\n[1] \"Iteration 1\"\n[1] \"Iteration 2\"\n[1] \"Iteration 3\"\n[1] \"Iteration 4\"\n[1] \"Iteration 5\"\n\n# While loop\ncounter &lt;- 1\nwhile (counter &lt;= 5) {\n  print(paste(\"Counter:\", counter))\n  counter &lt;- counter + 1\n}\n\n[1] \"Counter: 1\"\n[1] \"Counter: 2\"\n[1] \"Counter: 3\"\n[1] \"Counter: 4\"\n[1] \"Counter: 5\"\n\n\n\n\n\n1.14.3 Introduction to tidyverse\nThe tidyverse is a collection of R packages designed for data science. These packages share a common philosophy and are designed to work together seamlessly.\n\n1.14.3.1 Key tidyverse Packages\n\nggplot2: for data visualization\ndplyr: for data manipulation\ntidyr: for tidying data\nreadr: for reading rectangular data\npurrr: for functional programming\ntibble: modern reimagining of data frames\n\n\n\n1.14.3.2 Getting Started with tidyverse\n\n# Install tidyverse (run once)\n# install.packages(\"tidyverse\")\n\n# Load tidyverse\nlibrary(tidyverse)\n\n\n\n1.14.3.3 Data Import with readr\n\n# Reading CSV files\ndata &lt;- read_csv(\"social_data.csv\")\n\n# Reading other file formats\nread_tsv(\"data.tsv\")  # Tab-separated values\nread_delim(\"data.txt\", delim = \"|\")  # Custom delimiter\n\n\n\n1.14.3.4 Data Manipulation with dplyr\n\n# Let's use the built-in mtcars dataset\ndata(\"mtcars\")\n\n# Selecting columns\nmtcars %&gt;% \n  select(mpg, cyl, hp)\n\n                     mpg cyl  hp\nMazda RX4           21.0   6 110\nMazda RX4 Wag       21.0   6 110\nDatsun 710          22.8   4  93\nHornet 4 Drive      21.4   6 110\nHornet Sportabout   18.7   8 175\nValiant             18.1   6 105\nDuster 360          14.3   8 245\nMerc 240D           24.4   4  62\nMerc 230            22.8   4  95\nMerc 280            19.2   6 123\nMerc 280C           17.8   6 123\nMerc 450SE          16.4   8 180\nMerc 450SL          17.3   8 180\nMerc 450SLC         15.2   8 180\nCadillac Fleetwood  10.4   8 205\nLincoln Continental 10.4   8 215\nChrysler Imperial   14.7   8 230\nFiat 128            32.4   4  66\nHonda Civic         30.4   4  52\nToyota Corolla      33.9   4  65\nToyota Corona       21.5   4  97\nDodge Challenger    15.5   8 150\nAMC Javelin         15.2   8 150\nCamaro Z28          13.3   8 245\nPontiac Firebird    19.2   8 175\nFiat X1-9           27.3   4  66\nPorsche 914-2       26.0   4  91\nLotus Europa        30.4   4 113\nFord Pantera L      15.8   8 264\nFerrari Dino        19.7   6 175\nMaserati Bora       15.0   8 335\nVolvo 142E          21.4   4 109\n\n# Filtering rows\nmtcars %&gt;% \n  filter(cyl == 4)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n# Arranging data\nmtcars %&gt;% \n  arrange(desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n# Creating new variables\nmtcars %&gt;% \n  mutate(kpl = mpg * 0.425)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb     kpl\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  8.9250\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  8.9250\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1  9.6900\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  9.0950\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  7.9475\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  7.6925\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  6.0775\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 10.3700\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  9.6900\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  8.1600\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  7.5650\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  6.9700\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  7.3525\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  6.4600\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  4.4200\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  4.4200\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  6.2475\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 13.7700\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 12.9200\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 14.4075\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  9.1375\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  6.5875\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  6.4600\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  5.6525\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  8.1600\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 11.6025\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 11.0500\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 12.9200\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  6.7150\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  8.3725\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  6.3750\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2  9.0950\n\n# Summarizing data\nmtcars %&gt;% \n  group_by(cyl) %&gt;% \n  summarize(mean_mpg = mean(mpg),\n            count = n())\n\n# A tibble: 3 × 3\n    cyl mean_mpg count\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n1     4     26.7    11\n2     6     19.7     7\n3     8     15.1    14\n\n\n\n\n1.14.3.5 Data Visualization with ggplot2\n\n# Scatter plot\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Car Weight vs. Fuel Efficiency\",\n       x = \"Weight (1000 lbs)\",\n       y = \"Miles per Gallon\")\n\n\n\n\nCar Weight vs. Fuel Efficiency\n\n\n\n\n\n# Bar chart\nmtcars %&gt;% \n  count(cyl) %&gt;% \n  ggplot(aes(x = factor(cyl), y = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Number of Cars by Cylinder Count\",\n       x = \"Number of Cylinders\",\n       y = \"Count\")\n\n\n\n\nNumber of Cars by Cylinder Count\n\n\n\n\n\n# Box plot\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Fuel Efficiency by Number of Cylinders\",\n       x = \"Number of Cylinders\",\n       y = \"Miles per Gallon\")\n\n\n\n\nFuel Efficiency by Number of Cylinders\n\n\n\n\n\n\n\n1.14.4 Additional Resources\n\nR for Data Science\ntidyverse documentation\nRStudio Cheat Sheets\nQuarto Guide\nR Cookbook\n\nRemember to experiment with the code, modify examples, and don’t hesitate to use the built-in R help system (accessed by typing ?function_name in the console) when you encounter unfamiliar functions or concepts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to Data Science and Statistics for Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html",
    "href": "rozdzial1.html",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "",
    "text": "2.1 Czym są Statystyka i Nauka o Danych?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#czym-są-statystyka-i-nauka-o-danych",
    "href": "rozdzial1.html#czym-są-statystyka-i-nauka-o-danych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "",
    "text": "Important\n\n\n\nStatystyka i data science to sztuka i nauka (o metodach, technikach lub narzędziach) uczenia się z danych.\n\n\n\nNauka o danych i statystyka to potężne narzędzia, które pomagają nam zrozumieć złożone zjawiska w różnych naukach społecznych, w tym w politologii, ekonomii i socjologii. Te uzupełniające się dziedziny dostarczają badaczom i praktykom środków do analizy trendów, zachowań i wyników w społeczeństwie, oferując wgląd, który może kształtować politykę i pogłębiać nasze zrozumienie ludzkich zachowań.\nStatystyka dostarcza matematycznych podstaw do analizy trendów i wyników społecznych, oferując metody projektowania badań, podsumowywania danych i wyciągania wniosków. Nauka o danych rozszerza tę podstawę, włączając metody obliczeniowe i wiedzę dziedzinową, aby radzić sobie z większymi zbiorami danych i przeprowadzać bardziej złożone analizy.\nRazem te dyscypliny pozwalają nam zbierać i przetwarzać duże zbiory danych, wizualizować złożone informacje, odkrywać wzorce w interakcjach społecznych, oceniać wpływ polityk i wspierać podejmowanie decyzji opartych na dowodach. Ich zastosowania są rozległe i zróżnicowane, od badania wzorców głosowania i analizy wskaźników ekonomicznych po badanie nierówności społecznych i analizę zachowań ludzkich.\nW miarę jak nasz świat staje się coraz bardziej oparty na danych, znaczenie nauki o danych i statystyki w naukach społecznych nadal rośnie.\n\n\n\n\n\n\n\nNote\n\n\n\nW naukach społecznych nauka o danych łączy metody statystyczne, narzędzia obliczeniowe i wiedzę dziedzinową do analizy złożonych zjawisk społecznych i zachowań ludzkich.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#związek-między-statystyką-a-nauką-o-danych",
    "href": "rozdzial1.html#związek-między-statystyką-a-nauką-o-danych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.2 Związek Między Statystyką a Nauką o Danych",
    "text": "2.2 Związek Między Statystyką a Nauką o Danych\nStatystyka i data science to ściśle powiązane dziedziny o znaczącym nakładaniu się, szczególnie w naukach społecznych. Zamiast ścisłego podziału, trafniej jest postrzegać je jako komplementarne podejścia na pewnym kontinuum:\n\nTradycyjna StatystykaNowoczesna Data ScienceEwoluujący Krajobraz\n\n\n\nZakorzeniona w teoriach matematycznych i metodach analizy danych\nKładzie nacisk na wnioskowanie statystyczne, testowanie hipotez i teorię prawdopodobieństwa\nHistorycznie kluczowa w naukach społecznych do analizy badań ankietowych, eksperymentów i badań obserwacyjnych\n\n\n\n\nIntegruje metody statystyczne z nauką o komputerach i wiedzą dziedzinową\nPoszerza fokus o uczenie maszynowe, przetwarzanie big data i modelowanie predykcyjne\nW naukach społecznych często zajmuje się wielkoskalowymi danymi cyfrowymi i złożonymi zbiorami danych behawioralnych\n\n\n\n\nGranice między statystyką a data science są coraz bardziej rozmyte\nWiele technik i narzędzi jest wspólnych dla obu dziedzin\nNaukowcy społeczni często łączą tradycyjne podejścia statystyczne z nowszymi metodami data science\nWybór podejścia zależy od pytań badawczych, charakterystyki danych i konkretnych potrzeb analitycznych\n\n\n\n\nNauka o danych może być postrzegana jako wynik ewolucji i rozszerzenie tradycyjnej statystyki, włączając nowe technologie i metody do obsługi większych i bardziej złożonych zbiorów danych w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#podstawowe-koncepcje-w-nauce-o-danych-i-statystyce",
    "href": "rozdzial1.html#podstawowe-koncepcje-w-nauce-o-danych-i-statystyce",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.3 Podstawowe Koncepcje w Nauce o Danych i Statystyce",
    "text": "2.3 Podstawowe Koncepcje w Nauce o Danych i Statystyce\n\n2.3.1 Dane i Populacje (Data and Populations) oraz pojęcia pokrewne\n\n\n\n\n\n\nImportant\n\n\n\n\nDane: Obserwacje lub pomiary zebrane z próby lub populacji.\nPopulacja: Cały zbiór osób lub elementów badanych w określonym czasie.\n\nPrzykład: Wszyscy uprawnieni wyborcy w kraju podczas konkretnego roku wyborczego.\n\nPróba: Podzbiór populacji, który jest faktycznie mierzony. Reprezentatywna próba to podzbiór większej populacji, który dokładnie odzwierciedla cechy tej populacji. Próba powinna odzwierciedlać populację pod względem ważnych cech, takich jak wiek, płeć, status społeczno-ekonomiczny itp. Często wykorzystuje metody losowego doboru próby, aby uniknąć stronniczości. Jest wystarczająco duża, aby być statystycznie istotna, ale mniejsza niż cała populacja.\n\nPrzykład: 1500 losowo wybranych uprawnionych wyborców ankietowanych w przedwyborczym sondażu.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nProces Generowania Danych (PGD) i Superpopulacja: Rozszerzenie Tradycyjnych Koncepcji\nW tradycyjnej statystyce często pracujemy z dwoma kluczowymi pojęciami:\n\nPopulacja: Cała grupa, którą chcemy badać.\nPróba: Podzbiór populacji, który faktycznie obserwujemy i analizujemy.\n\nChociaż te pojęcia są fundamentalne, współczesne badania często wymagają myślenia wykraczającego poza ten dychotomiczny podział. Tu wkraczają koncepcje Procesu Generowania Danych (PGD) i superpopulacji, rozszerzając nasze rozumienie danych i populacji.\nProces Generowania Danych (PGD; Data Generating Process, DGP):\nPGD to podstawowy mechanizm, który produkuje dane obserwowane w rzeczywistym świecie, zarówno w naszej próbie, jak i w całej populacji.\nIntuicyjne wyjaśnienie: Wyobraź sobie PGD jako złożony system, który przyjmuje różne dane wejściowe i produkuje obserwowalne wyniki. To “czarna skrzynka”, która przekształca przyczyny w skutki, nie tylko dla naszej próby, ale dla całej populacji i poza nią.\nPrzykład: Rozważmy badanie zachowań wyborczych. Tradycyjne podejście mogłoby zdefiniować populację jako “wszyscy zarejestrowani wyborcy” i pobrać z tej grupy próbę. PGD natomiast obejmowałby czynniki takie jak cechy demograficzne, warunki ekonomiczne, wydarzenia polityczne i wpływ mediów, które kształtują zachowania wyborcze wszystkich wyborców, niezależnie od tego, czy zostali uwzględnieni w próbie, czy nie.\nSuperpopulacja:\nSuperpopulacja to teoretyczna koncepcja, która wykracza zarówno poza próbę, jak i obserwowalną populację, obejmując wszystkie potencjalne wyniki, które mogłyby wystąpić w podobnych warunkach lub procesach.\nPrzykłady:\n\nPodejście tradycyjne vs. podejście superpopulacyjne:\n\nTradycyjne: populacja (wszyscy zarejestrowani wyborcy w województwie), próba (1000 ankietowanych wyborców)\nSuperpopulacja: Wszyscy możliwi wyborcy i scenariusze głosowania, w tym przyszłe wybory i hipotetyczne konteksty polityczne\n\nGdy próba równa się populacji:\nW badaniach wszystkich 16 województw Polski:\n\nTradycyjne spojrzenie: Brak rozróżnienia między próbą a populacją\nSpojrzenie superpopulacyjne: Traktuje te 16 województw jako “próbę” z teoretycznego zbioru wszystkich możliwych interakcji między województwami a polityką\n\n\nZastosowanie w rzeczywistości: Załóżmy, że badacze studiują wpływ nowej polityki planowania urbanistycznego w kilku miastach:\n\nPodejście tradycyjne:\n\nPopulacja: Wszystkie miasta w kraju\nPróba: Miasta uwzględnione w badaniu\n\nPodejście superpopulacyjne:\n\nObserwowane dane: Miasta w badaniu\nSuperpopulacja: Wszystkie miasta (istniejące lub potencjalne), w których można by zastosować podobne zasady planowania urbanistycznego\n\n\nPGD (DGP) w tym przypadku byłby złożonym zestawem czynników, które determinują, jak polityki planowania urbanistycznego wpływają na wyniki miast, mające zastosowanie nie tylko do badanych miast czy nawet wszystkich istniejących miast, ale do szerszej koncepcji “miasta” jako takiego.\nWażne kwestie do rozważenia:\n\nZakres i ograniczenia: Badacze powinni jasno określić, jakie jednostki lub procesy starają się zrozumieć, wykraczając poza samo opisanie próby i populacji.\nMożliwość uogólnienia: Przy formułowaniu wniosków dotyczących superpopulacji, badacze powinni wyraźnie określić granice, w których ich ustalenia mają zastosowanie.\nSpecyfika kontekstu: Chociaż koncepcja superpopulacji pozwala na szersze wnioskowanie niż tradycyjne pobieranie próbek, ważne jest, aby zdawać sobie sprawę, że PGD może się różnić w zależności od kontekstu.\n\nPrzykład podsumowujący: Jakość Pizzy w Nowym Jorku\nPopulacja: Wszystkie obecnie działające pizzerie w Nowym Jorku. To skończona, policzalna grupa lokali istniejących w danym momencie.\nPróba: Wybór 50 pizzerii losowo wybranych z różnych dzielnic Nowego Jorku. To konkretne pizzerie, w których badacze będą degustować i oceniać pizze.\nSuperpopulacja: Wszystkie możliwe pizzerie, które mogłyby istnieć w Nowym Jorku, w tym:\n\nObecnie działające pizzerie\nPrzyszłe pizzerie, które jeszcze nie zostały otwarte\nPizzerie, które zostały zamknięte\nHipotetyczne pizzerie, które mogłyby istnieć w innych warunkach ekonomicznych lub kulturowych\n\nKoncepcja superpopulacji pozwala nam myśleć o jakości pizzy wykraczając poza obecny “zrzut ekranu” nowojorskich pizzerii.\nProces Generowania Danych (PGD): PGD to złożony zestaw czynników, które przyczyniają się do jakości pizzy w każdej pizzerii. Może to obejmować:\n\nSkładniki: Jakość i źródło mąki, pomidorów, sera itp.\nUmiejętności szefa kuchni: Szkolenie, doświadczenie i osobiste podejście pizzaiolo\nSprzęt: Rodzaj i stan pieca, używane narzędzia\nPrzepis: Proporcje składników, metody przygotowania\nCzynniki środowiskowe: Wilgotność, jakość wody w Nowym Jorku\nWpływy kulturowe: Lokalne tradycje robienia pizzy, preferencje klientów\nCzynniki ekonomiczne: Koszty składników, ceny wynajmu wpływające na decyzje biznesowe\n\nPGD jest jak “przepis na jakość pizzy”, który ma zastosowanie nie tylko do naszej próby czy nawet obecnej populacji, ale do wszystkich potencjalnych pizzerii w superpopulacji.\nIntuicyjne Wyjaśnienie:\n\nJeśli odwiedzisz wszystkie obecnie działające pizzerie w Nowym Jorku i je ocenisz, zbadałeś populację.\nJeśli losowo wybierzesz 50 pizzerii do odwiedzenia i oceny, pobrałeś próbę.\nJeśli zastanawiasz się, jak jakość pizzy mogłaby się różnić we wszystkich możliwych nowojorskich pizzeriach (przeszłych, obecnych, przyszłych i hipotetycznych), myślisz o superpopulacji.\nJeśli próbujesz zrozumieć wszystkie czynniki, które składają się na jakość pizzy w Nowym Jorku, niezależnie od tego, czy dana pizzeria obecnie istnieje czy nie, badasz Proces Generowania Danych.\n\n\n\n\n\n\n\n\ngraph TD\n    A[Data Generating Process DGP]\n    B(Population)\n    C[Sample]\n    A --&gt;|Generates| B\n    B --&gt;|Sampled from| C\n    C -.-&gt;|Inference| B\n    C -.-&gt;|Inference| A\n    B -.-&gt;|Inference| A\n    \n    classDef dgp fill:#f9f,stroke:#333,stroke-width:4px;\n    classDef pop fill:#bbf,stroke:#333,stroke-width:2px;\n    classDef sam fill:#bfb,stroke:#333,stroke-width:2px;\n    \n    class A dgp;\n    class B pop;\n    class C sam;\n\n\n\n\n\n\n\n\n\n\n\n\nObjaśnienie diagramu PGD, Populacji i Próby\n\n\n\nDiagram przedstawia relacje między Procesem Generującym Dane (PGD), populacją i próbą, wraz ze ścieżkami wnioskowania:\n\nRelacje bezpośrednie (ciągłe strzałki):\n\nPGD generuje populację\nZ populacji pobierane są próby\n\nŚcieżki wnioskowania (przerywane strzałki):\n\nOd Próby do Populacji: Tradycyjne wnioskowanie statystyczne\nOd Próby do PGD: Wnioskowanie o podstawowym procesie na podstawie danych z próby\nOd Populacji do PGD: Wnioskowanie o PGD przy użyciu pełnych danych populacji\n\n\nNa przykład, w naszym badaniu wpływu ordynacji wyborczej na frekwencję w polskich gminach (wybory samorządowe 1998-2010):\n\nDysponujemy danymi dla całej populacji gmin, więc nie musimy wnioskować z próby o populacji.\nSkupiamy się na wykorzystaniu pełnych danych populacyjnych (prawa przerywana strzałka) do wnioskowania o leżącym u podstaw PGD—złożonych procesach, poprzez które ordynacja wyborcza wpływa na frekwencję wyborczą w gminach.\nTakie podejście pozwala nam potencjalnie zrozumieć mechanizmy, dzięki którym różne systemy wyborcze (np. reprezentacja proporcjonalna vs. większościowa) wpływają na poziom frekwencji, oraz formułować uzasadnione przewidywania o tym, jak zmiany w ordynacji wyborczej mogłyby wpłynąć na przyszłą frekwencję lub jak te efekty mogłyby się uogólniać na podobne konteksty.\n\n\n\n\n\n\nPopulacja vs. próba. Retrieved from: https://allmodelsarewrong.github.io/mse.html\n\n\nDane stanowią podstawę analizy statystycznej. Mogą być:\n\nDane pierwotne (Primary data): Zebrane bezpośrednio w określonym celu\nDane wtórne (Secondary data): Uzyskane z istniejących źródeł\n\nPrzykład: W badaniu wzrostu studentów uniwersyteckich, populacją są wszyscy studenci uniwersyteccy w kraju, podczas gdy próba może składać się z 1000 losowo wybranych studentów.\n\n\n2.3.2 Zmienne i Stałe (Variables and Constants)\nZmienne to cechy, które mogą przyjmować różne wartości w zbiorze danych. Mogą być:\n\nIlościowe (Quantitative):\n\nCiągłe (Continuous): Wzrost, waga, temperatura\nDyskretne (Discrete): Liczba dzieci, liczba błędów w programie\n\nJakościowe (Qualitative):\n\nNominalne (Nominal): Grupa krwi, kolor oczu\nPorządkowe (Ordinal): Poziom wykształcenia, ocena satysfakcji klienta\n\n\nStałe to wartości, które pozostają niezmienne w trakcie analizy.\n\n2.3.2.1 Rodzaje Danych w Naukach Społecznych\nBadania w naukach społecznych zajmują się różnymi rodzajami danych:\n\nDane Ilościowe: Dane liczbowe (np. odpowiedzi z ankiet, wskaźniki ekonomiczne)\nDane Jakościowe: Dane nieliczbowe (np. transkrypcje wywiadów, odpowiedzi na pytania otwarte w ankietach)\nBig Data: Dane cyfrowe na dużą skalę (np. posty w mediach społecznościowych, logi zachowań online)\n\n\n\n\n2.3.3 Parametry Populacji i Estymanda (Population Parameters and Estimands)\nParametry populacji to liczbowe charakterystyki populacji. Kluczowe punkty:\n\nOpisują całą populację, nie tylko próbę.\nZwykle oznaczane są greckimi literami.\nW większości przypadków nie mogą być bezpośrednio obliczone, ponieważ nie możemy zmierzyć całej populacji.\nSą determinowane przez podstawowy Proces Generujący Dane (DGP).\n\nTypowe parametry populacji to:\n\nŚrednia populacji (Population mean) (\\mu): Średnia/oczekiwana wartość zmiennej w populacji.\nWariancja populacji (Population variance) (\\sigma^2): Miara zmienności w populacji.\nProporcja populacji (Population proportion) (p): Proporcja osób w populacji posiadających daną cechę.\n\nEstymand (Estimand) to cel estymacji - konkretny parametr populacji lub funkcja parametrów, którą chcemy oszacować. Definiuje to, co chcemy wiedzieć o populacji.\n\n\n\n\n\n\nPrzykład: Wzrost Studentów Uniwersyteckich\n\n\n\nRozważmy wzrost wszystkich studentów uniwersyteckich w kraju:\n\n\\mu (estymand): Prawdziwa średnia wysokość wszystkich studentów uniwersyteckich (średnia populacji)\n\\sigma^2 (estymand): Prawdziwa wariancja wysokości w populacji\n\nTe parametry są nieznanymi estymandami, które chcemy oszacować na podstawie danych z próby.\n\n\n\n\n2.3.4 Statystyki i Estymatory (Statistic(s) and Estimators)\nStatystyka (pojedyncza) lub statystyka z próby to dowolna wielkość obliczona na podstawie wartości z próby, która jest rozważana w celu statystycznym.\nGdy statystyka jest używana do oszacowania estymandy (parametru populacji), nazywana jest estymatorem. Estymatory są funkcjami danych z próby, które dostarczają przybliżonych wartości dla nieznanych parametrów populacji.\nPrzykłady statystyk/estymatorów:\n\nŚrednia z próby (Sample mean): \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i (szacuje \\mu)\nWariancja z próby (Sample variance): s^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2 (szacuje \\sigma^2)\nProporcja z próby (Sample proportion): \\hat{p} = \\frac{x}{n} (szacuje p)\n\n\n\n2.3.5 Oszacowania (Estimates)\nOszacowanie to konkretna wartość uzyskana przez zastosowanie estymatora do konkretnej próby. Jest to wartość punktowa, która przybliża prawdziwą estymandę (parametr populacji).\nPrzykład: Jeśli obliczamy średnią wysokość z próby wynoszącą 173 cm, to 173 cm jest naszym oszacowaniem estymandy \\mu (średniej wysokości populacji).\n\n\n2.3.6 Modele Statystyczne (Statistical Models)\n\n\n\n\n\n\nNote\n\n\n\nModel w nauce to uproszczona reprezentacja złożonego systemu lub zjawiska. Jest on ta zaprojektowany, aby pomóc nam zrozumieć, wyjaśnić i przewidywać zjawiska zachodzące w rzeczywistym świecie. Modele mogą przybierać różne formy, w tym równania matematyczne, symulacje komputerowe lub ramy koncepcyjne. Pozwalają naukowcom skupić się na kluczowych aspektach systemu, ignorując mniej istotne szczegóły, co sprawia, że złożone problemy stają się łatwiejsze do zrozumienia i badania.\n\n\nModele statystyczne reprezentują relacje między zmiennymi i pomagają w przewidywaniu lub wnioskowaniu o estymandach (parametrach populacji).\nPrzykład: Model regresji liniowej y = \\beta_0 + \\beta_1x + \\epsilon opisuje relację między zmienną niezależną x a zmienną zależną y, gdzie:\n\ny to zmienna zależna (np. wielkość popytu na dobro)\nx to zmienna niezależna (np. cena lub dochód konsumenta)\n\\beta_0 i \\beta_1 to parametry, estymandy do oszacowania\n\\epsilon to składnik błędu, reprezentujący niewyjaśnioną zmienność\n\n\n\n\n\n\n\nWnioskowanie przyczynowe i kontrfakty\n\n\n\nW naukach społecznych często chcemy zrozumieć co by się stało, gdybyśmy podjęli inne działanie - ten hipotetyczny scenariusz nazywamy kontrfaktem/wynikiem kontrfaktycznym. Na przykład:\n\nJakie byłyby zarobki danej osoby, gdyby poszła na studia vs. gdyby nie poszła?\nJak zmieniłaby się frekwencja wyborcza, gdyby głosowanie było obowiązkowe?\n\nPonieważ nie możemy obserwować obu scenariuszy jednocześnie, modele statystyczne pomagają nam oszacować te kontrfakty poprzez:\n\nKontrolowanie zmiennych zakłócających (confounders)\nPorównywanie podobnych grup, które różnią się tylko badanym czynnikiem\nWykorzystanie technik takich jak dopasowanie według współczynnika skłonności czy zmienne instrumentalne\n\n\n\n\nFundamentalny problem wnioskowania przyczynowego: We can think of causal inference as a PREDICTION problem. How could we predict the counterfactual given that we never observe it?\n\n\nPamiętaj: Korelacja ≠ Przyczynowość, ale staranny projekt badawczy i metody statystyczne mogą pomóc nam formułować wnioski przyczynowe.\n\n\n\nConfounding bias and spurious correlation (https://www.bradyneal.com/causal-inference-course) drinking the night before is a common cause of sleeping with shoes on and waking up with a headache :-)\n\n\n\n\n\nReverse causality: https://ff13.fastforwardlabs.com/\n\n\n\n\n\n\n2.3.7 Wnioskowanie (Inference)\nWnioskowanie statystyczne to proces wyciągania wniosków o estymandach (parametrach populacji) na podstawie danych z próby. Obejmuje dwa główne typy:\n\nEstymacja (Estimation): Używanie statystyk z próby (estymatorów) do oszacowania estymand (parametrów populacji)\nTestowanie hipotez (Hypothesis testing): Podejmowanie decyzji o estymandach na podstawie dowodów z próby\n\n\n\n\n\n\n\nEstymacja i testowanie hipotez: wstęp\n\n\n\n\nEstymacja\n\nEstymacja polega na określeniu prawdopodobnej wartości parametru populacji na podstawie danych z próby. W kontekście rozkładu dwumianowego możemy być zainteresowani oszacowaniem prawdopodobieństwa sukcesu (p) dla określonego zdarzenia.\nPrzykład: Rzucanie monetą\nPowiedzmy, że rzucamy monetą 100 razy i chcemy oszacować prawdopodobieństwo wypadnięcia orła.\n\nRzucamy monetą 100 razy i obserwujemy 55 orłów.\nNasze punktowe oszacowanie p (prawdopodobieństwo wypadnięcia orła) wynosi 55/100 = 0,55\nMożemy również obliczyć przedział ufności, np. 95% przedział ufności może wynosić (0,45; 0,65).\n\nPrzedział ufności mówi nam o zakresie, w którym może leżeć prawdziwe prawdopodobieństwo. Mówiąc prościej: “Jesteśmy w 95% pewni, że prawdziwe prawdopodobieństwo wypadnięcia orła mieści się między 45% a 65%.”\nCelem jest tutaj dostarczenie naszego najlepszego oszacowania prawdziwego prawdopodobieństwa wypadnięcia orła, wraz z zakresem prawdopodobnych wartości.\nWażne Pojęcia Teorii Estymacji:\n\nObciążenie (Bias)\n\nObciążenie odnosi się do tendencji estymatora do systematycznego przeszacowania lub niedoszacowania prawdziwej wartości parametru populacji (estymandy).\n\nEstymator nieobciążony to taki, którego średnia wartość (przy wielokrotnym powtórzeniu estymacji) jest równa prawdziwej wartości parametru.\nObciążenie można rozumieć jako różnicę między średnią wartością estymatora a prawdziwą wartością parametru.\n\n\nEfektywność (Efficiency)\n\nEfektywność odnosi się do precyzji estymatora. Bardziej efektywny estymator daje wyniki bliższe prawdziwej wartości parametru, czyli ma mniejsze rozproszenie wyników.\n\nMierzona jest najczęściej wariancją estymatora (im mniejsza wariancja, tym większa efektywność)\nDla nieobciążonych estymatorów efektywność często porównuje się za pomocą Błędu Średniokwadratowego (Mean Squared Error, MSE)\n\n\nTestowanie hipotez\n\nTestowanie hipotez z kolei polega na podejmowaniu decyzji między dwoma konkurencyjnymi twierdzeniami dotyczącymi parametru populacji. Zazwyczaj mamy hipotezę zerową (H0) i hipotezę alternatywną (H1).\nPrzykład: Czy moneta jest uczciwa?\nKorzystając z tego samego scenariusza rzucania monetą, powiedzmy, że chcemy sprawdzić, czy moneta jest uczciwa (p = 0,5), czy też stronnicza na korzyść orła (p &gt; 0,5).\n\nHipoteza zerowa (H0): p = 0,5 (moneta jest uczciwa)\nHipoteza alternatywna (H1): p &gt; 0,5 (moneta jest stronnicza na korzyść orła)\nObserwujemy 55 orłów na 100 rzutów\n\nP-wartość i jak testowanie hipotez działa jako rodzaj “probabilistycznego dowodu nie wprost”:\n\nZaczynamy od założenia, że hipoteza zerowa (H0) jest prawdziwa. W tym przypadku zakładamy, że moneta jest uczciwa.\nNastępnie pytamy: “Jeśli moneta byłaby naprawdę uczciwa, jakie byłoby prawdopodobieństwo zaobserwowania 55 lub więcej orłów na 100 rzutów?”\nTo prawdopodobieństwo nazywa się wartością p. Jest to prawdopodobieństwo zaobserwowania naszych danych (lub bardziej ekstremalnych) przy założeniu, że hipoteza zerowa jest prawdziwa.\nJeśli to prawdopodobieństwo (wartość p) jest bardzo małe, mamy sprzeczność: zaobserwowaliśmy coś, co powinno być bardzo rzadkie, gdyby nasze założenie (H0) było prawdziwe.\nZwykle ustalamy próg zwany poziomem istotności (często 0,05 lub 5%) dla tego, co uważamy za “bardzo małe”.\nJeśli wartość p jest mniejsza niż wybrany poziom istotności, odrzucamy H0. Wnioskujemy, że nasza obserwacja jest zbyt mało prawdopodobna przy H0, więc faworyzujemy hipotezę alternatywną.\nJeśli wartość p jest większa niż nasz poziom istotności, nie odrzucamy H0. Nie mamy wystarczających dowodów, aby stwierdzić, że moneta jest stronnicza.\n\nTen proces jest jak “probabilistyczny dowód nie wprost”, ponieważ:\n\nZaczynamy od założenia H0 (podobnie jak zakładamy przeciwieństwo tego, co chcemy udowodnić w dowodzie nie wprost).\nSprawdzamy, czy to założenie prowadzi do bardzo mało prawdopodobnej sytuacji (naszych zaobserwowanych danych).\nJeśli tak, odrzucamy założenie (H0) i faworyzujemy alternatywę.\n\nWartość p dokładnie określa, jak mało prawdopodobna jest nasza obserwacja przy założeniu H0. Bardzo mała wartość p (np. 0,01) oznacza: “Gdyby H0 była prawdziwa, spodziewalibyśmy się zobaczyć tak ekstremalne dane tylko około 1% czasu.”\nTestowanie hipotez i estymacja to powiązane, ale odrębne procedury statystyczne; testowanie hipotez może być wykorzystane do wyciągania wniosków o oszacowaniach i może uzupełniać estymację na kilka sposobów, np.:\n\nTestowanie oszacowań punktowych: Testowanie hipotez może być wykorzystane do oceny, czy oszacowanie punktowe różni się istotnie od hipotetycznej wartości. Na przykład, jeśli oszacujemy, że moneta ma prawdopodobieństwo 0,55 wypadnięcia orłem, możemy użyć testu hipotezy, aby określić, czy ta wartość różni się istotnie od 0,5 (uczciwa moneta).\nIstotność parametrów: W modelach wielowymiarowych, testy hipotez (takie jak testy t w regresji) mogą pomóc określić, które oszacowane parametry różnią się istotnie od zera, dając wgląd w to, które zmienne są ważne w modelu.\n\n\n\n\n\n2.3.8 Relacje Między Pojęciami\n\nProces Generujący Dane (DGP - Data Generating Process) określa rzeczywiste wartości parametrów populacji (estymand).\nEstymandy są szacowane za pomocą statystyk obliczonych na podstawie próby (estymatorów).\nJakość estymatorów ocenia się na podstawie właściwości takich jak obciążenie i efektywność w szacowaniu estymandy.\nModele statystyczne wykorzystują oszacowane parametry do opisania relacji między zmiennymi w populacji.\nWnioskowanie statystyczne polega na wyciąganiu wniosków o estymandach na podstawie danych z próby, wykorzystując właściwości estymatorów.\n\n\n\n\n\n\n\nPrzykład: Badanie Zachowań Wyborczych\n\n\n\n\nPopulacja: Wszyscy uprawnieni wyborcy w kraju\nEstymanda: p = rzeczywista proporcja wyborców popierających danego kandydata\nPróba: 1000 losowo wybranych uprawnionych wyborców\nEstymator: \\hat{p} = proporcja wyborców z próby popierających kandydata\nOszacowanie: Konkretna wartość \\hat{p} obliczona z próby (np. 0,52)\nDGP: Złożona interakcja czynników wpływających na decyzje wyborcze, takich jak przekonania polityczne, warunki ekonomiczne, ekspozycja na media i sieci społeczne.\n\nZrozumienie DGP pomaga badaczom interpretować, dlaczego estymanda p ma określoną wartość i jak może się zmieniać w czasie. Na przykład, nagła zmiana w gospodarce może wpłynąć na preferencje wyborców, zmieniając tym samym wartość p.\nObciążenie i efektywność w kontekście przykładu:\n\nJeśli \\hat{p} jest nieobciążonym estymatorem, oznacza to, że przy wielokrotnym powtórzeniu badania na różnych próbach, średnia wartość \\hat{p} będzie bliska rzeczywistej wartości p.\nEfektywność \\hat{p} określa, jak bardzo rozproszone są wyniki poszczególnych badań wokół tej średniej. Im mniejsze rozproszenie, tym estymator jest bardziej efektywny.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#główne-komponenty-nauki-o-danych-w-badaniach-naukowych",
    "href": "rozdzial1.html#główne-komponenty-nauki-o-danych-w-badaniach-naukowych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.4 Główne Komponenty Nauki o Danych w Badaniach Naukowych",
    "text": "2.4 Główne Komponenty Nauki o Danych w Badaniach Naukowych\n\nZbieranie DanychPrzetwarzanie DanychEksploracyjna Analiza Danych (EDA)Wnioskowanie StatystyczneUczenie MaszynoweWizualizacja Danych i KomunikacjaPowtarzalność i Otwarta Nauka\n\n\n\nMetody eksperymentalne: Kontrolowane badania, w których naukowcy manipulują zmiennymi, aby obserwować efekty\nBadania obserwacyjne: Gromadzenie danych poprzez obserwację i rejestrację bez ingerencji\nAnkiety i wywiady: Zbieranie informacji bezpośrednio od ludzi poprzez zadawanie pytań\nCyfrowe zbieranie danych: Gromadzenie danych ze źródeł internetowych, czujników lub systemów komputerowych\nAspekty etyczne: Zapewnienie, że badania respektują prawa i dobro uczestników\n\n\n\n\nCzyszczenie danych: Usuwanie błędów i niespójności z surowych danych\nObsługa brakujących wartości: Radzenie sobie z lukami w zbiorze danych, które mogłyby wpłynąć na analizę\nTransformacja danych: Konwertowanie danych na formaty odpowiednie do analizy, np. zmiana tekstu na liczby\n\n\n\n\nStatystyki opisowe: Podsumowanie danych za pomocą miar takich jak średnia, mediana i odchylenie standardowe\nWizualizacja danych: Tworzenie wykresów i diagramów do wizualnego przedstawienia wzorców w danych\nIdentyfikacja wzorców: Odkrywanie trendów lub zależności w danych\n\n\n\n\nTestowanie hipotez: Wykorzystanie danych do oceny twierdzeń o populacjach\nAnaliza regresji: Badanie zależności między zmiennymi i dokonywanie przewidywań\nWnioskowanie przyczynowe: Określanie, czy jedna zmienna bezpośrednio wpływa na inną\n\n\n\n\nUczenie nadzorowane: Trenowanie modeli do przewidywania wyników przy użyciu danych ze znanymi odpowiedziami\nUczenie nienadzorowane: Znajdowanie ukrytych wzorców w danych bez predefiniowanych kategorii\nPrzetwarzanie języka naturalnego (NLP): Nauczanie komputerów rozumienia i analizy ludzkiego języka\n\n\n\n\nEfektywne wizualizacje: Tworzenie czytelnych, informatywnych grafik do przedstawiania złożonych danych\nKomunikacja naukowa: Wyjaśnianie wyników różnym odbiorcom, od ekspertów po ogół społeczeństwa\nPisanie naukowe: Przygotowywanie artykułów i raportów naukowych w celu dzielenia się wynikami\n\n\n\n\nKontrola wersji: Śledzenie zmian w danych i kodzie w trakcie procesu badawczego\nPraktyki otwartych danych: Udostępnianie danych i metod badawczych do weryfikacji i dalszych badań\nPowtarzalne procesy badawcze: Dokumentowanie kroków badawczych, aby inni mogli powtórzyć badanie",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#narzędzia-do-nauki-o-danych-w-naukach-społecznych",
    "href": "rozdzial1.html#narzędzia-do-nauki-o-danych-w-naukach-społecznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.5 Narzędzia do Nauki o Danych w Naukach Społecznych",
    "text": "2.5 Narzędzia do Nauki o Danych w Naukach Społecznych\nW tym kursie będziemy głównie używać R do naszej analizy danych, ponieważ jest on szeroko stosowany w badaniach nauk społecznych.\n\n2.5.1 R w Analizie Danych Nauk Społecznych\nR oferuje potężne możliwości dla badań w naukach społecznych, od manipulacji danymi po zaawansowane modelowanie statystyczne.\n\n\nKliknij, aby pokazać/ukryć kod R\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nKliknij, aby pokazać/ukryć kod R\n# Set seed for reproducibility\nset.seed(42)\n\n# Generate example data with a Simpson's Paradox\nn &lt;- 1000\ndata &lt;- tibble(\n  age_group = sample(c(\"Young\", \"Middle\", \"Old\"), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)),\n  education_years = case_when(\n    age_group == \"Young\" ~ rnorm(n, mean = 10, sd = 1),\n    age_group == \"Middle\" ~ rnorm(n, mean = 13, sd = 1),\n    age_group == \"Old\" ~ rnorm(n, mean = 16, sd = 1)\n  ),\n  income = case_when(\n    age_group == \"Young\" ~ 70000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Middle\" ~ 50000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000),\n    age_group == \"Old\" ~ 30000 + 1000 * education_years + rnorm(n, mean = 0, sd = 5000)\n  )\n)\n\n# Basic data summary\nsummary(data)\n\n\n  age_group         education_years      income     \n Length:1000        Min.   : 6.628   Min.   :34068  \n Class :character   1st Qu.:10.913   1st Qu.:51508  \n Mode  :character   Median :13.004   Median :63376  \n                    Mean   :12.986   Mean   :63307  \n                    3rd Qu.:14.934   3rd Qu.:75023  \n                    Max.   :18.861   Max.   :96620  \n\n\nKliknij, aby pokazać/ukryć kod R\n# Correlation analysis\ncor(data %&gt;% select(education_years, income))\n\n\n                education_years     income\neducation_years       1.0000000 -0.8152477\nincome               -0.8152477  1.0000000\n\n\nKliknij, aby pokazać/ukryć kod R\n# Overall trend (Simpson's Paradox)\noverall_plot &lt;- ggplot(data, aes(x = education_years, y = income)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Overall Relationship between Education and Income\",\n       subtitle = \"Simpson's Paradox: Appears negative\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Trend by age group (Resolving Simpson's Paradox)\ngrouped_plot &lt;- ggplot(data, aes(x = education_years, y = income, color = age_group)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Relationship between Education and Income by Age Group\",\n       subtitle = \"Resolving Simpson's Paradox: Positive relationship within groups\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n# Statistical analysis\nmodel_overall &lt;- lm(income ~ education_years, data = data)\nmodel_by_age &lt;- lm(income ~ education_years + age_group, data = data)\n\n# Print results\nprint(overall_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(grouped_plot)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_overall))\n\n\n\nCall:\nlm(formula = income ~ education_years, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-24451  -5439    235   5262  34328 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     121814.7     1339.5   90.94   &lt;2e-16 ***\neducation_years  -4505.4      101.3  -44.47   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7976 on 998 degrees of freedom\nMultiple R-squared:  0.6646,    Adjusted R-squared:  0.6643 \nF-statistic:  1978 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(summary(model_by_age))\n\n\n\nCall:\nlm(formula = income ~ education_years + age_group, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-14827  -3369    118   3356  16388 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      48270.8     2028.4  23.797  &lt; 2e-16 ***\neducation_years   1135.5      154.6   7.345 4.26e-13 ***\nage_groupOld    -19942.8      593.2 -33.619  &lt; 2e-16 ***\nage_groupYoung   20461.1      600.7  34.064  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4950 on 996 degrees of freedom\nMultiple R-squared:  0.8711,    Adjusted R-squared:  0.8707 \nF-statistic:  2244 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\nKliknij, aby pokazać/ukryć kod R\n# Calculate and print correlations\noverall_cor &lt;- cor(data$education_years, data$income)\ngroup_cors &lt;- data %&gt;%\n  group_by(age_group) %&gt;%\n  summarize(correlation = cor(education_years, income))\n\nprint(\"Overall correlation:\")\n\n\n[1] \"Overall correlation:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(overall_cor)\n\n\n[1] -0.8152477\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(\"Correlations by age group:\")\n\n\n[1] \"Correlations by age group:\"\n\n\nKliknij, aby pokazać/ukryć kod R\nprint(group_cors)\n\n\n# A tibble: 3 × 2\n  age_group correlation\n  &lt;chr&gt;           &lt;dbl&gt;\n1 Middle          0.185\n2 Old             0.291\n3 Young           0.223\n\n\nTen przykład demonstruje podstawowe operacje na danych, statystyki opisowe i wizualizację danych przy użyciu R.\nCertainly. Here’s the Polish version of the section on causal inference versus observational studies:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#wnioskowanie-przyczynowe-a-badania-obserwacyjne",
    "href": "rozdzial1.html#wnioskowanie-przyczynowe-a-badania-obserwacyjne",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.6 Wnioskowanie przyczynowe a badania obserwacyjne",
    "text": "2.6 Wnioskowanie przyczynowe a badania obserwacyjne\nW naukach społecznych i nie tylko, zrozumienie relacji między zmiennymi jest kluczowe. Dwa główne podejścia to wnioskowanie przyczynowe i badania obserwacyjne, każde z własnymi mocnymi stronami i ograniczeniami.\n\nWnioskowanie przyczynoweBadania obserwacyjneKluczowe rozróżnienie: Korelacja vs. Przyczynowość\n\n\n\nDąży do ustalenia związków przyczynowo-skutkowych\nCzęsto obejmuje plany eksperymentalne lub zaawansowane techniki statystyczne\nStara się odpowiedzieć na pytania “Co by było, gdyby?” i określić wpływ interwencji\nPrzykłady: Randomizowane badania kontrolowane, projekty quasi-eksperymentalne, zmienne instrumentalne\n\n\n\n\nBadają relacje między zmiennymi bez bezpośredniej interwencji\nOpierają się na danych zebranych w naturalnych warunkach lub z istniejących zbiorów danych\nMogą identyfikować korelacje i wzorce, ale mają trudności z ustaleniem przyczynowości\nPrzykłady: Badania kohortowe, badania kliniczno-kontrolne, przekrojowe badania ankietowe\n\n\n\n\n\n\n\n\n\n\n\n\n\nPamiętaj: Korelacja nie implikuje przyczynowości\n\n\n\nFundamentalna zasada w badaniach głosi, że korelacja między dwiema zmiennymi niekoniecznie implikuje związek przyczynowy. Ta koncepcja jest kluczowa przy interpretacji wyników badań obserwacyjnych.\n\nKorelacja: Mierzy siłę i kierunek związku między zmiennymi\nPrzyczynowość: Wskazuje, że zmiany w jednej zmiennej bezpośrednio powodują zmiany w drugiej\n\nChociaż silne korelacje mogą sugerować potencjalne związki przyczynowe, do ustalenia przyczynowości wymagane są dodatkowe dowody i rygorystyczne metody.\n\n\n\nWyzwania w ustalaniu przyczynowościMetody wzmacniania twierdzeń przyczynowychZnaczenie w naukach społecznych\n\n\n\nZmienne zakłócające: Niezmierzone czynniki wpływające zarówno na domniemaną przyczynę, jak i skutek\nOdwrotna przyczynowość: Domniemany skutek może w rzeczywistości powodować domniemaną przyczynę\nBłąd selekcji: Nielosowy dobór uczestników do grup badawczych\n\n\n\n\nRandomizowane badania kontrolowane (gdy są etyczne i wykonalne)\nNaturalne eksperymenty lub projekty quasi-eksperymentalne\nDopasowanie według propensity score\nAnaliza różnicy w różnicach\nPodejścia oparte na zmiennych instrumentalnych\nSkierowane grafy acykliczne (DAG) do wizualizacji relacji przyczynowych\n\n\n\nZrozumienie różnicy między wnioskowaniem przyczynowym a badaniami obserwacyjnymi jest kluczowe w naukach społecznych, gdzie względy etyczne często ograniczają manipulacje eksperymentalne. Badacze muszą starannie projektować badania i interpretować wyniki, aby uniknąć wprowadzających w błąd wniosków dotyczących przyczynowości.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#modele-w-nauce-od-deterministycznych-do-stochastycznych",
    "href": "rozdzial1.html#modele-w-nauce-od-deterministycznych-do-stochastycznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.7 Modele w Nauce: Od Deterministycznych do Stochastycznych",
    "text": "2.7 Modele w Nauce: Od Deterministycznych do Stochastycznych\nModele są niezbędnymi narzędziami w badaniach naukowych, pomagając naukowcom reprezentować, rozumieć i przewidywać złożone zjawiska. Ta sekcja omawia główne typy modeli stosowanych w nauce, wraz z przykładami ich zastosowań. Należy pamiętać, że te kategorie często się nakładają, a wiele modeli naukowych łączy w sobie różne aspekty.\n\n2.7.1 Modele Matematyczne\nModele matematyczne wykorzystują równania i koncepcje matematyczne do opisywania i analizowania systemów lub zjawisk. Można je podzielić na kilka podkategorii, choć należy pamiętać, że niektóre złożone modele mogą zawierać elementy z wielu kategorii:\n\n2.7.1.1 a. Modele Deterministyczne\nModele deterministyczne dostarczają precyzyjnych przewidywań na podstawie zestawu zmiennych, bez uwzględniania losowości na poziomie makroskopowym.\nPrzykład: Prawa ruchu Newtona, które mogą precyzyjnie przewidzieć ruch obiektów pod wpływem znanych sił w mechanice klasycznej.\n\n\n2.7.1.2 b. Modele Stochastyczne\nModele stochastyczne uwzględniają losowość i prawdopodobieństwo. Jednak kluczowe jest rozróżnienie dwóch fundamentalnie różnych typów modeli stochastycznych:\n\n2.7.1.2.1 i. Klasyczne Modele Stochastyczne\nTe modele zajmują się losowością wynikającą z niepełnej informacji lub złożonych interakcji w systemach klasycznych. Podstawowy system jest deterministyczny, ale praktyczne ograniczenia w pomiarach lub obliczeniach prowadzą do użycia opisów probabilistycznych.\nPrzykład: Modele regresji w statystyce, gdzie losowość reprezentuje niewyjaśnioną zmienność lub błąd pomiaru:\ny = β_0 + β_1x + ε\nGdzie:\n\ny to zmienna zależna (np. wielkość popytu na dobro)\nx to zmienna niezależna (np. cena lub dochód konsumenta)\nβ_0 i β_1 to parametry\nε to składnik błędu, reprezentujący niewyjaśnioną zmienność\n\n\n\n2.7.1.2.2 ii. Kwantowe Modele Stochastyczne\nTe modele zajmują się fundamentalną, nieredukowalną losowością nieodłącznie związaną z systemami mechaniki kwantowej. Ta losowość nie wynika z braku informacji, ale jest podstawową cechą rzeczywistości kwantowej.\nPrzykład: Model Standardowy w fizyce cząstek elementarnych, który opisuje interakcje cząstek za pomocą kwantowej teorii pola. Na przykład, rozpad cząstki jest z natury probabilistyczny:\nP(t) = e^{-t/τ}\nGdzie:\n\nP(t) to prawdopodobieństwo, że cząstka nie rozpadła się po czasie t\nτ to średni czas życia cząstki\n\n\n\n\n2.7.1.3 c. Modele Symulacji Komputerowych\nSymulacje komputerowe wykorzystują algorytmy i metody obliczeniowe oparte na modelach matematycznych do symulowania złożonych systemów i przewidywania ich zachowania w czasie. Mogą być deterministyczne lub stochastyczne.\nPrzykład: Modele klimatyczne symulujące system klimatyczny Ziemi, uwzględniające czynniki takie jak skład atmosfery, prądy oceaniczne i promieniowanie słoneczne do prognozowania przyszłych scenariuszy klimatycznych.\n\n\n\n2.7.2 Modele Koncepcyjne\nModele koncepcyjne to abstrakcyjne reprezentacje systemów lub procesów, często wykorzystujące diagramy lub schematy blokowe do ilustrowania relacji między komponentami.\nPrzykład: Model obiegu wody w naukach o Ziemi, który ilustruje ciągły ruch wody w obrębie Ziemi i atmosfery poprzez procesy takie jak parowanie, opady i spływ powierzchniowy.\n\n\n2.7.3 Modele Fizyczne\nModele fizyczne to namacalne reprezentacje obiektów lub systemów, często w formie pomniejszonej lub uproszczonej wersji rzeczywistego obiektu.\nPrzykład: Modele tunelu aerodynamicznego w badaniach aerodynamiki, używane do badania efektów przepływu powietrza wokół obiektów stałych i optymalizacji projektów samolotów, pojazdów lub budynków.\n\n\n2.7.4 Modele Teoretyczne\nModele teoretyczne to abstrakcyjne ramy oparte na fundamentalnych zasadach i hipotezach, często używane do wyjaśniania obserwowanych zjawisk lub przewidywania nowych. Te modele często wykorzystują równania matematyczne i mogą być deterministyczne lub stochastyczne.\nPrzykład: Teoria ewolucji poprzez dobór naturalny, która dostarcza ram do zrozumienia różnorodności i adaptacji form życia w czasie.\n\n\n2.7.5 Podsumowanie\nTe różne formy modeli odgrywają kluczową rolę w badaniach naukowych, każda oferując unikalne zalety dla zrozumienia i przewidywania zjawisk naturalnych. Naukowcy często używają wielu typów modeli jednocześnie, aby uzyskać kompleksowy wgląd w złożone systemy i procesy.\nWażne jest, aby zdawać sobie sprawę, że te kategorie nie są wzajemnie wykluczające i często się nakładają:\n\nModele matematyczne stanowią podstawę dla wielu innych typów modeli, w tym symulacji komputerowych i niektórych modeli teoretycznych.\nModele symulacji komputerowych są zasadniczo modelami matematycznymi implementowanymi za pomocą metod obliczeniowych i mogą być deterministyczne lub stochastyczne.\nModele teoretyczne często wykorzystują sformułowania matematyczne i mogą być implementowane jako symulacje komputerowe.\nModele fizyczne mogą być projektowane na podstawie modeli matematycznych i mogą być używane do walidacji symulacji komputerowych.\n\nWybór typu modelu często zależy od konkretnego pytania badawczego, natury badanego systemu, dostępnych danych oraz zasobów obliczeniowych. W miarę postępu nauki granice między tymi typami modeli coraz bardziej się zacierają, prowadząc do coraz bardziej wyrafinowanych i interdyscyplinarnych podejść do modelowania złożonych zjawisk.\nKluczowe jest rozróżnienie różnych typów modeli stochastycznych. Klasyczne modele stochastyczne, takie jak te używane w analizie regresji, zajmują się losowością wynikającą z niepełnej informacji lub złożonych interakcji w systemach, które są zasadniczo deterministyczne. Z drugiej strony, kwantowe modele stochastyczne, jak te w fizyce cząstek, zajmują się fundamentalną, nieredukowalną losowością nieodłącznie związaną z systemami mechaniki kwantowej. To rozróżnienie odzwierciedla głębokie różnice między klasycznymi a kwantowymi paradygmatami w fizyce i podkreśla różnorodne sposoby, w jakie prawdopodobieństwo jest wykorzystywane w modelowaniu naukowym.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#zrozumienie-pozornych-korelacji-zmiennych-zakłócających-i-kolizyjnych",
    "href": "rozdzial1.html#zrozumienie-pozornych-korelacji-zmiennych-zakłócających-i-kolizyjnych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.8 Zrozumienie Pozornych Korelacji, Zmiennych Zakłócających i Kolizyjnych (*)",
    "text": "2.8 Zrozumienie Pozornych Korelacji, Zmiennych Zakłócających i Kolizyjnych (*)\nW tej sekcji zbadamy trzy ważne pojęcia w analizie statystycznej: pozorne korelacje, zmienne zakłócające i zmienne kolizyjne. Zrozumienie tych pojęć jest kluczowe dla uniknięcia błędnej interpretacji danych i wyciągania nieprawidłowych wniosków z analiz statystycznych.\nZacznijmy od załadowania niezbędnych bibliotek:\n\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\nset.seed(123) # dla powtarzalności\n\n\n2.8.1 Pozorne Korelacje\nPozorne korelacje to związki między zmiennymi, które wydają się przyczynowe, ale w rzeczywistości są przypadkowe lub spowodowane przez niewidoczny trzeci czynnik.\n\n2.8.1.1 Przykład: Sprzedaż lodów a przypadki utonięć\nStwórzmy zbiór danych, który pokazuje pozorną korelację między sprzedażą lodów a przypadkami utonięć:\n\nn &lt;- 100\ndane_pozorne &lt;- tibble(\n  temperatura = rnorm(n, mean = 25, sd = 5),\n  sprzedaz_lodow = 100 + 5 * temperatura + rnorm(n, sd = 10),\n  przypadki_utoniec = 1 + 0.5 * temperatura + rnorm(n, sd = 2)\n)\n\nggplot(dane_pozorne, aes(x = sprzedaz_lodow, y = przypadki_utoniec)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Pozorna Korelacja: Sprzedaż Lodów vs Przypadki Utonięć\",\n       x = \"Sprzedaż Lodów\", y = \"Przypadki Utonięć\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nTen wykres pokazuje pozytywną korelację między sprzedażą lodów a przypadkami utonięć. Jednak ta relacja jest pozorna. Prawdziwą przyczyną obu zjawisk jest temperatura:\n\nggplot(dane_pozorne, aes(x = temperatura)) +\n  geom_point(aes(y = sprzedaz_lodow), color = \"blue\") +\n  geom_point(aes(y = przypadki_utoniec * 10), color = \"red\") +\n  geom_smooth(aes(y = sprzedaz_lodow), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(aes(y = przypadki_utoniec * 10), method = \"lm\", se = FALSE, color = \"red\") +\n  scale_y_continuous(\n    name = \"Sprzedaż Lodów\",\n    sec.axis = sec_axis(~./10, name = \"Przypadki Utonięć\")\n  ) +\n  labs(title = \"Temperatura jako Wspólna Przyczyna\",\n       x = \"Temperatura\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n2.8.2 Zmienne Zakłócające\nZmienna zakłócająca to zmienna, która wpływa zarówno na zmienną zależną, jak i niezależną, powodując pozorny związek.\n\n2.8.2.1 Przykład: Edukacja, Dochód i Wiek\nStwórzmy zbiór danych, w którym wiek zakłóca relację między edukacją a dochodem:\n\nlibrary(tidyverse)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nn &lt;- 1000\nconfounder_data &lt;- tibble(\n  age = runif(n, 25, 65),\n  education = round(10 + 0.1 * age + rnorm(n, sd = 2)),\n  income = 20000 + 1000 * education + 500 * age + rnorm(n, sd = 5000)\n)\n\n# Without controlling for age\nmodel_naive &lt;- lm(income ~ education, data = confounder_data)\n# Controlling for age\nmodel_adjusted &lt;- lm(income ~ education + age, data = confounder_data)\n\n# Create age groups for visualization\nconfounder_data &lt;- confounder_data %&gt;%\n  mutate(age_group = cut(age, breaks = 3, labels = c(\"Young\", \"Middle\", \"Old\")))\n\n# Visualize\nggplot(confounder_data, aes(x = education, y = income)) +\n  geom_point(aes(color = age), alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1.2) +\n  geom_smooth(aes(group = age_group, color = as.numeric(age_group)), \n              method = \"lm\", se = FALSE, linewidth = 1) +\n  scale_color_viridis_c(name = \"Age\", \n                        breaks = c(30, 45, 60), \n                        labels = c(\"Young\", \"Middle\", \"Old\")) +\n  labs(title = \"Education vs Income, Confounded by Age\",\n       x = \"Years of Education\", y = \"Income\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPorównajmy współczynniki:\n\nsummary(model_naive)$coefficients[\"education\", \"Estimate\"]\n\n[1] 2328.718\n\nsummary(model_adjusted)$coefficients[\"education\", \"Estimate\"]\n\n[1] 1101.783\n\n\nEfekt edukacji na dochód jest przeszacowany, gdy nie kontrolujemy wieku.\n\n\n\n2.8.3 Zmienne Kolizyjne\nZmienna kolizyjna to zmienna, na którą wpływają zarówno zmienna niezależna, jak i zmienna zależna. Kontrolowanie zmiennej kolizyjnej może wprowadzić pozorną korelację.\n\n2.8.3.1 Przykład: Satysfakcja z pracy, Wynagrodzenie i Równowaga między pracą a życiem prywatnym\nStwórzmy zbiór danych, w którym równowaga między pracą a życiem prywatnym jest zmienną kolizyjną między satysfakcją z pracy a wynagrodzeniem:\n\nn &lt;- 1000\ndane_kolizyjne &lt;- tibble(\n  satysfakcja_z_pracy = rnorm(n),\n  wynagrodzenie = rnorm(n),\n  rownowaga_praca_zycie = -0.5 * satysfakcja_z_pracy - 0.5 * wynagrodzenie + rnorm(n, sd = 0.5)\n)\n\n# Bez kontrolowania równowagi praca-życie\nmodel_poprawny &lt;- lm(wynagrodzenie ~ satysfakcja_z_pracy, data = dane_kolizyjne)\n\n# Błędne kontrolowanie równowagi praca-życie\nmodel_kolizyjny &lt;- lm(wynagrodzenie ~ satysfakcja_z_pracy + rownowaga_praca_zycie, data = dane_kolizyjne)\n\n# Wizualizacja\nggplot(dane_kolizyjne, aes(x = satysfakcja_z_pracy, y = wynagrodzenie, color = rownowaga_praca_zycie)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_color_viridis_c() +\n  labs(title = \"Satysfakcja z Pracy vs Wynagrodzenie, Równowaga Praca-Życie jako Zmienna Kolizyjna\",\n       x = \"Satysfakcja z Pracy\", y = \"Wynagrodzenie\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPorównajmy współczynniki:\n\nsummary(model_poprawny)$coefficients[\"satysfakcja_z_pracy\", \"Estimate\"]\n\n[1] 0.02063487\n\nsummary(model_kolizyjny)$coefficients[\"satysfakcja_z_pracy\", \"Estimate\"]\n\n[1] -0.4794016\n\n\nKontrolowanie zmiennej kolizyjnej (równowaga praca-życie) wprowadza pozorną korelację między satysfakcją z pracy a wynagrodzeniem.\n\n\n\n2.8.4 Podsumowanie\nZrozumienie pozornych korelacji, zmiennych zakłócających i kolizyjnych jest kluczowe dla prawidłowej analizy statystycznej i wnioskowania przyczynowego. Zawsze rozważ podstawową strukturę przyczynową swoich danych i bądź ostrożny w kwestii tego, które zmienne kontrolujesz w swoich analizach.\n\n\n2.8.5 Dalsza Lektura\n\nPearl, J. (2009). Causality: Models, Reasoning, and Inference. Cambridge University Press.\nHernán, M. A., & Robins, J. M. (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#etyczne-aspekty-w-analizie-danych-nauk-społecznych",
    "href": "rozdzial1.html#etyczne-aspekty-w-analizie-danych-nauk-społecznych",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.9 Etyczne Aspekty w Analizie Danych Nauk Społecznych",
    "text": "2.9 Etyczne Aspekty w Analizie Danych Nauk Społecznych\nEtyka odgrywa kluczową rolę w badaniach nauk społecznych:\n\nPrywatność i Zgoda: Zapewnienie prywatności uczestników i świadomej zgody\nOchrona Danych: Bezpieczne przechowywanie i zarządzanie wrażliwymi danymi osobowymi\nBłędy i Reprezentacja: Adresowanie błędów próbkowania i zapewnienie różnorodnej reprezentacji\nPrzejrzystość: Jasne komunikowanie metod badawczych i ograniczeń\nWpływ Społeczny: Rozważanie potencjalnych społecznych implikacji wyników badań\n\n\n\n\n\n\n\nWarning\n\n\n\nNaukowcy społeczni muszą starannie rozważyć etyczne implikacje swoich praktyk zbierania, analizy i rozpowszechniania danych.\n\n\n\n2.9.1 Kluczowe Wnioski\n\nNauka o danych w naukach społecznych bazuje na tradycyjnych metodach statystycznych, włączając nowe technologie do analizy złożonych zjawisk społecznych.\nZrozumienie koncepcji takich jak populacja, próba i procesy generowania danych jest kluczowe dla prawidłowych badań w naukach społecznych.\nProces nauki o danych w badaniach społecznych obejmuje wiele etapów, od etycznego zbierania danych po komunikację wniosków.\nR jest potężnym narzędziem do analizy danych w naukach społecznych, oferującym szeroki zakres możliwości.\nAspekty etyczne powinny być na pierwszym planie każdego projektu związanego z danymi w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-a-losowość-klasyczna-a-kwantowa-zrozumienie-fundamentalnych-różnic",
    "href": "rozdzial1.html#appendix-a-losowość-klasyczna-a-kwantowa-zrozumienie-fundamentalnych-różnic",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.10 Appendix A: Losowość Klasyczna a Kwantowa: Zrozumienie Fundamentalnych Różnic",
    "text": "2.10 Appendix A: Losowość Klasyczna a Kwantowa: Zrozumienie Fundamentalnych Różnic\nAby zrozumieć, jak losowość w mechanice kwantowej różni się od losowości reprezentowanej przez składnik błędu w modelach regresji, musimy przeanalizować ich pochodzenie, naturę i implikacje.\n\n2.10.1 Pochodzenie Losowości\n\n2.10.1.1 Losowość Klasyczna (Modele Regresji)\n\nŹródło: Niekompletna informacja lub złożone interakcje w systemie, który w zasadzie jest deterministyczny.\nNatura: Niepewność epistemiczna (wynikająca z braku wiedzy).\nPrzykład: W modelu regresji, y = β_0 + β_1x + ε, składnik błędu ε reprezentuje niewyjaśnioną zmienność.\n\n\n\n2.10.1.2 Losowość Kwantowa\n\nŹródło: Fundamentalna właściwość systemów kwantowych.\nNatura: Niepewność ontyczna (nieodłączna cecha systemu, nie wynika z braku wiedzy).\nPrzykład: Dokładny moment rozpadu atomu radioaktywnego nie może być przewidziany, można określić jedynie jego prawdopodobieństwo.\n\n\n\n\n2.10.2 Implikacje Filozoficzne\n\n2.10.2.1 Losowość Klasyczna\n\nDeterminizm: Podstawowa rzeczywistość jest deterministyczna; losowość odzwierciedla naszą niewiedzę.\nUkryte Zmienne: W zasadzie, gdybyśmy mieli pełną informację, moglibyśmy dokładnie przewidzieć wyniki.\n\n\n\n2.10.2.2 Losowość Kwantowa\n\nIndeterminizm: Losowość jest fundamentalną cechą rzeczywistości, nie tylko naszego jej opisu.\nBrak Ukrytych Zmiennych: Nawet przy pełnej informacji o systemie kwantowym, niektóre wyniki pozostają nieprzewidywalne (co sugeruje twierdzenie Bella).\n\n\n\n\n2.10.3 Ujęcie Matematyczne\n\n2.10.3.1 Losowość Klasyczna\n\nTeoria Prawdopodobieństwa: Oparta na klasycznej teorii prawdopodobieństwa.\nRozkład: Często zakłada się znane rozkłady (np. rozkład normalny w wielu modelach regresji).\nCentralne Twierdzenie Graniczne: Stosuje się do dużych prób zmiennych losowych.\n\n\n\n2.10.3.2 Losowość Kwantowa\n\nPrawdopodobieństwo Kwantowe: Oparte na matematycznych podstawach mechaniki kwantowej.\nFunkcja Falowa: Opisuje stan kwantowy i jego ewolucję.\nReguła Borna: Określa prawdopodobieństwa wyników pomiarów na podstawie funkcji falowej.\n\n\n\n\n2.10.4 Przewidywalność i Kontrola\n\n2.10.4.1 Losowość Klasyczna\n\nRedukowalna: W zasadzie można ją zmniejszyć, zbierając więcej danych lub poprawiając dokładność pomiarów.\nKontrolowalna: Błędy systematyczne można zidentyfikować i skorygować.\n\n\n\n2.10.4.2 Losowość Kwantowa\n\nNieredukowalna: Nie można jej wyeliminować nawet przy idealnych pomiarach.\nFundamentalnie Niekontrolowalna: Sam akt pomiaru wpływa na system (problem pomiaru).\n\n\n\n\n2.10.5 Praktyczne Implikacje\n\n2.10.5.1 Losowość Klasyczna\n\nRedukcja Błędów: Koncentracja na udoskonalaniu technik pomiarowych i zbierania danych.\nUdoskonalanie Modelu: Dążenie do wyjaśnienia większej wariancji i zmniejszenia składnika błędu.\n\n\n\n2.10.5.2 Losowość Kwantowa\n\nNieodłączne Ograniczenie: Akceptacja fundamentalnych granic przewidywalności.\nPrzewidywania Probabilistyczne: Skupienie na dokładnych rozkładach prawdopodobieństwa zamiast na dokładnych wynikach.\n\n\n\n\n2.10.6 Przykłady Pomagające Zrozumieć Różnicę\n\n2.10.6.1 Przykład Losowości Klasycznej\nWyobraź sobie rzut monetą. Fizyka klasyczna mówi, że wynik jest zdeterminowany przez warunki początkowe (przyłożona siła, opór powietrza itp.). “Losowość” wynika z naszej niezdolności do precyzyjnego zmierzenia i uwzględnienia wszystkich tych czynników.\n\n\n2.10.6.2 Przykład Losowości Kwantowej\nW eksperymencie z podwójną szczeliną pojedyncze cząstki wykazują wzory interferencyjne, jakby przechodziły przez obie szczeliny jednocześnie. Dokładna ścieżka każdej pojedynczej cząstki jest fundamentalnie nieokreślona do momentu pomiaru, a tej nieokreśloności nie można rozwiązać przez bardziej precyzyjne pomiary.\n\n\n\n2.10.7 Podsumowanie\nChociaż oba rodzaje losowości prowadzą do probabilistycznych przewidywań, ich fundamentalne natury są zupełnie różne:\n\nLosowość klasyczna w modelach regresji jest odzwierciedleniem naszej niepełnej wiedzy lub ograniczeń pomiarowych w systemie, który w zasadzie jest deterministyczny.\nLosowość kwantowa jest fundamentalną właściwością systemów kwantowych, reprezentującą nieodłączną nieokreśloność w naturze, która utrzymuje się nawet przy doskonałej wiedzy i pomiarze.\n\nZrozumienie tych różnic jest kluczowe dla prawidłowej interpretacji i stosowania modeli statystycznych w różnych kontekstach naukowych, od nauk społecznych wykorzystujących analizę regresji po eksperymenty z fizyki kwantowej.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-b-duże-modele-językowe---zrozumienie-ich-stochastycznej-natury",
    "href": "rozdzial1.html#appendix-b-duże-modele-językowe---zrozumienie-ich-stochastycznej-natury",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.11 Appendix B: Duże Modele Językowe - Zrozumienie Ich Stochastycznej Natury",
    "text": "2.11 Appendix B: Duże Modele Językowe - Zrozumienie Ich Stochastycznej Natury\nDuże Modele Językowe (LLM), takie jak GPT-3, BERT i Claude, zrewolucjonizowały przetwarzanie języka naturalnego, ale mogą popełniać zagadkowe błędy, szczególnie w zadaniach matematycznych. Ten dodatek wyjaśnia funkcjonowanie LLM, ich stochastyczną naturę i porównuje je z klasycznymi modelami statystycznymi.\n\n2.11.1 Podstawy LLM i Ich Stochastyczna Natura\nLLM są trenowane na ogromnych zbiorach danych tekstowych, aby przewidywać rozkład prawdopodobieństwa następnego tokenu w sekwencji. Wykorzystują architektury transformerowe do przetwarzania i generowania tekstu. Kluczowe aspekty ich stochastycznej natury obejmują:\n\nProbabilistyczny wybór tokenów: LLM wybierają każde słowo na podstawie obliczonych prawdopodobieństw, a nie stałych reguł.\nLosowość kontrolowana temperaturą: Parametr “temperatury” dostosowuje losowość wyborów, równoważąc kreatywność i spójność.\nNiedeterministyczne wyniki: Te same dane wejściowe mogą prowadzić do różnych wyników w oddzielnych uruchomieniach.\nKontekstowa niejednoznaczność: LLM interpretują kontekst probabilistycznie, co czasami prowadzi do nieporozumień.\n\n\n\n2.11.2 Porównanie z Klasycznymi Modelami Statystycznymi\nAby lepiej zrozumieć LLM, porównajmy je z regresją Najmniejszych Kwadratów (OLS):\n\n\n\n\n\n\n\n\nAspekt\nRegresja OLS\nDuże Modele Językowe\n\n\n\n\nPodstawowa funkcja\nPrzewiduje ciągłe wyniki na podstawie zmiennych wejściowych\nPrzewiduje rozkład prawdopodobieństwa następnego tokenu na podstawie poprzednich tokenów\n\n\nWejście-Wyjście\nZmienne ciągłe, relacje liniowe\nDyskretne tokeny, relacje nieliniowe\n\n\nTyp predykcji\nPredykcje punktowe z przedziałami ufności\nRozkłady prawdopodobieństwa dla możliwych tokenów\n\n\nZłożoność modelu\nNiewiele parametrów\nMiliardy parametrów\n\n\nInterpretowalność\nJasne interpretacje współczynników\nLargely nieprzejrzyste działanie wewnętrzne\n\n\nObsługa szumu\nZakłada losowy szum w zmiennej wynikowej\nRadzi sobie ze zmiennością języka naturalnego\n\n\nEkstrapolacja\nMniej wiarygodna poza zakresem treningu\nMniej wiarygodna dla nieznanych tematów\n\n\n\nOba modele dążą do nauczenia się mapowania wejścia-wyjścia na podstawie wzorców w danych treningowych.\n\n\n2.11.3 Implikacje dla Zadań Matematycznych\nStochastyczna natura LLM wpływa na operacje matematyczne:\n\nZmienne wyniki dla powtarzanych obliczeń: Każda próba może dać inny wynik ze względu na probabilistyczny wybór tokenów.\nPewność nie gwarantuje poprawności: Wysoka pewność modelu może wystąpić nawet dla niepoprawnych odpowiedzi.\nAproksymacja zamiast dokładnych obliczeń: LLM dopasowują wzorce zamiast wykonywać precyzyjne obliczenia.\n\nOgraniczenia w zadaniach matematycznych wynikają z:\n\nNiedopasowania celu treningu: LLM są trenowane do przewidywania języka, nie dokładności matematycznej.\nBraku jawnego rozumowania matematycznego: Nie mają wbudowanych reguł czy operacji matematycznych.\nBraku pamięci roboczej: LLM nie mogą niezawodnie przechowywać i manipulować wynikami pośrednimi.\nOgraniczonego okna kontekstowego: Mogą tracić istotne informacje w długich problemach.\nOgraniczeń danych treningowych: Niedoreprezentowanie pewnych koncepcji matematycznych może prowadzić do słabych wyników.\nBraku kontroli spójności: LLM nie weryfikują logicznej spójności swoich wyników.\n\n\n\n2.11.4 Najlepsze Praktyki i Wnioski\nPrzy korzystaniu z LLM do zadań matematycznych:\n\nSkup się na wyjaśnieniach koncepcyjnych, nie na dokładnych obliczeniach: LLM doskonale wyjaśniają koncepcje, ale mogą zawodzić w dokładnych obliczeniach.\nWeryfikuj wyniki dedykowanym oprogramowaniem: Zawsze sprawdzaj obliczenia LLM odpowiednimi narzędziami matematycznymi.\nRozbijaj złożone problemy: Podział zadań na mniejsze kroki może poprawić wydajność LLM.\nBądź świadomy efektów przeformułowania: Różne sformułowania tego samego problemu mogą dawać różne wyniki.\nUżywaj jako narzędzi wspomagających, nie zamienników dla ekspertyzy: LLM powinny uzupełniać, a nie zastępować wiedzę matematyczną.\n\nZrozumienie probabilistycznej natury LLM pomaga wykorzystać ich mocne strony w zadaniach językowych, jednocześnie uznając ich ograniczenia w dziedzinach wymagających deterministycznej precyzji, takich jak matematyka.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-c-modele-deterministyczne-a-modele-stochastyczne",
    "href": "rozdzial1.html#appendix-c-modele-deterministyczne-a-modele-stochastyczne",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.12 Appendix C: Modele Deterministyczne a Modele Stochastyczne (*)",
    "text": "2.12 Appendix C: Modele Deterministyczne a Modele Stochastyczne (*)\n\n2.12.1 Modele Deterministyczne\nModele deterministyczne to te, w których wynik jest w pełni określony przez wartości parametrów i warunki początkowe. Modele te są często używane w fizyce i inżynierii.\n\n\n2.12.2 Przykład: Ruch Jednostajnie Przyspieszony\nKlasycznym przykładem modelu deterministycznego jest ruch jednostajnie przyspieszony, opisany równaniem:\nx(t) = x_0 + v_0t + \\frac{1}{2}at^2\nGdzie:\n\nx(t) to położenie w czasie t\nx_0 to położenie początkowe\nv_0 to prędkość początkowa\na to przyspieszenie\nt to czas\n\nZasymulujmy to w R:\n\n# Ruch jednostajnie przyspieszony\nsymuluj_ruch_przyspieszony &lt;- function(x0, v0, a, t) {\n  x0 + v0 * t + 0.5 * a * t^2\n}\n\n# Generowanie danych\nt &lt;- seq(0, 10, by = 0.1)\nx &lt;- symuluj_ruch_przyspieszony(x0 = 0, v0 = 2, a = 1, t = t)\n\n# Wykres\nplot(t, x, type = \"l\", xlab = \"Czas\", ylab = \"Położenie\", \n     main = \"Ruch Jednostajnie Przyspieszony\")\n\n\n\n\n\n\n\n\nTen kod wygeneruje wykres ruchu jednostajnie przyspieszonego, który jest intuicyjnym przykładem z dynamiki Newtona. W tym przypadku obiekt zaczyna ruch z początkową prędkością i przyspiesza jednostajnie, co prowadzi do parabolicznej trajektorii na wykresie położenia w funkcji czasu.\n\n\n2.12.3 Modele Stochastyczne w Naukach Społecznych\nModele stochastyczne uwzględniają losowość i są często używane w naukach społecznych, gdzie istnieje nieodłączna niepewność w badanych systemach.\n\n\n2.12.4 Przykład: Regresja Metodą Najmniejszych Kwadratów (OLS)\nOLS to podstawowy model stochastyczny w naukach społecznych. Jest reprezentowany jako:\nY = \\beta_0 + \\beta_1X + \\epsilon\nGdzie:\n\nY to zmienna zależna\nX to zmienna niezależna\n\\beta_0 i \\beta_1 to parametry\n\\epsilon to składnik błędu (komponent stochastyczny)\n\nZademonstrujmy OLS w R:\n\n# Generowanie przykładowych danych\nset.seed(123)\nX &lt;- rnorm(100)\nY &lt;- 2 + 3*X + rnorm(100, sd = 0.5)\n\n# Dopasowanie modelu OLS\nmodel &lt;- lm(Y ~ X)\n\n# Podsumowanie modelu\nsummary(model)\n\n\nCall:\nlm(formula = Y ~ X)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.95367 -0.34175 -0.04375  0.29032  1.64520 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.94860    0.04878   39.95   &lt;2e-16 ***\nX            2.97376    0.05344   55.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4854 on 98 degrees of freedom\nMultiple R-squared:  0.9693,    Adjusted R-squared:  0.969 \nF-statistic:  3097 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n# Wykres\nplot(X, Y, main = \"Regresja OLS\")\nabline(model, col = \"red\")\n\n\n\n\n\n\n\n\nTo dopasuje model OLS do symulowanych danych i wykreśli wyniki.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff\n\n\n\n\n2.12.5 Zaawansowane Modele Stochastyczne: Duże Modele Językowe\nDuże Modele Językowe (LLM), takie jak GPT-3, to złożone modele stochastyczne używane w przetwarzaniu języka naturalnego. Chociaż nie możemy zaimplementować pełnego LLM w tym tutorialu, możemy omówić jego zasady.\nLLM opierają się na architekturze transformatora i wykorzystują mechanizmy samouwagi. Są trenowane na ogromnych ilościach danych tekstowych i uczą się przewidywać następny token w sekwencji.\nRdzeń LLM można postrzegać jako warunkowy rozkład prawdopodobieństwa:\nP(x_t | x_{&lt;t}, \\theta)\nGdzie:\n\nx_t to aktualny token\nx_{&lt;t} reprezentuje wszystkie poprzednie tokeny\n\\theta to parametry modelu\n\n\n\n\n\n\n\nNote\n\n\n\nTokeny w Dużych Modelach Językowych (LLM) to podstawowe jednostki tekstu, które model przetwarza. Można je postrzegać jako części słów lub znaki interpunkcyjne. Oto kluczowe informacje o tokenach:\nDefinicja: Tokeny to najmniejsze jednostki tekstu, które LLM przetwarza. Mogą to być całe słowa, części słów, a nawet pojedyncze znaki lub znaki interpunkcyjne. Tokenizacja: Proces dzielenia tekstu na tokeny nazywa się tokenizacją. LLM używają specyficznych algorytmów do wykonania tego zadania. Przykłady:\nSłowo “kot” może być pojedynczym tokenem. Dłuższe słowo jak “zrozumienie” może być podzielone na wiele tokenów, np. “zrozum” i “ienie”. Znaki interpunkcyjne jak “.” czy “?” są często oddzielnymi tokenami. Powszechne przedrostki lub przyrostki mogą być własnymi tokenami.\nSłownictwo: LLM mają ustalone słownictwo tokenów, które rozpoznają. To słownictwo zazwyczaj obejmuje od dziesiątek tysięcy do setek tysięcy tokenów. Znaczenie: Sposób tokenizacji tekstu może wpływać na to, jak model rozumie i generuje język. Jest to szczególnie ważne przy obsłudze różnych języków, rzadkich słów lub specjalistycznego słownictwa. Kontekst: W równaniu dla LLM: P(x_t | x_{&lt;t}, \\theta) Gdzie:\nx_t reprezentuje bieżący token x_{&lt;t} reprezentuje wszystkie poprzednie tokeny w sekwencji \\theta reprezentuje parametry modelu\n\n\nW przeciwieństwie do modeli deterministycznych, LLM produkują różne wyniki nawet dla tego samego wejścia ze względu na ich stochastyczną naturę.\n\n\n2.12.6 Podsumowanie\nKażdy rodzaj modelu ma swoje miejsce w nauce, w zależności od badanego systemu i poziomu niepewności.\nPamiętaj, że wybór między modelami deterministycznymi a stochastycznymi często zależy od natury badanego systemu i pytań, na które próbujesz odpowiedzieć. Modele deterministyczne są świetne dla systemów o dobrze zrozumiałej mechanice, podczas gdy modele stochastyczne sprawdzają się przy radzeniu sobie z nieodłączną losowością lub złożonymi, nie w pełni zrozumiałymi systemami.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial1.html#appendix-d-wprowadzenie-do-r-rstudio-i-tidyverse",
    "href": "rozdzial1.html#appendix-d-wprowadzenie-do-r-rstudio-i-tidyverse",
    "title": "2  Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych",
    "section": "2.13 Appendix D: Wprowadzenie do R, RStudio i tidyverse",
    "text": "2.13 Appendix D: Wprowadzenie do R, RStudio i tidyverse\nR to potężny język programowania i środowisko do obliczeń statystycznych i grafiki. Jest szeroko stosowany w środowisku akademickim, szczególnie w naukach społecznych, do analizy danych i wizualizacji.\n\n2.13.0.1 Kluczowe cechy R:\n\nOtwarty kod źródłowy i darmowy\nRozbudowany ekosystem pakietów\nSilne wsparcie społeczności\nDoskonały do analizy statystycznej i wizualizacji danych\n\n\n\n2.13.1 Pierwsze kroki z RStudio\nRStudio to zintegrowane środowisko programistyczne (IDE) dla R, które ułatwia pracę z R.\n\n2.13.1.1 Instalacja R i RStudio\n\nPobierz i zainstaluj R ze strony CRAN\nPobierz i zainstaluj RStudio ze strony RStudio\n\n\n\n2.13.1.2 Interfejs RStudio\nRStudio ma cztery główne panele:\n\nEdytor źródłowy: Gdzie piszesz i edytujesz skrypty R\nKonsola: Gdzie możesz wpisywać polecenia R i widzieć wyniki\nŚrodowisko/Historia: Pokazuje wszystkie obiekty w twoim obszarze roboczym i historię poleceń\nPliki/Wykresy/Pakiety/Pomoc: Wielofunkcyjny panel do zarządzania plikami, przeglądania wykresów, zarządzania pakietami i dostępu do pomocy\n\n\n\n2.13.1.3 Podstawowe funkcje RStudio\n\nTworzenie nowego skryptu R: Plik &gt; Nowy plik &gt; Skrypt R\nUruchamianie kodu: Zaznacz kod i naciśnij Ctrl+Enter (Cmd+Enter na Macu)\nInstalowanie pakietów: Narzędzia &gt; Instaluj pakiety\nUzyskiwanie pomocy: Wpisz ?nazwa_funkcji w konsoli\n\n\n\n\n2.13.2 Podstawy R\n\n2.13.2.1 Typy danych w R\n\n# Numeryczny\nx &lt;- 10.5\nclass(x)\n\n[1] \"numeric\"\n\n# Całkowity\ny &lt;- 1L\nclass(y)\n\n[1] \"integer\"\n\n# Znakowy\nimie &lt;- \"Alicja\"\nclass(imie)\n\n[1] \"character\"\n\n# Logiczny\njest_studentem &lt;- TRUE\nclass(jest_studentem)\n\n[1] \"logical\"\n\n\n\n\n2.13.2.2 Struktury danych\n\n2.13.2.2.1 Wektory\n\n# Tworzenie wektora\nliczby &lt;- c(1, 2, 3, 4, 5)\nowoce &lt;- c(\"jabłko\", \"banan\", \"wiśnia\")\n\n# Operacje na wektorach\nliczby + 2\n\n[1] 3 4 5 6 7\n\nliczby * 2\n\n[1]  2  4  6  8 10\n\nmean(liczby)\n\n[1] 3\n\nlength(owoce)\n\n[1] 3\n\n\n\n\n2.13.2.2.2 Macierze\n\n# Tworzenie macierzy\nm &lt;- matrix(1:6, nrow = 2, ncol = 3)\nprint(m)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n# Operacje na macierzach\nt(m)  # transpozycja\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nm * 2  # mnożenie skalarne\n\n     [,1] [,2] [,3]\n[1,]    2    6   10\n[2,]    4    8   12\n\n\n\n\n2.13.2.2.3 Ramki danych\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(\n  imie = c(\"Alicja\", \"Bartek\", \"Celina\"),\n  wiek = c(25, 30, 35),\n  student = c(TRUE, FALSE, TRUE)\n)\nprint(df)\n\n    imie wiek student\n1 Alicja   25    TRUE\n2 Bartek   30   FALSE\n3 Celina   35    TRUE\n\n# Dostęp do elementów ramki danych\ndf$imie\n\n[1] \"Alicja\" \"Bartek\" \"Celina\"\n\ndf[1, 2]\n\n[1] 25\n\ndf[df$wiek &gt; 25, ]\n\n    imie wiek student\n2 Bartek   30   FALSE\n3 Celina   35    TRUE\n\n\n\n\n\n2.13.2.3 Funkcje\n\n# Definiowanie funkcji\npowitaj &lt;- function(imie) {\n  paste(\"Cześć,\", imie, \"!\")\n}\n\n# Użycie funkcji\npowitaj(\"Alicja\")\n\n[1] \"Cześć, Alicja !\"\n\n# Funkcja z wieloma argumentami\noblicz_bmi &lt;- function(waga, wzrost) {\n  bmi &lt;- waga / (wzrost^2)\n  return(bmi)\n}\n\noblicz_bmi(70, 1.75)\n\n[1] 22.85714\n\n\n\n\n2.13.2.4 Struktury kontrolne\n\n# Instrukcja if-else\nx &lt;- 10\nif (x &gt; 5) {\n  print(\"x jest większe niż 5\")\n} else {\n  print(\"x nie jest większe niż 5\")\n}\n\n[1] \"x jest większe niż 5\"\n\n# Pętla for\nfor (i in 1:5) {\n  print(paste(\"Iteracja\", i))\n}\n\n[1] \"Iteracja 1\"\n[1] \"Iteracja 2\"\n[1] \"Iteracja 3\"\n[1] \"Iteracja 4\"\n[1] \"Iteracja 5\"\n\n# Pętla while\nlicznik &lt;- 1\nwhile (licznik &lt;= 5) {\n  print(paste(\"Licznik:\", licznik))\n  licznik &lt;- licznik + 1\n}\n\n[1] \"Licznik: 1\"\n[1] \"Licznik: 2\"\n[1] \"Licznik: 3\"\n[1] \"Licznik: 4\"\n[1] \"Licznik: 5\"\n\n\n\n\n\n2.13.3 Wprowadzenie do tidyverse\nTidyverse to kolekcja pakietów R zaprojektowanych do nauki o danych. Te pakiety mają wspólną filozofię i są zaprojektowane do bezproblemowej współpracy.\n\n2.13.3.1 Kluczowe pakiety tidyverse\n\nggplot2: do wizualizacji danych\ndplyr: do manipulacji danymi\ntidyr: do porządkowania danych\nreadr: do odczytu danych prostokątnych\npurrr: do programowania funkcyjnego\ntibble: nowoczesne ujęcie ramek danych\n\n\n\n2.13.3.2 Rozpoczęcie pracy z tidyverse\n\n# Instalacja tidyverse (uruchom raz)\n# install.packages(\"tidyverse\")\n\n# Wczytanie tidyverse\nlibrary(tidyverse)\n\n\n\n2.13.3.3 Import danych z readr\n\n# Odczyt plików CSV\ndane &lt;- read_csv(\"dane_spoleczne.csv\")\n\n# Odczyt innych formatów plików\nread_tsv(\"dane.tsv\")  # Wartości oddzielone tabulatorem\nread_delim(\"dane.txt\", delim = \"|\")  # Niestandardowy separator\n\n\n\n2.13.3.4 Manipulacja danymi z dplyr\n\n# Użyjmy wbudowanego zbioru danych mtcars\ndata(\"mtcars\")\n\n# Wybieranie kolumn\nmtcars %&gt;% \n  select(mpg, cyl, hp)\n\n                     mpg cyl  hp\nMazda RX4           21.0   6 110\nMazda RX4 Wag       21.0   6 110\nDatsun 710          22.8   4  93\nHornet 4 Drive      21.4   6 110\nHornet Sportabout   18.7   8 175\nValiant             18.1   6 105\nDuster 360          14.3   8 245\nMerc 240D           24.4   4  62\nMerc 230            22.8   4  95\nMerc 280            19.2   6 123\nMerc 280C           17.8   6 123\nMerc 450SE          16.4   8 180\nMerc 450SL          17.3   8 180\nMerc 450SLC         15.2   8 180\nCadillac Fleetwood  10.4   8 205\nLincoln Continental 10.4   8 215\nChrysler Imperial   14.7   8 230\nFiat 128            32.4   4  66\nHonda Civic         30.4   4  52\nToyota Corolla      33.9   4  65\nToyota Corona       21.5   4  97\nDodge Challenger    15.5   8 150\nAMC Javelin         15.2   8 150\nCamaro Z28          13.3   8 245\nPontiac Firebird    19.2   8 175\nFiat X1-9           27.3   4  66\nPorsche 914-2       26.0   4  91\nLotus Europa        30.4   4 113\nFord Pantera L      15.8   8 264\nFerrari Dino        19.7   6 175\nMaserati Bora       15.0   8 335\nVolvo 142E          21.4   4 109\n\n# Filtrowanie wierszy\nmtcars %&gt;% \n  filter(cyl == 4)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDatsun 710     22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230       22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nVolvo 142E     21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n# Sortowanie danych\nmtcars %&gt;% \n  arrange(desc(mpg))\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n\n# Tworzenie nowych zmiennych\nmtcars %&gt;% \n  mutate(kpl = mpg * 0.425)\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb     kpl\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4  8.9250\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4  8.9250\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1  9.6900\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1  9.0950\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2  7.9475\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1  7.6925\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4  6.0775\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 10.3700\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2  9.6900\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4  8.1600\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4  7.5650\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3  6.9700\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3  7.3525\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3  6.4600\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4  4.4200\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4  4.4200\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4  6.2475\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 13.7700\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 12.9200\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 14.4075\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1  9.1375\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2  6.5875\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2  6.4600\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4  5.6525\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2  8.1600\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 11.6025\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 11.0500\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 12.9200\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4  6.7150\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6  8.3725\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8  6.3750\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2  9.0950\n\n# Podsumowywanie danych\nmtcars %&gt;% \n  group_by(cyl) %&gt;% \n  summarize(srednie_mpg = mean(mpg),\n            liczba = n())\n\n# A tibble: 3 × 3\n    cyl srednie_mpg liczba\n  &lt;dbl&gt;       &lt;dbl&gt;  &lt;int&gt;\n1     4        26.7     11\n2     6        19.7      7\n3     8        15.1     14\n\n\n\n\n2.13.3.5 Wizualizacja danych z ggplot2\n\n# Wykres rozrzutu\nggplot(mtcars, aes(x = wt, y = mpg)) +\n  geom_point() +\n  labs(title = \"Waga samochodu vs. Zużycie paliwa\",\n       x = \"Waga (1000 funtów)\",\n       y = \"Mile na galon\")\n\n\n\n\nWaga samochodu vs. Zużycie paliwa\n\n\n\n\n\n# Wykres słupkowy\nmtcars %&gt;% \n  count(cyl) %&gt;% \n  ggplot(aes(x = factor(cyl), y = n)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Liczba samochodów według liczby cylindrów\",\n       x = \"Liczba cylindrów\",\n       y = \"Liczba\")\n\n\n\n\nLiczba samochodów według liczby cylindrów\n\n\n\n\n\n# Wykres pudełkowy\nggplot(mtcars, aes(x = factor(cyl), y = mpg)) +\n  geom_boxplot() +\n  labs(title = \"Zużycie paliwa według liczby cylindrów\",\n       x = \"Liczba cylindrów\",\n       y = \"Mile na galon\")\n\n\n\n\nZużycie paliwa według liczby cylindrów\n\n\n\n\n\n\n\n2.13.4 Dodatkowe zasoby\n\nR for Data Science\nDokumentacja tidyverse\nŚciągawki RStudio\nPrzewodnik Quarto\nR Cookbook\n\nPamiętaj, aby eksperymentować z kodem, modyfikować przykłady i nie wahaj się korzystać z wbudowanego systemu pomocy R (dostępnego przez wpisanie ?nazwa_funkcji w konsoli), gdy napotkasz nieznane funkcje lub koncepcje.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Wprowadzenie do Nauki o Danych i Statystyki dla Nauk Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1 Foundations in Number Sets\nBefore diving into data types, it’s essential to understand the basic number sets that form the foundation of our understanding of data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#foundations-in-number-sets",
    "href": "chapter2.html#foundations-in-number-sets",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "",
    "text": "3.1.1 Basic Number Sets\n\nNatural Numbers (ℕ): The counting numbers {0, 1, 2, 3, …}\nIntegers (ℤ): Includes natural numbers, their negatives, and zero {…, -2, -1, 0, 1, 2, …}\nRational Numbers (ℚ): Numbers that can be expressed as a fraction of two integers\nReal Numbers (ℝ): All numbers on the number line, including rationals and irrationals\n\n\n\n3.1.2 Properties of Sets\n\nCountable Sets: Sets whose elements can be put in a one-to-one correspondence with the natural numbers. For example, the set of integers is countable.\nUncountable Sets: Sets that are not countable. The set of real numbers is uncountable.\nDiscrete Sets: Sets where each element is separated from other elements by a finite gap. The integers form a discrete set.\nDense Sets: Sets where between any two elements, there is always another element of the set. The rational numbers and real numbers are dense sets.\n\n\n\n\n\n\n\nNote\n\n\n\nUnderstanding these set properties is crucial for grasping the nature of different data types in social sciences.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#discrete-vs.-continuous-data",
    "href": "chapter2.html#discrete-vs.-continuous-data",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.2 Discrete vs. Continuous Data",
    "text": "3.2 Discrete vs. Continuous Data\nIn data science and statistics, we often categorize variables as either discrete or continuous. However, the distinction is not always clear-cut, and some variables exhibit characteristics of both types. This section explores the concepts of discrete and continuous data, their differences, and the interesting cases of variables that can be treated as both or challenge our intuitive understanding.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n3.2.1 Discrete Data\nDiscrete data can only take on specific, countable values. These values are often (but not always) integers.\n\n3.2.1.1 Characteristics of Discrete Data:\n\nCountable\nOften represented by integers\nCan be finite or infinite\nNo values between two adjacent data points\n\n\n\n3.2.1.2 Examples:\n\nNumber of students in a class\nNumber of cars sold by a dealership\nShoe sizes\n\n\n\n\n3.2.2 Continuous Data\nContinuous data can take on any value within a given range, including fractional and decimal values. It’s important to note that continuity is not solely determined by uncountability, but also by density.\n\n3.2.2.1 Characteristics of Continuous Data:\n\nCan be uncountable (like real numbers) or dense (like rational numbers)\nCan be measured to any level of precision (theoretically)\nRepresented by real numbers or dense subsets of real numbers\nThere are always values between any two data points\n\n\n\n3.2.2.2 Examples:\n\nHeight\nWeight\nTemperature\nPercentages (explained further below)\n\n\n\n\n3.2.3 The Discrete-Continuous Spectrum\nIn practice, some variables that are mathematically discrete are often treated as if they are continuous. This dual nature provides flexibility in how these variables can be analyzed and interpreted.\n\n3.2.3.1 Reasons for Treating Discrete Data as Continuous:\n\nDense Granularity\n\nWhen a discrete variable has a large number of possible values within a range, it can approximate continuity.\nExample: Income measured in individual cents. While technically discrete, the large number of possible values makes it behave similarly to a continuous variable.\n\nAnalytical Convenience\n\nContinuous methods often yield reasonable and useful results even for dense discrete variables.\nIt’s often easier to use existing statistical tools if continuity is assumed, as this allows the use of calculus-based methods.\n\nApproximation of Underlying Phenomena\n\nIn some cases, a discrete measurement might be an approximation of an underlying continuous process.\nExample: While we measure time in discrete units (seconds, minutes, hours), time itself is continuous.\n\n\n\n\n3.2.3.2 Examples of Variables with Dual Discrete-Continuous Nature:\n\nAge\n\nDiscrete: Typically measured in whole years\nContinuous: Can be considered as a continuous variable in many analyses, especially when dealing with large populations\n\nPrice and Income\n\nDiscrete: Prices and incomes are actually measured in discrete units (e.g., cents or smallest currency unit)\nContinuous: In economic models and many analyses, prices and incomes are treated as continuous variables due to their dense nature and analytical convenience\n\nTest Scores\n\nDiscrete: Often given as whole numbers\nContinuous: In statistical analyses, test scores might be treated as continuous, especially when the range of possible scores is large\n\n\n\n\n\n3.2.4 Special Case: Percentages and Rational Numbers\nPercentages present an interesting case in the discrete-continuous spectrum:\n\nRational Nature: Percentages are essentially fractions (m/100), making them rational numbers.\nDense but Countable: The set of rational numbers is dense (between any two rationals, there’s another rational) but also countable.\nPractical Continuity: In most practical applications, percentages are treated as continuous due to their dense nature.\nFinite Precision: In reality, percentages are often reported to a limited number of decimal places, creating a finite set of possible values.\n\n\n\n\n\n\n\nPercentages: Bridging Discrete and Continuous\n\n\n\nVariables measured in percentages, such as unemployment rates or voter turnout, challenge our intuitive understanding of discreteness and continuity:\n\nThey are rational numbers (fractions with denominator 100), which are technically countable.\nThey form a dense set within their range (0% to 100%), allowing for values between any two percentages.\nIn practice, they are often treated as continuous variables due to their dense nature and analytical convenience.\nThe precision of measurement (e.g., reporting to one or two decimal places) can impose a discrete structure on what is conceptually a dense set.\n\nThis duality allows for flexible analytical approaches, depending on the specific research context and required precision.\n\n\n\n\n3.2.5 Implications for Data Analysis\nUnderstanding the nuanced nature of variables as discrete, continuous, or somewhere in between has important implications for data analysis:\n\nFlexibility in Modeling: It allows for the use of a wider range of statistical techniques.\nSimplified Calculations: Treating dense discrete data as continuous can simplify calculations and make certain analyses more tractable.\nImproved Interpretability: In some cases, treating discrete data as continuous can lead to more intuitive or useful interpretations of results.\nPotential for Error: It’s important to be aware of when approximations are appropriate and when they might lead to misleading results.\nTheoretical vs. Practical Considerations: While the mathematical nature of the data is important, practical considerations in measurement and analysis often guide how we treat variables.\n\n\n\n3.2.6 Conclusion\nThe distinction between discrete and continuous data is not always rigid in social sciences. Many variables, including those involving money, percentages, or dense measurements, can be viewed through both discrete and continuous lenses. The choice of treatment should be guided by the nature of the data, the goals of the analysis, and the potential implications of the choice. This flexibility, when used thoughtfully, provides powerful tools for social science researchers to gain insights from their data.\n\n\n\n\n\n\nDiscrete vs. Continuous Numerical Data: A Language-Based Analogies\n\n\n\n\n3.2.6.1 The Language Connection\nThink about how you naturally ask questions about quantities:\n\n“How many cookies are in the jar?” (counting)\n“How much water is in the glass?” (measuring)\n\nThis natural language distinction reflects the two fundamental types of numerical data:\n\n\n3.2.6.2 Discrete Data = “How Many?” Questions\n\nLike counting whole objects (countable nouns)\nTakes specific values with gaps between them\nExamples:\n\nNumber of pets: 0, 1, 2, 3… (can’t have 2.5 pets)\nDice rolls: 1, 2, 3, 4, 5, 6\nStudents in a class: 20, 21, 22…\n\n\n🤔 Self-Check: Can you find a value between 2 and 3 students? Why not?\n\n\n3.2.6.3 Continuous Data = “How Much?” Questions\n\nLike measuring quantities (uncountable nouns)\nCan take any value within a range\nExamples:\n\nHeight: 1.7231… meters\nTemperature: 36.8325… °C\nTime: 3.5792… hours\n\n\n🤔 Self-Check: Write down three different values between 1.72 and 1.73 meters\n\n\n3.2.6.4 Quick Recognition Guide\n\nIf you naturally ask “How many?” → Discrete\nIf you naturally ask “How much?” → Continuous\nIf you can measure it more precisely → Continuous\nIf you can only use whole numbers → Discrete\n\n✍️ Practice: Classify these quantities as discrete or continuous\n\nYour age in years: _____\nYour height: _____\nNumber of songs in a playlist: _____\nVolume of water: _____\n\n\n\n\n\n\n3.2.7 R Code Example\nHere’s a simple R code example to illustrate how we might analyze variables treated as continuous:\n\n# Generate some sample data\nset.seed(123)\nages &lt;- round(runif(1000, min = 18, max = 80))\nunemployment_rates &lt;- round(runif(1000, min = 3, max = 10), 1)\n\n# Compare means and medians\ncat(\"Mean age:\", mean(ages), \"\\n\")\n\nMean age: 48.848 \n\ncat(\"Median age:\", median(ages), \"\\n\")\n\nMedian age: 48 \n\ncat(\"Mean unemployment rate:\", mean(unemployment_rates), \"\\n\")\n\nMean unemployment rate: 6.4871 \n\ncat(\"Median unemployment rate:\", median(unemployment_rates), \"\\n\")\n\nMedian unemployment rate: 6.5 \n\n# Linear regression (treating both as continuous)\nincome &lt;- 20000 + 500 * ages + 1000 * unemployment_rates + rnorm(1000, 0, 5000)\nmodel &lt;- lm(income ~ ages + unemployment_rates)\nsummary(model)\n\n\nCall:\nlm(formula = income ~ ages + unemployment_rates)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15354  -3471     54   3485  16684 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        20116.194    717.231   28.05   &lt;2e-16 ***\nages                 503.385      8.983   56.04   &lt;2e-16 ***\nunemployment_rates   989.333     80.004   12.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5053 on 997 degrees of freedom\nMultiple R-squared:  0.7637,    Adjusted R-squared:  0.7632 \nF-statistic:  1611 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n# Plot regression plane\nlibrary(plotly)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nplot_ly(x = ages, y = unemployment_rates, z = income, type = \"scatter3d\", mode = \"markers\") %&gt;%\n  add_trace(x = ages, y = unemployment_rates, z = fitted(model), type = \"scatter3d\", mode = \"lines\")\n\n\n\n\n\nThe 3D plot illustrates how both variables can be treated as continuous in a regression model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#introduction-to-stevens-data-typology",
    "href": "chapter2.html#introduction-to-stevens-data-typology",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.3 Introduction to Stevens’ Data Typology",
    "text": "3.3 Introduction to Stevens’ Data Typology\nStanley S. Stevens, an American psychologist, introduced a classification system for scales of measurement in his 1946 paper “On the Theory of Scales of Measurement.” This system, known as Stevens’ data typology or levels of measurement, has become fundamental in understanding how different types of data should be analyzed and interpreted.\nStevens proposed four levels of measurement:\n\nNominal\nOrdinal\nInterval\nRatio\n\nEach level has specific properties and allows for different types of statistical operations and analyses.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n3.3.1 Nominal Scale\n\n3.3.1.1 Definition\nThe nominal scale is the most basic level of measurement. It uses labels or categories to classify data without any quantitative value or order.\n\n\n3.3.1.2 Properties\n\nCategories are mutually exclusive\nNo inherent order among categories\nNo meaningful arithmetic operations can be performed\n\n\n\n3.3.1.3 Examples\n\nNationality (Polish, English, …)\nBlood types (A, B, AB, O)\nEye color (Blue, Brown, Green, Hazel)\nBinary variables (“Success” versus “Failure”)\n\n\n\n\n3.3.2 Ordinal Scale\n\n3.3.2.1 Definition\nThe ordinal scale categorizes data into ordered categories, but the intervals between categories are not necessarily equal or meaningful.\n\n\n3.3.2.2 Properties\n\nCategories have a defined order\nDifferences between categories are not quantifiable\nArithmetic operations on the numbers are not meaningful\n\n\n\n3.3.2.3 Examples\n\nEducation levels (High School, Bachelor’s, Master’s, PhD)\nLikert scales (Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree)\nSocioeconomic status (Low, Medium, High)\n\n\n\n\n3.3.3 Interval Scale\n\n3.3.3.1 Definition\nThe interval scale has ordered categories with equal intervals between adjacent categories. However, it lacks a true zero point.\n\n\n3.3.3.2 Properties\n\nEqual intervals between adjacent categories\nNo true zero point (zero is arbitrary)\nRatios between values are not meaningful\n\n\n\n3.3.3.3 Examples\n\nTemperature in Celsius or Fahrenheit\nCalendar years\npH scale (the difference between pH 4 and 5 represents the same change in hydrogen ion concentration as between pH 6 and 7)\nElevation above sea level\n\n\n\n\n3.3.4 Ratio Scale\n\n3.3.4.1 Definition\nThe ratio scale is the highest level of measurement. It has all the properties of the interval scale plus a true zero point, making ratios between values meaningful.\n\n\n3.3.4.2 Properties\n\nAll properties of interval scales\nTrue zero point\nRatios between values are meaningful\n\n\n\n3.3.4.3 Examples\n\nHeight\nWeight\nAge\nIncome\n\n\n\n\n\n\n\nUnderstanding Operations with Data Types\n\n\n\n\n3.3.5 Fundamental Concepts\n\n3.3.5.1 Temperature Scale Conversions\nBasic conversion formula: F = \\frac{9}{5}C + 32\n\n\n3.3.5.2 Data Type Classifications\n\nInterval Data (e.g., temperature): Has equal intervals but no true zero\nRatio Data (e.g., length): Has equal intervals and a true zero point\n\n\n\n\n3.3.6 Operations Analysis\n\n3.3.6.1 Differences (Works for Both Types)\n\n3.3.6.1.1 Temperature Proof\nFor any two temperatures C_1 and C_2: * F_1 = \\frac{9}{5}C_1 + 32 * F_2 = \\frac{9}{5}C_2 + 32\nTemperature difference: * \\Delta F = F_2 - F_1 * = (\\frac{9}{5}C_2 + 32) - (\\frac{9}{5}C_1 + 32) * = \\frac{9}{5}(C_2 - C_1) * = \\frac{9}{5}\\Delta C\n\n\n3.3.6.1.2 Why Differences Work\n\nInterval Data: Constant terms (like +32) cancel out\nRatio Data: Conversion factors preserve both differences and ratios\n\nExample: 20m - 10m = 10m converts to 65.6ft - 32.8ft = 32.8ft\nRatio preservation: 20m/10m = 65.6ft/32.8ft = 2\n\n\n\n\n\n3.3.6.2 Multiplication (Only Works for Ratio Data)\n\n3.3.6.2.1 Temperature Example (Fails)\nMethod A: Convert after multiplication\n\n10°C × 2 = 20°C → 68°F\n\nMethod B: Convert before multiplication\n\n10°C → 50°F → 100°F\n\nResults differ: 68°F ≠ 100°F\n\n\n3.3.6.2.2 Length Example (Works)\nMethod A: Convert after multiplication\n\n10m × 2 = 20m → 65.6ft\n\nMethod B: Convert before multiplication\n\n10m → 32.8ft × 2 = 65.6ft\n\nResults match: 65.6ft = 65.6ft\n\n\n\n\n3.3.7 Statistical Measures\n\n3.3.7.1 Mean (Works for Both Types)\n\nUses only differences\nTemperature example:\n\nCelsius: (10°C + 20°C + 30°C)/3 = 20°C\nFahrenheit: (50°F + 68°F + 86°F)/3 = 68°F\n\nMaintains proportional relationships\n\n\n\n3.3.7.2 Variance (Only Works for Ratio Data)\n\n3.3.7.2.1 Temperature Example (Fails)\nCelsius calculation:\n\nValues: 10°C, 20°C, 30°C\nMean = 20°C\nVariance = 66.67°C²\n\nFahrenheit calculation:\n\nValues: 50°F, 68°F, 86°F\nMean = 68°F\nVariance = 216°F²\n\nProblem: No consistent relationship between variances\n\n\n3.3.7.2.2 Length Example (Works)\nMeters:\n\nValues: 1.5m, 1.6m, 1.7m\nVariance = 0.01m²\n\nFeet:\n\nValues: 4.92ft, 5.25ft, 5.58ft\nVariance = 0.108ft²\n\nRelationship preserved: 0.108ft² = 0.01m² × (3.28084)²\n\n\n\n\n3.3.8 Key Principles\n\n3.3.8.1 Differences\n\nWork for both types because equal steps remain equal after conversion\nConstant terms cancel out\n\n\n\n3.3.8.2 Multiplication\n\nOnly works for ratio data because it requires true zero point\nMust preserve proportional relationships\n\n\n\n3.3.8.3 Statistical Measures\n\nMean: Valid for both types (uses differences)\nVariance: Only valid for ratio data (uses squares)\n\n\n\n3.3.8.4 Unit Conversions\n\nInterval: Preserves differences but not ratios\nRatio: Preserves both differences and ratios\n\n\n\n\n\n\n\n\n3.3.9 Importance in Research and Analysis\nUnderstanding Stevens’ data typology is crucial for several reasons:\n\nChoosing appropriate statistical tests: The level of measurement determines which statistical analyses are appropriate for a given dataset.\nInterpreting results: The meaning of statistical results depends on the level of measurement of the variables involved.\nDesigning measurement instruments: When creating surveys or other measurement tools, researchers must consider the level of measurement they want to achieve.\nData transformation: Sometimes, data can be transformed from one level to another, but this must be done carefully to avoid misinterpretation.\n\n\n\n3.3.10 Controversies and Limitations\nWhile Stevens’ typology is widely used, it has faced some criticisms:\n\nRigidity: Some argue that the typology is too rigid and that many real-world measurements fall between these categories.\nTreatment of ordinal data: There’s ongoing debate about when it’s appropriate to treat ordinal data as interval for certain analyses.\nPsychological scaling: Some psychological constructs (like intelligence) are difficult to categorize definitively within this system.\n\n\n\n3.3.11 Conclusion\nStevens’ data typology provides a fundamental framework for understanding different types of data and their properties. By recognizing the level of measurement of their variables, researchers can make informed decisions about data collection, analysis, and interpretation. However, it’s important to remember that while this typology is a useful guide, real-world data often requires nuanced consideration and may not always fit neatly into these categories.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not “twice as acidic” as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "href": "chapter2.html#common-ordinal-scales-in-behavioural-research",
    "title": "3  Understanding Data Types in Social Sciences",
    "section": "3.4 Common Ordinal Scales in Behavioural Research",
    "text": "3.4 Common Ordinal Scales in Behavioural Research\n\n3.4.1 Likert Scales\nLikert scales are widely used in psychology and social sciences to measure attitudes, opinions, and perceptions. Named after psychologist Rensis Likert, these scales typically consist of a series of statements or questions that respondents rate on a scale, often from “Strongly Disagree” to “Strongly Agree.”\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n3.4.1.1 Why Likert Scales are Ordinal Variables\nLikert scales are considered ordinal variables for several reasons:\n\nOrder without equal intervals: While the responses have a clear order (e.g., “Strongly Disagree” &lt; “Disagree” &lt; “Neutral” &lt; “Agree” &lt; “Strongly Agree”), the intervals between these categories are not necessarily equal.\nSubjective interpretation: The difference between “Strongly Disagree” and “Disagree” may not be the same as the difference between “Agree” and “Strongly Agree” for all respondents.\nLack of true zero point: Likert scales typically don’t have a true zero point, which is a characteristic of interval or ratio scales.\n\n\n\n\n3.4.2 IQ and Other Psychological Variables as Ordinal Measures\nMany psychological measures, including IQ, are often treated as interval scales but are, in fact, ordinal. Here’s why:\n\nIQ Scores:\n\nWhile IQ scores are presented as numbers, the difference between an IQ of 100 and 110 may not represent the same cognitive difference as between 130 and 140.\nThe scale is normalized and adjusted over time, making it difficult to claim true interval properties.\n\nOther Psychological Measures:\n\nDepression scales (e.g., Beck Depression Inventory)\nAnxiety measures (e.g., State-Trait Anxiety Inventory)\nPersonality assessments (e.g., Big Five Inventory)\n\n\nThese measures often use summed Likert-type items or other scoring methods that don’t guarantee equal intervals between scores.\n\n\n3.4.3 Implications for Analysis\nRecognizing these measures as ordinal has important implications for data analysis:\n\nAppropriate statistical tests: Use non-parametric tests (e.g., Mann-Whitney U, Kruskal-Wallis) instead of parametric ones.\nCorrelation analysis: Use Spearman’s rank correlation instead of Pearson’s correlation.\nCentral tendency: Report median and mode rather than mean.\nData visualization: Use methods appropriate for ordinal data, such as bar plots or stacked bar charts.\n\n\n\n3.4.4 Conclusion\nWhile Likert scales and many behavioural measures are often treated as interval data for practical reasons, it’s crucial to remember their ordinal nature.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\nExercise: Identifying Measurement Scales\n\n\n\nFor each of the following variables, determine the most appropriate scale of measurement (Nominal, Ordinal, Interval, or Ratio). Also evaluate whether the variable is discrete or continuous.\n\nGender: nominal level of measurement, and discrete;\nCustomer satisfaction: Poor, Fair, Good, Excellent\nHeight (questionnaire): “I am: very short, short, average, tall, very tall”\nHeight (inches)\nReaction time (milliseconds)\nPostal codes: e.g., 61548, 61761, 62461, 47424, 65233\nAge (years)\nNationality\nStreet addresses\nMilitary ranks\nLeft-Right political scale placement\nFamily size: 1 child, 2 children, 3 children, …\nIQ score\nShirt size (S, M, L, …)\nMovie ratings (1 star, 2 stars, 3 stars)\nTemperature (Celsius)\nTemperature (Kelvin)\nBlood types: A, B, AB, O\nIncome categories: low, medium, high\nVoter turnout\nPolitical party affiliation\nElectoral district magnitude\n\nRemember to justify your choices for each variable.\nFor instance: In Stevens’ typology of measurement scales, street addresses are nominal data. This is because:\nThey serve purely as labels/identifiers. They have no inherent ordering (123 Main St isn’t “more than” 23 Oak St). You can’t perform meaningful mathematical operations on them.The only valid operation is testing for equality/inequality (is this the same address or different?)\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\n3.4.5 Notes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Understanding Data Types in Social Sciences</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html",
    "href": "rozdzial2.html",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "",
    "text": "4.1 Podstawy Zbiorów Liczbowych\nZanim zagłębimy się w typy danych, istotne jest zrozumienie podstawowych zbiorów liczbowych, które stanowią fundament naszego rozumienia danych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "href": "rozdzial2.html#podstawy-zbiorów-liczbowych",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "",
    "text": "4.1.1 Podstawowe Zbiory Liczbowe\n\nLiczby Naturalne (ℕ): Liczby używane do liczenia obiektów {0, 1, 2, 3, …}\nLiczby Całkowite (ℤ): Obejmują liczby naturalne, ich przeciwności i zero {…, -2, -1, 0, 1, 2, …}\nLiczby Wymierne (ℚ): Liczby, które można wyrazić jako ułamek dwóch liczb całkowitych\nLiczby Rzeczywiste (ℝ): Wszystkie liczby na osi liczbowej, w tym wymierne i niewymierne\n\n\n\n4.1.2 Właściwości Zbiorów\n\nZbiory Przeliczalne: Zbiory, których elementy można ustawić w relacji jeden do jednego z liczbami naturalnymi. Na przykład, zbiór liczb całkowitych jest przeliczalny.\nZbiory Nieprzeliczalne: Zbiory, które nie są przeliczalne. Zbiór liczb rzeczywistych jest nieprzeliczalny.\nZbiory Dyskretne: Zbiory, w których każdy element jest oddzielony od innych elementów skończoną przerwą. Liczby całkowite tworzą zbiór dyskretny.\nZbiory Gęste: Zbiory, w których między dowolnymi dwoma elementami zawsze znajduje się inny element zbioru. Liczby wymierne i rzeczywiste są zbiorami gęstymi.\n\n\n\n\n\n\n\nNote\n\n\n\nZrozumienie tych właściwości zbiorów jest kluczowe dla uchwycenia natury różnych typów danych w naukach społecznych.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#dane-dyskretne-vs.-ciągłe",
    "href": "rozdzial2.html#dane-dyskretne-vs.-ciągłe",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.2 Dane Dyskretne vs. Ciągłe",
    "text": "4.2 Dane Dyskretne vs. Ciągłe\nW nauce o danych i statystyce często kategoryzujemy zmienne jako dyskretne lub ciągłe. Jednak rozróżnienie to nie zawsze jest jednoznaczne, a niektóre zmienne wykazują cechy obu typów. Ta sekcja bada koncepcje danych dyskretnych i ciągłych, ich różnice oraz interesujące przypadki zmiennych, które można traktować jako oba typy lub które kwestionują nasze intuicyjne rozumienie.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n4.2.1 Dane Dyskretne\nDane dyskretne mogą przyjmować tylko określone, przeliczalne wartości. Te wartości często (ale nie zawsze) są liczbami całkowitymi.\n\n4.2.1.1 Cechy Danych Dyskretnych:\n\nPrzeliczalne\nCzęsto reprezentowane przez liczby całkowite\nMogą być skończone lub nieskończone\nBrak wartości między dwoma sąsiednimi punktami danych\n\n\n\n4.2.1.2 Przykłady:\n\nLiczba studentów w klasie\nLiczba samochodów sprzedanych przez dealera\nRozmiary butów\n\n\n\n\n4.2.2 Dane Ciągłe\nDane ciągłe mogą przyjmować dowolną wartość w danym zakresie, w tym wartości ułamkowe i dziesiętne. Ważne jest, aby zauważyć, że ciągłość nie jest określona wyłącznie przez nieprzeliczalność, ale również przez gęstość.\n\n4.2.2.1 Cechy Danych Ciągłych:\n\nMogą być nieprzeliczalne (jak liczby rzeczywiste) lub gęste (jak liczby wymierne)\nMogą być mierzone z dowolną precyzją (teoretycznie)\nReprezentowane przez liczby rzeczywiste lub gęste podzbiory liczb rzeczywistych\nZawsze istnieją wartości między dowolnymi dwoma punktami danych\n\n\n\n4.2.2.2 Przykłady:\n\nWzrost\nWaga\nTemperatura\nProcenty (wyjaśnione dalej poniżej)\n\n\n\n\n4.2.3 Spektrum Dyskretno-Ciągłe\nW praktyce niektóre zmienne, które matematycznie są dyskretne, często są traktowane tak, jakby były ciągłe. Ta dwoista natura zapewnia elastyczność w analizie i interpretacji tych zmiennych.\n\n4.2.3.1 Powody Traktowania Danych Dyskretnych jako Ciągłych:\n\nGęsta Granularność\n\nGdy zmienna dyskretna ma dużą liczbę możliwych wartości w danym zakresie, może przybliżać ciągłość.\nPrzykład: Dochód mierzony w pojedynczych groszach. Choć technicznie dyskretny, duża liczba możliwych wartości sprawia, że zachowuje się podobnie do zmiennej ciągłej.\n\nWygoda Analityczna\n\nMetody ciągłe często dają rozsądne i użyteczne wyniki nawet dla gęstych zmiennych dyskretnych.\nCzęsto łatwiej jest używać istniejących narzędzi statystycznych, jeśli założymy ciągłość, ponieważ pozwala to na stosowanie metod opartych na rachunku różniczkowym.\n\nPrzybliżenie Zjawisk Bazowych\n\nW niektórych przypadkach dyskretny pomiar może być przybliżeniem bazowego procesu ciągłego.\nPrzykład: Chociaż mierzymy czas w dyskretnych jednostkach (sekundy, minuty, godziny), sam czas jest ciągły.\n\n\n\n\n4.2.3.2 Przykłady Zmiennych o Dwoistej Naturze Dyskretno-Ciągłej:\n\nWiek\n\nDyskretny: Typowo mierzony w pełnych latach\nCiągły: Może być uznany za zmienną ciągłą w wielu analizach, szczególnie przy dużych populacjach\n\nCena i Dochód\n\nDyskretne: Ceny i dochody są w rzeczywistości mierzone w dyskretnych jednostkach (np. grosze lub najmniejsza jednostka waluty)\nCiągłe: W modelach ekonomicznych i wielu analizach ceny i dochody są traktowane jako zmienne ciągłe ze względu na ich gęstą naturę i wygodę analityczną\n\nWyniki Testów\n\nDyskretne: Często podawane jako liczby całkowite\nCiągłe: W analizach statystycznych wyniki testów mogą być traktowane jako ciągłe, szczególnie gdy zakres możliwych wyników jest duży\n\n\n\n\n\n4.2.4 Przypadek Szczególny: Procenty i Liczby Wymierne\nProcenty przedstawiają interesujący przypadek w spektrum dyskretno-ciągłym:\n\nNatura Wymierna: Procenty są zasadniczo ułamkami (m/100), co czyni je liczbami wymiernymi.\nGęste, ale Przeliczalne: Zbiór liczb wymiernych jest gęsty (między dowolnymi dwoma wymiernymi jest inny wymierny), ale także przeliczalny.\nPraktyczna Ciągłość: W większości praktycznych zastosowań procenty są traktowane jako ciągłe ze względu na ich gęstą naturę.\nSkończona Precyzja: W rzeczywistości procenty są często podawane z ograniczoną liczbą miejsc po przecinku, tworząc skończony zbiór możliwych wartości.\n\n\n\n\n\n\n\nProcenty: Łączenie Dyskretnego i Ciągłego\n\n\n\nZmienne mierzone w procentach, takie jak stopy bezrobocia czy frekwencja wyborcza, kwestionują nasze intuicyjne rozumienie dyskretności i ciągłości:\n\nSą liczbami wymiernymi (ułamki z mianownikiem 100), które technicznie są przeliczalne.\nTworzą zbiór gęsty w swoim zakresie (od 0% do 100%), pozwalając na wartości między dowolnymi dwoma procentami.\nW praktyce są często traktowane jako zmienne ciągłe ze względu na ich gęstą naturę i wygodę analityczną.\nPrecyzja pomiaru (np. podawanie do jednego lub dwóch miejsc po przecinku) może narzucić dyskretną strukturę na to, co koncepcyjnie jest zbiorem gęstym.\n\nTa dwoistość pozwala na elastyczne podejścia analityczne, w zależności od konkretnego kontekstu badawczego i wymaganej precyzji.\n\n\n\n\n4.2.5 Implikacje dla Analizy Danych\nZrozumienie zniuansowanej natury zmiennych jako dyskretnych, ciągłych lub gdzieś pomiędzy ma ważne implikacje dla analizy danych:\n\nElastyczność w Modelowaniu: Pozwala na wykorzystanie szerszego zakresu technik statystycznych.\nUproszczone Obliczenia: Traktowanie gęstych danych dyskretnych jako ciągłych może uprościć obliczenia i uczynić niektóre analizy bardziej wykonalnymi.\nLepsza Interpretowalność: W niektórych przypadkach traktowanie danych dyskretnych jako ciągłych może prowadzić do bardziej intuicyjnych lub użytecznych interpretacji wyników.\nPotencjał Błędu: Ważne jest, aby być świadomym, kiedy przybliżenia są odpowiednie, a kiedy mogą prowadzić do mylących wyników.\nRozważania Teoretyczne vs. Praktyczne: Choć matematyczna natura danych jest ważna, praktyczne względy w pomiarze i analizie często kierują tym, jak traktujemy zmienne.\n\n\n\n4.2.6 Wnioski\nRozróżnienie między danymi dyskretnymi a ciągłymi nie zawsze jest sztywne w naukach społecznych. Wiele zmiennych, w tym te dotyczące pieniędzy, procentów czy gęstych pomiarów, można oglądać przez pryzmat zarówno dyskretny, jak i ciągły. Wybór sposobu traktowania powinien być kierowany naturą danych, celami analizy i potencjalnymi implikacjami tego wyboru. Ta elastyczność, gdy jest używana rozważnie, zapewnia potężne narzędzia dla badaczy nauk społecznych do uzyskiwania wglądu w ich dane.\n\n\n\n\n\n\nDane Dyskretne vs. Ciągłe: Analogia Językowa\n\n\n\n\n4.2.6.1 Kluczowe Rozróżnienie Językowe\nW języku polskim mamy precyzyjne rozróżnienie:\n\n“Liczba” → używamy dla rzeczy policzalnych\n“Ilość” → używamy dla rzeczy niepoliczalnych\n\nTo rozróżnienie doskonale odzwierciedla dwa podstawowe typy danych liczbowych:\n\n\n4.2.6.2 Dane Dyskretne = “Liczba czegoś”\n\nUżywamy słowa “liczba” (tak jak mówimy “liczba studentów”)\nWartości są rozdzielone jak pojedyncze elementy\nPrzykłady:\n\nLiczba książek: 0, 1, 2, 3…\nLiczba punktów w teście: 0, 1, 2…\nLiczba mieszkańców: 100, 101, 102…\n\n\n🤔 Czy poprawne jest powiedzenie “ilość studentów” czy “liczba studentów”? (Poprawna forma pomoże Ci rozpoznać typ danych)\n\n\n4.2.6.3 Dane Ciągłe = “Ilość czegoś”\n\nUżywamy słowa “ilość” (tak jak mówimy “ilość wody”)\nWartości płynnie przechodzą jedna w drugą\nPrzykłady:\n\nIlość cieczy: 1,5231… litra\nIlość czasu: 2,3891… godziny\nIlość energii: 5,7123… kWh\n\n\n🤔 Czy mówimy “ilość wody” czy “liczba wody”? (Poprawna forma wskazuje na typ danych)\n\n\n4.2.6.4 Sposób Rozpoznawania\n\nCzy użyłbyś słowa “liczba”? → Dane dyskretne\nCzy użyłbyś słowa “ilość”? → Dane ciągłe\n\n✍️ Ćwiczenie: Uzupełnij poprawnym słowem i określ typ danych\n\n_____ uczniów w klasie (liczba/ilość): typ _____\n_____ deszczu (liczba/ilość): typ _____\n_____ piosenek (liczba/ilość): typ _____\n_____ temperatury (liczba/ilość): typ _____\n\n\n\n\n\n\n4.2.7 Przykład Kodu R\nOto prosty przykład kodu R ilustrujący, jak możemy analizować zmienne traktowane jako ciągłe:\n\n# Generowanie przykładowych danych\nset.seed(123)\nwiek &lt;- round(runif(1000, min = 18, max = 80))\nstopy_bezrobocia &lt;- round(runif(1000, min = 3, max = 10), 1)\n\n# Traktowanie wieku jako dyskretnego\ntabela_wieku &lt;- table(wiek)\nbarplot(tabela_wieku, main = \"Rozkład Wieku (Dyskretny)\", xlab = \"Wiek\", ylab = \"Częstotliwość\")\n\n\n\n\n\n\n\n# Traktowanie wieku jako ciągłego\nhist(wiek, main = \"Rozkład Wieku (Ciągły)\", xlab = \"Wiek\", ylab = \"Częstotliwość\")\n\n\n\n\n\n\n\n# Traktowanie stopy bezrobocia jako dyskretnej\ntabela_stopy &lt;- table(stopy_bezrobocia)\nbarplot(tabela_stopy, main = \"Stopy Bezrobocia (Dyskretne)\", xlab = \"Stopa (%)\", ylab = \"Częstotliwość\")\n\n\n\n\n\n\n\n# Traktowanie stopy bezrobocia jako ciągłej\nhist(stopy_bezrobocia, breaks = 30, main = \"Stopy Bezrobocia (Ciągłe)\", xlab = \"Stopa (%)\", ylab = \"Częstotliwość\")\n\n# Porównanie średnich i median\ncat(\"Średni wiek:\", mean(wiek), \"\\n\")\n\nŚredni wiek: 48.848 \n\ncat(\"Mediana wieku:\", median(wiek), \"\\n\")\n\nMediana wieku: 48 \n\ncat(\"Średnia stopa bezrobocia:\", mean(stopy_bezrobocia), \"\\n\")\n\nŚrednia stopa bezrobocia: 6.4871 \n\ncat(\"Mediana stopy bezrobocia:\", median(stopy_bezrobocia), \"\\n\")\n\nMediana stopy bezrobocia: 6.5 \n\n# Regresja liniowa (traktowanie obu jako ciągłych)\ndochod &lt;- 20000 + 500 * wiek + 1000 * stopy_bezrobocia + rnorm(1000, 0, 5000)\nmodel &lt;- lm(dochod ~ wiek + stopy_bezrobocia)\nsummary(model)\n\n\nCall:\nlm(formula = dochod ~ wiek + stopy_bezrobocia)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15354  -3471     54   3485  16684 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      20116.194    717.231   28.05   &lt;2e-16 ***\nwiek               503.385      8.983   56.04   &lt;2e-16 ***\nstopy_bezrobocia   989.333     80.004   12.37   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5053 on 997 degrees of freedom\nMultiple R-squared:  0.7637,    Adjusted R-squared:  0.7632 \nF-statistic:  1611 on 2 and 997 DF,  p-value: &lt; 2.2e-16\n\n# Wykres płaszczyzny regresji\nlibrary(plotly)\n\nLoading required package: ggplot2\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\n\n\n\n\n\n\nplot_ly(x = wiek, y = stopy_bezrobocia, z = dochod, type = \"scatter3d\", mode = \"markers\") %&gt;%\n  add_trace(x = wiek, y = stopy_bezrobocia, z = fitted(model), type = \"scatter3d\", mode = \"lines\")\n\n\n\n\n\nWykres 3D ilustruje, jak obie zmienne mogą być traktowane jako ciągłe w modelu regresji.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "href": "rozdzial2.html#wprowadzenie-do-typologii-danych-stevensa",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.3 Wprowadzenie do Typologii Danych Stevensa",
    "text": "4.3 Wprowadzenie do Typologii Danych Stevensa\nStanley S. Stevens, amerykański psycholog, wprowadził system klasyfikacji skal pomiarowych w swoim artykule z 1946 roku “On the Theory of Scales of Measurement”. Ten system, znany jako typologia danych Stevensa lub poziomy pomiaru, stał się fundamentalny dla zrozumienia, jak różne typy danych powinny być analizowane i interpretowane.\nStevens zaproponował cztery poziomy pomiaru:\n\nNominalny\nPorządkowy\nInterwałowy\nIlorazowy\n\nKażdy poziom ma specyficzne właściwości i pozwala na różne rodzaje operacji statystycznych i analiz.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n4.3.1 Skala Nominalna\n\n4.3.1.1 Definicja\nSkala nominalna jest najbardziej podstawowym poziomem pomiaru. Używa etykiet lub kategorii do klasyfikacji danych bez żadnej wartości ilościowej ani porządku.\n\n\n4.3.1.2 Właściwości\n\nKategorie są wzajemnie wykluczające się\nBrak inherentnego porządku między kategoriami\nNie można wykonywać znaczących operacji arytmetycznych\n\n\n\n4.3.1.3 Przykłady\n\nNarodowość (Polak, Niemiec, …)\nGrupy krwi (A, B, AB, O)\nKolor oczu (Niebieskie, Brązowe, Zielone, Piwne)\nZmienne binarne (“Sukces” versus “Niepowodzenie”)\n\n\n\n\n4.3.2 Skala Porządkowa\n\n4.3.2.1 Definicja\nSkala porządkowa kategoryzuje dane w uporządkowane kategorie, ale odstępy między kategoriami niekoniecznie są równe lub znaczące.\n\n\n4.3.2.2 Właściwości\n\nKategorie mają zdefiniowany porządek\nRóżnice między kategoriami nie są kwantyfikowalne\nOperacje arytmetyczne na liczbach nie są znaczące\n\n\n\n4.3.2.3 Przykłady\n\nPoziomy wykształcenia (Szkoła Średnia, Licencjat, Magister, Doktorat)\nSkale Likerta (Zdecydowanie się nie zgadzam, Nie zgadzam się, Neutralnie, Zgadzam się, Zdecydowanie się zgadzam)\nStatus społeczno-ekonomiczny (Niski, Średni, Wysoki)\n\n\n\n\n4.3.3 Skala Interwałowa\n\n4.3.3.1 Definicja\nSkala interwałowa ma uporządkowane kategorie z równymi odstępami między sąsiednimi kategoriami. Jednak brakuje jej prawdziwego punktu zerowego.\n\n\n4.3.3.2 Właściwości\n\nRówne odstępy między sąsiednimi kategoriami\nBrak prawdziwego punktu zerowego (zero jest umowne)\nStosunki między wartościami nie są znaczące\n\n\n\n4.3.3.3 Przykłady\n\nTemperatura w stopniach Celsjusza lub Fahrenheita\nLata kalendarzowe\nSkala pH\nWysokość nad poziomem morza\n\n\n\n\n4.3.4 Skala Ilorazowa\n\n4.3.4.1 Definicja\nSkala ilorazowa jest najwyższym poziomem pomiaru. Ma wszystkie właściwości skali interwałowej plus prawdziwy punkt zerowy, co sprawia, że stosunki między wartościami są znaczące.\n\n\n4.3.4.2 Właściwości\n\nWszystkie właściwości skal interwałowych\nPrawdziwy punkt zerowy\nStosunki między wartościami są znaczące\n\n\n\n4.3.4.3 Przykłady\n\nWzrost\nWaga\nWiek\nDochód\n\n\n\n\n\n\n\nDlaczego Niektóre Statystyki Działają (a Inne Nie) dla Skal Interwałowych\n\n\n\n\n4.3.4.4 Kluczowa Idea\nW przypadku skal interwałowych (np. temperatury):\n\nDozwolone jest dodawanie/odejmowanie wartości oraz dzielenie przez liczby\nNiedozwolone jest mnożenie/dzielenie wartości przez siebie\n\n\n\n4.3.4.5 Własności Skali Interwałowej\n\nRówne interwały reprezentują takie same różnice:\n\nRóżnica między 20°C a 25°C (5°C) reprezentuje taką samą zmianę jak między 30°C a 35°C\nProporcje różnic są zachowane: 10°C to dwa razy większa zmiana niż 5°C\n\nPunkt zero jest umowny:\n\n0°C to punkt zamarzania wody, a nie brak temperatury\nTen sam stan fizyczny ma różne wartości: 0°C = 32°F\n\nTransformacja liniowa:\n\nWzór ogólny: y = ax + b, gdzie a \\neq 0\nDla temperatury: F = C \\times \\frac{9}{5} + 32\n\n\n\n\n4.3.4.6 Dlaczego Średnia Arytmetyczna Działa\nPrzykład analizy dla dwóch temperatur:\nDane: 20°C i 30°C\n\nMetoda 1: Średnia w Celsjuszach, potem konwersja\n1. Średnia: (20°C + 30°C) ÷ 2 = 25°C\n2. Konwersja: 25°C × (9/5) + 32 = 77°F\n\nMetoda 2: Konwersja na °F, potem średnia\n1. Konwersja: 20°C → 68°F, 30°C → 86°F\n2. Średnia: (68°F + 86°F) ÷ 2 = 77°F\n\nObie metody dają ten sam wynik! ✓\nMatematyczny dowód poprawności:\n\\begin{align*}\n\\bar{F} &= \\frac{F_1 + F_2}{2} = \\frac{(C_1 \\times \\frac{9}{5} + 32) + (C_2 \\times \\frac{9}{5} + 32)}{2} \\\\\n&= \\frac{(C_1 + C_2) \\times \\frac{9}{5} + 64}{2} \\\\\n&= (\\frac{C_1 + C_2}{2}) \\times \\frac{9}{5} + 32 \\\\\n&= \\bar{C} \\times \\frac{9}{5} + 32\n\\end{align*}\nTo działa ponieważ:\n\nUżywamy tylko dozwolonych operacji\nZachowana jest liniowość transformacji\nUmowny punkt zero nie wpływa na wynik\n\n\n\n4.3.4.7 Dlaczego Wariancja Jest Problematyczna\nAnaliza na tym samym przykładzie:\nTe same temperatury: 20°C i 30°C\n\nMetoda 1: Wariancja w Celsjuszach\n1. Średnia: 25°C\n2. Odchylenia: (20 - 25)°C = -5°C, (30 - 25)°C = 5°C\n3. Kwadraty odchyleń: (-5°C)² = 25(°C)², (5°C)² = 25(°C)²\n4. Średnia: (25 + 25)(°C)² ÷ 2 = 25(°C)²\n\nMetoda 2: Wariancja w Fahrenheitach\n1. Konwersja: 20°C → 68°F, 30°C → 86°F\n2. Średnia: 77°F\n3. Odchylenia: (68 - 77)°F = -9°F, (86 - 77)°F = 9°F\n4. Kwadraty odchyleń: (-9°F)² = 81(°F)², (9°F)² = 81(°F)²\n5. Średnia: (81 + 81)(°F)² ÷ 2 = 81(°F)²\n\nProblem: 25(°C)² i 81(°F)² nie są porównywalne!\nMatematyczna analiza problemu:\nDla odchylenia w skali Fahrenheita: \\begin{align*}\n(F_i - \\bar{F})^2 &= [(C_i \\times \\frac{9}{5} + 32) - (\\bar{C} \\times \\frac{9}{5} + 32)]^2 \\\\\n&= [(C_i - \\bar{C}) \\times \\frac{9}{5}]^2 \\\\\n&= (C_i - \\bar{C})^2 \\times (\\frac{9}{5})^2\n\\end{align*}\nTo nie działa dobrze ponieważ:\n\nMnożymy temperatury przez siebie (niedozwolone!)\nPowstają jednostki kwadratowe (°C² lub °F²) bez interpretacji fizycznej\nWyniki w różnych skalach nie są porównywalne\nKwadrat współczynnika skalowania ((\\frac{9}{5})^2) nie ma interpretacji fizycznej\n\n\n\n4.3.4.8 Wnioski Teoretyczne\n\nOperacje dozwolone:\n\nDodawanie/odejmowanie (zachowuje różnice)\nMnożenie przez stałe (skalowanie)\nŚrednie arytmetyczne\nPorównywanie różnic temperatur\n\nOperacje niedozwolone:\n\nMnożenie temperatur\nDzielenie temperatur\nŚrednie geometryczne\nWspółczynnik zmienności\n\nImplikacje praktyczne:\n\nDla wariancji i odchylenia standardowego potrzebna ostrożna interpretacja\nLepiej używać miar opartych na różnicach (np. MAD)\nPrzy porównywaniu zmienności warto standaryzować dane\n\n\n\n\n4.3.4.9 Zasada Praktyczna\nJeśli w obliczeniach pojawia się mnożenie wartości ze skali interwałowej przez siebie, należy zachować ostrożność w interpretacji wyników!\n\n\n\n\n\n\n\n\n\nProporcje w Skalach Pomiarowych: Przypadek Temperatury\n\n\n\n\n4.3.4.10 Dwa Rodzaje Proporcji\n\n4.3.4.10.1 A) Proporcje wartości (NIE zachowują się w skali interwałowej):\nWeźmy 80°C i 20°C:\nW Celsjuszach: 80°C/20°C = 4\nW Fahrenheitach: 176°F/68°F ≈ 2.59\nW Kelwinach: 353.15K/293.15K ≈ 1.20\n\nTe same temperatury dają różne proporcje! \n→ Proporcje wartości NIE mają sensu na skalach interwałowych; sens mają tylko na skali ilorazowej\n\n\n4.3.4.10.2 B) Proporcje różnic (zachowują się w skali interwałowej):\nWeźmy dwie pary różnic:\nPara 1: 30°C - 20°C = 10°C\nPara 2: 80°C - 60°C = 20°C\n\nProporcja różnic w Celsjuszach:\n20°C/10°C = 2\n\nTe same temperatury w Fahrenheitach:\nPara 1: 86°F - 68°F = 18°F\nPara 2: 176°F - 140°F = 36°F\n\nProporcja różnic w Fahrenheitach:\n36°F/18°F = 2\n\nProporcja różnic jest taka sama! ✓\n\n\n\n4.3.4.11 Matematyczne Wyjaśnienie\nDla transformacji F = \\frac{9}{5}C + 32:\n\nProporcje wartości NIE zachowują się: \\frac{F_1}{F_2} = \\frac{\\frac{9}{5}C_1 + 32}{\\frac{9}{5}C_2 + 32} \\neq \\frac{C_1}{C_2}\nProporcje różnic zachowują się: \\frac{F_1 - F_2}{F_3 - F_4} = \\frac{(\\frac{9}{5}C_1 + 32) - (\\frac{9}{5}C_2 + 32)}{(\\frac{9}{5}C_3 + 32) - (\\frac{9}{5}C_4 + 32)} = \\frac{\\frac{9}{5}(C_1 - C_2)}{\\frac{9}{5}(C_3 - C_4)} = \\frac{C_1 - C_2}{C_3 - C_4}\n\n\n\n4.3.4.12 Dlaczego To Jest Ważne?\n\nDla wartości:\n\nW skali Celsjusza: 40°C nie jest “dwa razy cieplejsze” niż 20°C\nW skali Fahrenheita: 100°F nie jest “dwa razy cieplejsze” niż 50°F\nTylko w Kelwinach proporcje wartości mają sens fizyczny\n\nDla różnic:\n\nWzrost o 20°C jest zawsze dwa razy większy niż wzrost o 10°C\nWzrost o 36°F jest zawsze dwa razy większy niż wzrost o 18°F\nProporcje różnic są niezależne od skali\n\n\n\n\n4.3.4.13 Implikacje dla Statystyk\n\nOperacje bazujące na różnicach (DZIAŁAJĄ):\n\nŚrednia arytmetyczna\nOdchylenie bezwzględne\nRozstęp\n\nOperacje bazujące na proporcjach wartości (NIE DZIAŁAJĄ):\n\nŚrednia geometryczna\nWspółczynnik zmienności\nWariancja (bo używa kwadratu wartości)\n\n\n\n\n\n\n\n\n4.3.5 Znaczenie w Badaniach i Analizie\nZrozumienie typologii danych Stevensa jest kluczowe z kilku powodów:\n\nWybór odpowiednich testów statystycznych: Poziom pomiaru determinuje, które analizy statystyczne są odpowiednie dla danego zbioru danych.\nInterpretacja wyników: Znaczenie wyników statystycznych zależy od poziomu pomiaru zaangażowanych zmiennych.\nProjektowanie narzędzi pomiarowych: Przy tworzeniu ankiet lub innych narzędzi pomiarowych badacze muszą wziąć pod uwagę poziom pomiaru, który chcą osiągnąć.\nTransformacja danych: Czasami dane mogą być przekształcane z jednego poziomu na drugi, ale musi to być robione ostrożnie, aby uniknąć błędnej interpretacji.\n\n\n\n4.3.6 Kontrowersje i Ograniczenia\nChociaż typologia Stevensa jest szeroko stosowana, spotkała się z pewnymi krytykami:\n\nSztywność: Niektórzy twierdzą, że typologia jest zbyt sztywna i że wiele rzeczywistych pomiarów mieści się pomiędzy tymi kategoriami.\nTraktowanie danych porządkowych: Trwa debata na temat tego, kiedy właściwe jest traktowanie danych porządkowych jako interwałowych dla pewnych analiz.\nSkalowanie psychologiczne: Niektóre konstrukty psychologiczne (jak inteligencja) są trudne do jednoznacznego skategoryzowania w ramach tego systemu.\n\n\n\n4.3.7 Podsumowanie\nTypologia danych Stevensa dostarcza fundamentalnych ram dla zrozumienia różnych rodzajów danych i ich właściwości. Rozpoznając poziom pomiaru swoich zmiennych, badacze mogą podejmować świadome decyzje dotyczące gromadzenia danych, analizy i interpretacji. Jednak ważne jest, aby pamiętać, że chociaż ta typologia jest użytecznym przewodnikiem, rzeczywiste dane często wymagają niuansowego podejścia i nie zawsze pasują idealnie do tych kategorii.\n\n\n\n\n\n\npH as an Interval Scale\n\n\n\npH is considered an interval scale because:\n\nIt has ordered categories: Lower pH values indicate higher acidity, while higher values indicate higher alkalinity.\nThe intervals between adjacent pH values are equal in terms of hydrogen ion concentration:\n\nEach whole number change in pH represents a tenfold change in hydrogen ion concentration.\nFor example, the difference in acidity between pH 4 and pH 5 is the same as the difference between pH 7 and pH 8.\n\nIt lacks a true zero point:\n\npH 0 does not represent a complete absence of hydrogen ions.\nNegative pH values and values above 14 are possible in extreme conditions.\n\nRatios are not meaningful:\n\nA pH of 4 is not “twice as acidic” as a pH of 2.\nThe ratio of hydrogen ion concentrations, not pH values, indicates relative acidity.\n\n\nThese characteristics align with the definition of an interval scale, where the differences between values are meaningful and consistent, but ratios are not interpretable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "rozdzial2.html#popularne-skale-porządkowe-w-badaniach-behawioralnych",
    "href": "rozdzial2.html#popularne-skale-porządkowe-w-badaniach-behawioralnych",
    "title": "4  Zrozumienie Typów Danych w Naukach Społecznych",
    "section": "4.4 Popularne Skale Porządkowe w Badaniach Behawioralnych",
    "text": "4.4 Popularne Skale Porządkowe w Badaniach Behawioralnych\n\n4.4.1 Skale Likerta\nSkale Likerta są szeroko stosowane w psychologii i naukach społecznych do pomiaru postaw, opinii i percepcji. Nazwane na cześć psychologa Rensisa Likerta, skale te zazwyczaj składają się z serii stwierdzeń lub pytań, które respondenci oceniają na skali, często od “Zdecydowanie się nie zgadzam” do “Zdecydowanie się zgadzam”.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n4.4.1.1 Dlaczego Skale Likerta są Zmiennymi Porządkowymi\nSkale Likerta są uważane za zmienne porządkowe z kilku powodów:\n\nPorządek bez równych odstępów: Chociaż odpowiedzi mają wyraźną kolejność (np. “Zdecydowanie się nie zgadzam” &lt; “Nie zgadzam się” &lt; “Neutralnie” &lt; “Zgadzam się” &lt; “Zdecydowanie się zgadzam”), odstępy między tymi kategoriami niekoniecznie są równe.\nSubiektywna interpretacja: Różnica między “Zdecydowanie się nie zgadzam” a “Nie zgadzam się” może nie być taka sama jak różnica między “Zgadzam się” a “Zdecydowanie się zgadzam” dla wszystkich respondentów.\nBrak prawdziwego punktu zerowego: Skale Likerta zazwyczaj nie mają prawdziwego punktu zerowego, co jest cechą charakterystyczną skal interwałowych lub ilorazowych.\n\n\n\n\n4.4.2 IQ i Inne Zmienne Psychologiczne jako Miary Porządkowe\nWiele miar psychologicznych, w tym IQ, jest często traktowanych jako skale interwałowe, ale w rzeczywistości są to skale porządkowe. Oto dlaczego:\n\nWyniki IQ:\n\nChociaż wyniki IQ są przedstawiane jako liczby, różnica między IQ 100 a 110 może nie reprezentować takiej samej różnicy poznawczej jak między 130 a 140.\nSkala jest normalizowana i dostosowywana w czasie, co utrudnia stwierdzenie, że ma właściwości prawdziwie interwałowe.\n\nInne Miary Psychologiczne:\n\nSkale depresji (np. Inwentarz Depresji Becka)\nMiary lęku (np. Inwentarz Stanu i Cechy Lęku)\nOceny osobowości (np. Inwentarz Wielkiej Piątki)\n\n\nTe miary często wykorzystują sumowane pozycje typu Likerta lub inne metody punktacji, które nie gwarantują równych odstępów między wynikami.\n\n\n4.4.3 Implikacje dla Analizy\nUznanie tych miar za porządkowe ma ważne implikacje dla analizy danych:\n\nOdpowiednie testy statystyczne: Używaj testów nieparametrycznych (np. test U Manna-Whitneya, test Kruskala-Wallisa) zamiast parametrycznych.\nAnaliza korelacji: Używaj korelacji rangowej Spearmana zamiast korelacji Pearsona.\nTendencja centralna: Raportuj medianę i dominantę zamiast średniej.\nWizualizacja danych: Stosuj metody odpowiednie dla danych porządkowych, takie jak wykresy słupkowe lub skumulowane wykresy słupkowe.\n\n\n\n4.4.4 Podsumowanie\nChociaż skale Likerta i wiele miar psychologicznych jest często traktowanych jako dane interwałowe ze względów praktycznych, ważne jest, aby pamiętać o ich porządkowym charakterze.\n\n\n\nhttps://individual-psychometrics.rbind.io/\n\n\n\n\n\n\n\n\nĆwiczenie: Identyfikacja Skal Pomiarowych\n\n\n\nDla każdej z poniższych zmiennych określ najbardziej odpowiednią skalę pomiaru (Nominalna, Porządkowa, Przedziałowa lub Stosunkowa). Czy zmienna jest dyskretna, czy ciągła?\n\nPłeć: skala nominalna; zmienna dyskretna;\nSatysfakcja klienta: Niska, Średnia, Dobra, Doskonała\nWzrost (ankieta): “Jestem: bardzo niski, niski, przeciętnego wzrostu, wysoki, bardzo wysoki”\nWzrost mierzony w centymetrach\nCzas reakcji (w milisekundach)\nKody pocztowe: np. 00-001, 00-950, 80-452, 31-072\nWiek (w latach)\nMarki samochodów\nNarodowość\nLiczba dzieci w rodzinie: 1 dziecko, 2 dzieci, 3 dzieci, …\nWynik testu IQ\nTemperatura (skala Celsjusza)\nTemperatura (skala Kelvina)\nFrekwencja wyborcza\nPrzynależność partyjna\nWielkość okręgu wyborczego\nWspółrzędne w układzie kartezjańskim\nData (względem określonej epoki, np. n.e.)\nWysokość nad poziomem morza\nGrupy krwi: A, B, AB, 0\nKategorie dochodów: niskie, średnie, wysokie\nStopnie wojskowe\n\nPamiętaj, aby uzasadnić swój wybór skali dla każdej zmiennej.\nDla przykładu: W typologii skal pomiarowych Stevensa, adresy uliczne są danymi nominalnymi. Dlaczego?\nPełnią wyłącznie funkcję etykiet/identyfikatorów Nie mają naturalnego uporządkowania (ul. Mickiewicza 5 nie jest “większa” niż ul. Słowackiego 10) Nie można wykonywać na nich sensownych operacji matematycznych Jedyna dozwolona operacja to sprawdzanie równości/nierówności (czy to ten sam adres czy inny?)\nMimo że numery domów są liczbami, w systemie adresowym funkcjonują jako etykiety, a nie wartości ilościowe. Liczba 100 w adresie “ul. Kilińskiego 100” nie jest używana matematycznie - równie dobrze mogłaby to być “ul. Jabłkowa” czy “ul. Zeusa”, jeśli chodzi o jej funkcję w adresie.\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\n4.4.5 Notes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Zrozumienie Typów Danych w Naukach Społecznych</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "",
    "text": "5.1 Introduction to Randomness\nRandomness is a cornerstone concept in statistics and scientific research. It refers to the unpredictability of individual outcomes, even when the overall pattern may be predictable. In the social sciences, understanding randomness is crucial for designing studies, collecting data, and interpreting results.\nConsider flipping a fair coin. While we know that the probability of getting heads is 50%, we can’t predict with certainty the outcome of any single flip. This unpredictability is the essence of randomness.\nExamples of random phenomena in social sciences include:\nUnderstanding randomness helps researchers distinguish between genuine effects and chance occurrences. For instance, if we observe a slight difference in test scores between two groups, randomness helps us determine whether this difference is likely due to a real effect or just chance variation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#introduction-to-randomness",
    "href": "chapter3.html#introduction-to-randomness",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "",
    "text": "Participant Selection: In a psychology experiment studying reaction times, the order in which participants arrive at the lab may be random.\nEconomic Behavior: The daily fluctuations in stock prices often exhibit random patterns, influenced by countless unpredictable factors.\nSocial Interactions: The occurrence of chance encounters between individuals in a community can be considered random events.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-bridging-sample-and-population",
    "href": "chapter3.html#sampling-bridging-sample-and-population",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.2 Sampling: Bridging Sample and Population",
    "text": "5.2 Sampling: Bridging Sample and Population\nSampling is the process of selecting a subset (sample) from a larger group (population) to make inferences about the population. It’s a critical skill in social science research, as studying entire populations is often impractical, too expensive, or sometimes impossible.\nKey Terms:\n\nPopulation: The entire group about which we want to draw conclusions.\nSample: A subset of the population that we actually study.\nSampling Frame: The list or procedure used to identify all members of the population.\n\nExample: Suppose we want to study the job satisfaction of all teachers in the United States (the population). Instead of surveying millions of teachers, we might select a sample of 5,000 teachers from various states, school districts, and grade levels.\nRandomness in sampling helps ensure that the sample is representative of the population, reducing bias and allowing for more accurate inferences. This is why probability sampling methods, which we’ll discuss next, are often preferred in scientific research.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-methods",
    "href": "chapter3.html#sampling-methods",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.3 Sampling Methods",
    "text": "5.3 Sampling Methods\n\n5.3.1 Probability Sampling\nProbability sampling methods involve random selection, giving each member of the population a known, non-zero chance of being selected.\n\nSimple Random Sampling: Each member of the population has an equal chance of being selected.\nExample: To select 100 students from a university with 10,000 students, you could assign each student a number from 1 to 10,000, then use a random number generator to select 100 numbers.\nStratified Random Sampling: The population is divided into subgroups (strata) based on shared characteristics, then samples are randomly selected from each stratum.\nExample: In a national political survey, you might divide the population into strata based on geographic regions (Northeast, Midwest, South, West) and then randomly sample from each region. This ensures representation from all areas of the country.\nCluster Sampling: The population is divided into clusters (usually geographic), some clusters are randomly selected, and all members within those clusters are studied.\nExample: To study high school students’ study habits, you might randomly select 20 high schools from across the country and then survey all students in those schools.\nSystematic Sampling: Selecting every kth item from a list after a random start.\nExample: At a busy shopping mall, you might survey every 20th person who enters the mall, starting with a randomly chosen number between 1 and 20.\n\n\n\n5.3.2 Non-probability Sampling\nNon-probability sampling doesn’t involve random selection. While it can introduce bias, it may be necessary in certain situations, especially when dealing with hard-to-reach populations or when resources are limited.\n\nConvenience Sampling: Selecting easily accessible subjects.\nExample: A researcher studying college students’ sleep patterns might survey students in their own classes or around campus.\nPurposive Sampling: Selecting subjects based on specific characteristics.\nExample: For a study on the experiences of CEOs in the tech industry, a researcher might intentionally seek out and interview CEOs from various tech companies.\nSnowball Sampling: Participants recruit other participants.\nExample: In a study of undocumented immigrants’ access to healthcare, researchers might ask initial participants to refer other potential participants from their community.\nQuota Sampling: Selecting participants to meet specific quotas for certain characteristics.\nExample: In a market research study, researchers might ensure they interview a specific number of people from different age groups, genders, and income levels to match the demographics of the target market.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#making-inferences-from-samples",
    "href": "chapter3.html#making-inferences-from-samples",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.4 Making Inferences from Samples",
    "text": "5.4 Making Inferences from Samples\nStatistical inference is the process of drawing conclusions about a population based on a sample. This allows researchers to estimate characteristics of the entire population (parameters) using characteristics of the sample (statistics).\n\n\n\n\n\n\nNote\n\n\n\nThe Soup Analogy: A Taste of Statistics\n\n\nWhen you taste a spoonful of soup and decide it isn’t salty enough, that’s exploratory/descriptive analysis.\nIf you generalize and conclude that your entire pot of soup needs salt, that’s an inference.\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population).\nIf the soup is not well stirred (heterogeneous population), it doesn’t matter how large a spoon you have (sample size), it will still not accurately represent the whole. If the soup is well stirred (homogeneous population), even a small spoon will suffice to test the soup.\n\n\n\n\n\n\n\n\ngraph TD\n    DGP[Data Generating Process] --&gt;|Generates| A[Population]\n    A --&gt;|Random Selection| B[Sample]\n    B --&gt;|Statistical Inference| C[Estimates & Conclusions]\n    C --&gt;|Generalize back to| A\n    C -.-&gt;|Attempt to understand| DGP\n\n    style DGP fill:#1E90FF,stroke:#000,stroke-width:4px,color:#FFF\n    style A fill:#DC143C,stroke:#000,stroke-width:4px,color:#FFF\n    style B fill:#228B22,stroke:#000,stroke-width:2px,color:#FFF\n    style C fill:#8B4513,stroke:#000,stroke-width:2px,color:#FFF\n    \n    classDef note fill:#F0F0F0,stroke:#000,stroke-width:1px;\n    D[[\"DGP:\n    Underlying process\n    that generates data\"]]\n    E[[\"Population:\n    Entire group of interest\"]]\n    F[[\"Sample:\n    Subset of population\"]]\n    G[[\"Inference:\n    Drawing conclusions\n    about population\n    and DGP\"]]\n    \n    class D,E,F,G note\n    \n    D --&gt; DGP\n    E --&gt; A\n    F --&gt; B\n    G --&gt; C\n\n\n\n\n\n\nKey Concepts:\n\nPoint Estimates: A single value used to estimate a population parameter.\nExample: The mean income of a sample of 1000 workers might be used to estimate the mean income of all workers in a country.\nConfidence Intervals: A range of values likely to contain the true population parameter.\nExample: We might say, “We are 95% confident that the true population mean income falls between $45,000 and $55,000.”\nMargin of Error: The range of values above and below the sample statistic in a confidence interval.\nExample: In political polling, you might see a statement like “Candidate A is preferred by 52% of voters, with a margin of error of ±3%.”\nHypothesis Testing: A method for making decisions about population parameters based on sample data.\nExample: A researcher might test whether there’s a significant difference in test scores between students who study with music and those who study in silence.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-and-non-sampling-errors",
    "href": "chapter3.html#sampling-and-non-sampling-errors",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.5 Sampling and Non-sampling Errors",
    "text": "5.5 Sampling and Non-sampling Errors\nUnderstanding potential errors in research is crucial for interpreting results accurately.\nSampling Error: The difference between a sample statistic and the true population parameter, occurring due to chance variations in the selection of sample members.\nExample: If we estimate the average height of all adult males in a country based on a sample, our estimate will likely differ somewhat from the true average due to sampling error.\nNon-sampling Errors: Errors not due to chance, which can occur in both sample surveys and censuses.\n\nCoverage Error: When the sampling frame doesn’t accurately represent the population.\nExample: A telephone survey that only calls landlines would miss people who only have cell phones, potentially biasing the results.\nNon-response Error: When selected participants fail to respond, potentially introducing bias.\nExample: In a survey about job satisfaction, highly satisfied or highly dissatisfied employees might be more likely to respond, skewing the results.\nMeasurement Error: Inaccuracies in the data collected.\nExample: A poorly worded survey question might be interpreted differently by different respondents, leading to inconsistent data.\nProcessing Error: Mistakes made during data entry, coding, or analysis.\nExample: Accidentally entering “99” instead of “9” for a participant’s response could significantly skew the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sample-size-and-power",
    "href": "chapter3.html#sample-size-and-power",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.6 Sample Size and Power",
    "text": "5.6 Sample Size and Power\nDetermining the appropriate sample size involves balancing the need for precision with available resources.\nSample Size Considerations: - Larger samples generally provide more precise estimates but are more costly and time-consuming to obtain. - The required sample size depends on factors such as the desired level of precision, the variability in the population, and the type of analysis planned.\nExample: To estimate the proportion of voters who support a particular policy with a margin of error of ±3% at a 95% confidence level, you would need a sample size of about 1067 voters (assuming maximum variability).\nStatistical Power: The probability that a study will detect an effect when there is an effect to be detected.\nFactors affecting power: 1. Sample size 2. Effect size (the magnitude of the difference or relationship you’re trying to detect) 3. Chosen significance level (usually 0.05)\nExample: In a study comparing two teaching methods, having a larger sample size would increase the likelihood of detecting a significant difference between the methods, if such a difference exists.\n\n\n\n\n\n\nNote\n\n\n\nWhat is Study Power?\nStudy power is about how likely we are to find something if it really exists. It’s like having a good flashlight when you’re looking for something in the dark - the better your flashlight, the more likely you are to find what you’re looking for.\n\nEffect Size: How big the thing (effect, difference) we’re looking for is.\nSample Size: How many people or things we look at in our study.\nStudy Power: How likely we are to find the effect if it’s really there.\n\nThe Relationship Between Effect Size and Sample Size:\nImagine you’re trying to find coins hidden in sand:\n\nBig Effects (Big Coins):\n\nIf you’re looking for big coins (like quarters), you don’t need to search through as much sand to find them.\nIn research, if the effect is big, you can use a smaller sample.\n\nExample: Testing if a new study method improves test scores by 20 points out of 100.\n\nYou might only need to test 30 students to see this big difference.\n\nSmall Effects (Small Coins):\n\nIf you’re looking for tiny coins (like pennies), you’ll need to search through more sand.\nIn research, if the effect is small, you need a larger sample.\n\nExample: Seeing if using social media affects happiness by a tiny amount.\n\nYou might need to study 500 or more people to detect this small effect.\n\n\nWhy Study Power Matters:\n\nNot Missing Real Effects:\n\nWith low power, you might miss real effects, like using a weak flashlight and missing something that’s actually there.\n\nConfidence in Results:\n\nHigher power gives you more confidence that what you found is real and not just luck.\n\n\nExample:\nLet’s say we want to study if a new teaching method helps students learn better:\n\nSmall Study (Low Power):\n\nWe try the method with just 10 students.\nEven if the method works, with such a small group, it’s hard to tell if improvements are due to the new method or just chance.\n\nLarger Study (Higher Power):\n\nWe use the method with 100 students.\nNow we’re more likely to see if the method really helps because we have more data to look at.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#sampling-in-the-digital-age",
    "href": "chapter3.html#sampling-in-the-digital-age",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.7 Sampling in the Digital Age",
    "text": "5.7 Sampling in the Digital Age\nThe advent of big data and digital technologies has transformed sampling practices in many fields.\nBig Data Opportunities and Challenges: - Unprecedented volumes of information available - Potential lack of representativeness - Data quality concerns - Privacy and ethical issues\nExample: Social media data can provide real-time insights into public opinion, but users of a particular platform may not be representative of the general population.\nWeb-based Surveys: - Offer new opportunities for data collection - Face challenges such as coverage bias (not everyone has internet access) and self-selection bias\nExample: An online survey about internet usage habits would inherently exclude people without internet access, potentially biasing the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#ethical-considerations-in-sampling",
    "href": "chapter3.html#ethical-considerations-in-sampling",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.8 Ethical Considerations in Sampling",
    "text": "5.8 Ethical Considerations in Sampling\nEthical sampling practices are crucial in social science research:\n\nInformed Consent: Participants should understand the study’s purpose and agree to participate.\nExample: Before conducting interviews about sensitive topics like mental health, researchers must clearly explain the study’s aims and potential risks to participants.\nPrivacy and Confidentiality: Researchers must protect participants’ personal information.\nExample: In a study on workplace harassment, researchers might use code numbers instead of names to protect participants’ identities.\nRepresentativeness and Inclusivity: Samples should fairly represent diverse populations, including marginalized groups.\nExample: A study on urban housing should make efforts to include participants from various socioeconomic backgrounds, ethnicities, and housing situations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#conclusion",
    "href": "chapter3.html#conclusion",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.9 Conclusion",
    "text": "5.9 Conclusion\nSampling remains a cornerstone of social science research, even in the era of big data. Understanding sampling principles helps researchers design studies, interpret results, and make valid inferences about populations. As we’ve seen, the journey from sample to population involves careful consideration of sampling methods, potential errors, ethical issues, and the ever-evolving landscape of data collection in the digital age.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#statistical-errors---summary",
    "href": "chapter3.html#statistical-errors---summary",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.10 Statistical Errors - Summary",
    "text": "5.10 Statistical Errors - Summary\n\n5.10.1 Systematic Error vs. Random Error\nSystematic errors and random errors are two fundamental types of errors in statistical measurements and experiments.\n\nSystematic Error:\n\nDefinition: Consistent, predictable deviations from the true value\nCharacteristics:\n\nBiases the results in a specific direction\nRepeatable and often constant across measurements\nCan be corrected if identified\n\nExamples:\n\nMiscalibrated measuring instrument\nConsistent rounding error in data entry\nBiased sampling method\n\n\nRandom Error:\n\nDefinition: Unpredictable fluctuations in measurements due to chance\nCharacteristics:\n\nVaries in magnitude and direction\nFollows a probability distribution (often normal)\nCan be reduced by increasing sample size or repeated measurements\n\nExamples:\n\nNatural variations in the phenomenon being measured\nSmall fluctuations in measuring instruments\nHuman errors in reading or recording data\n\n\n\n\n\n5.10.2 Sampling Errors vs. Non-Sampling Errors\nSampling and non-sampling errors are categories of errors that can occur in statistical studies, particularly in survey research.\n\nSampling Errors:\n\nDefinition: Errors that occur due to the sample not perfectly representing the population\nCharacteristics:\n\nInherent in any sample-based study\nCan be estimated and quantified using statistical methods\nDecreases as sample size increases\n\nExamples:\n\nRandom fluctuations in sample statistics\nOver- or under-representation of certain groups in the sample\n\n\nNon-Sampling Errors:\n\nDefinition: All errors in a study that are not related to sampling\nCharacteristics:\n\nCan occur in both sample and census studies\nOften more difficult to quantify and control than sampling errors\nCan introduce bias into results\nCan be either systematic or random\n\nExamples:\n\nResponse errors (e.g., misunderstanding questions, deliberate misreporting)\nNonresponse bias (when certain groups are less likely to respond)\nData processing errors (e.g., coding mistakes, data entry errors)\nCoverage errors (when the sampling frame doesn’t accurately represent the population)\n\n\n\nImportant clarification: Non-sampling errors can indeed be either systematic or random. This is a crucial distinction that should have been included in the original description. Non-sampling errors encompass a wide range of potential errors that are not directly related to the sampling process. Some of these can be systematic (e.g., a miscalibrated measuring instrument), while others can be random (e.g., occasional mistakes in data entry).\nThe distinction between sampling and non-sampling errors is independent of the division between systematic and random errors. In practice, non-sampling errors can fall into both of these categories, which makes their identification and control a particularly important aspect of statistical research.\nUnderstanding these types of errors is crucial for designing robust statistical studies, interpreting results accurately, and making valid inferences about populations based on sample data.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "chapter3.html#review-questions-and-exercises",
    "href": "chapter3.html#review-questions-and-exercises",
    "title": "5  From Sample to Population - Understanding Randomness, Sampling, and Inference",
    "section": "5.11 Review Questions and Exercises",
    "text": "5.11 Review Questions and Exercises\n(…)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>From Sample to Population - Understanding Randomness, Sampling, and Inference</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html",
    "href": "rozdzial3.html",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "",
    "text": "6.1 Wprowadzenie do Losowości\nLosowość jest fundamentalnym pojęciem w statystyce i badaniach naukowych. Odnosi się do nieprzewidywalności indywidualnych wyników, nawet gdy ogólny wzorzec może być przewidywalny. W naukach społecznych zrozumienie losowości jest kluczowe dla projektowania badań, zbierania danych i interpretacji wyników.\nRozważmy rzut uczciwą monetą. Chociaż wiemy, że prawdopodobieństwo wypadnięcia orła wynosi 50%, nie możemy z pewnością przewidzieć wyniku pojedynczego rzutu. Ta nieprzewidywalność jest istotą losowości.\nPrzykłady losowych zjawisk w naukach społecznych obejmują:\nZrozumienie losowości pomaga badaczom odróżnić rzeczywiste efekty od przypadkowych zdarzeń. Na przykład, jeśli zaobserwujemy niewielką różnicę w wynikach testów między dwiema grupami, losowość pomaga nam określić, czy ta różnica jest prawdopodobnie spowodowana rzeczywistym efektem, czy tylko przypadkową zmiennością.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wprowadzenie-do-losowości",
    "href": "rozdzial3.html#wprowadzenie-do-losowości",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "",
    "text": "Wybór uczestników: W eksperymencie psychologicznym badającym czasy reakcji, kolejność, w jakiej uczestnicy przybywają do laboratorium, może być losowa.\nZachowania ekonomiczne: Codzienne wahania cen akcji często wykazują losowe wzorce, na które wpływa niezliczona ilość nieprzewidywalnych czynników.\nInterakcje społeczne: Występowanie przypadkowych spotkań między osobami w społeczności można uznać za zdarzenia losowe.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#próbkowanie-łączenie-próby-i-populacji",
    "href": "rozdzial3.html#próbkowanie-łączenie-próby-i-populacji",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.2 Próbkowanie: Łączenie Próby i Populacji",
    "text": "6.2 Próbkowanie: Łączenie Próby i Populacji\nPróbkowanie to proces wybierania podzbioru (próby) z większej grupy (populacji) w celu wyciągnięcia wniosków o populacji. Jest to kluczowa umiejętność w badaniach nauk społecznych, ponieważ badanie całych populacji jest często niepraktyczne, zbyt kosztowne lub czasami niemożliwe.\nKluczowe pojęcia:\n\nPopulacja: Cała grupa, o której chcemy wyciągnąć wnioski.\nPróba: Podzbiór populacji, który faktycznie badamy.\nOperat losowania: Lista lub procedura używana do identyfikacji wszystkich członków populacji.\n\nPrzykład: Załóżmy, że chcemy zbadać satysfakcję z pracy wszystkich nauczycieli w Polsce (populacja). Zamiast ankietować setki tysięcy nauczycieli, możemy wybrać próbę 5000 nauczycieli z różnych województw, powiatów i poziomów nauczania.\nLosowość w próbkowaniu pomaga zapewnić, że próba jest reprezentatywna dla populacji, zmniejszając błędy systematyczne i umożliwiając dokładniejsze wnioskowanie. Dlatego metody próbkowania probabilistycznego, które omówimy dalej, są często preferowane w badaniach naukowych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#metody-próbkowania",
    "href": "rozdzial3.html#metody-próbkowania",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.3 Metody Próbkowania",
    "text": "6.3 Metody Próbkowania\n\n6.3.1 Próbkowanie Probabilistyczne\nMetody próbkowania probabilistycznego obejmują losowy wybór, dając każdemu członkowi populacji znaną, niezerową szansę na wybór.\n\nProsty Dobór Losowy: Każdy członek populacji ma równą szansę na wybór.\nPrzykład: Aby wybrać 100 studentów z uniwersytetu liczącego 10 000 studentów, można przypisać każdemu studentowi numer od 1 do 10 000, a następnie użyć generatora liczb losowych do wybrania 100 numerów.\nDobór Losowy Warstwowy: Populacja jest podzielona na podgrupy (warstwy) na podstawie wspólnych cech, a następnie próbki są losowo wybierane z każdej warstwy.\nPrzykład: W ogólnopolskim badaniu politycznym można podzielić populację na warstwy na podstawie regionów geograficznych (np. Polska Zachodnia, Centralna, Wschodnia) i losowo pobierać próbki z każdego regionu. Zapewnia to reprezentację ze wszystkich obszarów kraju.\nDobór Losowy Grupowy: Populacja jest podzielona na skupiska (zwykle geograficzne), niektóre skupiska są losowo wybierane, a wszyscy członkowie w tych skupiskach są badani.\nPrzykład: Aby zbadać nawyki uczenia się uczniów szkół średnich, można losowo wybrać 20 szkół z całego kraju, a następnie przeprowadzić ankietę wśród wszystkich uczniów w tych szkołach.\nDobór Systematyczny: Wybieranie co k-tego elementu z listy po losowym starcie.\nPrzykład: W ruchliwym centrum handlowym można ankietować co 20. osobę wchodzącą do centrum, zaczynając od losowo wybranej liczby między 1 a 20.\n\n\n\n6.3.2 Próbkowanie Nieprobabilistyczne\nPróbkowanie nieprobabilistyczne nie obejmuje losowego wyboru. Chociaż może wprowadzać błędy systematyczne, może być konieczne w niektórych sytuacjach, zwłaszcza w przypadku trudno dostępnych populacji lub gdy zasoby są ograniczone.\n\nDobór Wygodny: Wybieranie łatwo dostępnych podmiotów.\nPrzykład: Badacz studiujący wzorce snu studentów może przeprowadzić ankietę wśród studentów na własnych zajęciach lub na terenie kampusu.\nDobór Celowy: Wybieranie podmiotów na podstawie określonych cech.\nPrzykład: W badaniu doświadczeń prezesów w branży technologicznej badacz może celowo szukać i przeprowadzać wywiady z prezesami różnych firm technologicznych.\nDobór Metodą Kuli Śnieżnej: Uczestnicy rekrutują innych uczestników.\nPrzykład: W badaniu dostępu imigrantów bez dokumentów do opieki zdrowotnej, badacze mogą poprosić początkowych uczestników o polecenie innych potencjalnych uczestników z ich społeczności.\nDobór Kwotowy: Wybieranie uczestników w celu spełnienia określonych kwot dla pewnych cech.\nPrzykład: W badaniu rynku badacze mogą zapewnić, że przeprowadzają wywiady z określoną liczbą osób z różnych grup wiekowych, płci i poziomów dochodów, aby dopasować się do demografii rynku docelowego.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wnioskowanie-z-prób",
    "href": "rozdzial3.html#wnioskowanie-z-prób",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.4 Wnioskowanie z Prób",
    "text": "6.4 Wnioskowanie z Prób\nWnioskowanie statystyczne to proces wyciągania wniosków o populacji na podstawie próby. Pozwala to badaczom oszacować charakterystyki całej populacji (parametry) przy użyciu charakterystyk próby (statystyk).\n\n\n\n\n\n\nNote\n\n\n\nThe Soup Analogy: A Taste of Statistics\n\n\nWhen you taste a spoonful of soup and decide it isn’t salty enough, that’s exploratory/descriptive analysis.\nIf you generalize and conclude that your entire pot of soup needs salt, that’s an inference.\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population).\nIf the soup is not well stirred (heterogeneous population), it doesn’t matter how large a spoon you have (sample size), it will still not accurately represent the whole. If the soup is well stirred (homogeneous population), even a small spoon will suffice to test the soup.\n\n\n\nKluczowe pojęcia:\n\nEstymatory punktowe: Pojedyncza wartość używana do oszacowania parametru populacji.\nPrzykład: Średni dochód z próby 1000 pracowników może być użyty do oszacowania średniego dochodu wszystkich pracowników w kraju.\nPrzedziały ufności: Zakres wartości, który prawdopodobnie zawiera prawdziwy parametr populacji.\nPrzykład: Możemy powiedzieć: “Jesteśmy w 95% pewni, że prawdziwy średni dochód populacji mieści się między 4500 a 5500 złotych”.\nMargines błędu: Zakres wartości powyżej i poniżej statystyki z próby w przedziale ufności.\nPrzykład: W sondażach politycznych można zobaczyć stwierdzenie: “Kandydat A jest preferowany przez 52% wyborców, z marginesem błędu ±3%”.\nTestowanie hipotez: Metoda podejmowania decyzji o parametrach populacji na podstawie danych z próby.\nPrzykład: Badacz może testować, czy istnieje istotna różnica w wynikach testów między uczniami, którzy uczą się przy muzyce, a tymi, którzy uczą się w ciszy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#błędy-próbkowania-i-błędy-niepróbkowe",
    "href": "rozdzial3.html#błędy-próbkowania-i-błędy-niepróbkowe",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.5 Błędy Próbkowania i Błędy Niepróbkowe",
    "text": "6.5 Błędy Próbkowania i Błędy Niepróbkowe\nZrozumienie potencjalnych błędów w badaniach jest kluczowe dla dokładnej interpretacji wyników.\nBłąd próbkowania: Różnica między statystyką z próby a prawdziwym parametrem populacji, występująca z powodu przypadkowych wahań w wyborze członków próby.\nPrzykład: Jeśli oszacujemy średni wzrost wszystkich dorosłych mężczyzn w kraju na podstawie próby, nasze oszacowanie prawdopodobnie będzie się nieco różnić od prawdziwej średniej z powodu błędu próbkowania.\nBłędy niepróbkowe: Błędy nie wynikające z przypadku, które mogą wystąpić zarówno w badaniach próbkowych, jak i spisach.\n\nBłąd pokrycia: Gdy operat losowania nie reprezentuje dokładnie populacji.\nPrzykład: Badanie telefoniczne, które dzwoni tylko na telefony stacjonarne, pominęłoby osoby posiadające tylko telefony komórkowe, potencjalnie wypaczając wyniki.\nBłąd braku odpowiedzi: Gdy wybrani uczestnicy nie odpowiadają, potencjalnie wprowadzając błąd systematyczny.\nPrzykład: W badaniu satysfakcji z pracy, bardzo zadowoleni lub bardzo niezadowoleni pracownicy mogą być bardziej skłonni do odpowiedzi, wypaczając wyniki.\nBłąd pomiaru: Niedokładności w zebranych danych.\nPrzykład: Źle sformułowane pytanie ankietowe może być różnie interpretowane przez różnych respondentów, prowadząc do niespójnych danych.\nBłąd przetwarzania: Błędy popełnione podczas wprowadzania danych, kodowania lub analizy.\nPrzykład: Przypadkowe wprowadzenie “99” zamiast “9” dla odpowiedzi uczestnika mogłoby znacząco wypaczyć wyniki.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#wielkość-próby-i-moc-statystyczna",
    "href": "rozdzial3.html#wielkość-próby-i-moc-statystyczna",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.6 Wielkość Próby i Moc Statystyczna",
    "text": "6.6 Wielkość Próby i Moc Statystyczna\nOkreślenie odpowiedniej wielkości próby wymaga zrównoważenia potrzeby precyzji z dostępnymi zasobami.\nRozważania dotyczące wielkości próby: - Większe próby generalnie zapewniają bardziej precyzyjne oszacowania, ale są bardziej kosztowne i czasochłonne do uzyskania. - Wymagana wielkość próby zależy od czynników takich jak pożądany poziom precyzji, zmienność w populacji i rodzaj planowanej analizy.\nPrzykład: Aby oszacować proporcję wyborców popierających konkretną politykę z marginesem błędu ±3% na poziomie ufności 95%, potrzebna byłaby próba około 1067 wyborców (zakładając maksymalną zmienność).\nMoc statystyczna: Prawdopodobieństwo, że badanie wykryje efekt, gdy taki efekt istnieje.\nCzynniki wpływające na moc: 1. Wielkość próby 2. Wielkość efektu (wielkość różnicy lub związku, który próbujemy wykryć) 3. Wybrany poziom istotności (zwykle 0,05)\nPrzykład: W badaniu porównującym dwie metody nauczania, większa wielkość próby zwiększyłaby prawdopodobieństwo wykrycia istotnej różnicy między metodami, jeśli taka różnica istnieje.\n\n\n\n\n\n\nNote\n\n\n\nCzym jest Moc Badania?\nMoc badania dotyczy tego, jak prawdopodobne jest, że znajdziemy coś, jeśli to naprawdę istnieje. To jak posiadanie dobrej latarki, gdy szukasz czegoś w ciemności - im lepsza latarka, tym bardziej prawdopodobne, że znajdziesz to, czego szukasz.\n\nWielkość Efektu: Jak duża jest rzecz (efekt, różnica, itp.), której szukamy.\nWielkość Próby: Ile osób lub rzeczy badamy w naszym studium.\nMoc Badania: Jak prawdopodobne jest, że znajdziemy efekt, jeśli naprawdę istnieje.\n\nZwiązek Między Wielkością Efektu a Wielkością Próby:\nWyobraź sobie, że próbujesz znaleźć monety ukryte w piasku:\n\nDuże Efekty (Duże Monety):\n\nJeśli szukasz dużych monet (jak 5 złotych), nie musisz przeszukiwać tak dużo piasku, aby je znaleźć.\nW badaniach, jeśli efekt jest duży, możesz użyć mniejszej próby.\n\nPrzykład: Testowanie, czy nowa metoda nauki poprawia wyniki testów o 20 punktów na 100.\n\nMoże wystarczyć przetestować 30 uczniów, aby zobaczyć tę dużą różnicę.\n\nMałe Efekty (Małe Monety):\n\nJeśli szukasz maleńkich monet (jak 1 grosz), będziesz musiał przeszukać więcej piasku.\nW badaniach, jeśli efekt jest mały, potrzebujesz większej próby.\n\nPrzykład: Sprawdzanie, czy korzystanie z mediów społecznościowych wpływa na szczęście o niewielką ilość.\n\nMoże być potrzeba zbadania 500 lub więcej osób, aby wykryć ten mały efekt.\n\n\nDlaczego Moc Badania Jest Ważna:\n\nNie Przegapienie Rzeczywistych Efektów:\n\nPrzy niskiej mocy możesz przeoczyć rzeczywiste efekty, jak używanie słabej latarki i przeoczenie czegoś, co faktycznie tam jest.\n\nPewność Wyników:\n\nWyższa moc daje większą pewność, że to, co znalazłeś, jest prawdziwe, a nie tylko przypadkiem.\n\n\nPrzykład:\nZałóżmy, że chcemy zbadać, czy nowa metoda nauczania pomaga uczniom lepiej się uczyć:\n\nMałe Badanie (Niska Moc):\n\nPróbujemy metody z zaledwie 10 uczniami.\nNawet jeśli metoda działa, przy tak małej grupie trudno stwierdzić, czy poprawa wynika z nowej metody, czy to tylko przypadek.\n\nWiększe Badanie (Wyższa Moc):\n\nStosujemy metodę ze 100 uczniami.\nTeraz jest bardziej prawdopodobne, że zobaczymy, czy metoda naprawdę pomaga, ponieważ mamy więcej danych do analizy.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#próbkowanie-w-erze-cyfrowej",
    "href": "rozdzial3.html#próbkowanie-w-erze-cyfrowej",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.7 Próbkowanie w Erze Cyfrowej",
    "text": "6.7 Próbkowanie w Erze Cyfrowej\nPojawienie się big data i technologii cyfrowych zmieniło praktyki próbkowania w wielu dziedzinach.\nMożliwości i wyzwania Big Data: - Bezprecedensowe ilości dostępnych informacji - Potencjalny brak reprezentatywności - Problemy z jakością danych - Kwestie prywatności i etyki\nPrzykład: Dane z mediów społecznościowych mogą dostarczyć wglądu w opinię publiczną w czasie rzeczywistym, ale użytkownicy konkretnej platformy mogą nie być reprezentatywni dla ogólnej populacji.\nBadania internetowe: - Oferują nowe możliwości zbierania danych - Stają przed wyzwaniami takimi jak błąd pokrycia (nie każdy ma dostęp do internetu) i błąd samoselekcji\nPrzykład: Ankieta online na temat nawyków korzystania z internetu z natury wykluczałaby osoby bez dostępu do internetu, potencjalnie wypaczając wyniki.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#etyczne-aspekty-próbkowania",
    "href": "rozdzial3.html#etyczne-aspekty-próbkowania",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.8 Etyczne Aspekty Próbkowania",
    "text": "6.8 Etyczne Aspekty Próbkowania\nEtyczne praktyki próbkowania są kluczowe w badaniach nauk społecznych:\n\nŚwiadoma zgoda: Uczestnicy powinni rozumieć cel badania i zgodzić się na udział.\nPrzykład: Przed przeprowadzeniem wywiadów na temat wrażliwych tematów, takich jak zdrowie psychiczne, badacze muszą jasno wyjaśnić cele badania i potencjalne ryzyko uczestnikom.\nPrywatność i poufność: Badacze muszą chronić dane osobowe uczestników.\nPrzykład: W badaniu dotyczącym mobbingu w miejscu pracy, badacze mogą używać kodów numerycznych zamiast nazwisk, aby chronić tożsamość uczestników.\nReprezentatywność i inkluzywność: Próby powinny sprawiedliwie reprezentować zróżnicowane populacje, w tym grupy marginalizowane.\n\nPrzykład: Badanie dotyczące mieszkalnictwa miejskiego powinno dołożyć starań, aby uwzględnić uczestników z różnych środowisk społeczno-ekonomicznych, grup etnicznych i sytuacji mieszkaniowych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#podsumowanie",
    "href": "rozdzial3.html#podsumowanie",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.9 Podsumowanie",
    "text": "6.9 Podsumowanie\nPróbkowanie pozostaje fundamentem badań w naukach społecznych, nawet w erze big data. Zrozumienie zasad próbkowania pomaga badaczom projektować badania, interpretować wyniki i wyciągać trafne wnioski o populacjach. Jak widzieliśmy, droga od próby do populacji wymaga starannego rozważenia metod próbkowania, potencjalnych błędów, kwestii etycznych i stale ewoluującego krajobrazu gromadzenia danych w erze cyfrowej.\nDziękuję za to bardzo trafne pytanie. Ma Pan/Pani rację, i doceniam tę uwagę. Rzeczywiście, tłumaczenie “błąd samplowy vs. niesamplowy” jest bardziej precyzyjne i lepiej oddaje istotę tych pojęć w polskiej terminologii statystycznej. Pozwolę sobie wprowadzić odpowiednie korekty i wyjaśnienia:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "rozdzial3.html#błędy-statystyczne---podsumowanie",
    "href": "rozdzial3.html#błędy-statystyczne---podsumowanie",
    "title": "6  Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania",
    "section": "6.10 Błędy Statystyczne - podsumowanie",
    "text": "6.10 Błędy Statystyczne - podsumowanie\n\n6.10.1 Błąd Systematyczny vs. Błąd Losowy\nBłędy systematyczne i losowe to dwa podstawowe rodzaje błędów w pomiarach i eksperymentach statystycznych.\n\nBłąd Systematyczny:\n\nDefinicja: Konsekwentne, przewidywalne odchylenia od prawdziwej wartości\nCharakterystyka:\n\nZniekształca wyniki w określonym kierunku\nPowtarzalny i często stały w różnych pomiarach\nMoże być skorygowany, jeśli zostanie zidentyfikowany\n\nPrzykłady:\n\nŹle skalibrowany przyrząd pomiarowy\nKonsekwentny błąd zaokrąglania przy wprowadzaniu danych\nStronnicza metoda pobierania próbek\n\n\nBłąd Losowy:\n\nDefinicja: Nieprzewidywalne wahania w pomiarach wynikające z przypadku\nCharakterystyka:\n\nZmienia się co do wielkości i kierunku\nPodąża za rozkładem prawdopodobieństwa (często normalnym)\nMożna go zmniejszyć zwiększając wielkość próby lub powtarzając pomiary\n\nPrzykłady:\n\nNaturalne wahania w badanym zjawisku\nMałe fluktuacje w przyrządach pomiarowych\nBłędy ludzkie przy odczytywaniu lub zapisywaniu danych\n\n\n\n\n\n6.10.2 Błędy Samplowe vs. Błędy Niesamplowe\n\nBłędy Samplowe (lub Błędy Próbkowania):\n\nDefinicja: Błędy wynikające z tego, że próba (sample) nie reprezentuje idealnie populacji\nCharakterystyka:\n\nNieodłączne w każdym badaniu opartym na próbie\nMożna je oszacować i skwantyfikować za pomocą metod statystycznych\nZmniejszają się wraz ze wzrostem wielkości próby\n\nPrzykłady:\n\nLosowe wahania w statystykach (z) próby\nNadreprezentacja lub niedoreprezentowanie niektórych grup w próbie\n\n\nBłędy Niesamplowe:\n\nDefinicja: Wszystkie błędy w badaniu, które nie są związane z próbkowaniem (samplingiem)\nCharakterystyka:\n\nMogą wystąpić zarówno w badaniach próbkowych, jak i pełnych (spisach)\nCzęsto trudniejsze do skwantyfikowania i kontrolowania niż błędy samplowe\nMogą wprowadzać stronniczość do wyników\nMogą być systematyczne lub losowe\n\nPrzykłady:\n\nBłędy odpowiedzi (np. niezrozumienie pytań, celowe błędne raportowanie)\nBłąd braku odpowiedzi (gdy niektóre grupy są mniej skłonne do odpowiedzi)\nBłędy przetwarzania danych (np. błędy kodowania, błędy wprowadzania danych)\nBłędy pokrycia (gdy operat losowania nie reprezentuje dokładnie populacji)\n\n\n\nWażne wyjaśnienie: Błędy niesamplowe mogą być zarówno systematyczne, jak i losowe. To kluczowe rozróżnienie, które powinienem był uwzględnić w pierwotnym opisie. Błędy niesamplowe obejmują szeroki zakres możliwych błędów, które nie są bezpośrednio związane z procesem próbkowania. Niektóre z nich mogą być systematyczne (np. błędnie skalibrowany instrument pomiarowy), podczas gdy inne mogą być losowe (np. przypadkowe błędy przy wprowadzaniu danych).\nRozróżnienie na błędy samplowe i niesamplowe jest niezależne od podziału na błędy systematyczne i losowe. W praktyce, błędy niesamplowe mogą należeć do obu tych kategorii, co czyni ich identyfikację i kontrolę szczególnie ważnym aspektem badań statystycznych.\n\n\n\nRetrieved from: https://scientistcafe.com/ids/vbtradeoff\n\n\nKluczowe punkty do zapamiętania:\n\nLosowość jest podstawą wielu metod próbkowania i pomaga zapewnić reprezentatywność próby.\nIstnieją różne metody próbkowania, zarówno probabilistyczne, jak i nieprobabilistyczne, każda z własnymi zaletami i ograniczeniami.\nWnioskowanie statystyczne pozwala nam wyciągać wnioski o populacji na podstawie danych z próby.\nBłędy próbkowania i niepróbkowe mogą wpływać na jakość naszych wniosków, dlatego ważne jest ich zrozumienie i minimalizowanie.\nWielkość próby i moc statystyczna są kluczowe dla zapewnienia wiarygodności wyników badań.\nEra cyfrowa przynosi nowe możliwości i wyzwania w zakresie próbkowania i gromadzenia danych.\nEtyczne aspekty próbkowania, w tym świadoma zgoda, prywatność i reprezentatywność, są nieodłączną częścią procesu badawczego.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Od Próby do Populacji - Zrozumienie Losowości, Próbkowania i Wnioskowania</span>"
    ]
  },
  {
    "objectID": "chapter3b.html",
    "href": "chapter3b.html",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "",
    "text": "7.1 Defining Reliability and Validity\nReliability refers to the consistency of a measure. A reliable measurement or study produces similar results under consistent conditions.\nValidity refers to the accuracy of a measure. A valid measurement or study accurately represents what it claims to measure.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#the-four-combinations-of-reliability-and-validity",
    "href": "chapter3b.html#the-four-combinations-of-reliability-and-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.2 The Four Combinations of Reliability and Validity",
    "text": "7.2 The Four Combinations of Reliability and Validity\nThere are four possible combinations of reliability and validity:\n\nHigh Reliability, High Validity\nHigh Reliability, Low Validity\nLow Reliability, High Validity\nLow Reliability, Low Validity\n\nLet’s explore each of these combinations with examples and visualizations.\n\n7.2.1 1. High Reliability, High Validity\nThis is the ideal scenario in research. Measurements are both consistent and accurate.\nExample: A well-calibrated digital scale used to measure weight. It consistently gives the same reading for the same object and accurately represents the true weight.\n\n\n7.2.2 2. High Reliability, Low Validity\nIn this case, measurements are consistent but not accurate.\nExample: A miscalibrated scale that always measures 5 kg too heavy. It gives consistent results (high reliability) but doesn’t represent the true weight (low validity).\n\n\n7.2.3 3. Low Reliability, High Validity\nHere, measurements are accurate on average but inconsistent.\nExample: A scale that fluctuates around the true weight. Sometimes it’s a bit over, sometimes a bit under, but on average, it’s correct.\n\n\n7.2.4 4. Low Reliability, Low Validity\nThis is the worst-case scenario, where measurements are neither consistent nor accurate.\nExample: A broken scale that gives random readings unrelated to the true weight.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#visualizing-reliability-and-validity",
    "href": "chapter3b.html#visualizing-reliability-and-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.3 Visualizing Reliability and Validity",
    "text": "7.3 Visualizing Reliability and Validity\nTo better understand these concepts, let’s create visualizations using ggplot2 in R. We’ll simulate measurement data for each scenario and plot them.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generate data for each scenario\nn &lt;- 100\ntrue_value &lt;- 50\n\ndata &lt;- tibble(\n  high_rel_high_val = rnorm(n, mean = true_value, sd = 1),\n  high_rel_low_val = rnorm(n, mean = true_value + 5, sd = 1),\n  low_rel_high_val = rnorm(n, mean = true_value, sd = 5),\n  low_rel_low_val = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenario\", values_to = \"measurement\")\n\n# Create the scatterplot\nscatter_plot &lt;- ggplot(data, aes(x = id, y = measurement, color = scenario)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = true_value, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Scatterplots of Measurements\",\n       subtitle = \"Dashed line represents the true value\",\n       x = \"Measurement ID\",\n       y = \"Measured Value\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Create the histogram\nhist_plot &lt;- ggplot(data, aes(x = measurement, fill = scenario)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = true_value, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenario, ncol = 2, scales = \"free\",\n             labeller = labeller(scenario = c(\n               \"high_rel_high_val\" = \"High Reliability, High Validity\",\n               \"high_rel_low_val\" = \"High Reliability, Low Validity\",\n               \"low_rel_high_val\" = \"Low Reliability, High Validity\",\n               \"low_rel_low_val\" = \"Low Reliability, Low Validity\"\n             ))) +\n  labs(title = \"Histograms of Measurements\",\n       subtitle = \"Red dashed line represents the true value\",\n       x = \"Measured Value\",\n       y = \"Count\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Combine the plots\ncombined_plot &lt;- scatter_plot / hist_plot +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Reliability and Validity in Measurements\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Display the combined plot\ncombined_plot\n\n\n\n\n\n\n\n\n\n7.3.1 Interpreting the Visualizations\n\nHigh Reliability, High Validity: Points cluster tightly around the true value (dashed line).\nHigh Reliability, Low Validity: Points cluster tightly, but consistently above the true value.\nLow Reliability, High Validity: Points scatter widely but center around the true value.\nLow Reliability, Low Validity: Points scatter randomly with no clear pattern or relation to the true value.\n\nUnderstanding reliability and validity is crucial in data science and research. High reliability ensures consistent measurements, while high validity ensures accurate representations of what we intend to measure. By considering both aspects, researchers can design more robust studies and draw more meaningful conclusions from their data.\nWhen conducting your own research or analyzing others’ work, always consider: - How reliable are the measurements? - How valid is the approach for measuring the intended concept? - Do the methods used support both reliability and validity?\nBy keeping these questions in mind, you’ll be better equipped to produce and interpret high-quality research in data science.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#types-of-reliability",
    "href": "chapter3b.html#types-of-reliability",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.4 Types of Reliability",
    "text": "7.4 Types of Reliability\nReliability can be assessed in several ways, each focusing on a different aspect of consistency:\n\nTest-Retest Reliability: This measures the consistency of a test over time. It involves administering the same test to the same group of participants at different times and comparing the results.\nInter-Rater Reliability: This assesses the degree of agreement among different raters or observers. It’s crucial when subjective judgments are involved in data collection.\nInternal Consistency: This evaluates how well different items on a test or scale measure the same construct. Cronbach’s alpha is a common measure of internal consistency.\nParallel Forms Reliability: This involves creating two equivalent forms of a test and administering them to the same group. The correlation between the two sets of scores indicates reliability.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#types-of-validity",
    "href": "chapter3b.html#types-of-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.5 Types of Validity",
    "text": "7.5 Types of Validity\nValidity is a multifaceted concept, with several types that researchers need to consider:\n\nContent Validity: This ensures that a measure covers all aspects of the construct it aims to measure. It’s often assessed by expert judgment.\nConstruct Validity: This evaluates whether a test measures the intended theoretical construct. It includes:\n\nConvergent Validity: The degree to which the measure correlates with other measures of the same construct.\nDiscriminant Validity: The extent to which the measure does not correlate with measures of different constructs.\n\nCriterion Validity: This assesses how well a measure predicts an outcome. It includes:\n\nConcurrent Validity: How well the measure correlates with other measures of the same construct at the same time.\nPredictive Validity: How well the measure predicts future outcomes.\n\nFace Validity: Face validity describes how test subjects perceive the test and whether - from their point of view - it is adequate for the purpose it is supposed to serve. A lack of face validity, even though the test may be valid from the perspective of a specific purpose, can contribute to a decrease in motivation among test subjects, which directly affects the results achieved or may lead to rejection of the test. While not a scientific measure, it can be important for participant buy-in.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#internal-vs.-external-validity",
    "href": "chapter3b.html#internal-vs.-external-validity",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.6 Internal vs. External Validity",
    "text": "7.6 Internal vs. External Validity\nThese concepts are crucial in experimental design and the generalizability of research findings:\n\n7.6.1 Internal Validity\nInternal validity refers to the extent to which a study establishes a causal relationship between the independent and dependent variables. It answers the question: “Did the experimental treatment actually cause the observed effects?”\nFactors that can threaten internal validity include: - History: External events occurring between pre-test and post-test - Maturation: Natural changes in participants over time - Testing effects: Changes due to taking a pre-test - Instrumentation: Changes in the measurement tool or observers - Selection bias: Non-random assignment to groups - Attrition: Loss of participants during the study\n\n\n7.6.2 External Validity\nExternal validity refers to the extent to which the results of a study can be generalized to other situations, populations, or settings. It addresses the question: “To what extent can the findings be applied beyond the specific context of the study?”\nFactors that can affect external validity include: - Population validity: How well the sample represents the larger population - Ecological validity: How well the study setting represents real-world conditions - Temporal validity: Whether the results hold true across time",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#consistency-in-research",
    "href": "chapter3b.html#consistency-in-research",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.7 Consistency in Research",
    "text": "7.7 Consistency in Research\nConsistency is closely related to reliability but extends beyond just measurement. In research, consistency refers to the overall coherence and stability of results across different contexts, methods, or studies.\nKey aspects of consistency in research include:\n\nReplicability: The ability to reproduce study results using the same methods and data.\nRobustness: The stability of findings across different analytical approaches or slight variations in methodology.\nConvergence: The alignment of results from different studies or methods investigating the same phenomenon.\nLongitudinal Consistency: The stability of findings over time, especially important in longitudinal studies.\n\nEnsuring consistency in research involves: - Using standardized procedures and measures - Thoroughly documenting methods and analytical decisions - Conducting replication studies - Meta-analyses to synthesize findings across multiple studies",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#balancing-reliability-validity-and-consistency",
    "href": "chapter3b.html#balancing-reliability-validity-and-consistency",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.8 Balancing Reliability, Validity, and Consistency",
    "text": "7.8 Balancing Reliability, Validity, and Consistency\nWhile reliability, validity, and consistency are all crucial for high-quality research, they sometimes involve trade-offs:\n\nA highly reliable measure might lack validity if it consistently measures the wrong thing.\nStriving for perfect internal validity (e.g., in tightly controlled lab experiments) might reduce external validity.\nEnsuring high consistency across diverse contexts might require sacrificing some degree of precision or depth in specific situations.\n\nResearchers must carefully balance these aspects based on their research questions and the nature of their study. A comprehensive understanding of reliability, validity, and consistency helps in designing robust studies, interpreting results accurately, and contributing meaningfully to the body of scientific knowledge.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#bias-variance-tradeoff",
    "href": "chapter3b.html#bias-variance-tradeoff",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.9 Bias-Variance Tradeoff",
    "text": "7.9 Bias-Variance Tradeoff\nThe concepts of reliability and validity are closely related to the statistical notion of the bias-variance tradeoff. This tradeoff is fundamental in machine learning and statistical modeling.\n\nBias refers to the error introduced by approximating a real-world problem with a simplified model. High bias can lead to underfitting.\nVariance refers to the error introduced by the model’s sensitivity to small fluctuations in the training set. High variance can lead to overfitting.\n\nLet’s visualize this concept with a simplified plot:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_true &lt;- sin(x)\ny_low_bias_high_var &lt;- y_true + rnorm(100, 0, 0.3)\ny_high_bias_low_var &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_true, y_low_bias_high_var, y_high_bias_low_var),\n                 type = rep(c(\"True Function\", \"Low Bias, High Variance\", \"High Bias, Low Variance\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = type)) +\n  geom_line() +\n  geom_point(data = subset(df, type != \"True Function\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Bias-Variance Tradeoff\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Model Type\") +\n  theme_minimal()\n\n\n\n\nVisualization of Bias-Variance Tradeoff\n\n\n\n\nIn this plot: - The black line represents the true underlying function. - The blue points represent a model with low bias but high variance. It follows the true function closely on average but has a lot of noise. - The red line represents a model with high bias but low variance. It consistently underestimates the true function but has less noise.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#accuracy-and-precision",
    "href": "chapter3b.html#accuracy-and-precision",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.10 Accuracy and Precision",
    "text": "7.10 Accuracy and Precision\nThe concepts of accuracy and precision are closely related to validity and reliability:\n\nAccuracy refers to how close a measurement is to the true value (similar to validity).\nPrecision refers to how consistent or reproducible the measurements are (similar to reliability).\n\nWe can visualize these concepts using a simplified target analogy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"High Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Low Accuracy\\nHigh Precision\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"High Accuracy\\nLow Precision\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Low Accuracy\\nLow Precision\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Accuracy vs Precision\")\n\n\n\n\nVisualization of Accuracy vs Precision\n\n\n\n\nIn this visualization: - High accuracy means the points are close to the center (bullseye). - High precision means the points are tightly clustered. - Each panel represents a different combination of accuracy and precision.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#conclusion",
    "href": "chapter3b.html#conclusion",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.11 Conclusion",
    "text": "7.11 Conclusion\nUnderstanding reliability and validity is crucial for conducting robust research. These concepts help us ensure that our measurements are both consistent and accurate. By relating them to ideas like the bias-variance tradeoff and accuracy-precision, we gain a deeper appreciation of the challenges involved in measurement and modeling in scientific research. As researchers, we must strive to develop measures and models that are both reliable and valid, balancing the tradeoffs between bias and variance, and between accuracy and precision. This requires careful design of research methodologies, rigorous testing of our measurement instruments, and thoughtful interpretation of our results.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#understanding-bias-vs.-variance-in-statistical-measurement",
    "href": "chapter3b.html#understanding-bias-vs.-variance-in-statistical-measurement",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.12 Understanding Bias vs. Variance in Statistical Measurement",
    "text": "7.12 Understanding Bias vs. Variance in Statistical Measurement\n\n7.12.1 Introduction\nIn statistics and machine learning, two important concepts that affect the performance of our models are bias and variance. Understanding these concepts is crucial for building effective predictive models and avoiding common pitfalls like overfitting and underfitting.\n\nBias refers to the error introduced by approximating a real-world problem, which may be complex, by a simplified model. High bias can lead to underfitting.\n\nThink of bias as how far off our predictions are from the true values on average.\nIn terms of validity, high bias means our model isn’t capturing the true relationship in the data.\n\nVariance refers to the amount by which our model would change if we estimated it using a different training dataset. High variance can lead to overfitting.\n\nThink of variance as how much our predictions would fluctuate if we used different datasets.\nIn terms of reliability, high variance means our model is too sensitive to the specific data it was trained on.\n\n\nWe’ll explore four scenarios to illustrate different combinations of bias and variance using synthetic data and regression models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "chapter3b.html#data-generation-and-model-fitting-function",
    "href": "chapter3b.html#data-generation-and-model-fitting-function",
    "title": "7  Reliability and Validity in Data Science Research",
    "section": "7.13 Data Generation and Model Fitting Function",
    "text": "7.13 Data Generation and Model Fitting Function\nFirst, let’s create a function that will help us generate data and fit models for each scenario:\n\ngenerate_and_fit &lt;- function(n, intercept, slope, noise_sd, model_degree) {\n  # Generate synthetic data\n  x &lt;- runif(n, 0, 10)\n  y &lt;- intercept + slope * x + rnorm(n, 0, noise_sd)\n  \n  # Fit model\n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", model_degree, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  # Generate predictions\n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  # Plot\n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    geom_abline(intercept = intercept, slope = slope, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Model Degree:\", model_degree),\n         subtitle = paste(\"Noise SD:\", noise_sd),\n         x = \"X (Input Variable)\",\n         y = \"Y (Target Variable)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nThis function does the following: 1. Generates synthetic data based on our parameters 2. Fits a polynomial regression model 3. Creates a plot showing the true relationship (blue dashed line), our model’s predictions (red solid line), and the data points\nNow, let’s explore our four scenarios!\n\n7.13.1 Scenario 1: Low Bias, Low Variance\nIn this ideal scenario, we use a linear model to fit linear data with low noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 1, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The model (red line) closely follows the true relationship (blue dashed line). - Data points are clustered tightly around the line, indicating low noise. - This scenario represents a good fit: the model captures the underlying trend without being overly complex.\n\n\n7.13.2 Scenario 2: Low Bias, High Variance\nHere, we use a linear model to fit linear data, but with high noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 5, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The model still captures the general trend, but data points are more scattered. - This high variance means our model’s predictions would be less reliable. - In real-world terms, this might represent a situation where our measurements are correct on average but have a lot of random error.\n\n\n7.13.3 Scenario 3: High Bias, Low Variance\nIn this case, we use a linear model to fit quadratic (curved) data with low noise.\n\nquadratic_data &lt;- function(n, intercept, slope, noise_sd, model_degree) {\n  x &lt;- runif(n, 0, 10)\n  y &lt;- intercept + slope * x^2 + rnorm(n, 0, noise_sd)\n  \n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", model_degree, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    stat_function(fun = function(x) intercept + slope * x^2, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Model Degree:\", model_degree),\n         subtitle = paste(\"Noise SD:\", noise_sd),\n         x = \"X (Input Variable)\",\n         y = \"Y (Target Variable)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nquadratic_data(n = 100, intercept = 1, slope = 0.2, noise_sd = 1, model_degree = 1)\n\n\n\n\n\n\n\n\nExplanation: - The linear model (red line) fails to capture the curvature of the true relationship (blue dashed line). - This high bias means our model is consistently off in its predictions. - In real-world terms, this might represent using an overly simplistic model for a complex phenomenon.\n\n\n7.13.4 Scenario 4: High Bias, High Variance\nFinally, we use a high-degree polynomial to fit linear data with high noise.\n\ngenerate_and_fit(n = 100, intercept = 1, slope = 2, noise_sd = 5, model_degree = 5)\n\n\n\n\n\n\n\n\nExplanation: - The model (red line) is overly complex, trying to fit the noise rather than the underlying trend. - This combination of high bias and high variance leads to poor generalization. - In real-world terms, this might represent overcomplicating our analysis and drawing false conclusions from random fluctuations in our data.\n\n\n7.13.5 Conclusion\nUnderstanding the bias-variance trade-off is crucial in statistical modeling:\n\nLow Bias, Low Variance: The ideal scenario, where our model accurately captures the underlying relationship without being overly sensitive to noise.\nLow Bias, High Variance: Our model is correct on average but unreliable due to high sensitivity to individual data points.\nHigh Bias, Low Variance: Our model is consistently wrong due to oversimplification but gives stable predictions.\nHigh Bias, High Variance: The worst-case scenario, where our model is both inaccurate and unreliable.\n\nIn practice, we often need to balance bias and variance. Techniques like cross-validation, regularization, and ensemble methods can help find this balance.\nRemember: - A model with high bias is too simple and misses important patterns in the data. - A model with high variance is too complex and fits noise in the training data. - The goal is to find a sweet spot that captures true patterns without overfitting to noise.\nBy understanding these concepts, you’ll be better equipped to choose appropriate models, avoid overfitting and underfitting, and build more effective predictive models in your future statistical analyses!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Reliability and Validity in Data Science Research</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html",
    "href": "rozdzial3b.html",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "",
    "text": "8.1 Definiowanie Rzetelności i Trafności\nRzetelność odnosi się do spójności pomiaru. Rzetelny pomiar lub badanie daje podobne wyniki w spójnych warunkach.\nTrafność odnosi się do dokładności pomiaru. Trafny pomiar lub badanie dokładnie reprezentuje to, co twierdzi, że mierzy.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#cztery-kombinacje-rzetelności-i-trafności",
    "href": "rozdzial3b.html#cztery-kombinacje-rzetelności-i-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.2 Cztery Kombinacje Rzetelności i Trafności",
    "text": "8.2 Cztery Kombinacje Rzetelności i Trafności\nIstnieją cztery możliwe kombinacje rzetelności i trafności:\n\nWysoka Rzetelność, Wysoka Trafność\nWysoka Rzetelność, Niska Trafność\nNiska Rzetelność, Wysoka Trafność\nNiska Rzetelność, Niska Trafność\n\nPrzyjrzyjmy się każdej z tych kombinacji z przykładami i wizualizacjami.\n\n8.2.1 1. Wysoka Rzetelność, Wysoka Trafność\nTo idealny scenariusz w badaniach. Pomiary są zarówno spójne, jak i dokładne.\nPrzykład: Dobrze skalibrowana waga cyfrowa używana do pomiaru wagi. Konsekwentnie daje ten sam odczyt dla tego samego obiektu i dokładnie reprezentuje prawdziwą wagę.\n\n\n8.2.2 2. Wysoka Rzetelność, Niska Trafność\nW tym przypadku pomiary są spójne, ale niedokładne.\nPrzykład: Źle skalibrowana waga, która zawsze mierzy 5 kg za ciężko. Daje spójne wyniki (wysoka rzetelność), ale nie reprezentuje prawdziwej wagi (niska trafność).\n\n\n8.2.3 3. Niska Rzetelność, Wysoka Trafność\nTutaj pomiary są dokładne średnio, ale niespójne.\nPrzykład: Waga, która waha się wokół prawdziwej wagi. Czasami pokazuje trochę więcej, czasami trochę mniej, ale średnio jest poprawna.\n\n\n8.2.4 4. Niska Rzetelność, Niska Trafność\nTo najgorszy scenariusz, gdzie pomiary nie są ani spójne, ani dokładne.\nPrzykład: Zepsuta waga, która daje losowe odczyty niezwiązane z prawdziwą wagą.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#wizualizacja-rzetelności-i-trafności",
    "href": "rozdzial3b.html#wizualizacja-rzetelności-i-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.3 Wizualizacja Rzetelności i Trafności",
    "text": "8.3 Wizualizacja Rzetelności i Trafności\nAby lepiej zrozumieć te pojęcia, stwórzmy wizualizacje przy użyciu ggplot2 w R. Zasymulujemy dane pomiarowe dla każdego scenariusza i narysujemy je.\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(patchwork)\n\nset.seed(123)\n\n# Generowanie danych dla każdego scenariusza\nn &lt;- 100\nprawdziwa_wartosc &lt;- 50\n\ndane &lt;- tibble(\n  wysoka_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 1),\n  wysoka_rz_niska_tr = rnorm(n, mean = prawdziwa_wartosc + 5, sd = 1),\n  niska_rz_wysoka_tr = rnorm(n, mean = prawdziwa_wartosc, sd = 5),\n  niska_rz_niska_tr = runif(n, min = 0, max = 100)\n) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  pivot_longer(cols = -id, names_to = \"scenariusz\", values_to = \"pomiar\")\n\n# Tworzenie wykresu punktowego\nwykres_punktowy &lt;- ggplot(dane, aes(x = id, y = pomiar, color = scenariusz)) +\n  geom_point(alpha = 0.6, size = 2) +\n  geom_hline(yintercept = prawdziwa_wartosc, linetype = \"dashed\", color = \"black\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free_y\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelność, Wysoka Trafność\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelność, Niska Trafność\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelność, Wysoka Trafność\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelność, Niska Trafność\"\n             ))) +\n  labs(title = \"Wykresy punktowe pomiarów\",\n       subtitle = \"Przerywana linia reprezentuje prawdziwą wartość\",\n       x = \"ID pomiaru\",\n       y = \"Zmierzona wartość\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Tworzenie histogramu\nwykres_hist &lt;- ggplot(dane, aes(x = pomiar, fill = scenariusz)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  geom_vline(xintercept = prawdziwa_wartosc, color = \"red\", linetype = \"dashed\", size = 1) +\n  facet_wrap(~ scenariusz, ncol = 2, scales = \"free\",\n             labeller = labeller(scenariusz = c(\n               \"wysoka_rz_wysoka_tr\" = \"Wysoka Rzetelność, Wysoka Trafność\",\n               \"wysoka_rz_niska_tr\" = \"Wysoka Rzetelność, Niska Trafność\",\n               \"niska_rz_wysoka_tr\" = \"Niska Rzetelność, Wysoka Trafność\",\n               \"niska_rz_niska_tr\" = \"Niska Rzetelność, Niska Trafność\"\n             ))) +\n  labs(title = \"Histogramy pomiarów\",\n       subtitle = \"Czerwona przerywana linia reprezentuje prawdziwą wartość\",\n       x = \"Zmierzona wartość\",\n       y = \"Liczba\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\",\n        strip.text = element_text(size = 16, face = \"bold\"),\n        plot.title = element_text(size = 20, face = \"bold\"),\n        plot.subtitle = element_text(size = 16))\n\n# Łączenie wykresów\nwykres_polaczony &lt;- wykres_punktowy / wykres_hist +\n  plot_layout(heights = c(1, 1)) +\n  plot_annotation(\n    title = \"Rzetelność i Trafność w Pomiarach\",\n    theme = theme(plot.title = element_text(hjust = 0.5, size = 24, face = \"bold\"))\n  )\n\n# Wyświetlanie połączonego wykresu\nwykres_polaczony\n\n\n\n\n\n\n\n\n\n8.3.1 Interpretacja Wizualizacji\n\nWysoka Rzetelność, Wysoka Trafność: Punkty grupują się ciasno wokół prawdziwej wartości (przerywana linia).\nWysoka Rzetelność, Niska Trafność: Punkty grupują się ciasno, ale konsekwentnie powyżej prawdziwej wartości.\nNiska Rzetelność, Wysoka Trafność: Punkty rozpraszają się szeroko, ale centrują się wokół prawdziwej wartości.\nNiska Rzetelność, Niska Trafność: Punkty rozpraszają się losowo bez wyraźnego wzoru lub relacji do prawdziwej wartości.\n\nZrozumienie rzetelności i trafności jest kluczowe w naukach o danych i badaniach. Wysoka rzetelność zapewnia spójne pomiary, podczas gdy wysoka trafność zapewnia dokładne reprezentacje tego, co zamierzamy zmierzyć. Biorąc pod uwagę oba aspekty, badacze mogą projektować bardziej solidne badania i wyciągać bardziej znaczące wnioski ze swoich danych.\nProwadząc własne badania lub analizując pracę innych, zawsze należy rozważyć: - Jak rzetelne są pomiary? - Jak trafne jest podejście do pomiaru zamierzonego pojęcia? - Czy stosowane metody wspierają zarówno rzetelność, jak i trafność?\nMając na uwadze te pytania, będziesz lepiej przygotowany do prowadzenia i interpretowania wysokiej jakości badań w naukach o danych.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#rodzaje-rzetelności",
    "href": "rozdzial3b.html#rodzaje-rzetelności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.4 Rodzaje Rzetelności",
    "text": "8.4 Rodzaje Rzetelności\nRzetelność można oceniać na kilka sposobów, każdy skupiający się na innym aspekcie spójności:\n\nRzetelność test-retest: Mierzy spójność testu w czasie. Polega na przeprowadzeniu tego samego testu na tej samej grupie uczestników w różnych momentach i porównaniu wyników.\nRzetelność między oceniającymi: Ocenia stopień zgodności między różnymi oceniającymi lub obserwatorami. Jest kluczowa, gdy w zbieraniu danych biorą udział subiektywne osądy.\nSpójność wewnętrzna: Ocenia, jak dobrze różne elementy testu lub skali mierzą ten sam konstrukt. Alfa Cronbacha jest powszechną miarą spójności wewnętrznej.\nRzetelność form równoległych: Polega na stworzeniu dwóch równoważnych form testu i przeprowadzeniu ich na tej samej grupie. Korelacja między dwoma zestawami wyników wskazuje na rzetelność.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#rodzaje-trafności",
    "href": "rozdzial3b.html#rodzaje-trafności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.5 Rodzaje Trafności",
    "text": "8.5 Rodzaje Trafności\nTrafność jest pojęciem wieloaspektowym, z kilkoma rodzajami, które badacze muszą wziąć pod uwagę:\n\nTrafność treściowa: Zapewnia, że pomiar obejmuje wszystkie aspekty konstruktu, który ma mierzyć. Często jest oceniana przez osąd ekspertów.\nTrafność konstrukcyjna: Ocenia, czy test mierzy zamierzony konstrukt teoretyczny. Obejmuje:\n\nTrafność zbieżną: Stopień, w jakim pomiar koreluje z innymi pomiarami tego samego konstruktu.\nTrafność różnicową: Zakres, w jakim pomiar nie koreluje z pomiarami różnych konstruktów.\n\nTrafność kryterialną: Ocenia, jak dobrze pomiar przewiduje wynik. Obejmuje:\n\nTrafność współbieżną: Jak dobrze pomiar koreluje z innymi pomiarami tego samego konstruktu w tym samym czasie.\nTrafność predykcyjną: Jak dobrze pomiar przewiduje przyszłe wyniki.\n\nTrafność fasadowa: Trafność fasadowa odnosi się do tego, jak osoby badane postrzegają test i czy uważają go za odpowiedni do celu, któremu ma służyć. Brak trafności fasadowej może mieć negatywne konsekwencje, nawet jeśli test jest faktycznie trafny (czyli mierzy to, co powinien mierzyć) z punktu widzenia jego zamierzonego celu. Choć nie jest to naukowa miara, może być ważna dla zaangażowania uczestników.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#trafność-wewnętrzna-vs-zewnętrzna",
    "href": "rozdzial3b.html#trafność-wewnętrzna-vs-zewnętrzna",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.6 Trafność Wewnętrzna vs Zewnętrzna",
    "text": "8.6 Trafność Wewnętrzna vs Zewnętrzna\nTe pojęcia są kluczowe w projektowaniu eksperymentów i możliwości uogólniania wyników badań:\n\n8.6.1 Trafność Wewnętrzna\nTrafność wewnętrzna odnosi się do zakresu, w jakim badanie ustanawia związek przyczynowy między zmiennymi niezależnymi a zależnymi. Odpowiada na pytanie: “Czy eksperymentalne traktowanie rzeczywiście spowodowało zaobserwowane efekty?”\nCzynniki, które mogą zagrażać trafności wewnętrznej, obejmują: - Historia: Zewnętrzne wydarzenia występujące między pre-testem a post-testem - Dojrzewanie: Naturalne zmiany u uczestników w czasie - Efekty testowania: Zmiany wynikające z przeprowadzenia pre-testu - Instrumentacja: Zmiany w narzędziu pomiarowym lub obserwatorach - Błąd selekcji: Nielosowy przydział do grup - Utrata: Utrata uczestników podczas badania\n\n\n8.6.2 Trafność Zewnętrzna\nTrafność zewnętrzna odnosi się do zakresu, w jakim wyniki badania mogą być uogólnione na inne sytuacje, populacje lub ustawienia. Odpowiada na pytanie: “W jakim stopniu wyniki mogą być zastosowane poza konkretnym kontekstem badania?”\nCzynniki, które mogą wpływać na trafność zewnętrzną, obejmują: - Trafność populacyjna: Jak dobrze próba reprezentuje szerszą populację - Trafność ekologiczna: Jak dobrze ustawienie badania reprezentuje warunki świata rzeczywistego - Trafność czasowa: Czy wyniki pozostają prawdziwe w czasie",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#spójność-w-badaniach",
    "href": "rozdzial3b.html#spójność-w-badaniach",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.7 Spójność w Badaniach",
    "text": "8.7 Spójność w Badaniach\nSpójność jest ściśle związana z rzetelnością, ale wykracza poza sam pomiar. W badaniach spójność odnosi się do ogólnej koherencji i stabilności wyników w różnych kontekstach, metodach lub badaniach.\nKluczowe aspekty spójności w badaniach obejmują:\n\nReplikowalność: Zdolność do odtworzenia wyników badania przy użyciu tych samych metod i danych.\nOdporność: Stabilność wyników w różnych podejściach analitycznych lub niewielkich zmianach w metodologii.\nKonwergencja: Zbieżność wyników z różnych badań lub metod badających to samo zjawisko.\nSpójność długoterminowa: Stabilność wyników w czasie, szczególnie ważna w badaniach długoterminowych.\n\nZapewnienie spójności w badaniach obejmuje: - Stosowanie standaryzowanych procedur i miar - Dokładne dokumentowanie metod i decyzji analitycznych - Przeprowadzanie badań replikacyjnych - Meta-analizy w celu syntezy wyników z wielu badań",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#równoważenie-rzetelności-trafności-i-spójności",
    "href": "rozdzial3b.html#równoważenie-rzetelności-trafności-i-spójności",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.8 Równoważenie Rzetelności, Trafności i Spójności",
    "text": "8.8 Równoważenie Rzetelności, Trafności i Spójności\nChociaż rzetelność, trafność i spójność są kluczowe dla wysokiej jakości badań, czasami wiążą się z kompromisami:\n\nWysoce rzetelna miara może nie mieć trafności, jeśli konsekwentnie mierzy niewłaściwą rzecz.\nDążenie do idealnej trafności wewnętrznej (np. w ściśle kontrolowanych eksperymentach laboratoryjnych) może zmniejszyć trafność zewnętrzną.\nZapewnienie wysokiej spójności w różnych kontekstach może wymagać poświęcenia pewnego stopnia precyzji lub głębi w konkretnych sytuacjach.\n\nBadacze muszą starannie równoważyć te aspekty w oparciu o swoje pytania badawcze i charakter badania. Kompleksowe zrozumienie rzetelności, trafności i spójności pomaga w projektowaniu solidnych badań, dokładnej interpretacji wyników i znaczącym wkładzie do korpusu wiedzy naukowej.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#kompromis-między-obciążeniem-a-wariancją",
    "href": "rozdzial3b.html#kompromis-między-obciążeniem-a-wariancją",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.9 Kompromis między Obciążeniem a Wariancją",
    "text": "8.9 Kompromis między Obciążeniem a Wariancją\nPojęcia rzetelności i trafności są ściśle związane ze statystycznym pojęciem kompromisu między obciążeniem a wariancją. Ten kompromis jest fundamentalny w uczeniu maszynowym i modelowaniu statystycznym.\n\nObciążenie odnosi się do błędu wprowadzonego przez przybliżenie problemu ze świata rzeczywistego uproszczonym modelem. Wysokie obciążenie może prowadzić do niedopasowania.\nWariancja odnosi się do błędu wprowadzonego przez wrażliwość modelu na małe fluktuacje w zbiorze treningowym. Wysoka wariancja może prowadzić do przeuczenia.\n\nZobrazujmy to pojęcie za pomocą uproszczonego wykresu:\n\nx &lt;- seq(0, 10, length.out = 100)\ny_prawdziwa &lt;- sin(x)\ny_niskie_obciazenie_wysoka_wariancja &lt;- y_prawdziwa + rnorm(100, 0, 0.3)\ny_wysokie_obciazenie_niska_wariancja &lt;- 0.5 * x\n\ndf &lt;- data.frame(x = rep(x, 3),\n                 y = c(y_prawdziwa, y_niskie_obciazenie_wysoka_wariancja, y_wysokie_obciazenie_niska_wariancja),\n                 typ = rep(c(\"Prawdziwa Funkcja\", \"Niskie Obciążenie, Wysoka Wariancja\", \"Wysokie Obciążenie, Niska Wariancja\"), each = 100))\n\nggplot(df, aes(x = x, y = y, color = typ)) +\n  geom_line() +\n  geom_point(data = subset(df, typ != \"Prawdziwa Funkcja\"), alpha = 0.5) +\n  scale_color_manual(values = c(\"black\", \"blue\", \"red\")) +\n  labs(title = \"Kompromis między Obciążeniem a Wariancją\",\n       x = \"X\",\n       y = \"Y\",\n       color = \"Typ Modelu\") +\n  theme_minimal()\n\n\n\n\nWizualizacja kompromisu między obciążeniem a wariancją\n\n\n\n\nNa tym wykresie: - Czarna linia reprezentuje prawdziwą funkcję bazową. - Niebieskie punkty reprezentują model z niskim obciążeniem, ale wysoką wariancją. Średnio podąża blisko prawdziwej funkcji, ale ma dużo szumu. - Czerwona linia reprezentuje model z wysokim obciążeniem, ale niską wariancją. Konsekwentnie niedoszacowuje prawdziwej funkcji, ale ma mniej szumu.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#dokładność-i-precyzja",
    "href": "rozdzial3b.html#dokładność-i-precyzja",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.10 Dokładność i Precyzja",
    "text": "8.10 Dokładność i Precyzja\nPojęcia dokładności i precyzji są ściśle związane z trafnością i rzetelnością:\n\nDokładność odnosi się do tego, jak blisko pomiar jest prawdziwej wartości (podobnie do trafności).\nPrecyzja odnosi się do tego, jak spójne lub powtarzalne są pomiary (podobnie do rzetelności).\n\nMożemy zobrazować te pojęcia za pomocą uproszczonej analogii do tarczy:\n\nlibrary(ggplot2)\nlibrary(ggforce)\n\ncreate_points &lt;- function(n, x_center, y_center, spread) {\n  data.frame(\n    x = rnorm(n, x_center, spread),\n    y = rnorm(n, y_center, spread)\n  )\n}\n\nset.seed(101)\npoints &lt;- rbind(\n  cbind(create_points(20, 0, 0, 0.1), type = \"Wysoka Dokładność\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.1), type = \"Niska Dokładność\\nWysoka Precyzja\"),\n  cbind(create_points(20, 0, 0, 0.3), type = \"Wysoka Dokładność\\nNiska Precyzja\"),\n  cbind(create_points(20, 0.5, 0.5, 0.3), type = \"Niska Dokładność\\nNiska Precyzja\")\n)\n\nggplot(points, aes(x, y)) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 1), color = \"black\", fill = NA) +\n  geom_circle(aes(x0 = 0, y0 = 0, r = 0.5), color = \"black\", fill = NA) +\n  geom_point(color = \"red\", size = 2) +\n  facet_wrap(~type) +\n  coord_fixed(xlim = c(-1.1, 1.1), ylim = c(-1.1, 1.1)) +\n  theme_minimal() +\n  theme(axis.text = element_blank(), axis.title = element_blank()) +\n  labs(title = \"Dokładność vs Precyzja\")\n\n\n\n\nWizualizacja Dokładności vs Precyzji\n\n\n\n\nW tej wizualizacji: - Wysoka dokładność oznacza, że punkty są blisko środka (dziesiątki). - Wysoka precyzja oznacza, że punkty są ściśle zgrupowane. - Każdy panel reprezentuje inną kombinację dokładności i precyzji.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#podsumowanie",
    "href": "rozdzial3b.html#podsumowanie",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.11 Podsumowanie",
    "text": "8.11 Podsumowanie\nZrozumienie rzetelności i trafności jest kluczowe dla prowadzenia solidnych badań. Pojęcia te pomagają nam zapewnić, że nasze pomiary są zarówno spójne, jak i dokładne. Łącząc je z ideami takimi jak kompromis między obciążeniem a wariancją oraz dokładnością i precyzją, zyskujemy głębsze zrozumienie wyzwań związanych z pomiarem i modelowaniem w badaniach naukowych. Jako badacze musimy dążyć do opracowania miar i modeli, które są zarówno rzetelne, jak i trafne, równoważąc kompromisy między obciążeniem a wariancją oraz między dokładnością a precyzją. Wymaga to starannego projektowania metodologii badań, rygorystycznego testowania naszych instrumentów pomiarowych i przemyślanej interpretacji naszych wyników.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#zrozumienie-obciążenia-vs.-wariancji-w-pomiarach-statystycznych",
    "href": "rozdzial3b.html#zrozumienie-obciążenia-vs.-wariancji-w-pomiarach-statystycznych",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.12 Zrozumienie Obciążenia vs. Wariancji w Pomiarach Statystycznych",
    "text": "8.12 Zrozumienie Obciążenia vs. Wariancji w Pomiarach Statystycznych\n\n8.12.1 Wprowadzenie\nW statystyce i uczeniu maszynowym dwa ważne pojęcia, które wpływają na wydajność naszych modeli, to obciążenie (bias) i wariancja (variance). Zrozumienie tych pojęć jest kluczowe dla budowania efektywnych modeli predykcyjnych i unikania typowych pułapek, takich jak przeuczenie i niedouczenie.\n\nObciążenie odnosi się do błędu wprowadzonego przez przybliżenie rzeczywistego problemu, który może być złożony, za pomocą uproszczonego modelu. Wysokie obciążenie może prowadzić do niedouczenia.\n\nWyobraź sobie obciążenie jako średnią odległość naszych przewidywań od prawdziwych wartości.\nW kontekście trafności, wysokie obciążenie oznacza, że nasz model nie uchwycił prawdziwej zależności w danych.\n\nWariancja odnosi się do tego, jak bardzo nasz model zmieniłby się, gdybyśmy oszacowali go przy użyciu innego zbioru treningowego. Wysoka wariancja może prowadzić do przeuczenia.\n\nWyobraź sobie wariancję jako to, jak bardzo nasze przewidywania wahałyby się, gdybyśmy użyli różnych zbiorów danych.\nW kontekście rzetelności, wysoka wariancja oznacza, że nasz model jest zbyt wrażliwy na konkretne dane, na których został wytrenowany.\n\n\nZbadamy cztery scenariusze, aby zilustrować różne kombinacje obciążenia i wariancji przy użyciu syntetycznych danych i modeli regresji.\n\n\n8.12.2 Funkcja Generowania Danych i Dopasowywania Modelu\nNajpierw stwórzmy funkcję, która pomoże nam generować dane i dopasowywać modele dla każdego scenariusza:\n\ngeneruj_i_dopasuj &lt;- function(n, wyraz_wolny, nachylenie, odch_szumu, stopien_modelu) {\n  # Generowanie syntetycznych danych\n  x &lt;- runif(n, 0, 10)\n  y &lt;- wyraz_wolny + nachylenie * x + rnorm(n, 0, odch_szumu)\n  \n  # Dopasowanie modelu\n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", stopien_modelu, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  # Generowanie przewidywań\n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  # Wykres\n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    geom_abline(intercept = wyraz_wolny, slope = nachylenie, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Stopień Modelu:\", stopien_modelu),\n         subtitle = paste(\"Odchylenie Standardowe Szumu:\", odch_szumu),\n         x = \"X (Zmienna Wejściowa)\",\n         y = \"Y (Zmienna Docelowa)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\nTa funkcja wykonuje następujące czynności: 1. Generuje syntetyczne dane na podstawie naszych parametrów 2. Dopasowuje model regresji wielomianowej 3. Tworzy wykres pokazujący prawdziwą zależność (niebieska przerywana linia), przewidywania naszego modelu (czerwona ciągła linia) i punkty danych\nTeraz zbadajmy nasze cztery scenariusze!\n\n\n8.12.3 Scenariusz 1: Niskie Obciążenie, Niska Wariancja\nW tym idealnym scenariuszu używamy modelu liniowego do dopasowania danych liniowych z niskim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 1, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model (czerwona linia) ściśle podąża za prawdziwą zależnością (niebieska przerywana linia). - Punkty danych są skupione blisko linii, co wskazuje na niski szum. - Ten scenariusz reprezentuje dobre dopasowanie: model uchwycił podstawowy trend bez nadmiernej złożoności.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "rozdzial3b.html#scenariusz-2-niskie-obciążenie-wysoka-wariancja",
    "href": "rozdzial3b.html#scenariusz-2-niskie-obciążenie-wysoka-wariancja",
    "title": "8  Rzetelność i Trafność w Badaniach Nauk o Danych",
    "section": "8.13 Scenariusz 2: Niskie Obciążenie, Wysoka Wariancja",
    "text": "8.13 Scenariusz 2: Niskie Obciążenie, Wysoka Wariancja\nTutaj używamy modelu liniowego do dopasowania danych liniowych, ale z wysokim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 5, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model nadal uchwycił ogólny trend, ale punkty danych są bardziej rozproszone. - Ta wysoka wariancja oznacza, że przewidywania naszego modelu byłyby mniej wiarygodne. - W rzeczywistych warunkach mogłoby to reprezentować sytuację, w której nasze pomiary są średnio poprawne, ale mają dużo losowego błędu.\n\n8.13.1 Scenariusz 3: Wysokie Obciążenie, Niska Wariancja\nW tym przypadku używamy modelu liniowego do dopasowania danych kwadratowych (zakrzywionych) z niskim szumem.\n\ndane_kwadratowe &lt;- function(n, wyraz_wolny, nachylenie, odch_szumu, stopien_modelu) {\n  x &lt;- runif(n, 0, 10)\n  y &lt;- wyraz_wolny + nachylenie * x^2 + rnorm(n, 0, odch_szumu)\n  \n  formula &lt;- as.formula(paste(\"y ~\", paste0(\"poly(x, \", stopien_modelu, \", raw = TRUE)\")))\n  model &lt;- lm(formula, data = data.frame(x, y))\n  \n  x_pred &lt;- seq(0, 10, length.out = 100)\n  y_pred &lt;- predict(model, newdata = data.frame(x = x_pred))\n  \n  ggplot(data.frame(x, y), aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    geom_line(data = data.frame(x = x_pred, y = y_pred), color = \"red\", size = 1) +\n    stat_function(fun = function(x) wyraz_wolny + nachylenie * x^2, color = \"blue\", linetype = \"dashed\") +\n    labs(title = paste(\"Stopień Modelu:\", stopien_modelu),\n         subtitle = paste(\"Odchylenie Standardowe Szumu:\", odch_szumu),\n         x = \"X (Zmienna Wejściowa)\",\n         y = \"Y (Zmienna Docelowa)\") +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5),\n          plot.subtitle = element_text(hjust = 0.5))\n}\n\ndane_kwadratowe(n = 100, wyraz_wolny = 1, nachylenie = 0.2, odch_szumu = 1, stopien_modelu = 1)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model liniowy (czerwona linia) nie uchwycił krzywizny prawdziwej zależności (niebieska przerywana linia). - To wysokie obciążenie oznacza, że nasz model konsekwentnie myli się w swoich przewidywaniach. - W rzeczywistych warunkach mogłoby to reprezentować użycie zbyt uproszczonego modelu dla złożonego zjawiska.\n\n\n8.13.2 Scenariusz 4: Wysokie Obciążenie, Wysoka Wariancja\nNa koniec używamy wielomianu wysokiego stopnia do dopasowania danych liniowych z wysokim szumem.\n\ngeneruj_i_dopasuj(n = 100, wyraz_wolny = 1, nachylenie = 2, odch_szumu = 5, stopien_modelu = 5)\n\n\n\n\n\n\n\n\nWyjaśnienie: - Model (czerwona linia) jest zbyt złożony, próbując dopasować się do szumu zamiast do podstawowego trendu. - Ta kombinacja wysokiego obciążenia i wysokiej wariancji prowadzi do słabej generalizacji. - W rzeczywistych warunkach mogłoby to reprezentować nadmierne skomplikowanie naszej analizy i wyciąganie fałszywych wniosków z losowych fluktuacji w naszych danych.\n\n\n8.13.3 Podsumowanie\nZrozumienie kompromisu między obciążeniem a wariancją jest kluczowe w modelowaniu statystycznym:\n\nNiskie Obciążenie, Niska Wariancja: Idealny scenariusz, w którym nasz model dokładnie uchwycił podstawową zależność bez nadmiernej wrażliwości na szum.\nNiskie Obciążenie, Wysoka Wariancja: Nasz model jest średnio poprawny, ale niewiarygodny ze względu na wysoką wrażliwość na pojedyncze punkty danych.\nWysokie Obciążenie, Niska Wariancja: Nasz model jest konsekwentnie błędny z powodu nadmiernego uproszczenia, ale daje stabilne przewidywania.\nWysokie Obciążenie, Wysoka Wariancja: Najgorszy scenariusz, w którym nasz model jest zarówno niedokładny, jak i niewiarygodny.\n\nW praktyce często musimy zrównoważyć obciążenie i wariancję. Techniki takie jak walidacja krzyżowa, regularyzacja i metody zespołowe mogą pomóc w znalezieniu tej równowagi.\nPamiętaj: - Model z wysokim obciążeniem jest zbyt prosty i pomija ważne wzorce w danych. - Model z wysoką wariancją jest zbyt złożony i dopasowuje się do szumu w danych treningowych. - Celem jest znalezienie złotego środka, który uchwyci prawdziwe wzorce bez nadmiernego dopasowania do szumu.\nZrozumienie tych pojęć pomoże ci lepiej wybierać odpowiednie modele, unikać przeuczenia i niedouczenia oraz budować bardziej efektywne modele predykcyjne w przyszłych analizach statystycznych!",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Rzetelność i Trafność w Badaniach Nauk o Danych</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "",
    "text": "9.1 Introduction\nResearch designs are fundamental to the scientific process, providing structured approaches to investigate hypotheses and answer research questions. This chapter explores two main categories of research designs: experimental and non-experimental, with a focus on the Neyman-Rubin potential outcome framework. We’ll delve into various design types, their characteristics, and provide practical examples using R for data analysis and visualization.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#experimental-designs",
    "href": "chapter4.html#experimental-designs",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.2 Experimental Designs",
    "text": "9.2 Experimental Designs\nExperimental designs are characterized by the researcher’s control over the independent variable(s) and random assignment of subjects to different conditions. These designs are considered the gold standard for establishing causal relationships.\n\n9.2.1 Randomized Controlled Trials (RCTs)\nRCTs are the most rigorous form of experimental design. They involve:\n\nRandom assignment of subjects to treatment and control groups\nManipulation of the independent variable\nMeasurement of the dependent variable\n\nLet’s visualize a simple RCT design:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Create sample data\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  group = factor(rep(c(\"Control\", \"Treatment\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Simulate treatment effect\ndata$post_test &lt;- ifelse(data$group == \"Treatment\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Reshape data for plotting\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"time\", values_to = \"score\")\n\n# Create plot\nggplot(data_long, aes(x = time, y = score, color = group, group = interaction(id, group))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = group), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Pre-test and Post-test Scores in RCT\",\n       x = \"Time\", y = \"Score\", color = \"Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nRandomized Controlled Trial Design\n\n\n\n\nThis plot shows individual trajectories and group means for pre-test and post-test scores in a hypothetical RCT. The treatment group shows a clear increase in scores compared to the control group.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "href": "chapter4.html#ab-testing-an-example-and-comparison-with-rcts",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.3 A/B Testing: An Example and Comparison with RCTs",
    "text": "9.3 A/B Testing: An Example and Comparison with RCTs\nA/B testing is a widely used experimental method in digital marketing, user experience design, and product development. This chapter will present an example of A/B testing, explain its methodology, and discuss how it differs from Randomized Controlled Trials (RCTs).\n\n9.3.1 Example: Website Landing Page Conversion Rate\nLet’s consider an example where an e-commerce company wants to improve the conversion rate of their landing page. They decide to test two different layouts: the current layout (A) and a new layout (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Simulate data\nn_visitors &lt;- 10000\ndata &lt;- data.frame(\n  Version = sample(c(\"A\", \"B\"), n_visitors, replace = TRUE),\n  Converted = rbinom(n_visitors, 1, ifelse(sample(c(\"A\", \"B\"), n_visitors, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Calculate conversion rates\nconversion_rates &lt;- data %&gt;%\n  group_by(Version) %&gt;%\n  summarise(\n    Visitors = n(),\n    Conversions = sum(Converted),\n    ConversionRate = mean(Converted)\n  )\n\n# Visualize results\nggplot(conversion_rates, aes(x = Version, y = ConversionRate, fill = Version)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", ConversionRate * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"A/B Test: Landing Page Conversion Rates\",\n       x = \"Page Version\", y = \"Conversion Rate\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure 9.1: A/B Test Results: Landing Page Conversion Rates\n\n\n\n\n\nIn this example, we simulated data for 10,000 visitors randomly assigned to either version A or B of the landing page. The results show that version B has a slightly higher conversion rate (11.44%) compared to version A (10.94%).\n\n\n9.3.2 A/B Testing Methodology\nA/B testing typically follows these steps:\n\nIdentify the element to be tested (e.g., landing page layout).\nCreate two versions: the control (A) and the variant (B).\nRandomly assign visitors to either version.\nCollect data on the metric of interest (e.g., conversion rate).\nAnalyze the results using statistical methods.\nMake a decision based on the results.\n\n\n\n9.3.3 Differences between A/B Testing and RCTs\nWhile A/B testing and Randomized Controlled Trials (RCTs) share some similarities, they have several key differences:\n\nScope and Context:\n\nA/B Testing: Typically used in digital environments for quick, iterative improvements.\nRCTs: Used in various fields, including medicine, psychology, and social sciences, often for more complex interventions.\n\nDuration:\n\nA/B Testing: Usually shorter, often running for days or weeks.\nRCTs: Can last months or years, especially in medical research.\n\nSample Size:\n\nA/B Testing: Can involve very large sample sizes due to ease of implementation in digital platforms.\nRCTs: Sample sizes are often smaller due to practical and cost constraints.\n\nBlinding:\n\nA/B Testing: Participants are usually unaware they’re part of a test.\nRCTs: May involve single, double, or triple blinding to reduce bias.\n\nEthical Considerations:\n\nA/B Testing: Generally involves low-risk changes with minimal ethical concerns.\nRCTs: Often require extensive ethical review, especially in medical contexts.\n\nOutcome Measures:\n\nA/B Testing: Typically focuses on a single, easily measurable outcome (e.g., click-through rate).\nRCTs: Often measure multiple outcomes, including potential side effects or long-term impacts.\n\nGeneralizability:\n\nA/B Testing: Results are often specific to the platform or context tested.\nRCTs: Aim for broader generalizability, though this can vary.\n\nAnalysis Complexity:\n\nA/B Testing: Often uses simpler statistical analyses.\nRCTs: May involve more complex statistical methods to account for various factors.\n\n\nA/B testing is a powerful tool for making data-driven decisions in digital environments. While it shares the fundamental principle of randomization with RCTs, it is typically simpler, faster, and more focused on specific, measurable outcomes in digital contexts. Understanding these differences helps researchers and practitioners choose the most appropriate method for their specific needs and constraints.\n\n\n9.3.4 Example 1: Effect of Sleep Duration on Cognitive Performance\nResearch Question: Does increasing sleep duration improve cognitive performance in college students?\n\n# Generating sample data\nset.seed(456)\nn &lt;- 100\npre_experimental &lt;- rnorm(n, mean = 70, sd = 10)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = 8, sd = 5)\npre_control &lt;- rnorm(n, mean = 70, sd = 10)\npost_control &lt;- pre_control + rnorm(n, mean = 1, sd = 5)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Experimental\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  Score = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = Score, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Effect of Increased Sleep Duration on Cognitive Performance\") +\n  xlab(\"Time\") +\n  ylab(\"Cognitive Performance Score\")\n\n\n\n\n\n\n\nFigure 9.2: Effect of Sleep Duration on Cognitive Performance\n\n\n\n\n\n\n9.3.4.1 Interpretation\nThis plot demonstrates the effect of increased sleep duration on cognitive performance. The experimental group, which increased their sleep duration, shows a more substantial improvement in cognitive performance compared to the control group. This suggests that increasing sleep duration may positively impact cognitive abilities in college students.\n\n\n\n9.3.5 Example 2: Impact of Mindfulness Training on Stress Levels\nResearch Question: Can a short-term mindfulness training program reduce stress levels in healthcare workers?\n\n# Generating sample data\nset.seed(789)\nn &lt;- 120\npre_experimental &lt;- rnorm(n, mean = 60, sd = 15)\npost_experimental &lt;- pre_experimental + rnorm(n, mean = -12, sd = 8)\npre_control &lt;- rnorm(n, mean = 60, sd = 15)\npost_control &lt;- pre_control + rnorm(n, mean = -2, sd = 6)\n\ndata &lt;- data.frame(\n  Group = rep(c(\"Mindfulness\", \"Control\"), each = n*2),\n  Time = rep(rep(c(\"Pre\", \"Post\"), each = n), 2),\n  StressScore = c(pre_experimental, post_experimental, pre_control, post_control)\n)\n\n# Creating the plot\nggplot(data, aes(x = Time, y = StressScore, color = Group, group = Group)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Impact of Mindfulness Training on Stress Levels\") +\n  xlab(\"Time\") +\n  ylab(\"Stress Score\")\n\n\n\n\n\n\n\nFigure 9.3: Impact of Mindfulness Training on Stress Levels\n\n\n\n\n\n\n9.3.5.1 Interpretation\nThis visualization illustrates the impact of a mindfulness training program on stress levels in healthcare workers. The mindfulness group shows a more significant decrease in stress scores compared to the control group. This suggests that the mindfulness training program may be effective in reducing stress levels among healthcare workers.\nWhen interpreting such results, it’s important to consider:\n\nThe magnitude of the change in each group\nThe difference in change between the experimental and control groups\nThe variability within each group\nAny potential confounding factors not accounted for in the experimental design\n\nThese examples provide a template for visualizing and interpreting similar experimental designs across different research contexts.\n\n\n\n9.3.6 Factorial Designs\nFactorial designs allow researchers to study the effects of multiple independent variables simultaneously. They are efficient and can reveal interaction effects between variables.\nExample of a 2x2 factorial design:\n\n# Create sample data for 2x2 factorial design\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  factor_a = rep(rep(c(\"Low\", \"High\"), each = n_per_group), 2),\n  factor_b = rep(c(\"Control\", \"Treatment\"), each = n_per_group * 2),\n  outcome = NA\n)\n\n# Generate outcomes\nfactorial_data$outcome &lt;- ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Control\",\n                                 rnorm(n_per_group, 40, 5),\n                                 ifelse(factorial_data$factor_a == \"Low\" & factorial_data$factor_b == \"Treatment\",\n                                        rnorm(n_per_group, 45, 5),\n                                        ifelse(factorial_data$factor_a == \"High\" & factorial_data$factor_b == \"Control\",\n                                               rnorm(n_per_group, 50, 5),\n                                               rnorm(n_per_group, 60, 5))))\n\n# Create plot\nggplot(factorial_data, aes(x = factor_b, y = outcome, fill = factor_a)) +\n  geom_boxplot() +\n  facet_wrap(~factor_a, scales = \"free_x\") +\n  labs(title = \"2x2 Factorial Design\",\n       x = \"Factor B\", y = \"Outcome\", fill = \"Factor A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n2x2 Factorial Design\n\n\n\n\nThis plot illustrates a 2x2 factorial design, showing the effects of two factors (A and B) on the outcome variable. We can observe main effects for both factors and a potential interaction effect.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#non-experimental-designs",
    "href": "chapter4.html#non-experimental-designs",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.4 Non-Experimental Designs",
    "text": "9.4 Non-Experimental Designs\nNon-experimental designs are used when randomization or manipulation of variables is not possible or ethical. They include observational/descriptive studies and quasi-experimental designs.\n\n9.4.1 Observational Studies\nObservational studies involve collecting data without manipulating variables. They are useful for exploring relationships and generating hypotheses.\nExample: Correlation study\n\nset.seed(789)\nn &lt;- 100\nstudy_time &lt;- runif(n, 0, 10)\nexam_score &lt;- 50 + 5 * study_time + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(study_time, exam_score)\n\nggplot(correlation_data, aes(x = study_time, y = exam_score)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Correlation between Study Time and Exam Score\",\n       x = \"Study Time (hours)\", y = \"Exam Score\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCorrelation between Study Time and Exam Score\n\n\n\n\nThis scatter plot shows the relationship between study time and exam scores, illustrating a positive correlation typical in observational studies.\n\n\n9.4.2 Quasi-Experimental Designs\nQuasi-experimental designs lack random assignment but attempt to establish causal relationships. Common types include:\n\nDifference-in-Differences (DiD)\nRegression Discontinuity Design (RDD)\n\n\n9.4.2.1 Difference-in-Differences (DiD)\nDiD is used to estimate treatment effects by comparing the average change over time in the outcome variable for the treatment group to the average change over time for the control group.\nLet’s simulate a DiD analysis using the plm package:\n\nlibrary(plm)\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\nDifference-in-Differences Analysis\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nThe plot shows the average outcomes for treatment and control groups over time. The vertical dashed line indicates the intervention point. The DiD estimate is the difference between the two groups’ changes from pre- to post-intervention periods.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt’s important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\n9.4.2.2 Regression Discontinuity Design (RDD)\nRDD is used when treatment assignment is determined by a cutoff value on a continuous variable. It compares observations just above and below the cutoff to estimate the treatment effect.\nLet’s implement an RDD analysis using the rdrobust package:\n\nlibrary(rdrobust)\n\n# Generate synthetic RDD data\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# RDD analysis\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     4.092     0.231    17.723     0.000     [3.640 , 4.545]     \n        Robust         -         -    15.013     0.000     [3.600 , 4.680]     \n=============================================================================\n\n# Visualize RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regression Discontinuity Design\",\n       x = \"Running Variable\", y = \"Outcome\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nRegression Discontinuity Design Analysis\n\n\n\n\nThe plot shows the discontinuity at the cutoff point (x = 0), with separate regression lines fitted on either side. The treatment effect is estimated by the gap between these lines at the cutoff.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "href": "chapter4.html#the-neyman-rubin-potential-outcome-framework",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.5 The Neyman-Rubin Potential Outcome Framework",
    "text": "9.5 The Neyman-Rubin Potential Outcome Framework\nThe Neyman-Rubin potential outcome framework provides a formal approach to causal inference. It introduces the concept of potential outcomes: for each unit, we consider the outcome under treatment and the outcome under control, even though we can only observe one in reality.\nKey concepts:\n\nPotential Outcomes: Y_i(1) and Y_i(0) for treatment and control, respectively.\nObserved Outcome: Y_i = Y_i(1)T_i + Y_i(0)(1-T_i), where T_i is the treatment indicator.\nIndividual Treatment Effect: \\tau_i = Y_i(1) - Y_i(0)\nAverage Treatment Effect (ATE): E[\\tau_i] = E[Y_i(1) - Y_i(0)]\n\nThe framework emphasizes the “fundamental problem of causal inference”: we can never observe both potential outcomes for a single unit simultaneously.\n\n9.5.1 Example: Estimating ATE in an RCT\nIn an RCT, random assignment ensures that treatment is independent of potential outcomes, allowing unbiased estimation of the ATE:\n\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\nWhere n_1 and n_0 are the numbers of treated and control units, respectively.\n\n# Using the RCT data from earlier\nate_estimate &lt;- mean(data$post_test[data$group == \"Treatment\"]) - \n                mean(data$post_test[data$group == \"Control\"])\n\nWarning in mean.default(data$post_test[data$group == \"Treatment\"]): argument is\nnot numeric or logical: returning NA\n\n\nWarning in mean.default(data$post_test[data$group == \"Control\"]): argument is\nnot numeric or logical: returning NA\n\ncat(\"Estimated Average Treatment Effect:\", round(ate_estimate, 2))\n\nEstimated Average Treatment Effect: NA\n\n\nThis estimate represents the causal effect of the treatment under the assumptions of the potential outcome framework.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#conclusion",
    "href": "chapter4.html#conclusion",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.6 Conclusion",
    "text": "9.6 Conclusion\nThis chapter has explored various research designs, from experimental approaches like RCTs and factorial designs to non-experimental methods such as observational studies and quasi-experimental designs. We’ve demonstrated how to implement and visualize these designs using R, and introduced the Neyman-Rubin potential outcome framework for causal inference.\nUnderstanding these designs and their appropriate use is crucial for conducting rigorous research and drawing valid causal conclusions. Each design has its strengths and limitations, and the choice of design should be guided by the research question, ethical considerations, and practical constraints.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "chapter4.html#references",
    "href": "chapter4.html#references",
    "title": "9  Research Designs: Experimental and Non-Experimental Approaches",
    "section": "9.7 References",
    "text": "9.7 References\n\nImbens, G. W., & Rubin, D. B. (2015). Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction. Cambridge University Press.\nAngrist, J. D., & Pischke, J. S. (2008). Mostly Harmless Econometrics: An Empiricist’s Companion. Princeton University Press.\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and Quasi-Experimental Designs for Generalized Causal Inference. Houghton Mifflin.\nCunningham, S. (2021). Causal Inference: The Mixtape. Yale University Press.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Research Designs: Experimental and Non-Experimental Approaches</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html",
    "href": "rozdzial4.html",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "",
    "text": "10.1 Wstęp\nProjekty badawcze stanowią fundament procesu naukowego, zapewniając ustrukturyzowane podejście do badania hipotez i odpowiadania na pytania badawcze. Ten rozdział analizuje dwie główne kategorie projektów badawczych: eksperymentalne i nieeksperymentalne, ze szczególnym uwzględnieniem modelu potencjalnych wyników Neymana-Rubina. Zagłębimy się w różne typy projektów, ich charakterystykę i przedstawimy praktyczne przykłady wykorzystania R do analizy danych i wizualizacji.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-eksperymentalne",
    "href": "rozdzial4.html#projekty-eksperymentalne",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.2 Projekty Eksperymentalne",
    "text": "10.2 Projekty Eksperymentalne\nProjekty eksperymentalne charakteryzują się kontrolą badacza nad zmienną(ymi) niezależną(ymi) oraz losowym przydziałem uczestników do różnych warunków. Te projekty są uważane za złoty standard w ustalaniu związków przyczynowych.\n\n10.2.1 Randomizowane Badania Kontrolowane (RCT)\nRCT są najbardziej rygorystyczną formą projektu eksperymentalnego. Obejmują one:\n\nLosowy przydział uczestników do grup eksperymentalnej i kontrolnej\nManipulację zmienną niezależną\nPomiar zmiennej zależnej\n\nZobaczmy wizualizację prostego projektu RCT:\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(123)\n\n# Tworzenie przykładowych danych\nn &lt;- 100\ndata &lt;- data.frame(\n  id = 1:n,\n  grupa = factor(rep(c(\"Kontrolna\", \"Eksperymentalna\"), each = n/2)),\n  pre_test = rnorm(n, mean = 50, sd = 10),\n  post_test = NA\n)\n\n# Symulacja efektu leczenia\ndata$post_test &lt;- ifelse(data$grupa == \"Eksperymentalna\",\n                         data$pre_test + rnorm(n/2, mean = 10, sd = 5),\n                         data$pre_test + rnorm(n/2, mean = 0, sd = 5))\n\n# Przekształcenie danych do formatu długiego\ndata_long &lt;- tidyr::pivot_longer(data, cols = c(pre_test, post_test),\n                                 names_to = \"czas\", values_to = \"wynik\")\n\n# Tworzenie wykresu\nggplot(data_long, aes(x = czas, y = wynik, color = grupa, group = interaction(id, grupa))) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.5) +\n  stat_summary(aes(group = grupa), fun = mean, geom = \"line\", size = 1.5) +\n  labs(title = \"Wyniki Pre-test i Post-test w RCT\",\n       x = \"Czas\", y = \"Wynik\", color = \"Grupa\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\") +\n  scale_x_discrete(labels = c(\"pre_test\" = \"Pre-test\", \"post_test\" = \"Post-test\"))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nProjekt Randomizowanego Badania Kontrolowanego\n\n\n\n\nTen wykres pokazuje indywidualne trajektorie i średnie grupowe dla wyników pre-test i post-test w hipotetycznym RCT. Grupa eksperymentalna wykazuje wyraźny wzrost wyników w porównaniu do grupy kontrolnej.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#testy-ab-przykład-i-porównanie-z-rct",
    "href": "rozdzial4.html#testy-ab-przykład-i-porównanie-z-rct",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.3 Testy A/B: Przykład i Porównanie z RCT",
    "text": "10.3 Testy A/B: Przykład i Porównanie z RCT\nTesty A/B to szeroko stosowana metoda eksperymentalna w marketingu cyfrowym, projektowaniu doświadczeń użytkownika i rozwoju produktów. Ten rozdział przedstawi przykład testu A/B, wyjaśni jego metodologię i omówi, czym różni się od Randomizowanych Badań Kontrolowanych (RCT).\n\n10.3.1 Przykład: Współczynnik Konwersji Strony Docelowej\nRozważmy przykład, w którym firma e-commerce chce poprawić współczynnik konwersji swojej strony docelowej. Decydują się przetestować dwa różne układy: obecny układ (A) i nowy układ (B).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\nset.seed(1234)\n\n# Symulacja danych\nn_odwiedzajacych &lt;- 10000\ndane &lt;- data.frame(\n  Wersja = sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE),\n  Konwersja = rbinom(n_odwiedzajacych, 1, ifelse(sample(c(\"A\", \"B\"), n_odwiedzajacych, replace = TRUE) == \"A\", 0.10, 0.12))\n)\n\n# Obliczenie współczynników konwersji\nwspolczynniki_konwersji &lt;- dane %&gt;%\n  group_by(Wersja) %&gt;%\n  summarise(\n    Odwiedzajacy = n(),\n    Konwersje = sum(Konwersja),\n    WspolczynnikKonwersji = mean(Konwersja)\n  )\n\n# Wizualizacja wyników\nggplot(wspolczynniki_konwersji, aes(x = Wersja, y = WspolczynnikKonwersji, fill = Wersja)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = sprintf(\"%.2f%%\", WspolczynnikKonwersji * 100)), \n            vjust = -0.5, size = 4) +\n  theme_minimal() +\n  labs(title = \"Test A/B: Współczynniki Konwersji Strony Docelowej\",\n       x = \"Wersja Strony\", y = \"Współczynnik Konwersji\") +\n  scale_y_continuous(labels = scales::percent, limits = c(0, 0.15)) +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\nFigure 10.1: Wyniki Testu A/B: Współczynniki Konwersji Strony Docelowej\n\n\n\n\n\nW tym przykładzie zasymulowaliśmy dane dla 10 000 odwiedzających losowo przypisanych do wersji A lub B strony docelowej. Wyniki pokazują, że wersja B ma nieco wyższy współczynnik konwersji (11,44%) w porównaniu do wersji A (10,94%).\n\n\n10.3.2 Metodologia Testów A/B\nTesty A/B zazwyczaj przebiegają według następujących kroków:\n\nZidentyfikowanie elementu do przetestowania (np. układ strony docelowej).\nStworzenie dwóch wersji: kontrolnej (A) i wariantu (B).\nLosowe przypisanie odwiedzających do jednej z wersji.\nZbieranie danych o interesującej nas metryce (np. współczynniku konwersji).\nAnaliza wyników przy użyciu metod statystycznych.\nPodjęcie decyzji na podstawie wyników.\n\n\n\n10.3.3 Różnice między Testami A/B a RCT\nChoć testy A/B i Randomizowane Badania Kontrolowane (RCT) mają pewne podobieństwa, istnieje kilka kluczowych różnic:\n\nZakres i Kontekst:\n\nTesty A/B: Zazwyczaj stosowane w środowiskach cyfrowych do szybkich, iteracyjnych ulepszeń.\nRCT: Stosowane w różnych dziedzinach, w tym medycynie, psychologii i naukach społecznych, często dla bardziej złożonych interwencji.\n\nCzas Trwania:\n\nTesty A/B: Zwykle krótsze, często trwające dni lub tygodnie.\nRCT: Mogą trwać miesiące lub lata, szczególnie w badaniach medycznych.\n\nWielkość Próby:\n\nTesty A/B: Mogą obejmować bardzo duże próby ze względu na łatwość implementacji na platformach cyfrowych.\nRCT: Wielkości prób są często mniejsze ze względu na praktyczne i kosztowe ograniczenia.\n\nZaślepienie:\n\nTesty A/B: Uczestnicy zazwyczaj nie są świadomi, że biorą udział w teście.\nRCT: Mogą obejmować pojedyncze, podwójne lub potrójne zaślepienie w celu zmniejszenia błędu systematycznego.\n\nWzględy Etyczne:\n\nTesty A/B: Generalnie obejmują zmiany niskiego ryzyka z minimalnymi obawami etycznymi.\nRCT: Często wymagają obszernej oceny etycznej, szczególnie w kontekście medycznym.\n\nMiary Wyników:\n\nTesty A/B: Zazwyczaj skupiają się na pojedynczym, łatwo mierzalnym wyniku (np. współczynnik klikalności).\nRCT: Często mierzą wiele wyników, w tym potencjalne skutki uboczne lub długoterminowe efekty.\n\nMożliwość Uogólnienia:\n\nTesty A/B: Wyniki są często specyficzne dla testowanej platformy lub kontekstu.\nRCT: Dążą do szerszej możliwości uogólnienia, choć może to się różnić.\n\nZłożoność Analizy:\n\nTesty A/B: Często wykorzystują prostsze analizy statystyczne.\nRCT: Mogą obejmować bardziej złożone metody statystyczne, aby uwzględnić różne czynniki.\n\n\nTesty A/B są potężnym narzędziem do podejmowania decyzji opartych na danych w środowiskach cyfrowych. Choć dzielą podstawową zasadę randomizacji z RCT, są zazwyczaj prostsze, szybsze i bardziej skoncentrowane na konkretnych, mierzalnych wynikach w kontekstach cyfrowych. Zrozumienie tych różnic pomaga badaczom i praktykom wybrać najbardziej odpowiednią metodę do ich konkretnych potrzeb i ograniczeń.\nTesty A/B są szczególnie przydatne w optymalizacji stron internetowych, aplikacji mobilnych i kampanii marketingowych, gdzie szybkie iteracje i ciągłe ulepszenia są kluczowe. Z kolei RCT pozostają złotym standardem w badaniach naukowych, szczególnie w dziedzinach takich jak medycyna, gdzie rygorystyczna kontrola i długoterminowa obserwacja są niezbędne.\nNiezależnie od wybranej metody, kluczowe jest staranne planowanie, precyzyjne wykonanie i ostrożna interpretacja wyników. Zarówno testy A/B, jak i RCT, gdy są odpowiednio stosowane, mogą dostarczyć cennych informacji i przyczynić się do podejmowania lepszych decyzji opartych na danych.\n\n\n10.3.4 Przykład 1: Wpływ Długości Snu na Wydajność Poznawczą\nPytanie Badawcze: Czy zwiększenie długości snu poprawia wydajność poznawczą u studentów?\n\n# Generowanie przykładowych danych\nset.seed(456)\nn &lt;- 100\npre_eksperymentalna &lt;- rnorm(n, mean = 70, sd = 10)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = 8, sd = 5)\npre_kontrolna &lt;- rnorm(n, mean = 70, sd = 10)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = 1, sd = 5)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Eksperymentalna\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  Wynik = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = Wynik, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wpływ Zwiększonej Długości Snu na Wydajność Poznawczą\") +\n  xlab(\"Czas\") +\n  ylab(\"Wynik Wydajności Poznawczej\")\n\n\n\n\n\n\n\nFigure 10.2: Wpływ Długości Snu na Wydajność Poznawczą\n\n\n\n\n\n\n10.3.4.1 Interpretacja\nTen wykres pokazuje wpływ zwiększonej długości snu na wydajność poznawczą. Grupa eksperymentalna, która zwiększyła długość snu, wykazuje znacznie większą poprawę w wydajności poznawczej w porównaniu do grupy kontrolnej. Sugeruje to, że zwiększenie długości snu może pozytywnie wpływać na zdolności poznawcze studentów.\n\n\n\n10.3.5 Przykład 2: Wpływ Treningu Uważności na Poziom Stresu\nPytanie Badawcze: Czy krótkoterminowy program treningu uważności może obniżyć poziom stresu u pracowników służby zdrowia?\n\n# Generowanie przykładowych danych\nset.seed(789)\nn &lt;- 120\npre_eksperymentalna &lt;- rnorm(n, mean = 60, sd = 15)\npost_eksperymentalna &lt;- pre_eksperymentalna + rnorm(n, mean = -12, sd = 8)\npre_kontrolna &lt;- rnorm(n, mean = 60, sd = 15)\npost_kontrolna &lt;- pre_kontrolna + rnorm(n, mean = -2, sd = 6)\n\ndane &lt;- data.frame(\n  Grupa = rep(c(\"Uważność\", \"Kontrolna\"), each = n*2),\n  Czas = rep(rep(c(\"Przed\", \"Po\"), each = n), 2),\n  PoziomStresu = c(pre_eksperymentalna, post_eksperymentalna, pre_kontrolna, post_kontrolna)\n)\n\n# Tworzenie wykresu\nggplot(dane, aes(x = Czas, y = PoziomStresu, color = Grupa, group = Grupa)) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  theme_minimal() +\n  ggtitle(\"Wpływ Treningu Uważności na Poziom Stresu\") +\n  xlab(\"Czas\") +\n  ylab(\"Poziom Stresu\")\n\n\n\n\n\n\n\nFigure 10.3: Wpływ Treningu Uważności na Poziom Stresu\n\n\n\n\n\n\n10.3.5.1 Interpretacja\nTa wizualizacja ilustruje wpływ programu treningu uważności na poziom stresu u pracowników służby zdrowia. Grupa uważności wykazuje znacznie większy spadek poziomu stresu w porównaniu do grupy kontrolnej. Sugeruje to, że program treningu uważności może być skuteczny w redukcji poziomu stresu wśród pracowników służby zdrowia.\n\n\n\n10.3.6 Projekty Czynnikowe\nProjekty czynnikowe pozwalają badaczom na jednoczesne badanie efektów wielu zmiennych niezależnych. Są one efektywne i mogą ujawniać efekty interakcji między zmiennymi.\nPrzykład projektu czynnikowego 2x2:\n\n# Tworzenie przykładowych danych dla projektu czynnikowego 2x2\nset.seed(456)\nn_per_group &lt;- 25\n\nfactorial_data &lt;- data.frame(\n  czynnik_a = rep(rep(c(\"Niski\", \"Wysoki\"), each = n_per_group), 2),\n  czynnik_b = rep(c(\"Kontrola\", \"Interwencja\"), each = n_per_group * 2),\n  wynik = NA\n)\n\n# Generowanie wyników\nfactorial_data$wynik &lt;- ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Kontrola\",\n                               rnorm(n_per_group, 40, 5),\n                               ifelse(factorial_data$czynnik_a == \"Niski\" & factorial_data$czynnik_b == \"Interwencja\",\n                                      rnorm(n_per_group, 45, 5),\n                                      ifelse(factorial_data$czynnik_a == \"Wysoki\" & factorial_data$czynnik_b == \"Kontrola\",\n                                             rnorm(n_per_group, 50, 5),\n                                             rnorm(n_per_group, 60, 5))))\n\n# Tworzenie wykresu\nggplot(factorial_data, aes(x = czynnik_b, y = wynik, fill = czynnik_a)) +\n  geom_boxplot() +\n  facet_wrap(~czynnik_a, scales = \"free_x\") +\n  labs(title = \"Projekt Czynnikowy 2x2\",\n       x = \"Czynnik B\", y = \"Wynik\", fill = \"Czynnik A\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\nProjekt Czynnikowy 2x2\n\n\n\n\nTen wykres ilustruje projekt czynnikowy 2x2, pokazując efekty dwóch czynników (A i B) na zmienną wynikową. Możemy zaobserwować główne efekty dla obu czynników oraz potencjalny efekt interakcji.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#projekty-nieeksperymentalne",
    "href": "rozdzial4.html#projekty-nieeksperymentalne",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.4 Projekty Nieeksperymentalne",
    "text": "10.4 Projekty Nieeksperymentalne\nProjekty nieeksperymentalne są stosowane, gdy randomizacja lub manipulacja zmiennymi nie jest możliwa lub etyczna. Obejmują one badania obserwacyjne/opisowe i quasi-eksperymentalne.\n\n10.4.1 Badania Obserwacyjne\nBadania obserwacyjne polegają na zbieraniu danych bez manipulowania zmiennymi. Są one przydatne do eksploracji relacji i generowania hipotez.\nPrzykład: Badanie korelacyjne\n\nset.seed(789)\nn &lt;- 100\nczas_nauki &lt;- runif(n, 0, 10)\nwynik_egzaminu &lt;- 50 + 5 * czas_nauki + rnorm(n, 0, 10)\n\ncorrelation_data &lt;- data.frame(czas_nauki, wynik_egzaminu)\n\nggplot(correlation_data, aes(x = czas_nauki, y = wynik_egzaminu)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Korelacja między Czasem Nauki a Wynikiem Egzaminu\",\n       x = \"Czas Nauki (godziny)\", y = \"Wynik Egzaminu\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nKorelacja między Czasem Nauki a Wynikiem Egzaminu\n\n\n\n\nTen wykres punktowy pokazuje relację między czasem nauki a wynikami egzaminu, ilustrując pozytywną korelację typową dla badań obserwacyjnych.\n\n\n10.4.2 Projekty Quasi-Eksperymentalne\nProjekty quasi-eksperymentalne nie mają losowego przydziału, ale próbują ustalić związki przyczynowe. Popularne typy to:\n\nRóżnica w Różnicach (DiD)\nRegresja Nieciągła (RDD)\n\n\n10.4.2.1 Różnica w Różnicach (DiD)\nDiD jest używana do oszacowania efektów interwencji poprzez porównanie średniej zmiany w czasie w zmiennej wynikowej dla grupy eksperymentalnej ze średnią zmianą w czasie dla grupy kontrolnej.\nPrzeprowadźmy symulację analizy DiD przy użyciu pakietu plm:\n\nlibrary(plm)\n\n\nAttaching package: 'plm'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    between, lag, lead\n\nlibrary(ggplot2)\n\n# Set seed for reproducibility\nset.seed(101)\n\n# Generate synthetic panel data\nn &lt;- 1000\ntime_periods &lt;- 5\nintervention_time &lt;- 3\n\npanel_data &lt;- data.frame(\n  id = rep(1:n, each = time_periods),\n  time = rep(1:time_periods, times = n),\n  treatment = rep(sample(c(0, 1), n, replace = TRUE), each = time_periods)\n)\n\n# Generate outcomes\npanel_data$outcome &lt;- with(panel_data,\n                           10 + 2 * time + 5 * treatment + \n                           3 * (time &gt;= intervention_time & treatment == 1) + \n                           rnorm(n * time_periods, 0, 2))\n\n# Create post-treatment indicator\npanel_data$post &lt;- as.integer(panel_data$time &gt;= intervention_time)\n\n# Estimate DiD model\ndid_model &lt;- plm(outcome ~ treatment * post, \n                 data = panel_data, \n                 index = c(\"id\", \"time\"), \n                 model = \"within\")\n\n# Summarize results\nsummary_did &lt;- summary(did_model)\n\n# Calculate group means for each time period\ngroup_means &lt;- aggregate(outcome ~ time + treatment, data = panel_data, FUN = mean)\n\n# Visualize DiD\nggplot(group_means, aes(x = time, y = outcome, color = factor(treatment), group = treatment)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  geom_vline(xintercept = intervention_time, linetype = \"dashed\", color = \"gray50\") +\n  labs(title = \"Difference-in-Differences Analysis\",\n       subtitle = paste(\"Estimated treatment effect:\", round(coef(did_model)[\"treatment:post\"], 3)),\n       x = \"Time\", y = \"Outcome\", color = \"Treatment Group\") +\n  theme_minimal() +\n  scale_color_brewer(palette = \"Set1\", labels = c(\"Control\", \"Treatment\")) +\n  scale_x_continuous(breaks = 1:time_periods)\n\n\n\n\n\n\n\n# Print model summary\nprint(summary_did)\n\nOneway (individual) effect Within Model\n\nCall:\nplm(formula = outcome ~ treatment * post, data = panel_data, \n    model = \"within\", index = c(\"id\", \"time\"))\n\nBalanced Panel: n = 1000, T = 5, N = 5000\n\nResiduals:\n     Min.   1st Qu.    Median   3rd Qu.      Max. \n-7.509908 -1.625814  0.001753  1.610009  8.047479 \n\nCoefficients:\n               Estimate Std. Error t-value  Pr(&gt;|t|)    \npost            5.05692    0.10315  49.026 &lt; 2.2e-16 ***\ntreatment:post  2.89003    0.14935  19.351 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTotal Sum of Squares:    78894\nResidual Sum of Squares: 26696\nR-Squared:      0.66163\nAdj. R-Squared: 0.57691\nF-statistic: 3908.68 on 2 and 3998 DF, p-value: &lt; 2.22e-16\n\n\nWykres pokazuje średnie wyniki dla grup interwencji i kontrolnej w czasie. Pionowa przerywana linia wskazuje punkt interwencji. Oszacowanie DiD to różnica między zmianami obu grup od okresu przed do po interwencji.\nDiD Model:\n\nThe model outcome ~ treatment * post estimates:\nThe average treatment effect on the treated (ATT) after the intervention\nThe coefficient on treatment:post represents this effect\n\nInterpretation of Results: Looking at the model summary:\n\nThe coefficient for treatment:post is the DiD estimator. It represents the average treatment effect on the treated after the intervention.\nIf this coefficient is statistically significant, it suggests that the treatment had a causal effect on the outcome.\nThe magnitude of this coefficient tells us the size of the treatment effect.\n\nVisualization: The plot shows:\n\nSeparate trend lines for the treatment and control groups\nA vertical dashed line indicating the intervention time\nThe parallel trends assumption can be visually assessed by looking at the pre-intervention period\nThe divergence of the lines after the intervention represents the treatment effect\n\nAssumptions and Limitations:\nIt’s important to note some key assumptions of DiD:\n\nParallel trends: In the absence of treatment, the difference between the treatment and control groups would remain constant over time.\nNo spillover effects: The treatment does not affect the control group.\nNo compositional changes: The composition of treatment and control groups remains stable over time.\n\n\n\n10.4.2.2 Regresja Nieciągła (RDD)\nRDD jest stosowana, gdy przydział do interwencji jest określony przez wartość graniczną na ciągłej zmiennej. Porównuje obserwacje tuż powyżej i poniżej punktu granicznego, aby oszacować efekt interwencji.\nPrzeprowadźmy analizę RDD przy użyciu pakietu rdrobust:\n\nlibrary(rdrobust)\n\n# Generowanie syntetycznych danych RDD\nset.seed(202)\nn &lt;- 1000\nx &lt;- runif(n, -1, 1)\ny &lt;- 3 + 2 * x + 4 * (x &gt;= 0) + rnorm(n, 0, 1)\n\nrdd_data &lt;- data.frame(x, y)\n\n# Analiza RDD\nrdd_result &lt;- rdrobust(y, x, c = 0)\nsummary(rdd_result)\n\nSharp RD estimates using local polynomial regression.\n\nNumber of Obs.                 1000\nBW type                       mserd\nKernel                   Triangular\nVCE method                       NN\n\nNumber of Obs.                  499          501\nEff. Number of Obs.             182          175\nOrder est. (p)                    1            1\nOrder bias  (q)                   2            2\nBW est. (h)                   0.362        0.362\nBW bias (b)                   0.575        0.575\nrho (h/b)                     0.630        0.630\nUnique Obs.                     499          501\n\n=============================================================================\n        Method     Coef. Std. Err.         z     P&gt;|z|      [ 95% C.I. ]       \n=============================================================================\n  Conventional     4.092     0.231    17.723     0.000     [3.640 , 4.545]     \n        Robust         -         -    15.013     0.000     [3.600 , 4.680]     \n=============================================================================\n\n# Wizualizacja RDD\nggplot(rdd_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(data = subset(rdd_data, x &lt; 0), method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_smooth(data = subset(rdd_data, x &gt;= 0), method = \"lm\", se = FALSE, color = \"green\") +\n  labs(title = \"Regresja Nieciągła\",\n       x = \"Zmienna Bieżąca\", y = \"Wynik\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAnaliza Regresji Nieciągłej\n\n\n\n\nWykres pokazuje nieciągłość w punkcie granicznym (x = 0), z oddzielnymi liniami regresji dopasowanymi po obu stronach. Efekt interwencji jest szacowany przez różnicę między tymi liniami w punkcie granicznym.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "rozdzial4.html#model-potencjalnych-wyników-neymana-rubina",
    "href": "rozdzial4.html#model-potencjalnych-wyników-neymana-rubina",
    "title": "10  Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne",
    "section": "10.5 Model Potencjalnych Wyników Neymana-Rubina",
    "text": "10.5 Model Potencjalnych Wyników Neymana-Rubina\nModel potencjalnych wyników Neymana-Rubina zapewnia formalne podejście do wnioskowania przyczynowego. Wprowadza on koncepcję potencjalnych wyników: dla każdej jednostki rozważamy wynik w warunkach interwencji i w warunkach kontrolnych, mimo że w rzeczywistości możemy zaobserwować tylko jeden z nich.\nKluczowe pojęcia:\n\nPotencjalne Wyniki: Y_i(1) i Y_i(0) odpowiednio dla interwencji i kontroli.\nObserwowany Wynik: Y_i = Y_i(1)T_i + Y_i(0)(1-T_i), gdzie T_i to wskaźnik interwencji.\nIndywidualny Efekt Interwencji: \\tau_i = Y_i(1) - Y_i(0)\nPrzeciętny Efekt Interwencji (ATE): E[\\tau_i] = E[Y_i(1) - Y_i(0)]\n\nModel podkreśla “fundamentalny problem wnioskowania przyczynowego”: nigdy nie możemy zaobserwować obu potencjalnych wyników dla pojedynczej jednostki jednocześnie.\n\n10.5.1 Przykład: Szacowanie ATE w RCT\nW RCT, losowy przydział zapewnia, że interwencja jest niezależna od potencjalnych wyników, umożliwiając nieobciążone oszacowanie ATE:\n\n\\hat{ATE} = \\frac{1}{n_1} \\sum_{i:T_i=1} Y_i - \\frac{1}{n_0} \\sum_{i:T_i=0} Y_i\n\nGdzie n_1 i n_0 to odpowiednio liczby jednostek w grupie interwencji i kontrolnej.\n\n# Używając danych RCT z wcześniejszego przykładu\nate_estimate &lt;- mean(data$post_test[data$grupa == \"Eksperymentalna\"]) - \n                mean(data$post_test[data$grupa == \"Kontrolna\"])\n\ncat(\"Oszacowany Przeciętny Efekt Interwencji:\", round(ate_estimate, 2))\n\nOszacowany Przeciętny Efekt Interwencji: 9.66",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Projekty Badawcze: Podejścia Eksperymentalne i Nieeksperymentalne</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "11.1 Introduction to Sigma Notation (Σ) | Wprowadzenie do Notacji Sigma (Σ)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-sigma-notation-σ-wprowadzenie-do-notacji-sigma-σ",
    "href": "chapter5.html#introduction-to-sigma-notation-σ-wprowadzenie-do-notacji-sigma-σ",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "",
    "text": "What is Sigma? | Co to jest notacja sumacyjna Sigma? Sigma (Σ) is a mathematical operator that tells us to sum (add up) a sequence of terms - it functions as an instruction to perform addition of all elements in a specified range. | Sigma (Σ) to operator matematyczny, który nakazuje nam zsumować (dodać) sekwencję wyrazów - działa jak instrukcja wykonania dodawania wszystkich elementów w określonym zakresie.\nPurpose: | Cel: Provides a compact way to write sums of many similar terms using a single symbol, avoiding lengthy addition expressions. | Zapewnia zwięzły sposób zapisu sum wielu podobnych wyrazów za pomocą jednego symbolu, unikając długich wyrażeń dodawania.\n\n\n11.1.1 Basic Formula | Podstawowa formuła\n\nThe general form of a sigma notation is: | Ogólna forma notacji sigma to:\n\n\\sum_{i=a}^{b} f(i)\n\nIndex of Summation: | Indeks sumowania: i\nLower Limit: | Dolna granica: a\nUpper Limit: | Górna granica: b\nFunction: | Funkcja: f(i)\n\n\n\n11.1.2 Simple Example | Prosty przykład\n\nConsider you want to add the first five positive integers: | Załóżmy, że chcesz dodać pierwsze pięć dodatnich liczb całkowitych:\n\n\\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\n\nAdds the first five positive integers. | Dodaje pierwsze pięć dodatnich liczb całkowitych.\n\n\n\n11.1.3 Example with a Function | Przykład z funkcją\n\nSuppose you want to sum the squares of the first four positive integers: | Załóżmy, że chcesz zsumować kwadraty pierwszych czterech dodatnich liczb całkowitych:\n\n\\sum_{i=1}^{4} i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 30\n\nSum of the squares of the first four positive integers. | Suma kwadratów pierwszych czterech dodatnich liczb całkowitych.\n\n\n\n11.1.4 Practical Application in Statistics | Praktyczne zastosowanie w statystyce\n\nCalculating the Mean: | Obliczanie średniej:\n\nData Points: | Punkty danych: x_1, x_2, ..., x_n\nMean \\bar{x}: | Średnia \\bar{x}:\n\n\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\nExample: | Przykład: x_1, x_2, x_3, x_4 are 4, 8, 15, 16 | x_1, x_2, x_3, x_4 to 4, 8, 15, 16\n\n\\bar{x} = \\frac{43}{4} = 10.75\n\n\n11.1.5 Benefits of Using Sigma Notation | Korzyści z używania notacji Sigma\n\nClarity: | Jasność: Provides a clear and concise representation of various statistical formulas. | Zapewnia jasne i zwięzłe przedstawienie statystycznych formuł.\n\n\n\n\n\n\n\nSummation (Σ) and Product (Π) Operators\n\n\n\n\n11.1.5.1 Sigma (Σ) Operator\n\\sum is a summation operator that instructs us to add terms:\n\\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n\nwhere: - i is the index variable - The lower value under Σ (here i=1) is the starting point - The upper value (here n) is the ending point\n\n\n11.1.5.2 Pi (Π) Operator\n\\prod is a product operator that instructs us to multiply terms:\n\\prod_{i=1}^{n} x_i = x_1 \\times x_2 \\times ... \\times x_n\nwhere: - i is the index variable - The lower value under Π (here i=1) is the starting point - The upper value (here n) is the ending point\n\n\n\n\n\n\n\n\n\nExample of Σ\n\n\n\n\\sum_{i=1}^{4} i = 1 + 2 + 3 + 4 = 10\n\n\n\n\n\n\n\n\nExample of Π\n\n\n\n\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\n\n\n\n\n\n\n\n\nKey Differences\n\n\n\n\nΣ represents repeated addition\nΠ represents repeated multiplication",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#types-of-data-distributions",
    "href": "chapter5.html#types-of-data-distributions",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.2 Types of Data Distributions",
    "text": "11.2 Types of Data Distributions\n\n\n\n\n\n\nImportant\n\n\n\nData distribution informs what values a variable takes and how often.\n\n\nUnderstanding data distributions is crucial for data analysis and visualization. In this document, we’ll explore various types of distributions and how to visualize them using ggplot2 in R.\n\n11.2.1 Normal Distribution\nThe normal distribution, also known as the Gaussian distribution, is symmetric and bell-shaped.\n\n# Generate normal distribution data\nnormal_data &lt;- data.frame(x = rnorm(1000))\n\n# Plot\nggplot(normal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Normal Distribution\", x = \"Value\", y = \"Density\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\n11.2.2 Uniform Distribution\nIn a uniform distribution, all values have an equal probability of occurrence.\n\n# Generate uniform distribution data\nuniform_data &lt;- data.frame(x = runif(1000))\n\n# Plot\nggplot(uniform_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Uniform Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n11.2.3 Skewed Distributions\nSkewed distributions are asymmetric, with one tail longer than the other.\n\n# Generate right-skewed data\nright_skewed &lt;- data.frame(x = rlnorm(1000))\n\n# Plot\nggplot(right_skewed, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Right-Skewed Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n11.2.4 Bimodal Distribution\nA bimodal distribution has two peaks, indicating two distinct subgroups in the data.\n\n# Generate bimodal data\nbimodal_data &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Plot\nggplot(bimodal_data, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Bimodal Distribution\", x = \"Value\", y = \"Density\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nKey Properties\nSocial Examples\n\n\n\n\nNormal\nSymmetric, bell-shaped, most values near mean\nHeight, IQ scores, standardized test scores\n\n\nUniform\nEqual probability across range\nBirth dates in year, arrival times in hour\n\n\nBimodal\nTwo peaks, suggests subgroups\nAge in college towns, polarized opinions\n\n\nLog-normal\nRight-skewed, cannot be negative\nIncome, house prices, social media followers\n\n\nPower law\nExtreme skew, “rich get richer”\nCity sizes",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#visualizing-real-world-data-distributions",
    "href": "chapter5.html#visualizing-real-world-data-distributions",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.3 Visualizing Real-World Data Distributions",
    "text": "11.3 Visualizing Real-World Data Distributions\nLet’s use the palmerpenguins dataset to explore real-world data distributions.\n\n11.3.1 Histogram and Density Plot\n\n\n\n\n\n\nUnderstanding Histograms and Density\n\n\n\n⭐ A histogram is a special graph for numerical data where:\n\nData is grouped into ranges (called “bins”)\nBars touch each other (unlike bar charts!) because the data is continuous\nEach bar’s height shows how many values fall into that range\n\nThink of density as showing how common or concentrated certain values are in your data:\n\nA higher point on a density curve (or taller bar in a histogram) means those values appear more frequently in your data\nA lower point means those values are less common\n\nJust like a crowded area has more people per space (higher density), a taller part of the graph shows values that appear more often in your dataset!\n\n\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Distribution of Penguin Flipper Lengths\", \n       x = \"Flipper Length (mm)\", \n       y = \"Density\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\n11.3.2 Box Plot\nBox plots are useful for comparing distributions across categories.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n11.3.3 Violin Plot\nViolin plots combine box plot and density plot features.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Distribution of Penguin Body Mass by Species\", \n       x = \"Species\", \n       y = \"Body Mass (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n11.3.4 Ridgeline Plot\nRidgeline plots are useful for comparing multiple distributions.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Distribution of Flipper Length by Penguin Species\",\n       x = \"Flipper Length (mm)\",\n       y = \"Species\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n\n\n11.3.5 Conclusion\nUnderstanding and visualizing data distributions is crucial in data analysis. ggplot2 provides a flexible and powerful toolkit for creating various types of distribution plots. By exploring different visualization techniques, we can gain insights into the underlying patterns and characteristics of our data.\n\n\n\n\n\n\nUnderstanding Different Types of Data Sets and Their Formats\n\n\n\n\n11.3.6 Cross-sectional Data\nObservations collected at a single point in time across multiple entities/individuals:\n\n\n\nIndividual\nAge\nIncome\nEducation\n\n\n\n\n1\n25\n50000\nBachelor’s\n\n\n2\n35\n75000\nMaster’s\n\n\n3\n45\n90000\nPhD\n\n\n\n\n\n11.3.7 Time Series Data\nObservations of a single entity tracked over multiple time points:\n\n\n\nYear\nGDP (in billions)\nUnemployment Rate\n\n\n\n\n2018\n20,580\n3.9%\n\n\n2019\n21,433\n3.7%\n\n\n2020\n20,933\n8.1%\n\n\n\n\n\n11.3.8 Panel Data (Longitudinal Data)\nObservations of multiple entities tracked over time:\n\n\n\nCountry\nYear\nGDP per capita\nLife Expectancy\n\n\n\n\nUSA\n2018\n62,794\n78.7\n\n\nUSA\n2019\n65,118\n78.8\n\n\nCanada\n2018\n46,194\n81.9\n\n\nCanada\n2019\n46,194\n82.0\n\n\n\n\n\n11.3.9 Time-series Cross-sectional (TSCS) Data\nA special case of panel data where:\n\nNumber of time points &gt; Number of entities\nSimilar structure to panel data but with emphasis on temporal depth\nCommon in political science and economics research\n\n\n\n11.3.10 Data Formats\n\n11.3.10.1 Wide Format\nEach row represents an entity; columns represent variables/time points:\n\n\n\nCountry\nGDP_2018\nGDP_2019\nLE_2018\nLE_2019\n\n\n\n\nUSA\n62,794\n65,118\n78.7\n78.8\n\n\nCanada\n46,194\n46,194\n81.9\n82.0\n\n\n\n\n\n11.3.10.2 Long Format\nEach row represents a unique entity-time-variable combination:\n\n\n\nCountry\nYear\nVariable\nValue\n\n\n\n\nUSA\n2018\nGDP per capita\n62,794\n\n\nUSA\n2019\nGDP per capita\n65,118\n\n\nUSA\n2018\nLife Expectancy\n78.7\n\n\nUSA\n2019\nLife Expectancy\n78.8\n\n\nCanada\n2018\nGDP per capita\n46,194\n\n\nCanada\n2019\nGDP per capita\n46,194\n\n\nCanada\n2018\nLife Expectancy\n81.9\n\n\nCanada\n2019\nLife Expectancy\n82.0\n\n\n\nNote: Long format is generally preferred for:\n\nData manipulation in R and Python\nStatistical analysis\nData visualization\nMixed-effects modeling\nRepeated measures analyses",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-outliers",
    "href": "chapter5.html#understanding-outliers",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.4 Understanding Outliers",
    "text": "11.4 Understanding Outliers\nBefore diving into specific measures, it’s crucial to understand the concept of outliers, as they can significantly impact many descriptive statistics.\nOutliers are data points that differ significantly from other observations in the dataset. They can occur due to:\n\nMeasurement or recording errors\nGenuine extreme values in the population\nSampling from a different population\n\nOutliers can have a substantial effect on many statistical measures, especially those based on means or sums of squared deviations. Therefore, it’s essential to:\n\nIdentify outliers through both statistical methods and domain knowledge\nInvestigate the cause of outliers\nMake informed decisions about whether to include or exclude them in analyses\n\nThroughout this guide, we’ll discuss how different descriptive measures are affected by outliers.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#statistical-symbols-and-notations---summary",
    "href": "chapter5.html#statistical-symbols-and-notations---summary",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.5 Statistical Symbols and Notations - Summary",
    "text": "11.5 Statistical Symbols and Notations - Summary\n\n\n\n\n\n\n\n\n\n\nMeasure\nPopulation Parameter\nSample Statistic\nAlternative Notations\nUsage Notes\n\n\n\n\nSize\nN\nn\n-\nTotal count of observations\n\n\nMean\n\\mu\n\\bar{x}, m\nM, E(X)\nE(X) used in probability theory\n\n\nVariance\n\\sigma^2\ns^2\n\\text{Var}(X), V(X)\nSquared deviations from mean\n\n\nStandard Deviation\n\\sigma\ns\n\\text{SD}, \\text{std}\nSquare root of variance\n\n\nProportion\n\\pi, P\n\\hat{p}\n\\text{prop}\nRelative frequencies\n\n\nCorrelation\n\\rho\nr\n\\text{corr}(x,y)\nRanges from -1 to +1\n\n\nStandard Error\n\\sigma_{\\bar{x}}\ns_{\\bar{x}}\n\\text{SE}\nStandard error of mean\n\n\nSum\n\\sum\n\\sum\n\\sum_{i=1}^n\nWith indexing\n\n\nIndividual Value\nX_i\nx_i\n-\nith observation\n\n\nCovariance\n\\sigma_{xy}\ns_{xy}\n\\text{Cov}(X,Y)\nJoint variation\n\n\nMedian\n\\eta\n\\text{Med}\nM\nCentral value\n\n\nRange\nR\nr\n\\text{max}(X) - \\text{min}(X)\nSpread measure\n\n\nMode\n\\text{Mo}\n\\text{mo}\n\\text{mod}\nMost frequent value\n\n\nSkewness\n\\gamma_1\ng_1\n\\text{SK}\nDistribution asymmetry\n\n\nKurtosis\n\\gamma_2\ng_2\n\\text{KU}\nDistribution peakedness\n\n\n\nAdditional useful notations:\n\nSample moments: m_k = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^k\nPopulation moments: \\mu_k = E[(X - \\mu)^k]\nPopulation standard error: \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\nSample standard error: s_{\\bar{x}} = \\frac{s}{\\sqrt{n}}",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-central-tendency",
    "href": "chapter5.html#measures-of-central-tendency",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.6 Measures of Central Tendency",
    "text": "11.6 Measures of Central Tendency\nMeasures of central tendency aim to identify the “typical” or “central” value in a dataset. The three primary measures are mean, median, and mode.\n\n11.6.1 Arithmetic Mean\nThe arithmetic mean is the sum of all values divided by the number of values.\nFormula: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\nImportant Property: The mean is a balancing point in the data. The sum of deviations from the mean is always zero:\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0\nThis property makes the mean useful in many statistical calculations.\n\n\n\n\n\n\nUnderstanding Mean as a Balance Point 🎯\n\n\n\nLet’s consider a dataset X = \\{1, 2, 6, 7, 9\\} on a number line, imagining it as a seesaw:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\nThe mean (\\mu) acts as the perfect balance point of this seesaw. For our data:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\n11.6.2 What happens at different support points? 🤔\n\nSupport point at 6 (too high):\n\nLeft side: Values (1, 2) are below\nRight side: Values (7, 9) are above\n\\sum distances from left = (6-1) + (6-2) = 9\n\\sum distances from right = (7-6) + (9-6) = 4\nThe seesaw tilts left! ⬅️ because 9 &gt; 4\n\nSupport point at 4 (too low):\n\nLeft side: Values (1, 2) are below\nRight side: Values (6, 7, 9) are above\n\\sum distances from left = (4-1) + (4-2) = 5\n\\sum distances from right = (6-4) + (7-4) + (9-4) = 10\nThe seesaw tilts right! ➡️ because 5 &lt; 10\n\nSupport point at mean (5) (perfect balance):\n\n\\sum distances below = \\sum distances above\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ✨ Perfect balance!\n\n\nThis shows why the mean is the unique balance point, where:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nThe seesaw will always tilt unless the support point is placed exactly at the mean! 🎪\n\n\n\n\n\n\n\n\n\nMean as a Balance Point\n\n\n\nThis visualization shows how the arithmetic mean (5) acts as a balance point between clustered points on the left and dispersed points on the right:\nLeft side of the mean: - Points with values 2 and 3 - Close together (difference of 1 unit) - Distances from mean: 3 and 2 units - Sum of “pull” = 5 units\nRight side of the mean: - Points with values 6 and 9 - More spread out (difference of 3 units) - Distances from mean: 1 and 4 units - Sum of “pull” = 5 units\nKey observations:\n\nThe mean (5) is a balance point, even though:\n\nPoints on the left are clustered (2,3)\nPoints on the right are dispersed (6,9)\nGreen arrows show distances from the mean\n\nBalance is maintained because:\n\nSum of distances balances out: (5-2) + (5-3) = (6-5) + (9-5)\nTotal sum of distances = 5 units on each side\n\n\n\n\n\n\n\n\n\n\n\n\n\nManual Calculation Example:\nLet’s calculate the mean for the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nSum all values\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nCount the number of values\nn = 7\n\n\n3\nDivide the sum by n\n36 / 7 = 5.14\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(data)\n\n[1] 5.142857\n\n\nPros:\n\nEasy to calculate and understand\nUses all data points\nUseful for further statistical calculations\n\nCons:\n\nSensitive to outliers\nNot ideal for skewed distributions\n\nExample with outlier:\n\ndata_with_outlier &lt;- c(2, 4, 4, 5, 5, 7, 100)\nmean(data_with_outlier)\n\n[1] 18.14286\n\n\nAs we can see, the outlier (100) drastically affects the mean.\n\n\n11.6.3 Median\nThe median is the middle value when the data is ordered.\nManual Calculation Example:\nUsing the same dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nResult\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind the middle value\n5\n\n\n\nFor even number of values, take the average of the two middle values.\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmedian(data)\n\n[1] 5\n\nmedian(data_with_outlier)\n\n[1] 5\n\n\nPros:\n\nNot affected by extreme outliers\nBetter for skewed distributions\n\nCons:\n\nDoesn’t use all data points\nLess useful for further statistical calculations\n\n\n\n\n\n\n\nWarning\n\n\n\nTo find the position of the median in a dataset:\n\nFirst sort the data in ascending order\nIf n is odd:\n\nMedian position = \\frac{n + 1}{2}\n\nIf n is even:\n\nFirst median position = \\frac{n}{2}\nSecond median position = \\frac{n}{2} + 1\nMedian = \\frac{\\text{value at }\\frac{n}{2} + \\text{value at }(\\frac{n}{2}+1)}{2}\n\n\nFor example:\n\nOdd n=7: position = \\frac{7+1}{2} = 4th value\nEven n=8: positions = \\frac{8}{2} = 4th and 4+1 = 5th value\n\n\n\n\n\n11.6.4 Mode\nThe mode is the most frequently occurring value.\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nValue\nFrequency\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nThe mode is 4 and 5 (bimodal).\nR calculation:\n\nlibrary(modeest)\nmfv(data)  # Most frequent value\n\n[1] 4 5\n\n\nPros:\n\nOnly measure of central tendency for nominal data\nCan identify multiple peaks in the data\n\nCons:\n\nNot always uniquely defined\nNot useful for continuous data\n\n\n\n11.6.5 Weighted (arithmetic) Mean (*)\nThe weighted mean is used when some data points are more important than others. There are two types of weighted means: with not normalized weights and with normalized weights.\n\n11.6.5.1 Weighted Mean with Not Normalized Weights\nThis is the standard form of the weighted mean, where weights can be any positive numbers representing the importance of each data point.\nFormula: \\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\nManual Calculation Example:\nLet’s calculate the weighted mean for the dataset: 2, 4, 5, 7 with weights 1, 2, 3, 1\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 1) + (4 * 2) + (5 * 3) + (7 * 1) = 2 + 8 + 15 + 7 = 32\n\n\n2\nSum the weights\n1 + 2 + 3 + 1 = 7\n\n\n3\nDivide the result from step 1 by the result from step 2\n32 / 7 = 4.57\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\n11.6.5.2 Weighted Mean with Normalized Weights (Fractions)\nIn this case, the weights are fractions that sum to 1, representing the proportion of importance for each data point.\nFormula: \\bar{x}_w = \\sum_{i=1}^n w_i x_i, where \\sum_{i=1}^n w_i = 1\nManual Calculation Example:\nLet’s calculate the weighted mean for the dataset: 2, 4, 5, 7 with normalized weights 0.1, 0.3, 0.4, 0.2\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nMultiply each value by its weight\n(2 * 0.1) + (4 * 0.3) + (5 * 0.4) + (7 * 0.2)\n\n\n2\nSum the results\n0.2 + 1.2 + 2.0 + 1.4 = 4.8\n\n\n\nR calculation:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Note: these sum to 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nPros of Weighted Means:\n\nAccount for varying importance of data points\nUseful in survey analysis with different sample sizes or importance levels\nCan adjust for unequal probabilities in sampling designs\n\nCons of Weighted Means:\n\nRequire justification for weights\nCan be misused to manipulate results\nMay be less intuitive to interpret than simple arithmetic mean",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-variability",
    "href": "chapter5.html#measures-of-variability",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.7 Measures of Variability",
    "text": "11.7 Measures of Variability\nThese measures describe how spread out the data is. They are crucial for understanding the dispersion of data points around the central tendency.\n\n\n\n\n\n\nUnderstanding Variance\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.1: Three dot plots showing increasing variance with constant mean\n\n\n\n\n\nThe three dot plots above demonstrate how variance measures the spread of data around a central value:\n\nAll distributions have the same mean (μ = 10), shown by the dashed line\nLow Variance (σ² = 1): Points cluster tightly around the mean\nMedium Variance (σ² = 4): Points show moderate spread\nHigh Variance (σ² = 9): Points spread widely around the mean\n\n\n\n\n\n\n\n\n\nUnderstanding Different Levels of Variability\n\n\n\n\n\n\n\n\n\n\n\n\nThis visualization shows three normal distributions with the same mean (μ = 10) but different levels of variability:\n\nLow Variability (σ = 0.5)\n\nData points cluster tightly around the mean\nThe density curve is tall and narrow\nMost observations fall within ±0.5 units of the mean\n\nMedium Variability (σ = 2.0)\n\nData points spread out more from the mean\nThe density curve is lower and wider\nMost observations fall within ±2 units of the mean\n\nHigh Variability (σ = 4.0)\n\nData points spread widely from the mean\nThe density curve is much flatter and wider\nMost observations fall within ±4 units of the mean\n\n\n\n\n\n11.7.1 Range\nThe range is the difference between the maximum and minimum values.\nFormula: R = x_{max} - x_{min}\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nFind the maximum value\n9\n\n\n2\nFind the minimum value\n2\n\n\n3\nSubtract minimum from maximum\n9 - 2 = 7\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nrange(data)\n\n[1] 2 9\n\nmax(data) - min(data)\n\n[1] 7\n\n\nPros:\n\nSimple to calculate and understand\nGives an immediate sense of data spread\n\nCons:\n\nExtremely sensitive to outliers\nDoesn’t provide information about the distribution between extremes\n\n\n\n11.7.2 Interquartile Range (IQR)\nThe IQR is the difference between the 75th and 25th percentiles.\nFormula: IQR = Q_3 - Q_1\nTo find quartiles manually:\n\nFor odd number of values:\n\nQ2 (median) is the middle value\nQ1 is the median of the lower half (excluding the median of all observations)\nQ3 is the median of the upper half (excluding the median of all observations)\n\nFor even number of values:\n\nQ2 is the average of the two middle values\nQ1 is the median of the lower half (excluding the median of all observations)\nQ3 is the median of the upper half (excluding the median of all observations)\n\n\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nOrder the data\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nFind Q2 (median)\n5\n\n\n3\nFind Q1 (median of lower half)\n4\n\n\n4\nFind Q3 (median of upper half)\n7\n\n\n5\nCalculate IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nR calculation:\n\ndata &lt;- c(2, 4, 4, 5, 5, 7, 9)\nprint(data)\n\n[1] 2 4 4 5 5 7 9\n\nquantile(data, type = 1)\n\n  0%  25%  50%  75% 100% \n   2    4    5    7    9 \n\nIQR(data, type = 1)\n\n[1] 3\n\n\nPros:\n\nRobust to outliers\nProvides information about the spread of the middle 50% of the data\n\nCons:\n\nIgnores the tails of the distribution\nLess efficient than standard deviation for normal distributions\n\n\n\n11.7.3 Variance\nVariance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\n\n\n\n\n\nVariance: Understanding Average Squared Deviations\n\n\n\nWhat is Variance? Variance measures how “spread out” numbers are from their mean - it’s the average of squared deviations from the mean.\nFormula: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nSimple Example: Consider numbers: 2, 4, 6, 8, 10 Mean (\\bar{x}) = 6\n\n\n\n\n\n\n\n\n\nCalculating Deviations:\n\n\n\n\n\n\n\n\n\n\n\n\nValue\nDeviation from mean\nSquare of deviation\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n+2\n4\n\n\n10\n+4\n16\n\n\n\nVariance = \\frac{16 + 4 + 0 + 4 + 16}{4} = 10\nKey Points:\n\nMean acts as a reference line (blue dashed line)\nDeviations show distance from mean (red dotted lines)\nSquaring makes all deviations positive (blue bars)\nLarger deviations contribute more to variance\n\n\n\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\bar{x} = 5.14\n\n\n2\nSubtract the mean from each value and square the result\n(2 - 5.14)^2 = 9.86\n\n\n\n\n(4 - 5.14)^2 = 1.30\n\n\n\n\n(4 - 5.14)^2 = 1.30\n\n\n\n\n(5 - 5.14)^2 = 0.02\n\n\n\n\n(5 - 5.14)^2 = 0.02\n\n\n\n\n(7 - 5.14)^2 = 3.46\n\n\n\n\n(9 - 5.14)^2 = 14.90\n\n\n3\nSum the squared differences\n30.86\n\n\n4\nDivide by (n-1), i.e. by the number of observations - 1\n30.86 / 6 = 5.14\n\n\n\nR calculation:\n\nvar(data)\n\n[1] 5.142857\n\n\nPros:\n\nUses all data points\nFoundation for many statistical tests\n\nCons:\n\nUnits are squared, making interpretation less intuitive\nSensitive to outliers\n\n\n\n\n\n\n\nBessel’s Correction: Why We Divide by (n-1) And Not by n\n\n\n\nThe Key Insight:\nWhen we calculate deviations from the mean, they must sum to zero. This is a mathematical fact: \\sum(x_i - \\bar{x}) = 0\nThink of it Like This:\nIf you have 5 numbers and their mean:\n\nOnce you calculate 4 deviations from the mean\nThe 5th deviation MUST be whatever makes the sum zero\nYou don’t really have 5 independent deviations\nYou only have 4 truly “free” deviations\n\nSimple Example:\nNumbers: 2, 4, 6, 8, 10\n\nMean = 6\nDeviations: -4, -2, 0, +2, +4\nNotice they sum to zero\nIf you know any 4 deviations, the 5th is predetermined!\n\nThis is Why:\n\nWhen calculating variance: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nWe divide by (n-1) not n\nBecause only (n-1) deviations are truly independent\nThe last one is determined by the others\n\nDegrees of Freedom:\n\nn = number of observations\n1 = constraint (deviations must sum to zero)\nn-1 = degrees of freedom = number of truly independent deviations\n\nWhen to Use It:\n\nWhen calculating sample variance\nWhen calculating sample standard deviation\n\nWhen NOT to Use It:\n\nPopulation calculations (when you have all data)\n\nRemember:\n\nIt’s not just a statistical trick\nDeviations from the mean must sum to zero\nThis constraint costs us one degree of freedom\n\n\n\n\n\n11.7.4 Standard Deviation\nThe standard deviation is the square root of the variance and measures the average dispersion of the data about their arithmetic mean. In contrast to the variance, it has the advantage of being expressed in the same units as the original measurements, making its interpretation more intuitive.\nFormula: s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the variance\ns^2 = 5.14 (from previous calculation)\n\n\n2\nTake the square root\ns = \\sqrt{5.14} = 2.27\n\n\n\nR calculation:\n\nsd(data)\n\n[1] 2.267787\n\n\nPros:\n\nIn same units as original data\nWidely used and understood\n\nCons:\n\nStill sensitive to outliers\nAssumes data is roughly “normally” distributed\n\n\n\n11.7.5 Coefficient of Variation (*)\nThe coefficient of variation is the standard deviation divided by the mean, often expressed as a percentage.\nFormula: CV = \\frac{s}{\\bar{x}} \\times 100\\%\nManual Calculation Example:\nUsing the dataset: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate the mean\n\\bar{x} = 5.14\n\n\n2\nCalculate the standard deviation\ns = 2.27\n\n\n3\nDivide s by the mean and multiply by 100\n(2.27 / 5.14) * 100 = 44.16\\%\n\n\n\nR calculation:\n\n(sd(data) / mean(data)) * 100\n\n[1] 44.09586\n\n\nPros:\n\nAllows comparison of variability between datasets with different units or means\nUseful in fields like finance for risk assessment\n\nCons:\n\nNot meaningful for data with both positive and negative values\nCan be misleading when mean is close to zero\n\n\n\n\n\n\n\nLimitations of Coefficient of Variation (CV)\n\n\n\nThe coefficient of variation, calculated as (σ/μ) × 100\\%, has two important limitations:\n\n11.7.5.1 Not meaningful for data with both positive and negative values\n\nThe mean could be close to zero due to positive and negative values cancelling out\nExample: Dataset {-5, -3, 2, 6} has mean = 0\n\nCV = (std dev / 0) × 100%\nThis leads to division by zero\nEven if mean isn’t exactly zero, the CV doesn’t represent true relative variability when data cross zero\n\nThe CV assumes a natural zero point and meaningful ratios between values\n\n\n\n11.7.5.2 Misleading when mean is close to zero\n\nSince CV = (σ/μ) × 100\\%, as μ approaches zero:\n\nThe denominator becomes very small\nResults in extremely large CV values\nThese large values don’t meaningfully represent relative variability\n\nExample:\n\nDataset A: {0.001, 0.002, 0.003} has mean = 0.002\nEven small standard deviations will produce very large CVs\nThe resulting large CV might suggest extreme variability when the data are actually quite close together\n\n\n\n\n11.7.5.3 Best Use Cases\nCV is most useful for:\n\nStrictly positive data\nData measured on a ratio scale\nData with means well above zero\nComparing variability between datasets with different units or scales",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-relative-position-standing",
    "href": "chapter5.html#measures-of-relative-position-standing",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.8 Measures of Relative Position (Standing)",
    "text": "11.8 Measures of Relative Position (Standing)\nUnderstanding where values sit within a dataset is crucial for data analysis. Let’s explore these concepts step by step.\n\n11.8.1 Quartiles (Q): The Basics\nThink of quartiles as special numbers that split your ordered data into four equal parts.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\n11.8.1.1 What Are Quartiles?\nFirst Quartile (Q1):\n\nSeparates the lowest 25% of data from the rest\nAlso called the 25th percentile\nExample: If Q1 = 50 in a test score dataset, 25% of students scored below 50\n\nSecond Quartile (Q2):\n\nThe median - splits data in half\nAlso called the 50th percentile\nExample: If Q2 = 70, half the students scored below 70\n\nThird Quartile (Q3):\n\nSeparates the highest 25% of data from the rest\nAlso called the 75th percentile\nExample: If Q3 = 85, 75% of students scored below 85\n\n\n\n11.8.1.2 How to Calculate Quartiles (Step by Step) - Two Methods\nLet’s examine student test scores using both common quartile calculation methods:\nExample 1: Odd Number Case (11 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 11 values (odd)\nMedian position = (n + 1)/2 = 6\nQ2 = 78\n\nStep 2: Find Q1\n\nTukey’s Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3rd value)\n\n\nStep 3: Find Q3\n\nTukey’s Method:\n\nLook at upper half: 80, 82, 85, 88, 90\nQ3 = median of upper half = 85\n\nInterpolation Method:\n\nPosition = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9th value)\n\n\nExample 2: Even Number Case (10 scores)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nStep 1: Find Q2 (median) - Same for both methods\n\nWith n = 10 values (even)\nMedian positions = 5 and 6\nQ2 = (75 + 78)/2 = 76.5\n\nStep 2: Find Q1\n\nTukey’s Method:\n\nLook at lower half: 60, 65, 70, 72, 75\nQ1 = median of lower half = 70\n\nInterpolation Method:\n\nPosition = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nStep 3: Find Q3\n\nTukey’s Method:\n\nLook at upper half: 78, 80, 82, 85, 90\nQ3 = median of upper half = 82\n\nInterpolation Method:\n\nPosition = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nImportant Notes:\n\nTukey’s Method:\n\nFirst find the median (Q2)\nSplit the data into lower and upper halves\nFind Q1 as the median of the lower half\nFind Q3 as the median of the upper half\nWhen n is odd, the median is not included in either half\n\nInterpolation Method:\n\nUses positions (n+1)/4 for Q1 and 3(n+1)/4 for Q3\nWhen position falls between values, uses linear interpolation\nDoesn’t require splitting data into halves\n\n\nBoth methods give the same results for simple positions (Example 1) but can differ when interpolation is needed (Example 2).\n\n\n\n\n\n\nManual Construction of Tukey Boxplot\n\n\n\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\n\n\n\n11.8.2 Percentiles: A More Precise Measure of Relative Standing (*)\n\n11.8.2.1 What Are Percentiles?\nPercentiles give us a more detailed view by dividing data into 100 equal parts. Unlike quartiles, percentiles use linear interpolation for more precise measurements.\nKey Points:\n\nThe 25th percentile equals Q1\nThe 50th percentile equals Q2 (median)\nThe 75th percentile equals Q3\n\n\n\n11.8.2.2 Calculating Percentiles\nThe Formula: P_k = \\frac{k(n+1)}{100}\nWhere:\n\nP_k is the position for the kth percentile\nk is the percentile we want (1-100)\nn is the number of observations\n\nExample 3: Finding the 60th Percentile Let’s use student homework scores: 72, 75, 78, 80, 82, 85, 88, 90, 92, 95\nStep 1: Calculate position\n\nn = 10 scores\nFor 60th percentile: P_{60} = \\frac{60(10+1)}{100} = 6.6\n\nStep 2: Find surrounding values\n\nPosition 6: score of 85\nPosition 7: score of 88\n\nStep 3: Interpolate (important: percentiles use linear interpolation)\n\nWe need to go 0.6 of the way between 85 and 88 P_{60} = 85 + 0.6(88-85) P_{60} = 85 + 0.6(3) P_{60} = 85 + 1.8 = 86.8\n\nWhat this means: 60% of students scored 86.8 or below.\n\n\n\n11.8.3 Percentile Ranks (PR) (*)\n\n11.8.3.1 What is a Percentile Rank?\nWhile percentiles tell us the value at a certain position, percentile rank tells us what percentage of values fall below a specific score. Think of it as answering the question “What percentage of the class did I score higher than?”\nPR = \\frac{\\text{number of values below } + 0.5 \\times \\text{number of equal values}}{\\text{total number of values}} \\times 100\nExample 4: Finding a Percentile Rank Consider these exam scores:\n65, 70, 70, 75, 75, 75, 80, 85, 85, 90\nLet’s find the PR for a score of 75.\nStep 1: Count carefully\n\nValues below 75: 65, 70, 70 (3 values)\nValues equal to 75: 75, 75, 75 (3 values)\nTotal values: 10\n\nStep 2: Apply the formula\nPR = \\frac{3 + 0.5(3)}{10} \\times 100 PR = \\frac{3 + 1.5}{10} \\times 100 PR = \\frac{4.5}{10} \\times 100 = 45\\%\nInterpretation: A score of 75 is higher than 45% of the class scores.\nRemark:\nQ1: “Why do we use 0.5 for equal values in PR?”\nA1: This is because we’re assuming people with the same score are evenly spread across that position. It’s like saying they share the position equally.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#measures-of-shape",
    "href": "chapter5.html#measures-of-shape",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.9 Measures of Shape",
    "text": "11.9 Measures of Shape\n\n11.9.1 Skewness\n\n11.9.1.1 Definition\nSkewness quantifies the asymmetry of a data distribution. It indicates whether the data clusters more on one side of the mean than the other.\n\n\n11.9.1.2 Mathematical Expression\nSK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3\nwhere:\n\nn is the sample size\nx_i is the i-th observation\n\\bar{x} is the sample mean\ns is the sample standard deviation\n\n\n\n11.9.1.3 Example: Voter Turnout Analysis\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n# Generate example precinct-level voter turnout data\nset.seed(123)\nturnout_data &lt;- c(\n  # Urban precincts\n  rnorm(300, mean = 65, sd = 12),\n  # Suburban precincts\n  rnorm(400, mean = 70, sd = 10),\n  # Rural precincts\n  rnorm(300, mean = 68, sd = 15)\n) |&gt; \n  # Ensure turnout stays between 0-100%\n  pmax(0) |&gt; \n  pmin(100)\n\n# Calculate and visualize\nskew_value &lt;- skewness(turnout_data)\nskew_value\n\n[1] 0.02558143\n\nggplot(data.frame(x = turnout_data), aes(x = x)) +\n  geom_histogram(bins = 50, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = mean(turnout_data), color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = median(turnout_data), color = \"blue\", linetype = \"dashed\") +\n  labs(\n    title = str_glue(\"Precinct-Level Voter Turnout Distribution (Skewness = {round(skew_value, 4)})\"),\n    subtitle = \"Red: Mean, Blue: Median\",\n    x = \"Voter Turnout (%)\",\n    y = \"Number of Precincts\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n11.9.1.4 Interpretation Guide\n\nPositive Skewness (&gt; 0): Distribution has a longer right tail\nNegative Skewness (&lt; 0): Distribution has a longer left tail\nZero Skewness: Approximately symmetric distribution\n\n\n\n\n11.9.2 Kurtosis\n\n\n11.9.3 Definition\nKurtosis measures the “tailedness” of a distribution, indicating the presence of extreme values relative to a normal distribution.\n\n11.9.3.1 Mathematical Expression\nK = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\n\n11.9.3.2 Example: Legislative Voting Analysis\n\n# Generate example legislative voting agreement scores\nset.seed(456)\nvoting_agreement &lt;- c(\n  # Regular voting patterns\n  rnorm(400, mean = 75, sd = 10),\n  # Cross-party cooperation instances\n  rnorm(80, mean = 50, sd = 15),\n  # Party-line votes\n  rnorm(20, mean = 95, sd = 5)\n) |&gt; \n  pmax(0) |&gt; \n  pmin(100)\n\nkurt_value &lt;- kurtosis(voting_agreement)\nkurt_value\n\n[1] 3.849939\n\n# Visualization with normal distribution comparison\nx_range &lt;- seq(min(voting_agreement)-1, max(voting_agreement)+1, length.out = 100)\nnormal_dist &lt;- dnorm(x_range, mean = mean(voting_agreement), sd = sd(voting_agreement))\n\nggplot() +\n  geom_density(\n    data = data.frame(x = voting_agreement), \n    aes(x = x), \n    fill = \"skyblue\", \n    alpha = 0.5\n  ) +\n  geom_line(\n    data = data.frame(x = x_range, y = normal_dist),\n    aes(x = x, y = y),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) +\n  labs(\n    title = str_glue(\"Legislative Voting Agreement Distribution (Kurtosis = {round(kurt_value, 4)})\"),\n    subtitle = \"Observed distribution (blue) vs. Normal distribution (red)\",\n    x = \"Voting Agreement Score (%)\",\n    y = \"Density\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n11.9.3.3 Interpretation Guide\n\nExcess Kurtosis: Difference from normal distribution’s kurtosis\n\n\n\nLeptokurtic (&gt; 3): More extreme values than normal distribution\nPlatykurtic (&lt; 3): Fewer extreme values than normal distribution\nMesokurtic (= 3): Similar to normal distribution",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "href": "chapter5.html#exercise-1.-center-and-dispersion-of-data",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.10 Exercise 1. Center and dispersion of data",
    "text": "11.10 Exercise 1. Center and dispersion of data\n\n11.10.1 Data\nWe have salary data (in thousands of euros) from two small European companies:\n\n\n\nIndex\nCompany X\nCompany Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\nThis table presents the data for both Company X and Company Y side by side, with an index column for easy reference.\n\n\n11.10.2 Measures of Central Tendency\n\n11.10.2.1 Mean\nThe mean is the average of all values in a dataset.\nFormula: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\n11.10.2.1.1 Manual Calculation for Company X\n\n\n\nValue (x_i)\nFrequency (f_i)\nx_i \\cdot f_i\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nTotal\nn = 20\nSum = 119\n\n\n\n\\bar{x} = \\frac{119}{20} = 5.95\n\n\n11.10.2.1.2 Manual Calculation for Company Y\n\n\n\nValue (x_i)\nFrequency (f_i)\nx_i \\cdot f_i\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nTotal\nn = 20\nSum = 100\n\n\n\n\\bar{y} = \\frac{100}{20} = 5\n\n\n11.10.2.1.3 R Verification\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\n11.10.2.2 Median\nThe median is the middle value when the data is ordered.\n\n11.10.2.2.1 Manual Calculation for Company X\nOrdered data: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\frac{4 + 4}{2} = 4\n\n\n11.10.2.2.2 Manual Calculation for Company Y\nOrdered data: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\nn = 20 (even), so we take the average of the 10th and 11th values:\nMedian = \\frac{5 + 5}{2} = 5\n\n\n11.10.2.2.3 R Verification\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\n11.10.2.3 Mode\nThe mode is the most frequent value in the dataset.\nFor Company X, the mode is 3 (appears 6 times). For Company Y, there are two modes: 4 and 5 (both appear 6 times).\n\n# Function to calculate mode\nget_mode &lt;- function(x) {\n  unique_x &lt;- unique(x)\n  unique_x[which.max(tabulate(match(x, unique_x)))]\n}\n\nget_mode(X)\n\n[1] 3\n\nget_mode(Y)\n\n[1] 4\n\n\n\n\n\n11.10.3 Measures of Dispersion\n\n11.10.3.1 Variance\nThe variance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\n11.10.3.1.1 Manual Calculation for Company X\n\n\n\n\n\n\n\n\n\n\nx_i\nf_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\nf_i(x_i - \\bar{x})^2\n\n\n\n\n2\n3\n-3.95\n15.6025\n46.8075\n\n\n3\n6\n-2.95\n8.7025\n52.215\n\n\n4\n5\n-1.95\n3.8025\n19.0125\n\n\n5\n4\n-0.95\n0.9025\n3.61\n\n\n20\n1\n14.05\n197.4025\n197.4025\n\n\n35\n1\n29.05\n843.9025\n843.9025\n\n\nTotal\n20\n\n\n1162.95\n\n\n\ns^2 = \\frac{1162.95}{19} = 61.21\n\n\n11.10.3.1.2 Manual Calculation for Company Y\n\n\n\n\n\n\n\n\n\n\ny_i\nf_i\ny_i - \\bar{y}\n(y_i - \\bar{y})^2\nf_i(y_i - \\bar{y})^2\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nTotal\n20\n\n\n34\n\n\n\ns^2 = \\frac{34}{19} = 1.79\n\n\n11.10.3.1.3 R Verification\n\nvar(X)\n\n[1] 61.20789\n\nvar(Y)\n\n[1] 1.789474\n\n\n\n\n\n11.10.3.2 Standard Deviation\nThe standard deviation is the square root of the variance.\nFormula: s = \\sqrt{s^2}\n\nFor Company X: s = \\sqrt{61.21} = 7.82\nFor Company Y: s = \\sqrt{1.79} = 1.34\n\n\n11.10.3.2.1 R Verification\n\nsd(X)\n\n[1] 7.823547\n\nsd(Y)\n\n[1] 1.337712\n\n\n\n\n\n\n11.10.4 Quartiles\nQuartiles divide the dataset into four equal parts.\n\n11.10.4.1 Manual Calculation for Company X\nOrdered data: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\n\nQ1 (25th percentile): median of first 10 numbers = 3\nQ2 (50th percentile, median): 4\nQ3 (75th percentile): median of last 10 numbers = 5\n\n\n\n11.10.4.2 Manual Calculation for Company Y\nOrdered data: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\n\nQ1 (25th percentile): median of first 10 numbers = 4\nQ2 (50th percentile, median): 5\nQ3 (75th percentile): median of last 10 numbers = 6\n\n\n\n11.10.4.3 R Verification\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\n11.10.4.4 IQR\n\nIQR_x = 5 - 3 = 2\nIQR_y = 6 - 4 = 2\n\n\n\n\n11.10.5 Tukey Box Plot\nA Tukey box plot visually represents the distribution of data based on quartiles. We’ll use ggplot2 to create the plot.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Prepare the data\ndata &lt;- data.frame(\n  Company = rep(c(\"X\", \"Y\"), each = 20),\n  Salary = c(X, Y)\n)\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot() +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n# Create the box plot\nggplot(data, aes(x = Company, y = Salary, fill = Company)) +\n  geom_boxplot(outliers = F) +\n  labs(title = \"Salary Distribution in Companies X and Y\",\n       x = \"Company\",\n       y = \"Salary (thousands of euros)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\n11.10.5.1 Interpreting the Box Plot\n\nThe box represents the interquartile range (IQR) from Q1 to Q3.\nThe line inside the box is the median (Q2).\nWhiskers extend to the smallest and largest values within 1.5 * IQR.\nPoints beyond the whiskers are considered outliers.\n\n\n\n\n11.10.6 Comparison of Results\n\n\n\nMeasure\nCompany X\nCompany Y\n\n\n\n\nMean\n5.95\n5.00\n\n\nMedian\n4\n5\n\n\nMode\n3\n4 and 5\n\n\nVariance\n61.21\n1.79\n\n\nStandard Deviation\n7.82\n1.34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\n11.10.6.1 Key Observations:\n\nCentral Tendency: Company X has a higher mean but lower median than Company Y, indicating a right-skewed distribution for Company X.\nDispersion: Company X shows much higher variance and standard deviation, suggesting greater salary disparities.\nDistribution Shape: Company Y’s salaries are more tightly clustered, while Company X has extreme values (potential outliers) that significantly affect its mean and variance.\nQuartiles: Company Y’s interquartile range (Q3 - Q1) is slightly larger, but its overall range is much smaller than Company X’s.\n\n\n\n\n11.10.7 Conclusion\nThis comparative analysis reveals significant differences in salary structures between the two companies. Company X shows greater variability and potential inequality in its pay scale, while Company Y demonstrates a more consistent and narrowly distributed salary range.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise-2.-comparing-electoral-district-size-variation-between-countries",
    "href": "chapter5.html#exercise-2.-comparing-electoral-district-size-variation-between-countries",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.11 Exercise 2. Comparing Electoral District Size Variation Between Countries",
    "text": "11.11 Exercise 2. Comparing Electoral District Size Variation Between Countries\n\n11.11.1 Data\nWe have electoral district size data from two countries:\n\nx &lt;- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Country high variance\ny &lt;- c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13)  # Country low variance\n\nkable(data.frame(\n  \"Country X (High var.)\" = x,\n  \"Country Y (Low var.)\" = y\n))\n\n\n\n\nCountry.X..High.var..\nCountry.Y..Low.var..\n\n\n\n\n1\n8\n\n\n3\n9\n\n\n5\n9\n\n\n7\n10\n\n\n9\n10\n\n\n11\n11\n\n\n13\n11\n\n\n15\n12\n\n\n17\n12\n\n\n19\n13\n\n\n\n\n\n\n\n11.11.2 Measures of Central Tendency\n\n11.11.2.1 Arithmetic Mean\nFormula: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\n11.11.2.1.1 Calculations for Country X\n\n\n\nElement\nValue\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n5\n\n\n4\n7\n\n\n5\n9\n\n\n6\n11\n\n\n7\n13\n\n\n8\n15\n\n\n9\n17\n\n\n10\n19\n\n\nSum\n100\n\n\n\n\\bar{x} = \\frac{100}{10} = 10\n\nmean_x &lt;- mean(x)\nc(\"Manual\" = 10, \"R\" = mean_x)\n\nManual      R \n    10     10 \n\n\n\n\n11.11.2.1.2 Calculations for Country Y\n\n\n\nElement\nValue\n\n\n\n\n1\n8\n\n\n2\n9\n\n\n3\n9\n\n\n4\n10\n\n\n5\n10\n\n\n6\n11\n\n\n7\n11\n\n\n8\n12\n\n\n9\n12\n\n\n10\n13\n\n\nSum\n105\n\n\n\n\\bar{y} = \\frac{105}{10} = 10.5\n\nmean_y &lt;- mean(y)\nc(\"Manual\" = 10.5, \"R\" = mean_y)\n\nManual      R \n  10.5   10.5 \n\n\n\n\n\n11.11.2.2 Median\nThe median is the middle value in an ordered dataset.\n\n11.11.2.2.1 Calculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nFor n = 10 (even number of observations): Middle positions: 5 and 6 Middle values: 9 and 11\nMedian = \\frac{9 + 11}{2} = 10\n\nmedian_x &lt;- median(x)\nc(\"Manual\" = 10, \"R\" = median_x)\n\nManual      R \n    10     10 \n\n\n\n\n11.11.2.2.2 Calculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nFor n = 10 (even number of observations): Middle positions: 5 and 6 Middle values: 10 and 11\nMedian = \\frac{10 + 11}{2} = 10.5\n\nmedian_y &lt;- median(y)\nc(\"Manual\" = 10.5, \"R\" = median_y)\n\nManual      R \n  10.5   10.5 \n\n\n\n\n\n11.11.2.3 Mode\n\n11.11.2.3.1 Calculations for Country X\n\n\n\nValue\nFrequency\n\n\n\n\n1\n1\n\n\n3\n1\n\n\n5\n1\n\n\n7\n1\n\n\n9\n1\n\n\n11\n1\n\n\n13\n1\n\n\n15\n1\n\n\n17\n1\n\n\n19\n1\n\n\n\nConclusion: No mode (all values occur once)\n\n\n11.11.2.3.2 Calculations for Country Y\n\n\n\nValue\nFrequency\n\n\n\n\n8\n1\n\n\n9\n2\n\n\n10\n2\n\n\n11\n2\n\n\n12\n2\n\n\n13\n1\n\n\n\nConclusion: Four modes: 9, 10, 11, 12 (each occurs twice)\n\n# Frequency tables\ntable_x &lt;- table(x)\ntable_y &lt;- table(y)\n\nlist(\n  \"Country X\" = table_x,\n  \"Country Y\" = table_y\n)\n\n$`Country X`\nx\n 1  3  5  7  9 11 13 15 17 19 \n 1  1  1  1  1  1  1  1  1  1 \n\n$`Country Y`\ny\n 8  9 10 11 12 13 \n 1  2  2  2  2  1 \n\n\n\n\n\n11.11.2.4 Variance\nVariance measures the average squared deviation from the mean.\nFormula: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\n11.11.2.4.1 Calculations for Country X\n\n\n\nx_i\n(x_i - \\bar{x})\n(x_i - \\bar{x})^2\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSum\n\n330\n\n\n\ns^2_X = \\frac{330}{9} = 36.67\n\nvar_x &lt;- var(x)\nc(\"Manual\" = 36.67, \"R\" = var_x)\n\nManual      R \n 36.67  36.67 \n\n\n\n\n11.11.2.4.2 Calculations for Country Y\n\n\n\nx_i\n(y_i - \\bar{y})\n(y_i - \\bar{y})^2\n\n\n\n\n8\n-2.5\n6.25\n\n\n9\n-1.5\n2.25\n\n\n9\n-1.5\n2.25\n\n\n10\n-0.5\n0.25\n\n\n10\n-0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n11\n0.5\n0.25\n\n\n12\n1.5\n2.25\n\n\n12\n1.5\n2.25\n\n\n13\n2.5\n6.25\n\n\nSum\n\n22.5\n\n\n\ns^2_Y = \\frac{22.5}{9} = 2.5\n\nvar_y &lt;- var(y)\nc(\"Manual\" = 2.5, \"R\" = var_y)\n\nManual      R \n   2.5    2.5 \n\n\n\n\n\n11.11.2.5 Standard Deviation\nStandard deviation is the square root of variance. It measures variability in the same units as the data.\nFormula: s = \\sqrt{s^2}\n\n11.11.2.5.1 Calculations for Country X\nUsing previously calculated variance: s^2_X = 36.67\nCalculate square root: s_X = \\sqrt{36.67} \\approx 6.06\n\n\n\nStep\nCalculation\nResult\n\n\n\n\n1. Variance\ns^2_X\n36.67\n\n\n2. Square root\n\\sqrt{36.67}\n6.06\n\n\n\n\nsd_x &lt;- sd(x)\nc(\"Manual\" = 6.06, \"R\" = sd_x)\n\nManual      R \n 6.060  6.055 \n\n\n\n\n11.11.2.5.2 Calculations for Country Y\nUsing previously calculated variance: s^2_Y = 2.5\nCalculate square root: s_Y = \\sqrt{2.5} \\approx 1.58\n\n\n\nStep\nCalculation\nResult\n\n\n\n\n1. Variance\ns^2_Y\n2.5\n\n\n2. Square root\n\\sqrt{2.5}\n1.58\n\n\n\n\nsd_y &lt;- sd(y)\nc(\"Manual\" = 1.58, \"R\" = sd_y)\n\nManual      R \n 1.580  1.581 \n\n\nInterpretation:\n\nCountry X: Average deviation from the mean is about 6 seats\nCountry Y: Average deviation from the mean is about 1.6 seats\n\n\n\n\n\n11.11.3 Coefficient of Variation (CV)\nThe coefficient of variation is the ratio of standard deviation to mean, expressed as a percentage.\nFormula: CV = \\frac{s}{\\bar{x}} \\times 100\\%\n\n11.11.3.1 Calculations for Country X\nCV_X = \\frac{6.06}{10} \\times 100\\% = 60.6\\%\n\n\n\nComponent\nValue\n\n\n\n\nStandard deviation (s)\n6.06\n\n\nMean (\\bar{x})\n10\n\n\nCV\n60.6%\n\n\n\n\ncv_x &lt;- sd(x) / mean(x) * 100\nc(\"Manual\" = 60.6, \"R\" = cv_x)\n\nManual      R \n 60.60  60.55 \n\n\n\n\n11.11.3.2 Calculations for Country Y\nCV_Y = \\frac{1.58}{10.5} \\times 100\\% = 15.0\\%\n\n\n\nComponent\nValue\n\n\n\n\nStandard deviation (s)\n1.58\n\n\nMean (\\bar{x})\n10.5\n\n\nCV\n15.0%\n\n\n\n\ncv_y &lt;- sd(y) / mean(y) * 100\nc(\"Manual\" = 15.0, \"R\" = cv_y)\n\nManual      R \n 15.00  15.06 \n\n\n\n\n\n11.11.4 Quartiles and Interquartile Range (IQR)\n\n11.11.4.1 Methods for Calculating Quartiles\nThere are different methods for calculating quartiles. In our manual calculations, we’ll use the median-excluding method:\n\nSplit the series at the median\nMedian is not included in quartile calculations\nCalculate median of each part - these will be Q1 and Q3 respectively\n\n\n\n11.11.4.2 Calculations for Country X\nOrdered data: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMedian = 10 (not included in quartile calculations)\nLower half: 1, 3, 5, 7, 9 Q1 = median of lower half = 5\nUpper half: 11, 13, 15, 17, 19 Q3 = median of upper half = 15\nIQR = Q3 - Q1 = 15 - 5 = 10\n\n\n11.11.4.3 Calculations for Country Y\nOrdered data: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMedian = 10.5 (not included in quartile calculations)\nLower half: 8, 9, 9, 10, 10 Q1 = median of lower half = 9\nUpper half: 11, 11, 12, 12, 13 Q3 = median of upper half = 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n# Comparison of different quartile calculation methods in R\nmethods_comparison &lt;- data.frame(\n  Method = c(\"Manual (excl. median)\", \n             \"R type=1\", \"R type=2\", \"R type=7 (default)\"),\n  \"Q1 Country X\" = c(5, \n                    quantile(x, 0.25, type=1),\n                    quantile(x, 0.25, type=2),\n                    quantile(x, 0.25, type=7)),\n  \"Q3 Country X\" = c(15,\n                    quantile(x, 0.75, type=1),\n                    quantile(x, 0.75, type=2),\n                    quantile(x, 0.75, type=7)),\n  \"Q1 Country Y\" = c(9,\n                    quantile(y, 0.25, type=1),\n                    quantile(y, 0.25, type=2),\n                    quantile(y, 0.25, type=7)),\n  \"Q3 Country Y\" = c(12,\n                    quantile(y, 0.75, type=1),\n                    quantile(y, 0.75, type=2),\n                    quantile(y, 0.75, type=7))\n)\n\nkable(methods_comparison, digits = 2,\n      caption = \"Comparison of different quartile calculation methods\")\n\n\nComparison of different quartile calculation methods\n\n\n\n\n\n\n\n\n\nMethod\nQ1.Country.X\nQ3.Country.X\nQ1.Country.Y\nQ3.Country.Y\n\n\n\n\nManual (excl. median)\n5.0\n15.0\n9.00\n12.00\n\n\nR type=1\n5.0\n15.0\n9.00\n12.00\n\n\nR type=2\n5.0\n15.0\n9.00\n12.00\n\n\nR type=7 (default)\n5.5\n14.5\n9.25\n11.75\n\n\n\n\n\n\n\n11.11.4.4 Explanation of Different Quartile Calculation Methods\n\nManual method (excluding median):\n\nSplits data into two parts\nExcludes median\nFinds median of each part\n\nR type=1:\n\nFirst method in R\nUses whole positions\nNo interpolation\n\nR type=2:\n\nSecond method in R\nUses whole positions\nInterpolates when position is not whole\n\nR type=7 (default):\n\nDefault method in R\nUses quantile()[5] from SAS\nInterpolates according to Hyndman and Fan method\n\n\n\n\n\n11.11.5 Results Comparison\n\nsummary_df &lt;- data.frame(\n  Measure = c(\"Mean\", \"Median\", \"Mode\", \"Range\", \"Variance\", \n              \"Std. Dev.\", \"Q1\", \"Q3\", \"IQR\", \"CV (%)\"),\n  \"Country X\" = c(10, 10, \"none\", 18, 36.67, 6.06, 5, 15, 10, 60.6),\n  \"Country Y\" = c(10.5, 10.5, \"9,10,11,12\", 5, 2.5, 1.58, 9, 12, 3, 15.0)\n)\n\nkable(summary_df, \n      caption = \"Summary of all statistical measures\",\n      align = c('l', 'r', 'r'))\n\n\nSummary of all statistical measures\n\n\nMeasure\nCountry.X\nCountry.Y\n\n\n\n\nMean\n10\n10.5\n\n\nMedian\n10\n10.5\n\n\nMode\nnone\n9,10,11,12\n\n\nRange\n18\n5\n\n\nVariance\n36.67\n2.5\n\n\nStd. Dev.\n6.06\n1.58\n\n\nQ1\n5\n9\n\n\nQ3\n15\n12\n\n\nIQR\n10\n3\n\n\nCV (%)\n60.6\n15\n\n\n\n\n\n\n\n11.11.6 Comparison using Box Plot\n\ndf_long &lt;- data.frame(\n  country = rep(c(\"X\", \"Y\"), each = 10),\n  size = c(x, y)\n)\n\n# Basic plot\np &lt;- ggplot(df_long, aes(x = country, y = size, fill = country)) +\n  geom_boxplot(outlier.shape = NA) +  # Disable default outlier points\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Add points with transparency\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\")) +\n  labs(\n    title = \"Comparison of Electoral District Size Variation\",\n    subtitle = paste(\"CV: Country X =\", round(cv_x, 1), \"%, Country Y =\", round(cv_y, 1), \"%\"),\n    x = \"Country\",\n    y = \"District Size\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Add quartile annotations\np + annotate(\n  \"text\", \n  x = c(1, 1, 1, 2, 2, 2), \n  y = c(max(x)+1, mean(x), min(x)-1, max(y)+1, mean(y), min(y)-1),\n  label = c(\n    paste(\"Q3 =\", quantile(x, 0.75, type=1)),\n    paste(\"M =\", median(x)),\n    paste(\"Q1 =\", quantile(x, 0.25, type=1)),\n    paste(\"Q3 =\", quantile(y, 0.75, type=1)),\n    paste(\"M =\", median(y)),\n    paste(\"Q1 =\", quantile(y, 0.25, type=1))\n  ),\n  size = 3\n)\n\n\n\n\n\n\n\n\n\n\n11.11.7 Methodological Notes\n\nQuartile Calculations:\n\nThe median-excluding method used may give different results than R’s default functions\nDifferences in calculation methods don’t affect overall conclusions\nAlways important to specify the method used in reports\n\nVisualization:\n\nBox plot effectively shows differences in distributions\nAdditional points show actual values\nAnnotations facilitate interpretation\n\n\n\n\n11.11.8 Application Notes\n\nUsing the Analysis:\n\nAll calculations can be reproduced using the provided R code\nCode chunks are self-contained and documented\nData format requirements are clearly specified\n\nCustomization:\n\nAnalysis can be adapted for different district size datasets\nVisualization parameters can be adjusted for different presentation needs\nStatistical methods can be modified based on specific requirements\n\n\n\n\n11.11.9 Conclusion\n\n11.11.9.1 Summary Statistics Comparison\n\n\n\nMeasure\nCountry X\nCountry Y\nRelative Difference\n\n\n\n\nMean\n10.0\n10.5\nSimilar\n\n\nMedian\n10.0\n10.5\nSimilar\n\n\nMode\nNone\nMultiple (9,10,11,12)\n-\n\n\nRange\n18\n5\n3.6× larger in X\n\n\nVariance\n36.67\n2.5\n14.7× larger in X\n\n\nIQR\n10\n3\n3.3× larger in X\n\n\nCV\n60.6%\n15.0%\n4.0× larger in X\n\n\n\n\n\n11.11.9.2 Distribution Characteristics\nCountry X:\n\nUniform distribution pattern\nNo dominant district size (no mode)\nWide range: 1 to 19 seats\nHigh variability (CV = 60.6%) - Even spread of values across range\n\nCountry Y:\n\nClustered distribution pattern\nMultiple common sizes (four modes)\nNarrow range: 8 to 13 seats\nLow variability (CV = 15.0%) - Values concentrated around mean\n\n\n\n11.11.9.3 Box Plot Interpretation\nThe box plot visualization reveals:\nStructure Elements:\n\nBox: Shows interquartile range (IQR)\nLower edge: First quartile (Q1)\nUpper edge: Third quartile (Q3)\nInternal line: Median (Q2)\nWhiskers: Extend to ±1.5 IQR - Points: Individual district sizes\n\nKey Visual Findings:\n\nBox Size:\n\n\nCountry X: Large box indicates wide spread of middle 50%\nCountry Y: Small box shows tight clustering of middle values\n\n\nWhisker Length:\n\nCountry X: Long whiskers indicate broad overall distribution\nCountry Y: Short whiskers show limited total spread\n\nPoint Distribution:\n\nCountry X: Points widely dispersed\nCountry Y: Points densely clustered\n\n\n\n\n11.11.9.4 Key Observations\n\nCentral Tendency:\n\nSimilar average district sizes\nDifferent distribution patterns\nDistinct approaches to standardization\n\nVariability Measures:\n\nAll metrics show Country X with 3-15 times more variation\nConsistent pattern across different statistical measures\nSystematic difference in district design\n\nSystem Design:\n\nCountry X: Flexible, varied approach\nCountry Y: Standardized, uniform approach\nDifferent philosophical approaches to representation\n\nRepresentative Implications:\n\nCountry X: Variable voter-to-representative ratios\nCountry Y: More consistent representation levels\nDifferent approaches to democratic representation\n\n\nThis analysis demonstrates fundamental differences in electoral system design between the two countries, with Country X adopting a more varied approach and Country Y maintaining greater uniformity in district sizes.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exercise.-understanding-boxplots-through-life-expectancy-data",
    "href": "chapter5.html#exercise.-understanding-boxplots-through-life-expectancy-data",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.12 Exercise. Understanding Boxplots Through Life Expectancy Data",
    "text": "11.12 Exercise. Understanding Boxplots Through Life Expectancy Data\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Prepare data\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007)",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#introduction-to-boxplots",
    "href": "chapter5.html#introduction-to-boxplots",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.13 Introduction to Boxplots",
    "text": "11.13 Introduction to Boxplots\nA boxplot (also known as a box-and-whisker plot) reveals key statistics about your data:\n\nMedian: The middle line in the box (50th percentile)\nFirst quartile (Q1): Bottom of the box (25th percentile)\nThird quartile (Q3): Top of the box (75th percentile)\nInterquartile Range (IQR): The height of the box (Q3 - Q1)\nWhiskers: Extend to the most extreme non-outlier values (Tukey’s method: 1.5 × IQR)\nOutliers: Individual points beyond the whiskers\n\n\n11.13.1 Visualizing Life Expectancy\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 24, \n               outlier.fill = \"red\", outlier.alpha = 0.6, outlier.size = 4) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"darkblue\") +\n  labs(title = \"Life Expectancy by Continent (2007)\",\n       subtitle = \"Individual points show raw data; red points indicate outliers\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#understanding-the-data",
    "href": "chapter5.html#understanding-the-data",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.14 Understanding the Data",
    "text": "11.14 Understanding the Data\n\n11.14.1 Median and Distribution\nAnswer True or False:\n\n50% of African countries have life expectancy below 54 years\nThe median life expectancy in Europe is approximately 78 years\nMore than 75% of countries in Oceania have life expectancy above 74 years\n25% of Asian countries have life expectancy below 65 years\nThe middle 50% of life expectancies in Europe fall between 74 and 80 years\n\n\n\n11.14.2 Spread and Variation\nAnswer True or False:\n\nAsia shows the largest spread (IQR) in life expectancy\nEurope has the smallest IQR among all continents\nThe variation in Africa’s life expectancy is greater than in the Americas\nOceania shows the least variation in life expectancy\nThe range (excluding outliers) in Asia is approximately 20 years\n\n\n\n11.14.3 Outliers and Extremes\nAnswer True or False:\n\nAfrica has two countries with unusually low life expectancy\nThere are no outliers in Oceania’s distribution\nAsia has both high and low outliers",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#changes-over-time",
    "href": "chapter5.html#changes-over-time",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.15 Changes Over Time",
    "text": "11.15 Changes Over Time\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = \"dodge\", outlier.shape = 21,\n               outlier.alpha = 0.6) +\n  labs(title = \"Life Expectancy: 1957 vs 2007\",\n       subtitle = \"Comparing distribution changes over 50 years\",\n       x = \"Continent\",\n       y = \"Life Expectancy (years)\",\n       fill = \"Year\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\n11.15.1 Time Comparison Questions\nAnswer True or False:\n\nThe median life expectancy increased in all continents between 1957 and 2007\nThe variation in life expectancy (IQR) decreased in most continents over time\nAfrica showed the smallest improvement in median life expectancy\nThe spread of life expectancies in Asia decreased substantially from 1957 to 2007\nOceania maintained the highest median life expectancy in both time periods\n\n\n\n11.15.2 Statistical Summary\n\n# Calculate summary statistics\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    n_outliers = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics by Continent and Year\")\n\n\nSummary Statistics by Continent and Year\n\n\ncontinent\nyear\nmedian\nq1\nq3\niqr\nn_outliers\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n0",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#key-learning-points",
    "href": "chapter5.html#key-learning-points",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.16 Key Learning Points",
    "text": "11.16 Key Learning Points\n\nDistribution Center:\n\nMedian shows the typical life expectancy\nChanges in median reflect overall improvements\n\nSpread and Variation:\n\nIQR (box height) indicates data dispersion\nWider boxes suggest more inequality in life expectancy\n\nOutliers and Extremes:\n\nOutliers often represent countries with unique circumstances\n\nTime Comparison:\n\nShows both absolute improvements and changes in variation\nHighlights persistent regional disparities\nReveals different rates of progress across continents",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "href": "chapter5.html#appendix-summary-tables-for-data-types-and-applicable-statistical-measures",
    "title": "11  Fundamentals of Univariate Descriptive Statistics",
    "section": "11.17 Appendix: Summary Tables for Data Types and Applicable Statistical Measures",
    "text": "11.17 Appendix: Summary Tables for Data Types and Applicable Statistical Measures\n\n11.17.1 Table 1: Pros and Cons of Various Statistical Measures\n\n11.17.1.1 Measures of Center\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nMean\n- Uses all data points- Allows for further statistical calculations- Ideal for normally distributed data\n- Sensitive to outliers- Not ideal for skewed distributions- Not meaningful for nominal data\nInterval, Ratio, some Discrete, Continuous\n\n\nMedian\n- Not affected by outliers- Good for skewed distributions- Can be used with ordinal data\n- Ignores the actual values of most data points- Less useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nMode\n- Can be used with any data type- Good for finding most common category\n- May not be unique (multimodal)- Not useful for many types of analyses- Ignores magnitude of differences between values\nAll types\n\n\n\n\n\n11.17.1.2 Measures of Variability\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nRange\n- Simple to calculate and understand- Gives quick idea of data spread\n- Very sensitive to outliers- Ignores all data between extremes- Not useful for further statistical analyses\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nInterquartile Range (IQR)\n- Not affected by outliers- Good for skewed distributions\n- Ignores 50% of the data- Less intuitive than range\nOrdinal, Interval, Ratio, Discrete, Continuous\n\n\nVariance\n- Uses all data points- Basis for many statistical procedures\n- Sensitive to outliers- Units are squared (less intuitive)\nInterval, Ratio, some Discrete, Continuous\n\n\nStandard Deviation\n- Uses all data points- Same units as original data- Widely used and understood\n- Sensitive to outliers- Assumes roughly normal distribution for interpretation\nInterval, Ratio, some Discrete, Continuous\n\n\nCoefficient of Variation\n- Allows comparison between datasets with different units or means\n- Can be misleading when means are close to zero- Not meaningful for data with negative values\nRatio, some Interval\n\n\n\n\n\n11.17.1.3 Measures of Correlation/Association\n\n\n\n\n\n\n\n\n\nMeasure\nPros\nCons\nApplicable to\n\n\n\n\nPearson’s r\n- Measures linear relationship- Widely used and understood\n- Assumes normal distribution- Sensitive to outliers- Only captures linear relationships\nInterval, Ratio, Continuous\n\n\nSpearman’s rho\n- Can be used with ordinal data- Captures monotonic relationships- Less sensitive to outliers\n- Loses information by converting to ranks- May miss some types of relationships\nOrdinal, Interval, Ratio\n\n\nKendall’s tau\n- Can be used with ordinal data- More robust than Spearman’s for small samples- Has nice interpretation (probability of concordance)\n- Loses information by only considering order- Computationally more intensive\nOrdinal, Interval, Ratio\n\n\nChi-square\n- Can be used with nominal data- Tests independence of categorical variables\n- Requires large sample sizes- Sensitive to sample size- Doesn’t measure strength of association\nNominal, Ordinal\n\n\nCramér’s V\n- Can be used with nominal data- Provides measure of strength of association- Normalized to [0,1] range\n- Interpretation can be subjective- May overestimate association in small samples\nNominal, Ordinal\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓*\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\n11.17.2 Notes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fundamentals of Univariate Descriptive Statistics</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html",
    "href": "rozdzial5.html",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "12.1 Introduction to Sigma Notation (Σ) | Wprowadzenie do Notacji Sigma (Σ)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#introduction-to-sigma-notation-σ-wprowadzenie-do-notacji-sigma-σ",
    "href": "rozdzial5.html#introduction-to-sigma-notation-σ-wprowadzenie-do-notacji-sigma-σ",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "",
    "text": "What is Sigma? | Co to jest notacja sumacyjna Sigma? Sigma (Σ) is a mathematical operator that tells us to sum (add up) a sequence of terms - it functions as an instruction to perform addition of all elements in a specified range. | Sigma (Σ) to operator matematyczny, który nakazuje nam zsumować (dodać) sekwencję wyrazów - działa jak instrukcja wykonania dodawania wszystkich elementów w określonym zakresie.\nPurpose: | Cel: Provides a compact way to write sums of many similar terms using a single symbol, avoiding lengthy addition expressions. | Zapewnia zwięzły sposób zapisu sum wielu podobnych wyrazów za pomocą jednego symbolu, unikając długich wyrażeń dodawania.\n\n\n12.1.1 Basic Formula | Podstawowa formuła\n\nThe general form of a sigma notation is: | Ogólna forma notacji sigma to:\n\n\\sum_{i=a}^{b} f(i)\n\nIndex of Summation: | Indeks sumowania: i\nLower Limit: | Dolna granica: a\nUpper Limit: | Górna granica: b\nFunction: | Funkcja: f(i)\n\n\n\n12.1.2 Simple Example | Prosty przykład\n\nConsider you want to add the first five positive integers: | Załóżmy, że chcesz dodać pierwsze pięć dodatnich liczb całkowitych:\n\n\\sum_{i=1}^{5} i = 1 + 2 + 3 + 4 + 5 = 15\n\nAdds the first five positive integers. | Dodaje pierwsze pięć dodatnich liczb całkowitych.\n\n\n\n12.1.3 Example with a Function | Przykład z funkcją\n\nSuppose you want to sum the squares of the first four positive integers: | Załóżmy, że chcesz zsumować kwadraty pierwszych czterech dodatnich liczb całkowitych:\n\n\\sum_{i=1}^{4} i^2 = 1^2 + 2^2 + 3^2 + 4^2 = 30\n\nSum of the squares of the first four positive integers. | Suma kwadratów pierwszych czterech dodatnich liczb całkowitych.\n\n\n\n12.1.4 Practical Application in Statistics | Praktyczne zastosowanie w statystyce\n\nCalculating the Mean: | Obliczanie średniej:\n\nData Points: | Punkty danych: x_1, x_2, ..., x_n\nMean \\bar{x}: | Średnia \\bar{x}:\n\n\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\nExample: | Przykład: x_1, x_2, x_3, x_4 are 4, 8, 15, 16 | x_1, x_2, x_3, x_4 to 4, 8, 15, 16\n\n\\bar{x} = \\frac{43}{4} = 10.75\n\n\n12.1.5 Benefits of Using Sigma Notation | Korzyści z używania notacji Sigma\n\nClarity: | Jasność: Provides a clear and concise representation of various statistical formulas. | Zapewnia jasne i zwięzłe przedstawienie statystycznych formuł.\n\n\n\n\n\n\n\nOperatory Sumy (Σ) i Iloczynu (Π)\n\n\n\n\n12.1.5.1 Operator Sigma (Σ)\n\\sum to operator sumowania, który nakazuje nam dodać wyrazy:\n\\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n\ngdzie: - i to zmienna indeksowa - Dolna wartość pod Σ (tutaj i=1) to punkt początkowy - Górna wartość (tutaj n) to punkt końcowy\n\n\n12.1.5.2 Operator Pi (Π)\n\\prod to operator iloczynu, który nakazuje nam pomnożyć wyrazy:\n\\prod_{i=1}^{n} x_i = x_1 \\times x_2 \\times ... \\times x_n\ngdzie: - i to zmienna indeksowa - Dolna wartość pod Π (tutaj i=1) to punkt początkowy - Górna wartość (tutaj n) to punkt końcowy\n\n\n\n\n\n\n\n\n\nPrzykład Σ\n\n\n\n\\sum_{i=1}^{4} i = 1 + 2 + 3 + 4 = 10\n\n\n\n\n\n\n\n\nPrzykład Π\n\n\n\n\\prod_{i=1}^{4} i = 1 \\times 2 \\times 3 \\times 4 = 24\n\n\n\n\n\n\n\n\nKluczowe Różnice\n\n\n\n\nΣ oznacza wielokrotne dodawanie\nΠ oznacza wielokrotne mnożenie",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#typy-rozkładów-danych",
    "href": "rozdzial5.html#typy-rozkładów-danych",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.2 Typy rozkładów danych",
    "text": "12.2 Typy rozkładów danych\n\n\n\n\n\n\nImportant\n\n\n\nRozkład danych informuje o tym, jakie wartości przyjmuje zmienna i jak często.\n\n\nZrozumienie rozkładów danych jest kluczowe dla analizy i wizualizacji danych. W tym dokumencie przyjrzymy się różnym typom rozkładów i sposobom ich wizualizacji przy użyciu ggplot2 w R.\n\n12.2.1 Rozkład normalny\nRozkład normalny, znany również jako rozkład Gaussa, jest symetryczny i ma kształt dzwonu.\n\n# Generowanie danych o rozkładzie normalnym\ndane_normalne &lt;- data.frame(x = rnorm(1000))\n\n# Wykres\nggplot(dane_normalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład normalny\", x = \"Wartość\", y = \"Gęstość\")\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\n\n\n12.2.2 Rozkład jednostajny\nW rozkładzie jednostajnym wszystkie wartości mają równe prawdopodobieństwo wystąpienia.\n\n# Generowanie danych o rozkładzie jednostajnym\ndane_jednostajne &lt;- data.frame(x = runif(1000))\n\n# Wykres\nggplot(dane_jednostajne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightgreen\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład jednostajny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\n12.2.3 Rozkłady skośne\nRozkłady skośne są asymetryczne, z jednym ogonem dłuższym niż drugi.\n\n# Generowanie danych o rozkładzie prawoskośnym\ndane_prawoskosne &lt;- data.frame(x = rlnorm(1000))\n\n# Wykres\nggplot(dane_prawoskosne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightyellow\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład prawoskośny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\n12.2.4 Rozkład bimodalny\nRozkład bimodalny ma dwa szczyty (dwie dominanty), wskazujące na dwie odrębne podgrupy w danych.\n\n# Generowanie danych bimodalnych\ndane_bimodalne &lt;- data.frame(x = c(rnorm(500, mean = -2), rnorm(500, mean = 2)))\n\n# Wykres\nggplot(dane_bimodalne, aes(x)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightpink\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład bimodalny\", x = \"Wartość\", y = \"Gęstość\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nKey Properties\nSocial Examples\n\n\n\n\nNormal\nSymmetric, bell-shaped, most values near mean\nHeight, IQ scores, standardized test scores\n\n\nUniform\nEqual probability across range\nBirth dates in year, arrival times in hour\n\n\nBimodal\nTwo peaks, suggests subgroups\nAge in college towns, polarized opinions\n\n\nLog-normal\nRight-skewed, cannot be negative\nIncome, house prices, social media followers\n\n\nPower law\nExtreme skew, “rich get richer”\nCity sizes",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wizualizacja-rozkładów-danych-rzeczywistych",
    "href": "rozdzial5.html#wizualizacja-rozkładów-danych-rzeczywistych",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.3 Wizualizacja rozkładów danych rzeczywistych",
    "text": "12.3 Wizualizacja rozkładów danych rzeczywistych\nUżyjemy zbioru danych palmerpenguins do zbadania rozkładów danych rzeczywistych.\n\n12.3.1 Histogram i wykres gęstości\n\n\n\n\n\n\nUnderstanding Histograms and Density\n\n\n\n⭐ A histogram is a special graph for numerical data where:\n\nData is grouped into ranges (called “bins”)\nBars touch each other (unlike bar charts!) because the data is continuous\nEach bar’s height shows how many values fall into that range\n\nThink of density as showing how common or concentrated certain values are in your data:\n\nA higher point on a density curve (or taller bar in a histogram) means those values appear more frequently in your data\nA lower point means those values are less common\n\nJust like a crowded area has more people per space (higher density), a taller part of the graph shows values that appear more often in your dataset!\n\n\n\nggplot(penguins, aes(x = flipper_length_mm)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"lightblue\", color = \"black\") +\n  geom_density(color = \"red\") +\n  labs(title = \"Rozkład długości płetw pingwinów\", \n       x = \"Długość płetwy (mm)\", \n       y = \"Gęstość\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density()`).\n\n\n\n\n\n\n\n\n\n\n\n12.3.2 Wykres pudełkowy\nWykresy pudełkowe są przydatne do porównywania rozkładów między kategoriami.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_boxplot() +\n  labs(title = \"Rozkład masy ciała pingwinów według gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa ciała (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n12.3.3 Wykres skrzypcowy\nWykresy skrzypcowe łączą cechy wykresu pudełkowego i wykresu gęstości.\n\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) +\n  geom_violin(trim = FALSE) +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Rozkład masy ciała pingwinów według gatunku\", \n       x = \"Gatunek\", \n       y = \"Masa ciała (g)\")\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_ydensity()`).\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\n12.3.4 Wykres grzbietowy\nWykresy grzbietowe są przydatne do porównywania wielu rozkładów.\n\nlibrary(ggridges)\n\nggplot(penguins, aes(x = flipper_length_mm, y = species, fill = species)) +\n  geom_density_ridges(alpha = 0.6) +\n  labs(title = \"Rozkład długości płetw według gatunku pingwina\",\n       x = \"Długość płetwy (mm)\",\n       y = \"Gatunek\")\n\nPicking joint bandwidth of 2.38\n\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_density_ridges()`).\n\n\n\n\n\n\n\n\n\n\n\n12.3.5 Podsumowanie\nZrozumienie i wizualizacja rozkładów danych są kluczowe w analizie danych. ggplot2 zapewnia elastyczny i potężny zestaw narzędzi do tworzenia różnych typów wykresów rozkładów. Badając różne techniki wizualizacji, możemy uzyskać wgląd w podstawowe wzorce i charakterystyki naszych danych.\n\n\n\n\n\n\nRodzaje i Formaty Zbiorów Danych\n\n\n\n\n12.3.6 Dane Przekrojowe\nObserwacje zebrane w jednym punkcie czasowym dla wielu podmiotów:\n\n\n\nOsoba\nWiek\nDochód\nWykształcenie\n\n\n\n\n1\n25\n5000\nLicencjat\n\n\n2\n35\n7500\nMagister\n\n\n3\n45\n9000\nDoktorat\n\n\n\n\n\n12.3.7 Szeregi Czasowe\nObserwacje jednego podmiotu w kolejnych punktach czasowych:\n\n\n\nRok\nPKB (w mld)\nStopa Bezrobocia\n\n\n\n\n2018\n20.580\n3,9%\n\n\n2019\n21.433\n3,7%\n\n\n2020\n20.933\n8,1%\n\n\n\n\n\n12.3.8 Dane Panelowe (Longitudinalne)\nObserwacje wielu podmiotów w czasie:\n\n\n\nKraj\nRok\nPKB per capita\nDługość życia\n\n\n\n\nPolska\n2018\n32.794\n76,7\n\n\nPolska\n2019\n35.118\n76,8\n\n\nNiemcy\n2018\n46.194\n81,9\n\n\nNiemcy\n2019\n46.194\n82,0\n\n\n\n\n\n12.3.9 Dane Przekrojowo-Czasowe (TSCS)\nSzczególny przypadek danych panelowych gdzie:\n\nLiczba punktów czasowych &gt; liczba podmiotów\nStruktura podobna do danych panelowych\nCzęsto stosowane w ekonomii i naukach politycznych\n\n\n\n12.3.10 Formaty Danych\n\n12.3.10.1 Format Szeroki\nKażdy wiersz to podmiot; kolumny to zmienne/punkty czasowe:\n\n\n\nKraj\nPKB_2018\nPKB_2019\nDŻ_2018\nDŻ_2019\n\n\n\n\nPolska\n32.794\n35.118\n76,7\n76,8\n\n\nNiemcy\n46.194\n46.194\n81,9\n82,0\n\n\n\n\n\n12.3.10.2 Format Długi\nKażdy wiersz to unikalna kombinacja podmiot-czas-zmienna:\n\n\n\nKraj\nRok\nZmienna\nWartość\n\n\n\n\nPolska\n2018\nPKB per capita\n32.794\n\n\nPolska\n2019\nPKB per capita\n35.118\n\n\nPolska\n2018\nDługość życia\n76,7\n\n\nPolska\n2019\nDługość życia\n76,8\n\n\nNiemcy\n2018\nPKB per capita\n46.194\n\n\nNiemcy\n2019\nPKB per capita\n46.194\n\n\nNiemcy\n2018\nDługość życia\n81,9\n\n\nNiemcy\n2019\nDługość życia\n82,0\n\n\n\nUwaga: Format długi jest zazwyczaj preferowany do:\n\nManipulacji danymi w R i Pythonie\nAnaliz statystycznych\nWizualizacji danych\nModelowania efektów mieszanych\nAnaliz powtarzanych pomiarów",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wartości-odstające-outliers",
    "href": "rozdzial5.html#wartości-odstające-outliers",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.4 Wartości Odstające (Outliers)",
    "text": "12.4 Wartości Odstające (Outliers)\nPrzed zagłębieniem się w konkretne miary, kluczowe jest zrozumienie pojęcia wartości odstających, ponieważ mogą one znacząco wpływać na wiele statystyk opisowych.\nWartości odstające to punkty danych, które znacznie różnią się od innych obserwacji w zbiorze danych. Mogą wystąpić z powodu:\n\nBłędów pomiaru lub zapisu\nPrawdziwych ekstremalnych wartości w populacji\nPróbkowania z innej populacji\n\nWartości odstające mogą mieć istotny wpływ na wiele miar statystycznych, szczególnie tych opartych na średnich lub sumach kwadratów odchyleń. Dlatego ważne jest, aby:\n\nIdentyfikować wartości odstające zarówno poprzez metody statystyczne, jak i wiedzę dziedzinową\nBadać przyczyny wartości odstających\nPodejmować świadome decyzje o tym, czy włączać je do analiz, czy nie\n\nW tym przewodniku omówimy, jak różne miary opisowe są dotknięte przez wartości odstające.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#symbole-stosowane-w-statystyce---podsumowanie",
    "href": "rozdzial5.html#symbole-stosowane-w-statystyce---podsumowanie",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.5 Symbole Stosowane w Statystyce - podsumowanie",
    "text": "12.5 Symbole Stosowane w Statystyce - podsumowanie\n\n\n\n\n\n\n\n\n\n\nMiara\nParametr Populacji\nStatystyka z Próby\nAlternatywne Oznaczenia\nUwagi\n\n\n\n\nLiczebność\nN\nn\n-\nCałkowita liczba obserwacji\n\n\nŚrednia\n\\mu\n\\bar{x}\nE(X), M\nE(X) stosowane w rachunku prawdopodobieństwa\n\n\nWariancja\n\\sigma^2\ns^2\n\\text{Var}(X), V(X)\nKwadrat odchyleń od średniej\n\n\nOdchylenie standardowe\n\\sigma\ns\n\\text{OS}, \\text{std}\nPierwiastek z wariancji\n\n\nFrakcja/Proporcja\n\\pi, P\n\\hat{p}\n\\text{fr}\nCzęstości względne\n\n\nWspółczynnik korelacji\n\\rho\nr\n\\text{kor}(x,y)\nWartości od -1 do +1\n\n\nBłąd standardowy\n\\sigma_{\\bar{x}}\ns_{\\bar{x}}\n\\text{BS}\nBłąd standardowy średniej\n\n\nSuma\n\\sum\n\\sum\n\\sum_{i=1}^n\nZ indeksowaniem\n\n\nPojedyncza obserwacja\nX_i\nx_i\n-\ni-ta obserwacja\n\n\nKowariancja\n\\sigma_{xy}\ns_{xy}\n\\text{Cov}(X,Y)\nWspólna zmienność\n\n\nMediana\n\\eta\n\\text{Me}\nM\nWartość środkowa\n\n\nRozstęp\nR\nr\n\\text{max}(X) - \\text{min}(X)\nMiara rozproszenia\n\n\nDominanta\n\\text{Mo}\n\\text{mo}\n\\text{mod}\nWartość najczęstsza\n\n\nSkośność\n\\gamma_1\ng_1\n\\text{SK}\nAsymetria rozkładu\n\n\nKurtoza\n\\gamma_2\ng_2\n\\text{KU}\nSpłaszczenie rozkładu\n\n\n\nDodatkowe ważne wzory:\n\nMomenty z próby: m_k = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^k\nMomenty populacji: \\mu_k = E[(X - \\mu)^k]\nBłąd standardowy dla populacji: \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\nBłąd standardowy z próby: s_{\\bar{x}} = \\frac{s}{\\sqrt{n}}",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-tendencji-centralnej",
    "href": "rozdzial5.html#miary-tendencji-centralnej",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.6 Miary Tendencji Centralnej",
    "text": "12.6 Miary Tendencji Centralnej\nMiary tendencji centralnej mają na celu identyfikację “typowej” lub “centralnej” wartości w zbiorze danych. Trzy podstawowe miary to średnia, mediana i moda.\n\n12.6.1 Średnia Arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez liczbę wartości.\nWzór: \\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i\nWażna Właściwość: Średnia jest punktem równowagi w danych. Suma odchyleń od średniej zawsze wynosi zero:\n\\sum_{i=1}^n (x_i - \\bar{x}) = 0\nTa właściwość sprawia, że średnia jest użyteczna w wielu obliczeniach statystycznych.\n\n\n\n\n\n\nZrozumienie średniej jako punktu równowagi 🎯\n\n\n\nRozważmy zbiór danych X = \\{1, 2, 6, 7, 9\\} na osi liczbowej, wyobrażając go sobie jako huśtawkę:\n\n\n\nhttps://www.gastonsanchez.com/matrix4sl/mean-as-a-balancing-point.html\n\n\nŚrednia (\\mu) działa jak idealny punkt równowagi tej huśtawki. Dla naszych danych:\n\\mu = \\frac{1 + 2 + 6 + 7 + 9}{5} = 5\n\n12.6.2 Co się dzieje przy różnych punktach podparcia? 🤔\n\nPunkt podparcia w 6 (za wysoko):\n\nLewa strona: Wartości (1, 2) są poniżej\nPrawa strona: Wartości (7, 9) są powyżej\n\\sum odległości z lewej = (6-1) + (6-2) = 9\n\\sum odległości z prawej = (7-6) + (9-6) = 4\nHuśtawka przechyla się w lewo! ⬅️ bo 9 &gt; 4\n\nPunkt podparcia w 4 (za nisko):\n\nLewa strona: Wartości (1, 2) są poniżej\nPrawa strona: Wartości (6, 7, 9) są powyżej\n\\sum odległości z lewej = (4-1) + (4-2) = 5\n\\sum odległości z prawej = (6-4) + (7-4) + (9-4) = 10\nHuśtawka przechyla się w prawo! ➡️ bo 5 &lt; 10\n\nPunkt podparcia w średniej (5) (idealna równowaga):\n\n\\sum odległości poniżej = \\sum odległości powyżej\n((5-1) + (5-2)) = ((6-5) + (7-5) + (9-5))\n7 = 7 ✨ Idealna równowaga!\n\n\nTo pokazuje, dlaczego średnia jest unikalnym punktem równowagi, gdzie:\n\\sum_{i=1}^n (x_i - \\mu) = 0\nHuśtawka zawsze będzie się przechylać, chyba że punkt podparcia zostanie umieszczony dokładnie w średniej! 🎪\n\n\n\n\n\n\n\n\n\nŚrednia jako punkt równowagi\n\n\n\nTa wizualizacja pokazuje, jak średnia arytmetyczna (5) działa jako punkt równowagi pomiędzy skupionymi punktami z lewej strony a rozproszonymi punktami z prawej strony:\nLewa strona średniej:\n\nPunkty o wartościach 2 i 3\nBlisko siebie (różnica 1 jednostka)\nOdległości od średniej: 3 i 2 jednostki\nSuma “ciążenia” = 5 jednostek\n\nPrawa strona średniej:\n\nPunkty o wartościach 6 i 9\nBardziej oddalone (różnica 3 jednostki)\nOdległości od średniej: 1 i 4 jednostki\nSuma “ciążenia” = 5 jednostek\n\nKluczowe obserwacje:\n\nŚrednia (5) jest punktem równowagi, mimo że:\n\nPunkty po lewej są skupione (2,3)\nPunkty po prawej są rozproszone (6,9)\nZielone strzałki pokazują odległości od średniej\n\nRównowaga jest zachowana ponieważ:\n\nSuma odległości się równoważy: (5-2) + (5-3) = (6-5) + (9-5)\nCałkowita suma odległości = 5 jednostek po każdej stronie\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrzykład Ręcznego Obliczenia:\nObliczmy średnią dla zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nSumuj wszystkie wartości\n2 + 4 + 4 + 5 + 5 + 7 + 9 = 36\n\n\n2\nPolicz liczbę wartości\nn = 7\n\n\n3\nPodziel sumę przez n\n36 / 7 = 5,14\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmean(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nŁatwa do obliczenia i zrozumienia\nWykorzystuje wszystkie punkty danych\nPrzydatna do dalszych obliczeń statystycznych\n\nWady:\n\nWrażliwa na wartości odstające\nNie idealna dla rozkładów skośnych\n\n\n\n12.6.3 Mediana\nMediana to środkowa wartość, gdy dane są uporządkowane.\nPrzykład Ręcznego Obliczenia:\nUżywając tego samego zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nWynik\n\n\n\n\n1\nUporządkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajdź środkową wartość\n5\n\n\n\nDla parzystej liczby wartości, weź średnią z dwóch środkowych wartości.\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nmedian(dane)\n\n[1] 5\n\n\nZalety:\n\nNie jest zniekształcona przez skrajne wartości odstające (outliers)\nLepsza dla rozkładów skośnych\n\nWady:\n\nNie wykorzystuje wszystkich punktów danych\nMniej przydatna do dalszych obliczeń statystycznych\n\n\n\n\n\n\n\nWarning\n\n\n\nJak znaleźć pozycję mediany w zbiorze danych:\n\nNajpierw posortuj dane rosnąco\nGdy n jest nieparzyste:\n\nPozycja mediany = \\frac{n + 1}{2}\n\nGdy n jest parzyste:\n\nPierwsza pozycja mediany = \\frac{n}{2}\nDruga pozycja mediany = \\frac{n}{2} + 1\nMediana = \\frac{\\text{wartość na pozycji }\\frac{n}{2} + \\text{wartość na pozycji }(\\frac{n}{2}+1)}{2}\n\n\nPrzykłady:\n\nNieparzyste n=7: pozycja = \\frac{7+1}{2} = 4-ta wartość\nParzyste n=8: pozycje = \\frac{8}{2} = 4-ta i 4+1 = 5-ta wartość\n\n\n\n\n\n12.6.4 Moda (Dominanta)\nModa to najczęściej występująca wartość.\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nWartość\nCzęstość\n\n\n\n\n2\n1\n\n\n4\n2\n\n\n5\n2\n\n\n7\n1\n\n\n9\n1\n\n\n\nModa to 4 i 5 (rozkład bimodalny).\nObliczenie w R:\n\nlibrary(modeest)\nmfv(dane)  # Najczęściej występująca wartość\n\n[1] 4 5\n\n\nZalety:\n\nJedyna miara tendencji centralnej dla danych nominalnych\nMoże identyfikować wiele punktów szczytowych (dominujących) w danych\n\nWady:\n\nNie zawsze jednoznacznie zdefiniowana\nNie przydatna dla danych ciągłych\n\n\n\n12.6.5 Średnia (arytmetyczna) Ważona (*)\nŚrednia ważona jest używana, gdy niektóre punkty danych są ważniejsze niż inne. Występują dwa typy średnich ważonych: z wagami nienormalizowanymi i z wagami znormalizowanymi.\n\n12.6.5.1 Średnia Ważona z Wagami Nienormalizowanymi\nJest to standardowa forma średniej ważonej, gdzie wagi mogą być dowolnymi liczbami dodatnimi reprezentującymi ważność każdego punktu danych.\nWzór: \\bar{x}_w = \\frac{\\sum_{i=1}^n w_i x_i}{\\sum_{i=1}^n w_i}\nPrzykład Obliczeń Ręcznych: Obliczmy średnią ważoną dla zbioru danych: 2, 4, 5, 7 z wagami 1, 2, 3, 1\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPomnóż każdą wartość przez jej wagę\n(2 * 1) + (4 * 2) + (5 * 3) + (7 * 1) = 2 + 8 + 15 + 7 = 32\n\n\n2\nZsumuj wagi\n1 + 2 + 3 + 1 = 7\n\n\n3\nPodziel wynik z kroku 1 przez wynik z kroku 2\n32 / 7 = 4.57\n\n\n\nObliczenia w R:\n\nx &lt;- c(2, 4, 5, 7)\nw &lt;- c(1, 2, 3, 1)\nweighted.mean(x, w)\n\n[1] 4.571429\n\n\n\n\n12.6.5.2 Średnia Ważona z Wagami Znormalizowanymi (Ułamki)\nW tym przypadku wagi są ułamkami sumującymi się do 1, reprezentującymi proporcję ważności dla każdego punktu danych.\nWzór: \\bar{x}_w = \\sum_{i=1}^n w_i x_i, gdzie \\sum_{i=1}^n w_i = 1\nPrzykład Obliczeń Ręcznych:\nObliczmy średnią ważoną dla zbioru danych: 2, 4, 5, 7 z wagami znormalizowanymi 0.1, 0.3, 0.4, 0.2\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPomnóż każdą wartość przez jej wagę\n(2 * 0.1) + (4 * 0.3) + (5 * 0.4) + (7 * 0.2)\n\n\n2\nZsumuj wyniki\n0.2 + 1.2 + 2.0 + 1.4 = 4.8\n\n\n\nObliczenia w R:\n\nx &lt;- c(2, 4, 5, 7)\nw_normalized &lt;- c(0.1, 0.3, 0.4, 0.2)  # Uwaga: sumują się do 1\nsum(x * w_normalized)\n\n[1] 4.8\n\n\nZalety Średnich Ważonych:\n\nUwzględniają różną ważność punktów danych\nPrzydatne w analizie ankiet o różnych wielkościach próby lub poziomach ważności\nMogą korygować nierówne prawdopodobieństwa w projektach próbkowania\n\nWady Średnich Ważonych:\n\nWymagają uzasadnienia dla wag\nMogą być niewłaściwie wykorzystane do manipulacji wynikami\nMogą być mniej intuicyjne w interpretacji niż prosta średnia arytmetyczna",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-zmienności-rozproszenia",
    "href": "rozdzial5.html#miary-zmienności-rozproszenia",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.7 Miary Zmienności (Rozproszenia)",
    "text": "12.7 Miary Zmienności (Rozproszenia)\nTe miary opisują, jak bardzo rozproszone są dane.\n\n\n\n\n\n\nZrozumienie Wariancji\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.1: Trzy wykresy punktowe pokazujące rosnącą wariancję przy stałej średniej\n\n\n\n\n\nPowyższe trzy wykresy punktowe pokazują, w jaki sposób wariancja mierzy rozproszenie danych wokół wartości centralnej:\n\nWszystkie rozkłady mają tę samą średnią (μ = 10), oznaczoną linią przerywaną\nMała Wariancja (σ² = 1): Punkty są skupione blisko średniej\nŚrednia Wariancja (σ² = 4): Punkty wykazują umiarkowane rozproszenie\nDuża Wariancja (σ² = 9): Punkty są szeroko rozproszone wokół średniej\n\n\n\n\n\n\n\n\n\nZrozumienie Różnych Poziomów Zmienności\n\n\n\n\n\n\n\n\n\n\n\n\nTa wizualizacja przedstawia trzy rozkłady normalne o tej samej średniej (μ = 10), ale różnych poziomach zmienności:\n\nMała zmienność (σ = 0.5)\n\nPunkty danych grupują się ściśle wokół średniej\nKrzywa gęstości jest wysoka i wąska\nWiększość obserwacji mieści się w przedziale ±0.5 jednostki (odchylenia stand.) od średniej\n\nŚrednia zmienność (σ = 2.0)\n\nPunkty danych są bardziej rozproszone wokół średniej\nKrzywa gęstości jest niższa i szersza\nWiększość obserwacji mieści się w przedziale ±2 jednostki od średniej\n\nDuża zmienność (σ = 4.0)\n\nPunkty danych są szeroko rozproszone wokół średniej\nKrzywa gęstości jest znacznie bardziej płaska i szeroka\nWiększość obserwacji mieści się w przedziale ±4 jednostki od średniej\n\n\nZwróć uwagę, jak odchylenie standardowe (σ) bezpośrednio wpływa na rozproszenie rozkładu - większe wartości σ wskazują na większą zmienność danych, podczas gdy mniejsze wartości oznaczają, że punkty danych mają tendencję do grupowania się bliżej średniej.\n\n\n\n12.7.1 Rozstęp\nRozstęp to różnica między wartością maksymalną a minimalną.\nWzór: R = x_{max} - x_{min}\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nZnajdź wartość maksymalną\n9\n\n\n2\nZnajdź wartość minimalną\n2\n\n\n3\nOdejmij minimum od maksimum\n9 - 2 = 7\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nrange(dane)\n\n[1] 2 9\n\nmax(dane) - min(dane)\n\n[1] 7\n\n\nZalety:\n\nProsty do obliczenia i zrozumienia\nSzybka informacja o ogólnym rozproszeniu danych\n\nWady:\n\nBardzo wrażliwy na wartości odstające\nNie dostarcza informacji o rozkładzie między skrajnościami\n\n\n\n12.7.2 Rozstęp Międzykwartylowy (IQR)\nIQR to różnica między 75. a 25. percentylem (3. a 1. kwartylem).\nWzór: IQR = Q_3 - Q_1\nAby znaleźć kwartyle ręcznie:\n\nDla nieparzystej liczby wartości:\n\nQ2 (mediana) to środkowa wartość\nQ1 to mediana dolnej połowy (wyłączając medianę dla wszystkich obserwacji)\nQ3 to mediana górnej połowy (wyłączając medianę dla wszystkich obserwacji)\n\nDla parzystej liczby wartości:\n\nQ2 to średnia z dwóch środkowych wartości\nQ1 to mediana dolnej połowy (wyłączając medianę dla wszystkich obserwacji)\nQ3 to mediana górnej połowy (wyłączając medianę dla wszystkich obserwacji)\n\n\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nUporządkuj dane\n2, 4, 4, 5, 5, 7, 9\n\n\n2\nZnajdź Q2 (medianę)\n5\n\n\n3\nZnajdź Q1 (medianę dolnej połowy)\n4\n\n\n4\nZnajdź Q3 (medianę górnej połowy)\n7\n\n\n5\nOblicz IQR\nQ3 - Q1 = 7 - 4 = 3\n\n\n\nObliczenie w R:\n\ndane &lt;- c(2, 4, 4, 5, 5, 7, 9)\nprint(dane)\n\n[1] 2 4 4 5 5 7 9\n\nquantile(dane, type = 1)\n\n  0%  25%  50%  75% 100% \n   2    4    5    7    9 \n\nIQR(dane, type = 1)\n\n[1] 3\n\n\nZalety:\n\nOdporny na wartości odstające\nDostarcza informacji o rozproszeniu środkowych 50% danych\n\nWady:\n\nIgnoruje ogony rozkładu\nMniej efektywny niż odchylenie standardowe dla rozkładów normalnych\n\n\n\n12.7.3 Wariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: s^2 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}\n\n\n\n\n\n\nWariancja: Zrozumienie Średniego Odchylenia Kwadratowego\n\n\n\nCzym jest Wariancja? Wariancja mierzy, jak bardzo punkty danych są “rozrzucone” wokół średniej - jest średnią kwadratów odchyleń od średniej.\nWzór: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nProsty Przykład: Rozważmy liczby: 2, 4, 6, 8, 10 Średnia (\\bar{x}) = 6\n\n\n\n\n\n\n\n\n\nObliczanie Odchyleń:\n\n\n\n\n\n\n\n\n\n\n\n\nWartość\nOdchylenie od średniej\nKwadrat odchylenia\n\n\n\n\n2\n-4\n16\n\n\n4\n-2\n4\n\n\n6\n0\n0\n\n\n8\n+2\n4\n\n\n10\n+4\n16\n\n\n\nWariancja = \\frac{16 + 4 + 0 + 4 + 16}{4} = 10\nKluczowe Punkty:\n\nŚrednia służy jako punkt odniesienia (niebieska przerywana linia)\nOdchylenia pokazują odległość od średniej (czerwone kropkowane linie)\nPodniesienie do kwadratu sprawia, że wszystkie odchylenia są dodatnie (niebieskie słupki)\nWiększe odchylenia mają większy wpływ na wariancję\n\n\n\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz średnią\n\\bar{x} = 5,14\n\n\n2\nOdejmij średnią od każdej obserwacji i podnieś wynik do kwadratu\n(2 - 5,14)^2 = 9,86\n\n\n\n\n(4 - 5,14)^2 = 1,30\n\n\n\n\n(4 - 5,14)^2 = 1,30\n\n\n\n\n(5 - 5,14)^2 = 0,02\n\n\n\n\n(5 - 5,14)^2 = 0,02\n\n\n\n\n(7 - 5,14)^2 = 3,46\n\n\n\n\n(9 - 5,14)^2 = 14,90\n\n\n3\nSumuj kwadraty różnic\n30,86\n\n\n4\nPodziel przez (n-1), czyli przez liczbę obserwacji - 1\n30,86 / 6 = 5,14\n\n\n\nObliczenie w R:\n\nvar(dane)\n\n[1] 5.142857\n\n\nZalety:\n\nWykorzystuje wszystkie punkty danych\nPodstawa dla wielu testów statystycznych*\n\nWady:\n\nJednostki są podniesione do kwadratu, co utrudnia interpretację\nWrażliwa na wartości odstające\n\n\n\n\n\n\n\nPoprawka Bessela: Dlaczego Dzielimy przez (n-1), a nie po prostu przez n\n\n\n\nGdy obliczamy odchylenia od średniej, ich suma musi wynosić zero. To matematyczny fakt: \\sum(x_i - \\bar{x}) = 0\nPomyśl o tym Tak:\nJeśli masz 5 liczb i ich średnią:\n\nPo obliczeniu 4 odchyleń od średniej\n5-te odchylenie MUSI być takie, żeby suma była zero\nNie masz tak naprawdę 5 niezależnych odchyleń\nMasz tylko 4 prawdziwie “swobodne” odchylenia\n\nProsty Przykład:\nLiczby: 2, 4, 6, 8, 10\n\nŚrednia = 6\nOdchylenia: -4, -2, 0, +2, +4\nZauważ, że sumują się do zera\nJeśli znasz dowolne 4 odchylenia, 5-te jest z góry określone!\n\nDlatego Właśnie:\n\nPrzy obliczaniu wariancji: s^2 = \\frac{\\sum(x_i - \\bar{x})^2}{n-1}\nDzielimy przez (n-1), a nie n\nPonieważ tylko (n-1) odchyleń jest naprawdę niezależnych\nOstatnie jest określone przez pozostałe\n\nStopnie Swobody:\n\nn = liczba obserwacji\n1 = ograniczenie (odchylenia muszą sumować się do zera)\nn-1 = stopnie swobody = liczba prawdziwie niezależnych odchyleń\n\nKiedy Stosować:\n\nPrzy obliczaniu wariancji z próby\nPrzy obliczaniu odchylenia standardowego z próby\n\nKiedy NIE Stosować:\n\nW obliczeniach dla całej populacji (gdy mamy wszystkie dane)\nPrzy obliczaniu odchylenia od ustalonej, znanej wartości (nie obliczonej średniej)\n\nPamiętaj:\n\nTo nie jest tylko statystyczny trik\nOdchylenia od średniej muszą sumować się do zera\nTo ograniczenie kosztuje nas jeden stopień swobody\n\n\n\n\n\n12.7.4 Odchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji i mierzy przeciętne rozproszenie danych względem ich średniej arytmetycznej. W przeciwieństwie do wariancji, jest to miara mianowana i interpretowana w jednostkach bdanej zmiennej.\nWzór: s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n - 1}}\nPrzykład Ręcznego Obliczenia:\nUżywając zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz wariancję\ns^2 = 5,14 (z poprzedniego obliczenia)\n\n\n2\nWyciągnij pierwiastek kwadratowy\ns = \\sqrt{5,14} = 2,27\n\n\n\nObliczenie w R:\n\nsd(dane)\n\n[1] 2.267787\n\n\nZalety:\n\nW tych samych jednostkach co oryginalne dane\nSzeroko stosowane i zrozumiałe\n\nWady:\n\nNadal wrażliwe na wartości odstające\nZakłada, że dane są w przybliżeniu “normalnie” rozłożone\n\n\n\n12.7.5 Współczynnik zmienności (*)\nWspółczynnik zmienności to odchylenie standardowe podzielone przez średnią arytmetyczną, często wyrażany jako wartość procentowa.\nWzór: CV = \\frac{s}{\\bar{x}} \\times 100\\%\nPrzykład obliczeń ręcznych:\nDla zbioru danych: 2, 4, 4, 5, 5, 7, 9\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenia\n\n\n\n\n1\nOblicz średnią arytmetyczną\n\\bar{x} = 5,14\n\n\n2\nOblicz odchylenie standardowe\ns = 2,27\n\n\n3\nPodziel s przez średnią i pomnóż przez 100\n(2,27 / 5,14) * 100 = 44,16\\%\n\n\n\nObliczenia w R:\n\n(sd(dane) / mean(dane)) * 100\n\n[1] 44.09586\n\n\nZalety:\n- Umożliwia porównanie zmienności między zbiorami danych o różnych jednostkach lub średnich\n- Przydatny w dziedzinach takich jak finanse do oceny ryzyka\nWady:\n- Nie ma znaczenia dla danych zawierających zarówno wartości dodatnie, jak i ujemne\n- Może być mylący, gdy średnia jest bliska zeru\n\n\n\n\n\n\nOgraniczenia Współczynnika Zmienności (CV)\n\n\n\nWspółczynnik zmienności, obliczany jako (σ/μ) × 100\\%, ma dwa istotne ograniczenia:\n\n12.7.5.1 Nie ma interpretacji dla danych zawierających wartości dodatnie i ujemne\n\nŚrednia może być bliska zeru ze względu na wzajemne znoszenie się wartości dodatnich i ujemnych\nPrzykład: Zbiór danych {-5, -3, 2, 6} ma średnią = 0\n\nCV = (odch. std. / 0) × 100%\nProwadzi to do dzielenia przez zero\nNawet gdy średnia nie jest dokładnie zero, CV nie reprezentuje prawdziwej względnej zmienności, gdy dane przechodzą przez zero\n\nCV zakłada naturalny punkt zerowy i sensowne proporcje między wartościami\n\n\n\n12.7.5.2 Mylący gdy średnia jest bliska zeru\n\nPonieważ CV = (σ/μ) × 100\\%, gdy μ zbliża się do zera:\n\nMianownik staje się bardzo mały\nSkutkuje to ekstremalnie dużymi wartościami CV\nTe duże wartości nie reprezentują sensownie względnej zmienności\n\nPrzykład:\n\nZbiór danych A: {0.001, 0.002, 0.003} ma średnią = 0.002\nNawet małe odchylenia standardowe dadzą bardzo duże CV\nWynikający z tego duży CV może sugerować ekstremalne zróżnicowanie, gdy w rzeczywistości dane są dość skoncentrowane\n\n\n\n\n12.7.5.3 Najlepsze zastosowania\nCV jest najbardziej użyteczny dla:\n\nDanych ściśle dodatnich\nDanych mierzonych na skali ilorazowej\nDanych ze średnią znacznie powyżej zera\nPorównywania zmienności między zbiorami danych o różnych jednostkach lub skalach",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-położenia-względnego-względnej-pozycji",
    "href": "rozdzial5.html#miary-położenia-względnego-względnej-pozycji",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.8 Miary Położenia Względnego (Względnej Pozycji)",
    "text": "12.8 Miary Położenia Względnego (Względnej Pozycji)\nZrozumienie relatywnej (względnej) pozycji wartości w zbiorze danych.\n\n12.8.1 Kwartyle (Q): Podstawy\nKwartyle to specjalne liczby, które dzielą uporządkowane dane na cztery równe części.\n\n\n\nDoane, D. P., & Seward, L. W. (2016). Applied statistics in business and economics. Mcgraw-Hill.\n\n\n\n12.8.1.1 Czym są Kwartyle?\nPierwszy Kwartyl (Q1):\n\nOddziela najniższe 25% danych od reszty\nNazywany również 25-tym percentylem\nPrzykład: Jeśli Q1 = 50 w zbiorze wyników testu, 25% uczniów uzyskało wynik poniżej 50\n\nDrugi Kwartyl (Q2):\n\nMediana - dzieli dane na pół\nNazywany również 50-tym percentylem\nPrzykład: Jeśli Q2 = 70, połowa uczniów uzyskała wynik poniżej 70\n\nTrzeci Kwartyl (Q3):\n\nOddziela najwyższe 25% danych od reszty\nNazywany również 75-tym percentylem\nPrzykład: Jeśli Q3 = 85, 75% uczniów uzyskało wynik poniżej 85\n\n\n\n12.8.1.2 Jak Obliczać Kwartyle (Krok po Kroku) - Dwie Metody\nPrzeanalizujmy wyniki testów uczniów używając obu popularnych metod wyznaczania kwartyli:\nPrzykład 1: Przypadek Nieparzystej Liczby Wyników (11 wyników)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 88, 90\nKrok 1: Znajdź Q2 (medianę) - Tak samo dla obu metod\n\nPrzy n = 11 wartościach (nieparzyste)\nPozycja mediany = (n + 1)/2 = 6\nQ2 = 78\n\nKrok 2: Znajdź Q1\n\nMetoda Tukeya:\n\nSpójrz na dolną połowę: 60, 65, 70, 72, 75\nQ1 = mediana dolnej połowy = 70\n\nMetoda Interpolacji:\n\nPozycja = (n + 1)/4 = (11 + 1)/4 = 3\nQ1 = 70 (3-cia wartość)\n\n\nKrok 3: Znajdź Q3\n\nMetoda Tukeya:\n\nSpójrz na górną połowę: 80, 82, 85, 88, 90\nQ3 = mediana górnej połowy = 85\n\nMetoda Interpolacji:\n\nPozycja = 3(n + 1)/4 = 3(12)/4 = 9\nQ3 = 85 (9-ta wartość)\n\n\nPrzykład 2: Przypadek Parzystej Liczby (10 wyników)\n60, 65, 70, 72, 75, 78, 80, 82, 85, 90\nKrok 1: Znajdź Q2 (medianę) - Tak samo dla obu metod\n\nPrzy n = 10 wartościach (parzyste)\nPozycje mediany = 5 i 6\nQ2 = (75 + 78)/2 = 76.5\n\nKrok 2: Znajdź Q1\n\nMetoda Tukeya:\n\nSpójrz na dolną połowę: 60, 65, 70, 72, 75\nQ1 = mediana dolnej połowy = 70\n\nMetoda Interpolacji:\n\nPozycja = (10 + 1)/4 = 2.75\nQ1 = 65 + 0.75(70 - 65) = 68.75\n\n\nKrok 3: Znajdź Q3\n\nMetoda Tukeya:\n\nSpójrz na górną połowę: 78, 80, 82, 85, 90\nQ3 = mediana górnej połowy = 82\n\nMetoda Interpolacji:\n\nPozycja = 3(10 + 1)/4 = 8.25\nQ3 = 82 + 0.25(85 - 82) = 82.75\n\n\nWażne Uwagi:\n\nMetoda Tukeya:\n\nNajpierw znajdź medianę (Q2)\nPodziel dane na dolną i górną połowę\nZnajdź Q1 jako medianę dolnej połowy\nZnajdź Q3 jako medianę górnej połowy\nGdy n jest nieparzyste, mediana nie jest uwzględniana w żadnej połowie\n\nMetoda Interpolacji:\n\nUżywa pozycji (n+1)/4 dla Q1 i 3(n+1)/4 dla Q3\nGdy pozycja wypada między wartościami, stosuje interpolację liniową\nNie wymaga podziału danych na połowy\n\n\nObie metody dają te same wyniki dla prostych pozycji (Przykład 1), ale mogą się różnić, gdy potrzebna jest interpolacja (Przykład 2).\n\n\n\n\n\n\nManual Construction of Tukey Boxplot\n\n\n\nStep 1: Calculate Key Components\n\nFind quartiles: Q_1, Q_2 (median), Q_3\nCalculate Interquartile Range: IQR = Q_3 - Q_1\n\nStep 2: Determine Whisker Boundaries\n\nLower fence: Q_1 - 1.5 \\times IQR\nUpper fence: Q_3 + 1.5 \\times IQR\n\nStep 3: Identify Outliers Data points are outliers if they are:\n\nBelow lower fence: x &lt; Q_1 - 1.5 \\times IQR\nAbove upper fence: x &gt; Q_3 + 1.5 \\times IQR\n\nExample: Given data: 2, 4, 6, 8, 9, 10, 11, 12, 14, 16, 50\n\nFind quartiles:\n\nQ_1 = 6\nQ_2 = 10\nQ_3 = 14\n\nCalculate IQR:\n\nIQR = 14 - 6 = 8\n\nCalculate fences:\n\nLower: 6 - (1.5 \\times 8) = -6\nUpper: 14 + (1.5 \\times 8) = 26\n\nIdentify outliers:\n\n50 &gt; 26, therefore 50 is an outlier\n\n\nGraphical Elements:\n\nBox: Draw from Q_1 to Q_3\nLine inside box: Draw at Q_2\nWhiskers: Extend to most extreme non-outlier points\nPoints: Plot outliers individually beyond whiskers\n\n\n\n\n\n\n12.8.2 Percentyle: Bardziej Precyzyjna Miara Względnej Pozycji (*)\n\n12.8.2.1 Czym są Percentyle?\nPercentyle dają nam bardziej szczegółowy obraz, dzieląc dane na 100 równych części. W przeciwieństwie do kwartyli, percentyle używają interpolacji liniowej.\nKluczowe Punkty:\n\n25-ty percentyl równa się Q1\n50-ty percentyl równa się Q2 (mediana)\n75-ty percentyl równa się Q3\n\n\n\n12.8.2.2 Obliczanie Percentyli\nWzór: P_k = \\frac{k(n+1)}{100}\nGdzie:\n\nP_k to pozycja dla k-tego percentyla\nk to percentyl, który chcemy znaleźć (1-100)\nn to liczba obserwacji\n\nPrzykład 3: Znajdowanie 60-tego Percentyla Użyjmy wyników zadań domowych uczniów: 72, 75, 78, 80, 82, 85, 88, 90, 92, 95\nKrok 1: Oblicz pozycję\n\nn = 10 wyników\nDla 60-tego percentyla: P_{60} = \\frac{60(10+1)}{100} = 6.6\n\nKrok 2: Znajdź otaczające wartości\n\nPozycja 6: wynik 85\nPozycja 7: wynik 88\n\nKrok 3: Interpoluj (ważne: percentyle używają interpolacji liniowej)\n\nMusimy przejść 0.6 drogi między 85 a 88 P_{60} = 85 + 0.6(88-85) P_{60} = 85 + 0.6(3) P_{60} = 85 + 1.8 = 86.8\n\nCo to oznacza: 60% uczniów uzyskało wynik 86.8 lub niższy.\n\n\n\n12.8.3 Rangi Percentylowe (PR) (*)\n\n12.8.3.1 Czym jest Ranga Percentylowa?\nPodczas gdy percentyle mówią nam o wartości na określonej pozycji, ranga percentylowa mówi nam, jaki procent wartości znajduje się poniżej określonego wyniku. Można to traktować jako odpowiedź na pytanie “Jaki procent klasy uzyskał wynik niższy niż ja?”\nPR = \\frac{\\text{liczba wartości poniżej } + 0.5 \\times \\text{liczba równych wartości}}{\\text{całkowita liczba wartości}} \\times 100\nPrzykład 4: Znajdowanie Rangi Percentylowej Rozważmy te wyniki egzaminu:\n65, 70, 70, 75, 75, 75, 80, 85, 85, 90\nZnajdźmy PR dla wyniku 75.\nKrok 1: Dokładnie policz\n\nWartości poniżej 75: 65, 70, 70 (3 wartości)\nWartości równe 75: 75, 75, 75 (3 wartości)\nCałkowita liczba wartości: 10\n\nKrok 2: Zastosuj wzór\nPR = \\frac{3 + 0.5(3)}{10} \\times 100 PR = \\frac{3 + 1.5}{10} \\times 100 PR = \\frac{4.5}{10} \\times 100 = 45\\%\nInterpretacja: Wynik 75 jest wyższy niż 45% wyników w klasie.\nUwaga:\nP1: “Dlaczego używamy 0.5 dla równych wartości w PR?”\nO1: Jest tak, ponieważ zakładamy, że osoby z tym samym wynikiem są równomiernie rozłożone na tej pozycji. To jak powiedzenie, że dzielą pozycję po równo.\n\n\n\n12.8.4 Zadania z Rozwiązaniami\nZadanie 1: Podstawowe Kwartyle\nDane: 10, 12, 15, 15, 18, 20, 22, 25, 25 Znajdź: Q1, Q2, Q3\nRozwiązanie:\n\nQ2 (n = 9, nieparzyste)\n\nPozycja = (9 + 1)/2 = 5\nQ2 = 18\n\nQ1\n\nPozycja = (9 + 1)/4 = 2.5\nMiędzy 12 a 15\nQ1 = (12 + 15)/2 = 13.5\n\nQ3\n\nPozycja = 3(9 + 1)/4 = 7.5\nMiędzy 22 a 25\nQ3 = (22 + 25)/2 = 23.5\n\n\n\n\n\n\n\n\nPodwójna Rola Mediany\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.2: Wizualizacja podwójnej roli mediany\n\n\n\n\n\nMediana pełni dwie odrębne, ale powiązane ze sobą role:\nA. Jako Miara Centrum:\n\nReprezentuje środkowy punkt danych\nRównoważy liczbę obserwacji po obu stronach\nJest odporna na wartości odstające (w przeciwieństwie do średniej arytmetycznej)\n\nB. Jako Miara Pozycji Względnej:\n\nWyznacza 50-ty percentyl\nDzieli dane na dwie równe części\nKażdą wartość można do niej odnieść:\n\nPoniżej mediany: dolne 50%\nPowyżej mediany: górne 50%\n\n\nTa podwójna natura sprawia, że mediana jest szczególnie przydatna do:\n\nOpisywania wartości typowych (tendencja centralna)\nZrozumienia pozycji w rozkładzie (pozycja względna)\nDokonywania porównań między różnymi zbiorami danych",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#miary-kształtu",
    "href": "rozdzial5.html#miary-kształtu",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.9 Miary Kształtu",
    "text": "12.9 Miary Kształtu\n\n12.9.1 Skośność\n\n12.9.1.1 Definicja\nSkośność kwantyfikuje asymetrię rozkładu danych. Wskazuje, czy dane grupują się bardziej po jednej stronie średniej niż po drugiej.\n\n\n12.9.1.2 Wyrażenie Matematyczne\nSK = \\frac{n}{(n-1)(n-2)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^3 gdzie:\n\nn to wielkość próby\nx_i to i-ta obserwacja\n\\bar{x} to średnia z próby\ns to odchylenie standardowe z próby\n\n\n\n12.9.1.3 Przykład: Analiza Frekwencji Wyborczej\n\nlibrary(moments)\n\n\nAttaching package: 'moments'\n\n\nThe following object is masked from 'package:modeest':\n\n    skewness\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n# Generowanie przykładowych danych frekwencji w obwodach wyborczych\nset.seed(123)\ndane_frekwencji &lt;- c(\n  # Obwody miejskie\n  rnorm(300, mean = 65, sd = 12),\n  # Obwody podmiejskie\n  rnorm(400, mean = 70, sd = 10),\n  # Obwody wiejskie\n  rnorm(300, mean = 68, sd = 15)\n) |&gt; \n  # Zapewnienie, że frekwencja mieści się w przedziale 0-100%\n  pmax(0) |&gt; \n  pmin(100)\n\n# Obliczenie i wizualizacja\nwartosc_skosnosci &lt;- skewness(dane_frekwencji)\nwartosc_skosnosci\n\n[1] 0.02558143\n\nggplot(data.frame(x = dane_frekwencji), aes(x = x)) +\n  geom_histogram(bins = 50, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = mean(dane_frekwencji), color = \"red\", linetype = \"dashed\") +\n  geom_vline(xintercept = median(dane_frekwencji), color = \"blue\", linetype = \"dashed\") +\n  labs(\n    title = str_glue(\"Rozkład Frekwencji w Obwodach Wyborczych (Skośność = {round(wartosc_skosnosci, 4)})\"),\n    subtitle = \"Czerwona: Średnia, Niebieska: Mediana\",\n    x = \"Frekwencja Wyborcza (%)\",\n    y = \"Liczba Obwodów\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n12.9.1.4 Przewodnik Interpretacji\n\nSkośność Dodatnia (&gt; 0): Rozkład ma dłuższy ogon prawy\nSkośność Ujemna (&lt; 0): Rozkład ma dłuższy ogon lewy\nSkośność Zero: Rozkład w przybliżeniu symetryczny\n\n\n\n\n12.9.2 Kurtoza\n\n12.9.2.1 Definicja\nKurtoza mierzy “ogoniastość” rozkładu, wskazując na obecność wartości ekstremalnych w porównaniu z rozkładem normalnym.\n\n\n12.9.2.2 Wyrażenie Matematyczne\nK = \\frac{n(n+1)}{(n-1)(n-2)(n-3)} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{s})^4 - \\frac{3(n-1)^2}{(n-2)(n-3)}\n\n\n12.9.2.3 Przykład: Analiza Głosowań Parlamentarnych\n\n# Generowanie przykładowych wyników zgodności głosowań\nset.seed(456)\nzgodnosc_glosowan &lt;- c(\n  # Standardowe wzorce głosowania\n  rnorm(400, mean = 75, sd = 10),\n  # Przypadki współpracy międzypartyjnej\n  rnorm(80, mean = 50, sd = 15),\n  # Głosowania zgodne z linią partii\n  rnorm(20, mean = 95, sd = 5)\n) |&gt; \n  pmax(0) |&gt; \n  pmin(100)\n\nwartosc_kurtozy &lt;- kurtosis(zgodnosc_glosowan)\nwartosc_kurtozy\n\n[1] 3.849939\n\n# Wizualizacja z porównaniem do rozkładu normalnego\nzakres_x &lt;- seq(min(zgodnosc_glosowan)-1, max(zgodnosc_glosowan)+1, length.out = 100)\nrozklad_normalny &lt;- dnorm(zakres_x, mean = mean(zgodnosc_glosowan), sd = sd(zgodnosc_glosowan))\n\nggplot() +\n  geom_density(\n    data = data.frame(x = zgodnosc_glosowan), \n    aes(x = x), \n    fill = \"skyblue\", \n    alpha = 0.5\n  ) +\n  geom_line(\n    data = data.frame(x = zakres_x, y = rozklad_normalny),\n    aes(x = x, y = y),\n    color = \"red\",\n    linetype = \"dashed\"\n  ) +\n  labs(\n    title = str_glue(\"Rozkład Zgodności Głosowań (Kurtoza = {round(wartosc_kurtozy, 4)})\"),\n    subtitle = \"Rozkład obserwowany (niebieski) vs. Rozkład normalny (czerwony)\",\n    x = \"Wskaźnik Zgodności Głosowań (%)\",\n    y = \"Gęstość\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n12.9.2.4 Przewodnik Interpretacji\n\nNadwyżka Kurtozy: Różnica względem kurtozy rozkładu normalnego\n\n\n\nLeptokurtyczny (&gt; 3): Więcej wartości ekstremalnych niż w rozkładzie normalnym\nPlatykurtyczny (&lt; 3): Mniej wartości ekstremalnych niż w rozkładzie normalnym\nMezokurtyczny (= 3): Podobny do rozkładu normalnego",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ćwiczenie-1.-porównanie-wynagrodzeń",
    "href": "rozdzial5.html#ćwiczenie-1.-porównanie-wynagrodzeń",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.10 Ćwiczenie 1. Porównanie wynagrodzeń",
    "text": "12.10 Ćwiczenie 1. Porównanie wynagrodzeń\n\n12.10.1 Dane\nMamy dane o wynagrodzeniach (w tysiącach euro) z dwóch małych firm europejskich:\n\n\n\nIndex\nFirma X\nFirma Y\n\n\n\n\n1\n2\n3\n\n\n2\n2\n3\n\n\n3\n2\n4\n\n\n4\n3\n4\n\n\n5\n3\n4\n\n\n6\n3\n4\n\n\n7\n3\n4\n\n\n8\n3\n4\n\n\n9\n3\n5\n\n\n10\n4\n5\n\n\n11\n4\n5\n\n\n12\n4\n5\n\n\n13\n4\n5\n\n\n14\n4\n5\n\n\n15\n5\n6\n\n\n16\n5\n6\n\n\n17\n5\n6\n\n\n18\n5\n7\n\n\n19\n20\n7\n\n\n20\n35\n8\n\n\n\n\n\n12.10.2 Miary tendencji centralnej\n\n12.10.2.1 Średnia arytmetyczna\nŚrednia arytmetyczna to suma wszystkich wartości podzielona przez ich liczbę.\nWzór: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\n12.10.2.1.1 Obliczenia ręczne dla Firmy X\n\n\n\nWartość (x_i)\nCzęstość (f_i)\nx_i \\cdot f_i\n\n\n\n\n2\n3\n6\n\n\n3\n6\n18\n\n\n4\n5\n20\n\n\n5\n4\n20\n\n\n20\n1\n20\n\n\n35\n1\n35\n\n\nSuma\nn = 20\nSuma = 119\n\n\n\n\\bar{x} = \\frac{119}{20} = 5,95\n\n\n12.10.2.1.2 Obliczenia ręczne dla Firmy Y\n\n\n\nWartość (x_i)\nCzęstość (f_i)\nx_i \\cdot f_i\n\n\n\n\n3\n2\n6\n\n\n4\n6\n24\n\n\n5\n6\n30\n\n\n6\n3\n18\n\n\n7\n2\n14\n\n\n8\n1\n8\n\n\nSuma\nn = 20\nSuma = 100\n\n\n\n\\bar{y} = \\frac{100}{20} = 5\n\n\n12.10.2.1.3 Weryfikacja w R\n\nX &lt;- c(2,2,2,3,3,3,3,3,3,4,4,4,4,4,5,5,5,5,20,35)\nY &lt;- c(3,3,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,7,7,8)\n\nmean(X)\n\n[1] 5.95\n\nmean(Y)\n\n[1] 5\n\n\n\n\n\n12.10.2.2 Mediana\nMediana to wartość środkowa w uporządkowanym zbiorze danych.\n\n12.10.2.2.1 Obliczenia ręczne dla Firmy X\nUporządkowane dane: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\nn = 20 (parzyste), więc bierzemy średnią z 10. i 11. wartości:\nMediana = \\frac{4 + 4}{2} = 4\n\n\n12.10.2.2.2 Obliczenia ręczne dla Firmy Y\nUporządkowane dane: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\nn = 20 (parzyste), więc bierzemy średnią z 10. i 11. wartości:\nMediana = \\frac{5 + 5}{2} = 5\n\n\n12.10.2.2.3 Weryfikacja w R\n\nmedian(X)\n\n[1] 4\n\nmedian(Y)\n\n[1] 5\n\n\n\n\n\n12.10.2.3 Dominanta (moda)\nDominanta to najczęściej występująca wartość w zbiorze danych.\nDla Firmy X dominanta wynosi 3 (występuje 6 razy). Dla Firmy Y są dwie dominanty: 4 i 5 (obie występują 6 razy).\n\n# Funkcja do obliczania dominanty\nznajdz_dominante &lt;- function(x) {\n  unikalne_x &lt;- unique(x)\n  unikalne_x[which.max(tabulate(match(x, unikalne_x)))]\n}\n\nznajdz_dominante(X)\n\n[1] 3\n\nznajdz_dominante(Y)\n\n[1] 4\n\n\n\n\n\n12.10.3 Miary rozproszenia\n\n12.10.3.1 Wariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\n12.10.3.1.1 Obliczenia ręczne dla Firmy X\n\n\n\n\n\n\n\n\n\n\nx_i\nf_i\nx_i - \\bar{x}\n(x_i - \\bar{x})^2\nf_i(x_i - \\bar{x})^2\n\n\n\n\n2\n3\n-3,95\n15,6025\n46,8075\n\n\n3\n6\n-2,95\n8,7025\n52,215\n\n\n4\n5\n-1,95\n3,8025\n19,0125\n\n\n5\n4\n-0,95\n0,9025\n3,61\n\n\n20\n1\n14,05\n197,4025\n197,4025\n\n\n35\n1\n29,05\n843,9025\n843,9025\n\n\nSuma\n20\n\n\n1162,95\n\n\n\ns^2 = \\frac{1162,95}{19} = 61,21\n\n\n12.10.3.1.2 Obliczenia ręczne dla Firmy Y\n\n\n\n\n\n\n\n\n\n\ny_i\nf_i\ny_i - \\bar{x}\n(y_i - \\bar{y})^2\nf_i(y_i - \\bar{y})^2\n\n\n\n\n3\n2\n-2\n4\n8\n\n\n4\n6\n-1\n1\n6\n\n\n5\n6\n0\n0\n0\n\n\n6\n3\n1\n1\n3\n\n\n7\n2\n2\n4\n8\n\n\n8\n1\n3\n9\n9\n\n\nSuma\n20\n\n\n34\n\n\n\ns^2 = \\frac{34}{19} = 1,79\n\n\n12.10.3.1.3 Weryfikacja w R\n\nvar(X)\n\n[1] 61.20789\n\nvar(Y)\n\n[1] 1.789474\n\n\n\n\n\n12.10.3.2 Odchylenie standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji.\nWzór: s = \\sqrt{s^2}\n\nDla Firmy X: s = \\sqrt{61,21} = 7,82\nDla Firmy Y: s = \\sqrt{1,79} = 1,34\n\n\n12.10.3.2.1 Weryfikacja w R\n\nsd(X)\n\n[1] 7.823547\n\nsd(Y)\n\n[1] 1.337712\n\n\n\n\n\n\n12.10.4 Kwartyle\nKwartyle dzielą zbiór danych na cztery równe części.\n\n12.10.4.1 Obliczenia ręczne dla Firmy X\nUporządkowane dane: [2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 20, 35]\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 3\nQ2 (50. percentyl, mediana): 4\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 5\n\n\n\n12.10.4.2 Obliczenia ręczne dla Firmy Y\nUporządkowane dane: [3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8]\n\nQ1 (25. percentyl): mediana pierwszych 10 liczb = 4\nQ2 (50. percentyl, mediana): 5\nQ3 (75. percentyl): mediana ostatnich 10 liczb = 6\n\n\n\n12.10.4.3 Weryfikacja w R\n\nquantile(X)\n\n  0%  25%  50%  75% 100% \n   2    3    4    5   35 \n\nquantile(Y)\n\n  0%  25%  50%  75% 100% \n   3    4    5    6    8 \n\n\n\n\n12.10.4.4 IQR\n\nIQR_x = 5 - 3 = 2\nIQR_y = 6 - 4 = 2\n\n\n\n\n12.10.5 Wykres pudełkowy Tukeya\nWykres pudełkowy Tukeya wizualnie przedstawia rozkład danych na podstawie kwartyli. Użyjemy biblioteki ggplot2 do stworzenia wykresu.\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Przygotowanie danych\ndane &lt;- data.frame(\n  Firma = rep(c(\"X\", \"Y\"), each = 20),\n  Wynagrodzenie = c(X, Y)\n)\n\n# Tworzenie wykresu pudełkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot() +\n  labs(title = \"Rozkład wynagrodzeń w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiące euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n# Tworzenie wykresu pudełkowego\nggplot(dane, aes(x = Firma, y = Wynagrodzenie, fill = Firma)) +\n  geom_boxplot(outliers = F) +\n  labs(title = \"Rozkład wynagrodzeń w firmach X i Y\",\n       x = \"Firma\",\n       y = \"Wynagrodzenie (tysiące euro)\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"X\" = \"#69b3a2\", \"Y\" = \"#404080\"))\n\n\n\n\n\n\n\n\n\n12.10.5.1 Interpretacja wykresu pudełkowego\n\nPudełko reprezentuje rozstęp międzykwartylowy (IQR) od Q1 do Q3.\nLinia wewnątrz pudełka to mediana (Q2).\nWąsy rozciągają się do najmniejszych i największych wartości w granicach 1,5 * IQR.\nPunkty poza wąsami są uznawane za wartości odstające.\n\n\n\n\n12.10.6 Porównanie wyników\n\n\n\nMiara\nFirma X\nFirma Y\n\n\n\n\nŚrednia\n5,95\n5,00\n\n\nMediana\n4\n5\n\n\nDominanta\n3\n4 i 5\n\n\nWariancja\n61,21\n1,79\n\n\nOdchylenie standard.\n7,82\n1,34\n\n\nQ1\n3\n4\n\n\nQ3\n5\n6\n\n\n\n\n12.10.6.1 Kluczowe obserwacje:\n\nTendencja centralna: Firma X ma wyższą średnią, ale niższą medianę niż Firma Y, co wskazuje na prawostronnie skośny rozkład dla Firmy X.\n\nRozproszenie: Firma X wykazuje znacznie wyższą wariancję i odchylenie standardowe, sugerując większe dysproporcje w wynagrodzeniach.\nKształt rozkładu: Wynagrodzenia w Firmie Y są bardziej skupione, podczas gdy Firma X ma wartości ekstremalne (potencjalne wartości odstające), które znacząco wpływają na jej średnią i wariancję.\nKwartyle: Rozstęp międzykwartylowy (Q3 - Q1) Firmy Y jest nieznacznie większy, ale jej ogólny zakres jest znacznie mniejszy niż Firmy X.\n\n\n\n\n12.10.7 Wnioski\nTa analiza porównawcza ujawnia znaczące różnice w strukturach wynagrodzeń między dwiema firmami. Firma X wykazuje większą zmienność i potencjalną nierówność w swojej skali płac, podczas gdy Firma Y demonstruje bardziej spójny i wąsko rozłożony zakres wynagrodzeń.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#ćwiczenie-2.-porównanie-zmienności-wielkości-okręgów-wyborczych-między-krajami",
    "href": "rozdzial5.html#ćwiczenie-2.-porównanie-zmienności-wielkości-okręgów-wyborczych-między-krajami",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.11 Ćwiczenie 2. Porównanie Zmienności Wielkości Okręgów Wyborczych Między Krajami",
    "text": "12.11 Ćwiczenie 2. Porównanie Zmienności Wielkości Okręgów Wyborczych Między Krajami\n\n12.11.1 Dane\nMamy dane o wielkości okręgów wyborczych z dwóch krajów:\n\nx &lt;- c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19)  # Kraj wysoka zmienność\ny &lt;- c(8, 9, 9, 10, 10, 11, 11, 12, 12, 13)  # Kraj niska zmienność\n\nkable(data.frame(\n  \"Kraj X (Wysoka zm.)\" = x,\n  \"Kraj Y (Niska zm.)\" = y\n))\n\n\n\n\nKraj.X..Wysoka.zm..\nKraj.Y..Niska.zm..\n\n\n\n\n1\n8\n\n\n3\n9\n\n\n5\n9\n\n\n7\n10\n\n\n9\n10\n\n\n11\n11\n\n\n13\n11\n\n\n15\n12\n\n\n17\n12\n\n\n19\n13\n\n\n\n\n\n\n\n12.11.2 Miary Tendencji Centralnej\n\n12.11.2.1 Średnia Arytmetyczna\nWzór: \\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\n12.11.2.1.1 Obliczenia dla Kraju X\n\n\n\nElement\nWartość\n\n\n\n\n1\n1\n\n\n2\n3\n\n\n3\n5\n\n\n4\n7\n\n\n5\n9\n\n\n6\n11\n\n\n7\n13\n\n\n8\n15\n\n\n9\n17\n\n\n10\n19\n\n\nSuma\n100\n\n\n\n\\bar{x} = \\frac{100}{10} = 10\n\nmean_x &lt;- mean(x)\nc(\"Ręcznie\" = 10, \"R\" = mean_x)\n\nRęcznie       R \n     10      10 \n\n\n\n\n12.11.2.1.2 Obliczenia dla Kraju Y\n\n\n\nElement\nWartość\n\n\n\n\n1\n8\n\n\n2\n9\n\n\n3\n9\n\n\n4\n10\n\n\n5\n10\n\n\n6\n11\n\n\n7\n11\n\n\n8\n12\n\n\n9\n12\n\n\n10\n13\n\n\nSuma\n105\n\n\n\n\\bar{y} = \\frac{105}{10} = 10,5\n\nmean_y &lt;- mean(y)\nc(\"Ręcznie\" = 10.5, \"R\" = mean_y)\n\nRęcznie       R \n   10.5    10.5 \n\n\n\n\n\n12.11.2.2 Mediana\nMediana to wartość środkowa w uporządkowanym zbiorze danych.\n\n12.11.2.2.1 Obliczenia dla Kraju X\nUporządkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nDla n = 10 (parzysta liczba obserwacji): Pozycje środkowe: 5 i 6 Wartości środkowe: 9 i 11\nMediana = \\frac{9 + 11}{2} = 10\n\nmedian_x &lt;- median(x)\nc(\"Ręcznie\" = 10, \"R\" = median_x)\n\nRęcznie       R \n     10      10 \n\n\n\n\n12.11.2.2.2 Obliczenia dla Kraju Y\nUporządkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nDla n = 10 (parzysta liczba obserwacji): Pozycje środkowe: 5 i 6 Wartości środkowe: 10 i 11\nMediana = \\frac{10 + 11}{2} = 10,5\n\nmedian_y &lt;- median(y)\nc(\"Ręcznie\" = 10.5, \"R\" = median_y)\n\nRęcznie       R \n   10.5    10.5 \n\n\n\n\n\n12.11.2.3 Dominanta\n\n12.11.2.3.1 Obliczenia dla Kraju X\n\n\n\nWartość\nCzęstość\n\n\n\n\n1\n1\n\n\n3\n1\n\n\n5\n1\n\n\n7\n1\n\n\n9\n1\n\n\n11\n1\n\n\n13\n1\n\n\n15\n1\n\n\n17\n1\n\n\n19\n1\n\n\n\nWniosek: Brak dominanty (wszystkie wartości występują jednokrotnie)\n\n\n12.11.2.3.2 Obliczenia dla Kraju Y\n\n\n\nWartość\nCzęstość\n\n\n\n\n8\n1\n\n\n9\n2\n\n\n10\n2\n\n\n11\n2\n\n\n12\n2\n\n\n13\n1\n\n\n\nWniosek: Cztery dominanty: 9, 10, 11, 12 (każda występuje dwukrotnie)\n\n# Tabele częstości\ntable_x &lt;- table(x)\ntable_y &lt;- table(y)\n\nlist(\n  \"Kraj X\" = table_x,\n  \"Kraj Y\" = table_y\n)\n\n$`Kraj X`\nx\n 1  3  5  7  9 11 13 15 17 19 \n 1  1  1  1  1  1  1  1  1  1 \n\n$`Kraj Y`\ny\n 8  9 10 11 12 13 \n 1  2  2  2  2  1 \n\n\n\n\n\n12.11.2.4 Wariancja\nWariancja mierzy średnie kwadratowe odchylenie od średniej.\nWzór: s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\n12.11.2.4.1 Obliczenia dla Kraju X\n\n\n\nx_i\n(x_i - \\bar{x})\n(x_i - \\bar{x})^2\n\n\n\n\n1\n-9\n81\n\n\n3\n-7\n49\n\n\n5\n-5\n25\n\n\n7\n-3\n9\n\n\n9\n-1\n1\n\n\n11\n1\n1\n\n\n13\n3\n9\n\n\n15\n5\n25\n\n\n17\n7\n49\n\n\n19\n9\n81\n\n\nSuma\n\n330\n\n\n\ns^2_X = \\frac{330}{9} = 36,67\n\nvar_x &lt;- var(x)\nc(\"Ręcznie\" = 36.67, \"R\" = var_x)\n\nRęcznie       R \n  36.67   36.67 \n\n\n\n\n12.11.2.4.2 Obliczenia dla Kraju Y\n\n\n\nx_i\n(y_i - \\bar{y})\n(y_i - \\bar{y})^2\n\n\n\n\n8\n-2,5\n6,25\n\n\n9\n-1,5\n2,25\n\n\n9\n-1,5\n2,25\n\n\n10\n-0,5\n0,25\n\n\n10\n-0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n11\n0,5\n0,25\n\n\n12\n1,5\n2,25\n\n\n12\n1,5\n2,25\n\n\n13\n2,5\n6,25\n\n\nSuma\n\n22,5\n\n\n\ns^2_Y = \\frac{22,5}{9} = 2,5\n\nvar_y &lt;- var(y)\nc(\"Ręcznie\" = 2.5, \"R\" = var_y)\n\nRęcznie       R \n    2.5     2.5 \n\n\n\n\n\n12.11.2.5 Odchylenie Standardowe\nOdchylenie standardowe to pierwiastek kwadratowy z wariancji. Jest miarą zmienności wyrażoną w tych samych jednostkach co dane.\nWzór: s = \\sqrt{s^2}\n\n12.11.2.5.1 Obliczenia dla Kraju X\nWykorzystujemy wcześniej obliczoną wariancję: s^2_X = 36,67\nObliczamy pierwiastek: s_X = \\sqrt{36,67} \\approx 6,06\n\n\n\nKrok\nObliczenie\nWynik\n\n\n\n\n1. Wariancja\ns^2_X\n36,67\n\n\n2. Pierwiastek\n\\sqrt{36,67}\n6,06\n\n\n\n\nsd_x &lt;- sd(x)\nc(\"Ręcznie\" = 6.06, \"R\" = sd_x)\n\nRęcznie       R \n  6.060   6.055 \n\n\n\n\n12.11.2.5.2 Obliczenia dla Kraju Y\nWykorzystujemy wcześniej obliczoną wariancję: s^2_Y = 2,5\nObliczamy pierwiastek: s_Y = \\sqrt{2,5} \\approx 1,58\n\n\n\nKrok\nObliczenie\nWynik\n\n\n\n\n1. Wariancja\ns^2_Y\n2,5\n\n\n2. Pierwiastek\n\\sqrt{2,5}\n1,58\n\n\n\n\nsd_y &lt;- sd(y)\nc(\"Ręcznie\" = 1.58, \"R\" = sd_y)\n\nRęcznie       R \n  1.580   1.581 \n\n\nInterpretacja:\n\nKraj X: Przeciętne odchylenie wielkości okręgu od średniej wynosi około 6 mandatów\nKraj Y: Przeciętne odchylenie wielkości okręgu od średniej wynosi około 1,6 mandatu\n\n\n\n\n\n12.11.3 Współczynnik Zmienności (CV)\nWspółczynnik zmienności to stosunek odchylenia standardowego do średniej, wyrażony w procentach.\nWzór: CV = \\frac{s}{\\bar{x}} \\times 100\\%\n\n12.11.3.1 Obliczenia dla Kraju X\nCV_X = \\frac{6,06}{10} \\times 100\\% = 60,6\\%\n\n\n\nSkładowa\nWartość\n\n\n\n\nOdchylenie standardowe (s)\n6,06\n\n\nŚrednia (\\bar{x})\n10\n\n\nCV\n60,6%\n\n\n\n\ncv_x &lt;- sd(x) / mean(x) * 100\nc(\"Ręcznie\" = 60.6, \"R\" = cv_x)\n\nRęcznie       R \n  60.60   60.55 \n\n\n\n\n12.11.3.2 Obliczenia dla Kraju Y\nCV_Y = \\frac{1,58}{10,5} \\times 100\\% = 15,0\\%\n\n\n\nSkładowa\nWartość\n\n\n\n\nOdchylenie standardowe (s)\n1,58\n\n\nŚrednia (\\bar{x})\n10,5\n\n\nCV\n15,0%\n\n\n\n\ncv_y &lt;- sd(y) / mean(y) * 100\nc(\"Ręcznie\" = 15.0, \"R\" = cv_y)\n\nRęcznie       R \n  15.00   15.06 \n\n\n\n\n\n12.11.4 Kwartyle i Rozstęp Międzykwartylowy (IQR)\n\n12.11.4.1 Metody obliczania kwartyli\nIstnieją różne metody obliczania kwartyli. W naszych obliczeniach ręcznych zastosujemy metodę wyłączającą medianę:\n\nDzielimy szereg na dwie części względem mediany\nMediana nie jest uwzględniana w obliczeniach kwartyli\nDla każdej części obliczamy jej medianę - będzie to odpowiednio Q1 i Q3\n\n\n\n12.11.4.2 Obliczenia dla Kraju X\nUporządkowane dane: 1, 3, 5, 7, 9, 11, 13, 15, 17, 19\nMediana = 10 (nie uwzględniamy w obliczeniach kwartyli)\nDolna połowa: 1, 3, 5, 7, 9 Q1 = mediana dolnej połowy = 5\nGórna połowa: 11, 13, 15, 17, 19 Q3 = mediana górnej połowy = 15\nIQR = Q3 - Q1 = 15 - 5 = 10\n\n\n12.11.4.3 Obliczenia dla Kraju Y\nUporządkowane dane: 8, 9, 9, 10, 10, 11, 11, 12, 12, 13\nMediana = 10.5 (nie uwzględniamy w obliczeniach kwartyli)\nDolna połowa: 8, 9, 9, 10, 10 Q1 = mediana dolnej połowy = 9\nGórna połowa: 11, 11, 12, 12, 13 Q3 = mediana górnej połowy = 12\nIQR = Q3 - Q1 = 12 - 9 = 3\n\n# Porównanie różnych metod obliczania kwartyli w R\nmethods_comparison &lt;- data.frame(\n  Metoda = c(\"Ręcznie (bez mediany)\", \n             \"R type=1\", \"R type=2\", \"R type=7 (domyślna)\"),\n  \"Q1 Kraj X\" = c(5, \n                  quantile(x, 0.25, type=1),\n                  quantile(x, 0.25, type=2),\n                  quantile(x, 0.25, type=7)),\n  \"Q3 Kraj X\" = c(15,\n                  quantile(x, 0.75, type=1),\n                  quantile(x, 0.75, type=2),\n                  quantile(x, 0.75, type=7)),\n  \"Q1 Kraj Y\" = c(9,\n                  quantile(y, 0.25, type=1),\n                  quantile(y, 0.25, type=2),\n                  quantile(y, 0.25, type=7)),\n  \"Q3 Kraj Y\" = c(12,\n                  quantile(y, 0.75, type=1),\n                  quantile(y, 0.75, type=2),\n                  quantile(y, 0.75, type=7))\n)\n\nkable(methods_comparison, digits = 2,\n      caption = \"Porównanie różnych metod obliczania kwartyli\")\n\n\nPorównanie różnych metod obliczania kwartyli\n\n\nMetoda\nQ1.Kraj.X\nQ3.Kraj.X\nQ1.Kraj.Y\nQ3.Kraj.Y\n\n\n\n\nRęcznie (bez mediany)\n5.0\n15.0\n9.00\n12.00\n\n\nR type=1\n5.0\n15.0\n9.00\n12.00\n\n\nR type=2\n5.0\n15.0\n9.00\n12.00\n\n\nR type=7 (domyślna)\n5.5\n14.5\n9.25\n11.75\n\n\n\n\n\n\n\n12.11.4.4 Wyjaśnienie różnic w metodach obliczania kwartyli\n\nMetoda ręczna (bez mediany):\n\nDzieli dane na dwie części\nNie uwzględnia mediany\nZnajduje medianę każdej części\n\nR type=1:\n\nMetoda pierwsza w R\nUżywa pozycji całkowitych\nNie interpoluje\n\nR type=2:\n\nMetoda druga w R\nUżywa pozycji całkowitych\nInterpoluje gdy pozycja nie jest całkowita\n\nR type=7 (domyślna):\n\nMetoda domyślna w R\nUżywa quantile()[5] z SAS\nInterpoluje według metody opisanej przez Hyndmana i Fana\n\n\n\n\n\n12.11.5 Porównanie Wyników\n\nsummary_df &lt;- data.frame(\n  Miara = c(\"Średnia\", \"Mediana\", \"Dominanta\", \"Rozstęp\", \"Wariancja\", \n            \"Odch. Stand.\", \"Q1\", \"Q3\", \"IQR\", \"CV (%)\"),\n  \"Kraj X\" = c(10, 10, \"brak\", 18, 36.67, 6.06, 5, 15, 10, 60.6),\n  \"Kraj Y\" = c(10.5, 10.5, \"9,10,11,12\", 5, 2.5, 1.58, 9, 12, 3, 15.0)\n)\n\nkable(summary_df, \n      caption = \"Zestawienie wszystkich miar statystycznych\",\n      align = c('l', 'r', 'r'))\n\n\nZestawienie wszystkich miar statystycznych\n\n\nMiara\nKraj.X\nKraj.Y\n\n\n\n\nŚrednia\n10\n10.5\n\n\nMediana\n10\n10.5\n\n\nDominanta\nbrak\n9,10,11,12\n\n\nRozstęp\n18\n5\n\n\nWariancja\n36.67\n2.5\n\n\nOdch. Stand.\n6.06\n1.58\n\n\nQ1\n5\n9\n\n\nQ3\n15\n12\n\n\nIQR\n10\n3\n\n\nCV (%)\n60.6\n15\n\n\n\n\n\n\n\n12.11.6 Porównanie za pomocą Wykresu Pudełkowego\n\ndf_long &lt;- data.frame(\n  kraj = rep(c(\"X\", \"Y\"), each = 10),\n  wielkosc = c(x, y)\n)\n\n# Wykres podstawowy\np &lt;- ggplot(df_long, aes(x = kraj, y = wielkosc, fill = kraj)) +\n  geom_boxplot(outlier.shape = NA) +  # Wyłączamy domyślne punkty odstające\n  geom_jitter(width = 0.2, alpha = 0.5) +  # Dodajemy punkty z przezroczystością\n  scale_fill_manual(values = c(\"X\" = \"#FFA07A\", \"Y\" = \"#98FB98\")) +\n  labs(\n    title = \"Porównanie Zmienności Wielkości Okręgów Wyborczych\",\n    subtitle = paste(\"CV: Kraj X =\", round(cv_x, 1), \"%, Kraj Y =\", round(cv_y, 1), \"%\"),\n    x = \"Kraj\",\n    y = \"Wielkość Okręgu\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Dodajemy adnotacje z kwartylami\np + annotate(\n  \"text\", \n  x = c(1, 1, 1, 2, 2, 2), \n  y = c(max(x)+1, mean(x), min(x)-1, max(y)+1, mean(y), min(y)-1),\n  label = c(\n    paste(\"Q3 =\", quantile(x, 0.75, type=1)),\n    paste(\"M =\", median(x)),\n    paste(\"Q1 =\", quantile(x, 0.25, type=1)),\n    paste(\"Q3 =\", quantile(y, 0.75, type=1)),\n    paste(\"M =\", median(y)),\n    paste(\"Q1 =\", quantile(y, 0.25, type=1))\n  ),\n  size = 3\n)\n\n\n\n\n\n\n\n\n\n\n12.11.7 Uwagi Metodologiczne\n\nObliczenia kwartyli:\n\nZastosowana metoda wyłączająca medianę może dawać inne wyniki niż domyślne funkcje R\nRóżnice w metodach obliczeniowych nie wpływają na ogólne wnioski\nWarto zawsze zaznaczyć stosowaną metodę w raportach\n\nWizualizacja:\n\nWykres pudełkowy skutecznie pokazuje różnice w rozkładach\nDodatkowe punkty pokazują rzeczywiste wartości\nAdnotacje ułatwiają interpretację\n\n\n\n\n12.11.8 Podsumowanie\n\n12.11.8.1 Porównanie Miar Statystycznych\n\n\n\nMiara\nKraj X\nKraj Y\nRóżnica względna\n\n\n\n\nŚrednia\n10,0\n10,5\nPodobna\n\n\nMediana\n10,0\n10,5\nPodobna\n\n\nDominanta\nBrak\nWielokrotna (9,10,11,12)\n-\n\n\nRozstęp\n18\n5\n3,6× większy w X\n\n\nWariancja\n36,67\n2,5\n14,7× większa w X\n\n\nIQR\n10\n3\n3,3× większy w X\n\n\nCV\n60,6%\n15,0%\n4,0× większy w X\n\n\n\n\n\n12.11.8.2 Charakterystyka Rozkładów\nKraj X:\n\nRozkład równomierny\nBrak dominującej wielkości okręgu (brak dominanty)\nSzeroki zakres: od 1 do 19 mandatów\nWysoka zmienność (CV = 60,6%)\nRównomierne rozłożenie wartości w zakresie\n\nKraj Y:\n\nRozkład skupiony\nWiele typowych wielkości (cztery dominanty)\nWąski zakres: od 8 do 13 mandatów\nNiska zmienność (CV = 15,0%)\nWartości skoncentrowane wokół średniej\n\n\n\n12.11.8.3 Interpretacja Wykresu Pudełkowego\nWizualizacja w formie wykresu pudełkowego pokazuje:\nElementy Struktury:\n\nPudełko: Pokazuje rozstęp międzykwartylowy (IQR)\nDolna krawędź: Pierwszy kwartyl (Q1)\nGórna krawędź: Trzeci kwartyl (Q3)\nLinia wewnętrzna: Mediana (Q2)\nWąsy: Rozciągają się do ±1,5 IQR - Punkty: Pojedyncze wielkości okręgów\n\nGłówne Wnioski Wizualne:\n\nRozmiar Pudełka:\n\n\nKraj X: Duże pudełko wskazuje na szeroki rozrzut środkowych 50%\nKraj Y: Małe pudełko pokazuje skupienie wartości środkowych\n\n\nDługość Wąsów:\n\nKraj X: Długie wąsy wskazują na szeroki rozkład całkowity\nKraj Y: Krótkie wąsy pokazują ograniczony rozrzut\n\nRozkład Punktów:\n\nKraj X: Punkty szeroko rozproszone\nKraj Y: Punkty gęsto skupione\n\n\n\n\n12.11.8.4 Kluczowe Obserwacje\n\nTendencja Centralna:\n\nPodobne średnie wielkości okręgów\nRóżne wzorce rozkładu\nOdmienne podejścia do standaryzacji\n\nMiary Zmienności:\n\nWszystkie miary pokazują 3-15 razy większą zmienność w Kraju X\nSpójny wzorzec w różnych miarach statystycznych\nSystematyczna różnica w projekcie okręgów\n\nProjekt Systemu:\n\nKraj X: Elastyczne, zróżnicowane podejście\nKraj Y: Ustandaryzowane, jednolite podejście\nRóżne filozoficzne podejścia do reprezentacji\n\nImplikacje Reprezentatywności:\n\nKraj X: Zmienna proporcja wyborców do przedstawicieli\nKraj Y: Bardziej spójne poziomy reprezentacji\nRóżne podejścia do reprezentacji demokratycznej\n\n\nAnaliza ta pokazuje fundamentalne różnice w projektowaniu systemów wyborczych między dwoma krajami, gdzie Kraj X przyjmuje bardziej zróżnicowane podejście, a Kraj Y utrzymuje większą jednolitość w wielkości okręgów wyborczych.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#zrozumienie-wykresów-pudełkowych-na-przykładzie-danych-o-długości-życia",
    "href": "rozdzial5.html#zrozumienie-wykresów-pudełkowych-na-przykładzie-danych-o-długości-życia",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.12 Zrozumienie Wykresów Pudełkowych na Przykładzie Danych o Długości Życia",
    "text": "12.12 Zrozumienie Wykresów Pudełkowych na Przykładzie Danych o Długości Życia\n\nlibrary(tidyverse)\nlibrary(gapminder)\n\n# Przygotowanie danych\ndata_2007 &lt;- gapminder %&gt;%\n  filter(year == 2007)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#wprowadzenie-do-wykresów-pudełkowych",
    "href": "rozdzial5.html#wprowadzenie-do-wykresów-pudełkowych",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.13 Wprowadzenie do Wykresów Pudełkowych",
    "text": "12.13 Wprowadzenie do Wykresów Pudełkowych\nWykres pudełkowy (ang. box-and-whisker plot) przedstawia pięć kluczowych statystyk opisowych danych:\n\nMediana: Środkowa linia w pudełku (50. percentyl)\nPierwszy kwartyl (Q1): Dolna krawędź pudełka (25. percentyl)\nTrzeci kwartyl (Q3): Górna krawędź pudełka (75. percentyl)\nRozstęp międzykwartylowy (IQR): Wysokość pudełka (Q3 - Q1)\nWąsy: Rozciągają się do najbardziej skrajnych wartości niebędących obserwacjami odstającymi (metoda Tukeya: 1.5 × IQR)\nObserwacje odstające: Pojedyncze punkty poza wąsami\n\n\n12.13.1 Wizualizacja Długości Życia\n\nggplot(data_2007, aes(x = reorder(continent, lifeExp, FUN = median), y = lifeExp)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.7, outlier.shape = 24, \n               outlier.fill = \"red\", outlier.alpha = 0.6, outlier.size = 4) +\n  geom_jitter(width = 0.2, alpha = 0.4, color = \"darkblue\") +\n  labs(title = \"Długość Życia według Kontynentów (2007)\",\n       subtitle = \"Pojedyncze punkty pokazują surowe dane; czerwone punkty oznaczają wartości odstające\",\n       x = \"Kontynent\",\n       y = \"Długość życia (w latach)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_y_continuous(breaks = seq(40, 85, by = 5))",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#analiza-danych",
    "href": "rozdzial5.html#analiza-danych",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.14 Analiza Danych",
    "text": "12.14 Analiza Danych\n\n12.14.1 Mediana i Rozkład\nOdpowiedz Prawda lub Fałsz:\n\n50% krajów afrykańskich ma długość życia poniżej 52 lat\nMediana długości życia w Europie wynosi około 78 lat\nPonad 75% krajów Oceanii ma długość życia powyżej 75 lat\n25% krajów azjatyckich ma długość życia poniżej 68 lat\nŚrodkowe 50% długości życia w Europie mieści się między 76 a 80 lat\n\n\n\n12.14.2 Rozrzut i Zmienność\nOdpowiedz Prawda lub Fałsz:\n\nAzja wykazuje największy rozrzut (IQR) w długości życia\nEuropa ma najmniejszy IQR wśród wszystkich kontynentów\nZmienność długości życia w Afryce jest większa niż w obu Amerykach\nOceania wykazuje najmniejszą zmienność w długości życia\nWąsy dla Azji rozciągają się w przybliżeniu od 58 do 82 lat (z wyłączeniem wartości odstających)\n\n\n\n12.14.3 Wartości Odstające i Ekstrema\nOdpowiedz Prawda lub Fałsz:\n\nAfryka ma dwa kraje z wyjątkowo niską długością życia\nW rozkładzie dla Oceanii nie ma wartości odstających\nAzja ma kilka niskich wartości odstających (poniżej 55 lat)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#zmiany-w-czasie",
    "href": "rozdzial5.html#zmiany-w-czasie",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.15 Zmiany w Czasie",
    "text": "12.15 Zmiany w Czasie\n\ntime_comparison &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  mutate(year = factor(year))\n\nggplot(time_comparison, aes(x = continent, y = lifeExp, fill = year)) +\n  geom_boxplot(alpha = 0.7, position = \"dodge\", outlier.shape = 21,\n               outlier.alpha = 0.6) +\n  labs(title = \"Długość Życia: 1957 vs 2007\",\n       subtitle = \"Porównanie zmian rozkładu na przestrzeni 50 lat\",\n       x = \"Kontynent\",\n       y = \"Długość życia (w latach)\",\n       fill = \"Rok\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    axis.text = element_text(size = 10)\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(breaks = seq(30, 85, by = 5))\n\n\n\n\n\n\n\n\n\n12.15.1 Pytania dotyczące Zmian w Czasie\nOdpowiedz Prawda lub Fałsz:\n\nMediana długości życia wzrosła na wszystkich kontynentach między 1957 a 2007 rokiem\nZmienność długości życia (IQR) zmniejszyła się na większości kontynentów w czasie\nAfryka wykazała najmniejszą poprawę mediany długości życia\nRozrzut długości życia w Azji znacząco się zmniejszył od 1957 do 2007 roku\nOceania utrzymała najwyższą medianę długości życia w obu okresach\n\n\n\n12.15.2 Podsumowanie Statystyczne\n\n# Obliczenie statystyk opisowych\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    mediana = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    liczba_odstających = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Statystyki Opisowe według Kontynentu i Roku\")\n\n\nStatystyki Opisowe według Kontynentu i Roku\n\n\n\n\n\n\n\n\n\n\n\n\n\ncontinent\nyear\nmediana\nq1\nq3\niqr\nmin\nmax\nliczba_odstających\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n31.6\n58.1\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n40.7\n70.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n30.3\n67.8\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n48.1\n73.5\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n70.3\n70.3\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n80.2\n81.2\n0",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#najważniejsze-wnioski",
    "href": "rozdzial5.html#najważniejsze-wnioski",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.16 Najważniejsze Wnioski",
    "text": "12.16 Najważniejsze Wnioski\n\nCentrum Rozkładu:\n\nMediana pokazuje typową długość życia\nZmiany mediany odzwierciedlają ogólną poprawę\n\nRozrzut i Zmienność:\n\nIQR (wysokość pudełka) wskazuje na rozproszenie danych\nSzersze pudełka sugerują większe nierówności w długości życia\n\nWartości Odstające i Ekstrema:\n\nWartości odstające często reprezentują kraje o wyjątkowej sytuacji\n\nPorównanie w Czasie:\n\nPokazuje zarówno bezwzględną poprawę, jak i zmiany w wariancji\nUwydatnia utrzymujące się różnice regionalne\nUjawnia różne tempo postępu na poszczególnych kontynentach",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#statistical-summary",
    "href": "rozdzial5.html#statistical-summary",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.17 Statistical Summary",
    "text": "12.17 Statistical Summary\n\n# Calculate summary statistics\nsummary_stats &lt;- gapminder %&gt;%\n  filter(year %in% c(1957, 2007)) %&gt;%\n  group_by(continent, year) %&gt;%\n  summarise(\n    median = median(lifeExp),\n    q1 = quantile(lifeExp, 0.25),\n    q3 = quantile(lifeExp, 0.75),\n    iqr = IQR(lifeExp),\n    min = min(lifeExp),\n    max = max(lifeExp),\n    n_outliers = sum(lifeExp &lt; (q1 - 1.5 * iqr) | lifeExp &gt; (q3 + 1.5 * iqr))\n  ) %&gt;%\n  arrange(continent, year)\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\nknitr::kable(summary_stats, digits = 1,\n             caption = \"Summary Statistics by Continent and Year\")\n\n\nSummary Statistics by Continent and Year\n\n\ncontinent\nyear\nmedian\nq1\nq3\niqr\nmin\nmax\nn_outliers\n\n\n\n\nAfrica\n1957\n40.6\n37.4\n44.8\n7.4\n31.6\n58.1\n1\n\n\nAfrica\n2007\n52.9\n47.8\n59.4\n11.6\n39.6\n76.4\n0\n\n\nAmericas\n1957\n56.1\n48.6\n62.6\n14.0\n40.7\n70.0\n0\n\n\nAmericas\n2007\n72.9\n71.8\n76.4\n4.6\n60.9\n80.7\n1\n\n\nAsia\n1957\n48.3\n41.9\n54.1\n12.2\n30.3\n67.8\n0\n\n\nAsia\n2007\n72.4\n65.5\n75.6\n10.2\n43.8\n82.6\n1\n\n\nEurope\n1957\n67.7\n65.0\n69.2\n4.2\n48.1\n73.5\n2\n\n\nEurope\n2007\n78.6\n75.0\n79.8\n4.8\n71.8\n81.8\n0\n\n\nOceania\n1957\n70.3\n70.3\n70.3\n0.0\n70.3\n70.3\n0\n\n\nOceania\n2007\n80.7\n80.5\n81.0\n0.5\n80.2\n81.2\n0",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "rozdzial5.html#appendix-tabele-podsumowujące-typy-danych-i-odpowiednie-miary-statystyczne",
    "href": "rozdzial5.html#appendix-tabele-podsumowujące-typy-danych-i-odpowiednie-miary-statystyczne",
    "title": "12  Podstawy Jednowymiarowej Statystyki Opisowej",
    "section": "12.18 Appendix: Tabele Podsumowujące Typy Danych i Odpowiednie Miary Statystyczne",
    "text": "12.18 Appendix: Tabele Podsumowujące Typy Danych i Odpowiednie Miary Statystyczne\n\n12.18.1 Zalety i Wady Różnych Miar Statystycznych\n\n12.18.1.1 Miary Tendencji Centralnej\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nŚrednia\n- Wykorzystuje wszystkie punkty danych- Pozwala na dalsze obliczenia statystyczne- Idealna dla danych o rozkładzie normalnym\n- Wrażliwa na wartości odstające- Nieodpowiednia dla rozkładów skośnych- Bez znaczenia dla danych nominalnych\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nMediana\n- Niewrażliwa na wartości odstające- Dobra dla rozkładów skośnych- Może być stosowana do danych porządkowych\n- Ignoruje rzeczywiste wartości większości punktów danych- Mniej użyteczna do dalszych analiz statystycznych\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nModa\n- Może być stosowana do każdego typu danych- Dobra do znajdowania najczęstszej kategorii\n- Może nie być unikalna (rozkłady multimodalne)- Nieprzydatna do wielu typów analiz- Ignoruje wielkość różnic między wartościami\nWszystkie typy\n\n\n\n\n\n12.18.1.2 Miary Zmienności\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nZakres\n- Prosty do obliczenia i zrozumienia- Daje szybki obraz rozproszenia danych\n- Bardzo wrażliwy na wartości odstające- Ignoruje wszystkie dane między ekstremami- Nieprzydatny do dalszych analiz statystycznych\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nRozstęp międzykwartylowy (IQR)\n- Niewrażliwy na wartości odstające- Dobry dla rozkładów skośnych\n- Ignoruje 50% danych- Mniej intuicyjny niż zakres\nPorządkowe, Interwałowe, Ilorazowe, Dyskretne, Ciągłe\n\n\nWariancja\n- Wykorzystuje wszystkie punkty danych- Podstawa wielu procedur statystycznych\n- Wrażliwa na wartości odstające- Jednostki są podniesione do kwadratu (mniej intuicyjne)\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nOdchylenie standardowe\n- Wykorzystuje wszystkie punkty danych- Te same jednostki co oryginalne dane- Szeroko stosowane i zrozumiałe\n- Wrażliwe na wartości odstające- Zakłada w przybliżeniu rozkład normalny dla interpretacji\nInterwałowe, Ilorazowe, niektóre Dyskretne, Ciągłe\n\n\nWspółczynnik zmienności\n- Pozwala na porównanie między zbiorami danych o różnych jednostkach lub średnich\n- Może być mylący, gdy średnie są bliskie zeru- Bez znaczenia dla danych z wartościami ujemnymi\nIlorazowe, niektóre Interwałowe\n\n\n\n\n\n12.18.1.3 Miary Korelacji/Asocjacji\n\n\n\n\n\n\n\n\n\nMiara\nZalety\nWady\nZastosowanie do\n\n\n\n\nr Pearsona\n- Mierzy zależność liniową- Szeroko stosowany i zrozumiały\n- Zakłada rozkład normalny- Wrażliwy na wartości odstające- Uchwytuje tylko zależności liniowe\nInterwałowe, Ilorazowe, Ciągłe\n\n\nRho Spearmana\n- Może być stosowany do danych porządkowych- Uchwytuje zależności monotoniczne- Mniej wrażliwy na wartości odstające\n- Traci informacje przez konwersję na rangi- Może pominąć niektóre typy zależności\nPorządkowe, Interwałowe, Ilorazowe\n\n\nTau Kendalla\n- Może być stosowany do danych porządkowych- Bardziej odporny niż Spearman dla małych próbek- Ma ładną interpretację (prawdopodobieństwo zgodności)\n- Traci informacje, biorąc pod uwagę tylko porządek- Bardziej intensywny obliczeniowo\nPorządkowe, Interwałowe, Ilorazowe\n\n\nChi-kwadrat\n- Może być stosowany do danych nominalnych- Testuje niezależność zmiennych kategorycznych\n- Wymaga dużych rozmiarów próbek- Wrażliwy na rozmiar próbki- Nie mierzy siły asocjacji\nNominalne, Porządkowe\n\n\nV Craméra\n- Może być stosowany do danych nominalnych- Dostarcza miarę siły asocjacji- Znormalizowany do zakresu [0,1]\n- Interpretacja może być subiektywna- Może przeszacować asocjację w małych próbkach\nNominalne, Porządkowe\n\n\n\n\n\n\n\n\n\nStatistical Measures Applicability / Zastosowanie miar statystycznych\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure (EN)\nMiara (PL)\nNominal\nOrdinal\nInterval\nRatio\n\n\n\n\nCentral Tendency / Tendencja centralna:\n\n\n\n\n\n\n\nMode\nDominanta\n✓\n✓\n✓\n✓\n\n\nMedian\nMediana\n-\n✓\n✓\n✓\n\n\nArithmetic Mean\nŚrednia arytmetyczna\n-\n-\n✓*\n✓\n\n\nGeometric Mean\nŚrednia geometryczna\n-\n-\n-\n✓\n\n\nHarmonic Mean\nŚrednia harmoniczna\n-\n-\n-\n✓\n\n\nDispersion / Rozproszenie:\n\n\n\n\n\n\n\nRange\nRozstęp\n-\n✓\n✓\n✓\n\n\nInterquartile Range\nRozstęp międzykwartylowy\n-\n✓\n✓\n✓\n\n\nMean Absolute Deviation\nŚrednie odchylenie bezwzględne\n-\n-\n✓\n✓\n\n\nVariance\nWariancja\n-\n-\n✓*\n✓\n\n\nStandard Deviation\nOdchylenie standardowe\n-\n-\n✓*\n✓\n\n\nCoefficient of Variation\nWspółczynnik zmienności\n-\n-\n-\n✓\n\n\nAssociation / Współzależność:\n\n\n\n\n\n\n\nChi-square\nChi-kwadrat\n✓\n✓\n✓\n✓\n\n\nSpearman Correlation\nKorelacja Spearmana\n-\n✓\n✓\n✓\n\n\nKendall’s Tau\nTau Kendalla\n-\n✓\n✓\n✓\n\n\nPearson Correlation\nKorelacja Pearsona\n-\n-\n✓*\n✓\n\n\nCovariance\nKowariancja\n-\n-\n✓*\n✓\n\n\n\n* Theoretically problematic but commonly used in practice / Teoretycznie problematyczne, ale powszechnie stosowane w praktyce\n\n12.18.2 Notes / Uwagi:\n\nMeasurement Scales / Skale pomiarowe:\n\n\nNominal: Categories without order / Kategorie bez uporządkowania\nOrdinal: Ordered categories / Kategorie uporządkowane\nInterval: Equal intervals, arbitrary zero / Równe interwały, umowne zero\nRatio: Equal intervals, absolute zero / Równe interwały, absolutne zero\n\n\nPractical Considerations / Aspekty praktyczne:\n\n\nSome measures marked with ✓* are commonly used for interval data despite theoretical issues / Niektóre miary oznaczone ✓* są powszechnie stosowane dla danych przedziałowych pomimo problemów teoretycznych\nChoice of measure should consider both theoretical appropriateness and practical utility / Wybór miary powinien uwzględniać zarówno poprawność teoretyczną jak i użyteczność praktyczną\nMore restrictive scales (ratio) allow all measures from less restrictive scales / Bardziej restrykcyjne skale (ilorazowe) pozwalają na wszystkie miary z mniej restrykcyjnych skal",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Podstawy Jednowymiarowej Statystyki Opisowej</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "13  Data Visualization: with examples in R",
    "section": "",
    "text": "13.1 Introduction to Data Types and Visualization\nBefore diving into specific visualization techniques, it’s crucial to understand the different types of data you might encounter and how they influence your choice of visualization method. We’ll explore these concepts with practical examples using the ggplot2 library in R.\nFirst, let’s load the necessary libraries:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#bar-plots",
    "href": "chapter6.html#bar-plots",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.2 Bar Plots",
    "text": "13.2 Bar Plots\nBar plots are excellent for displaying categorical data or summarizing continuous data by groups.\n\n13.2.1 Understanding Bar Plots\nA bar plot represents data using rectangular bars with heights proportional to the values they represent. They are used to compare different categories or groups.\nKey components of a bar plot: 1. X-axis: Represents categories 2. Y-axis: Represents values (can be counts, percentages, or any numerical value) 3. Bars: Rectangle for each category, height corresponds to its value\n\n13.2.1.1 Example Data\nLet’s use a simple dataset of fruit sales:\n\nfruits &lt;- c(\"Apple\", \"Banana\", \"Orange\", \"Grape\")\nsales &lt;- c(120, 85, 70, 100)\n\n# Create a data frame\ndf &lt;- data.frame(fruit = fruits, sales = sales)\n\n\n\n\n13.2.2 Hand-Drawn Bar Plot\nTo create a bar plot by hand:\n\nDraw a horizontal line (x-axis) and a vertical line (y-axis) perpendicular to each other.\nLabel the x-axis with your categories (fruits), evenly spaced.\nLabel the y-axis with a suitable scale for your values (sales, 0 to 120 in increments of 20).\nFor each category, draw a rectangle (bar) whose height corresponds to its value on the y-axis scale.\nColor or shade each bar if desired.\nAdd a title and labels for both axes.\n\n\n\n\n\n\n\nTip\n\n\n\nWhen drawing by hand, use graph paper for more precise measurements and straighter lines. Choose a scale that allows all your data to fit while maximizing the use of space.\n\n\n\n\n13.2.3 Bar Plot in Base R\n\n# Create bar plot\nbarplot(sales, names.arg = fruits, \n        main = \"Fruit Sales\",\n        xlab = \"Fruit Types\", ylab = \"Sales\")\n\n\n\n\n\n\n\n\n\n\n13.2.4 Bar Plot with ggplot2\n\n# Create bar plot with ggplot2\nggplot(df, aes(x = fruit, y = sales)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Fruit Sales\",\n       x = \"Fruit Types\", y = \"Sales\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n13.2.5 Interpreting Bar Plots\nWhen interpreting a bar plot, consider the following:\n\nRelative Heights: Compare the heights of the bars to understand which categories have higher or lower values.\nOrdering: Sometimes, bars are ordered by height to make comparisons easier.\nPatterns: Look for any patterns or trends across categories.\nOutliers: Identify any bars that are much taller or shorter than the others.\n\n\n13.2.5.1 Example Interpretation\nFor our fruit sales data:\n\nApples have the highest sales (120), followed by Grapes (100).\nOranges have the lowest sales (70).\nThere’s a considerable difference between the highest (Apples) and lowest (Oranges) sales.\nBananas and Grapes have similar sales figures, in the middle range.\n\nThis information could be useful for inventory management or marketing strategies in a fruit shop.\n\n\n\n\n\n\nNote\n\n\n\nBar plots are great for comparing categories, but they don’t show the distribution within each category. For that, you might need other plot types like box plots.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#histograms",
    "href": "chapter6.html#histograms",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.3 Histograms",
    "text": "13.3 Histograms\nHistograms visualize the distribution of a continuous variable by dividing it into intervals (bins) and showing the frequency or density of data points in each bin.\n\n13.3.1 Understanding Histograms\nKey components of a histogram: 1. X-axis: Represents the variable’s values, divided into bins 2. Y-axis: Represents frequency, relative frequency, or density 3. Bars: Rectangle for each bin, height corresponds to the y-axis measure\nThere are three main types of histograms:\n\nFrequency Histogram: The y-axis shows the count of data points in each bin.\nRelative Frequency Histogram: The y-axis shows the proportion of data points in each bin (frequency divided by total number of data points).\nDensity Histogram: The y-axis shows the density, which is the relative frequency divided by the bin width. The total area of all bars sums to 1.\n\n\n13.3.1.1 Example Data\nLet’s use a dataset of 50 student exam scores (out of 100):\n\nset.seed(123)  # for reproducibility\nscores &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\n13.3.2 Hand-Drawn Histogram\nTo create a frequency histogram by hand:\n\nFind the range of your data.\nChoose a number of bins (let’s use 7 bins).\nCreate a frequency table.\nDraw x and y axes.\nLabel x-axis with bin ranges and y-axis with frequency.\nDraw a rectangle for each bin, with height corresponding to its frequency.\nAdd a title and labels for both axes.\n\nFor a relative frequency histogram, divide each frequency by the total number of data points before drawing the bars.\nFor a density histogram, divide the relative frequency by the bin width before drawing the bars.\n\n\n\n\n\n\nTip\n\n\n\nThe number of bins can affect the interpretation. Too few bins may obscure important features, while too many may introduce noise. A common rule of thumb is to use the square root of the number of data points as the number of bins.\n\n\n\n\n13.3.3 Histograms in Base R\n\n# Frequency Histogram\nhist(scores, breaks = 7, \n     main = \"Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Relative Frequency Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Relative Frequency\")\n\n\n\n\n\n\n\n# Density Histogram\nhist(scores, breaks = 7, freq = FALSE,\n     main = \"Density Histogram of Exam Scores\",\n     xlab = \"Scores\", ylab = \"Density\")\nlines(density(scores), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n13.3.4 Histograms with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(score = scores)\n\n# Frequency Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Relative Frequency Histogram\nggplot(df, aes(x = score, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Relative Frequency Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Relative Frequency\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Density Histogram\nggplot(df, aes(x = score)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Density Histogram of Exam Scores\",\n       x = \"Scores\", y = \"Density\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n13.3.5 Interpreting Histograms\nWhen interpreting a histogram, consider:\n\nCentral Tendency: Where is the peak of the distribution?\nSpread: How wide is the distribution?\nShape: Is it symmetric, skewed, or multi-modal?\nOutliers: Are there any unusual values far from the main distribution?",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#box-plots-and-tukey-box-plots",
    "href": "chapter6.html#box-plots-and-tukey-box-plots",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.4 Box Plots and Tukey Box Plots",
    "text": "13.4 Box Plots and Tukey Box Plots\nBox plots, also known as box-and-whisker plots, provide a concise summary of a distribution. We’ll focus on the Tukey-style box plot, named after the statistician John Tukey who popularized this type of plot.\n\n13.4.1 Understanding Box Plots\nA box plot represents five key statistics:\n\nMinimum value (excluding outliers)\nFirst quartile (Q1)\nMedian\nThird quartile (Q3)\nMaximum value (excluding outliers)\n\nAdditionally, box plots show:\n\nWhiskers: Lines extending from the box to the minimum and maximum values (excluding outliers)\nOutliers: Individual points beyond the whiskers\n\n\n13.4.1.1 Calculating Quartiles and Outliers\nTo create a box plot, follow these steps:\n\nOrder your data from smallest to largest.\nFind the median (middle value if odd number of data points, average of two middle values if even).\nFind Q1 (median of lower half of data) and Q3 (median of upper half of data).\nCalculate the Interquartile Range (IQR) = Q3 - Q1\nDetermine outliers using Tukey’s rule:\n\nLower outliers: &lt; Q1 - 1.5 * IQR\nUpper outliers: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe factor 1.5 in Tukey’s outlier rule is based on the properties of the normal distribution. For normally distributed data, this rule identifies about 0.7% of the data as potential outliers.\n\n\n\n\n13.4.1.2 Example Data\nLet’s use a small dataset to illustrate:\n\ndata &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\n13.4.2 Hand-Drawn Tukey Box Plot\nTo create a Tukey box plot by hand:\n\nDraw a vertical line representing the range from minimum to maximum (2 to 15 in our example, excluding the outlier).\nDraw a box from Q1 to Q3.\nDraw a horizontal line through the box at the median.\nDraw whiskers from the box to the minimum and maximum values (excluding outliers).\nRepresent the outlier (50) as an individual point beyond the whisker.\nAdd a scale to the vertical axis and label it.\n\n\n\n13.4.3 Box Plot in Base R\n\n# Create box plot\nboxplot(data, main = \"Box Plot of Sample Data\",\n        ylab = \"Values\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\n13.4.4 Tukey Box Plot with ggplot2\n\n# Create a data frame\ndf &lt;- data.frame(value = data)\n\n# Create Tukey box plot with ggplot2\nggplot(df, aes(x = \"\", y = value)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Tukey Box Plot of Sample Data\",\n       x = \"\", y = \"Values\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n13.4.5 Interpreting Box Plots\nWhen interpreting a box plot, consider the following:\n\nCentral Tendency: The median shows the center of the distribution.\nSpread: The box (IQR) represents the middle 50% of the data.\nSkewness: If the median line is closer to one end of the box, the distribution is skewed.\nOutliers: Points beyond the whiskers are potential outliers.\nComparisons: When comparing multiple box plots, look at relative positions of medians, box sizes, and presence of outliers.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "chapter6.html#conclusion",
    "href": "chapter6.html#conclusion",
    "title": "13  Data Visualization: with examples in R",
    "section": "13.5 Conclusion",
    "text": "13.5 Conclusion\nIn this chapter, we explored three fundamental types of data visualizations: bar plots, histograms, and box plots. We demonstrated how to create these plots by hand, using R’s base plotting system, and using the ggplot2 library.\nEach type of plot serves a different purpose: - Bar plots are excellent for comparing categories. - Histograms show the distribution of a continuous variable. - Box plots provide a concise summary of a distribution, highlighting central tendency, spread, and outliers.\nRemember, the choice of visualization depends on your data type and the insights you want to convey. Always consider your audience and the story you want to tell with your data when selecting and designing your visualizations.\nPractice creating these plots by hand to deepen your understanding of their construction and interpretation. Then, leverage the power of R and ggplot2 to quickly create and customize these visualizations for larger datasets and more complex analyses.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Data Visualization: with examples in R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html",
    "href": "rozdzial6.html",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "",
    "text": "14.1 Wprowadzenie do Typów Danych i Wizualizacji\nPrzed zagłębieniem się w konkretne techniki wizualizacji, ważne jest zrozumienie różnych typów danych i ich wpływu na wybór metody wizualizacji. Przeanalizujemy te koncepcje na praktycznych przykładach z użyciem biblioteki ggplot2 w R.\nNajpierw załadujmy niezbędne biblioteki:\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(scales)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-słupkowe",
    "href": "rozdzial6.html#wykresy-słupkowe",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.2 Wykresy Słupkowe",
    "text": "14.2 Wykresy Słupkowe\nWykresy słupkowe doskonale nadają się do prezentacji danych kategorycznych lub podsumowania danych ciągłych w grupach.\n\n14.2.1 Zrozumienie Wykresów Słupkowych\nWykres słupkowy przedstawia dane za pomocą prostokątnych słupków, których wysokość jest proporcjonalna do reprezentowanych przez nie wartości. Służą do porównywania różnych kategorii lub grup.\nGłówne elementy wykresu słupkowego: 1. Oś X: Reprezentuje kategorie 2. Oś Y: Reprezentuje wartości (mogą to być liczebności, procenty lub dowolne wartości numeryczne) 3. Słupki: Prostokąt dla każdej kategorii, wysokość odpowiada jej wartości\n\n14.2.1.1 Przykładowe Dane\nUżyjmy prostego zestawu danych dotyczącego sprzedaży owoców:\n\nowoce &lt;- c(\"Jabłko\", \"Banan\", \"Pomarańcza\", \"Winogrono\")\nsprzedaz &lt;- c(120, 85, 70, 100)\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(owoc = owoce, sprzedaz = sprzedaz)\n\n\n\n\n14.2.2 Ręcznie Rysowany Wykres Słupkowy\nAby stworzyć wykres słupkowy ręcznie:\n\nNarysuj linię poziomą (oś X) i pionową (oś Y) prostopadłe do siebie.\nOznacz oś X swoimi kategoriami (owocami), równomiernie rozmieszczonymi.\nOznacz oś Y odpowiednią skalą dla Twoich wartości (sprzedaż, od 0 do 120 z przyrostami co 20).\nDla każdej kategorii narysuj prostokąt (słupek), którego wysokość odpowiada jej wartości na skali osi Y.\nJeśli chcesz, pokoloruj lub zacienuj każdy słupek.\nDodaj tytuł i etykiety dla obu osi.\n\n\n\n\n\n\n\nTip\n\n\n\nPrzy rysowaniu ręcznym użyj papieru milimetrowego dla dokładniejszych pomiarów i prostszych linii. Wybierz skalę, która pozwoli zmieścić wszystkie dane, maksymalnie wykorzystując dostępną przestrzeń.\n\n\n\n\n14.2.3 Wykres Słupkowy w Podstawowym R\n\n# Tworzenie wykresu słupkowego\nbarplot(sprzedaz, names.arg = owoce, \n        main = \"Sprzedaż Owoców\",\n        xlab = \"Rodzaje Owoców\", ylab = \"Sprzedaż\")\n\n\n\n\n\n\n\n\n\n\n14.2.4 Wykres Słupkowy z ggplot2\n\n# Tworzenie wykresu słupkowego z ggplot2\nggplot(df, aes(x = owoc, y = sprzedaz)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  labs(title = \"Sprzedaż Owoców\",\n       x = \"Rodzaje Owoców\", y = \"Sprzedaż\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n14.2.5 Interpretacja Wykresów Słupkowych\nPodczas interpretacji wykresu słupkowego zwróć uwagę na:\n\nWzględne Wysokości: Porównaj wysokości słupków, aby zrozumieć, które kategorie mają wyższe lub niższe wartości.\nKolejność: Czasami słupki są uporządkowane według wysokości, aby ułatwić porównania.\nWzorce: Poszukaj wzorców lub trendów między kategoriami.\nWartości Odstające: Zidentyfikuj słupki, które są znacznie wyższe lub niższe od pozostałych.\n\n\n14.2.5.1 Przykładowa Interpretacja\nDla naszych danych o sprzedaży owoców:\n\nJabłka mają najwyższą sprzedaż (120), następnie Winogrona (100).\nPomarańcze mają najniższą sprzedaż (70).\nIstnieje znaczna różnica między najwyższą (Jabłka) a najniższą (Pomarańcze) sprzedażą.\nBanany i Winogrona mają podobne wartości sprzedaży, w średnim zakresie.\n\nTa informacja może być przydatna dla zarządzania zapasami lub strategii marketingowych w sklepie owocowym.\n\n\n\n\n\n\nNote\n\n\n\nWykresy słupkowe są świetne do porównywania kategorii, ale nie pokazują rozkładu wewnątrz każdej kategorii. Do tego mogą być potrzebne inne typy wykresów, jak wykresy pudełkowe.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#histogramy",
    "href": "rozdzial6.html#histogramy",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.3 Histogramy",
    "text": "14.3 Histogramy\nHistogramy wizualizują rozkład zmiennej ciągłej poprzez podzielenie jej na przedziały (bins) i pokazanie częstości lub gęstości punktów danych w każdym przedziale.\n\n14.3.1 Zrozumienie Histogramów\nGłówne elementy histogramu: 1. Oś X: Reprezentuje wartości zmiennej, podzielone na przedziały 2. Oś Y: Reprezentuje częstość, względną częstość lub gęstość 3. Słupki: Prostokąt dla każdego przedziału, wysokość odpowiada mierze na osi Y\nIstnieją trzy główne typy histogramów:\n\nHistogram Częstości: Oś Y pokazuje liczbę punktów danych w każdym przedziale.\nHistogram Częstości Względnej: Oś Y pokazuje proporcję punktów danych w każdym przedziale (częstość podzielona przez całkowitą liczbę punktów danych).\nHistogram Gęstości: Oś Y pokazuje gęstość, która jest częstością względną podzieloną przez szerokość przedziału. Całkowita powierzchnia wszystkich słupków sumuje się do 1.\n\n\n14.3.1.1 Przykładowe Dane\nUżyjmy zbioru 50 wyników egzaminów studentów (na 100 punktów):\n\nset.seed(123)  # dla powtarzalności\nwyniki &lt;- round(runif(50, min = 60, max = 100))\n\n\n\n\n14.3.2 Ręcznie Rysowany Histogram\nAby stworzyć histogram częstości ręcznie:\n\nZnajdź zakres danych.\nWybierz liczbę przedziałów (użyjmy 7 przedziałów).\nUtwórz tabelę częstości.\nNarysuj osie X i Y.\nOznacz oś X zakresami przedziałów, a oś Y częstością.\nNarysuj prostokąt dla każdego przedziału, z wysokością odpowiadającą jego częstości.\nDodaj tytuł i etykiety dla obu osi.\n\nDla histogramu częstości względnej, podziel każdą częstość przez całkowitą liczbę punktów danych przed narysowaniem słupków.\nDla histogramu gęstości, podziel częstość względną przez szerokość przedziału przed narysowaniem słupków.\n\n\n\n\n\n\nTip\n\n\n\nLiczba przedziałów może wpłynąć na interpretację. Zbyt mało przedziałów może ukryć ważne cechy, podczas gdy zbyt wiele może wprowadzić szum. Powszechną regułą jest użycie pierwiastka kwadratowego z liczby punktów danych jako liczby przedziałów.\n\n\n\n\n14.3.3 Histogramy w Podstawowym R\n\n# Histogram Częstości\nhist(wyniki, breaks = 7, \n     main = \"Histogram Częstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość\")\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Częstości Względnej Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Częstość Względna\")\n\n\n\n\n\n\n\n# Histogram Gęstości\nhist(wyniki, breaks = 7, freq = FALSE,\n     main = \"Histogram Gęstości Wyników Egzaminu\",\n     xlab = \"Wyniki\", ylab = \"Gęstość\")\nlines(density(wyniki), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n14.3.4 Histogramy z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wynik = wyniki)\n\n# Histogram Częstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Histogram Częstości Względnej\nggplot(df, aes(x = wynik, y = ..count.. / sum(..count..))) +\n  geom_histogram(bins = 7, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Histogram Częstości Względnej Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Częstość Względna\") +\n  theme_minimal()\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n# Histogram Gęstości\nggplot(df, aes(x = wynik)) +\n  geom_histogram(aes(y = ..density..), bins = 7, fill = \"skyblue\", color = \"black\") +\n  geom_density(color = \"red\", size = 1) +\n  labs(title = \"Histogram Gęstości Wyników Egzaminu\",\n       x = \"Wyniki\", y = \"Gęstość\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n14.3.5 Interpretacja Histogramów\nPodczas interpretacji histogramu zwróć uwagę na:\n\nTendencję Centralną: Gdzie znajduje się szczyt rozkładu?\nRozrzut: Jak szeroki jest rozkład?\nKształt: Czy jest symetryczny, skośny, czy wielomodalny?\nWartości Odstające: Czy są nietypowe wartości daleko od głównego rozkładu?",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "href": "rozdzial6.html#wykresy-pudełkowe-i-wykresy-pudełkowe-tukeya",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya",
    "text": "14.4 Wykresy Pudełkowe i Wykresy Pudełkowe Tukeya\nWykresy pudełkowe, znane również jako wykresy skrzynkowe, dostarczają zwięzłego podsumowania rozkładu. Skupimy się na wykresie pudełkowym w stylu Tukeya, nazwanym na cześć statystyka Johna Tukeya, który spopularyzował ten typ wykresu.\n\n14.4.1 Zrozumienie Wykresów Pudełkowych\nWykres pudełkowy przedstawia pięć kluczowych statystyk:\n\nWartość minimalna (z wyłączeniem wartości odstających)\nPierwszy kwartyl (Q1)\nMediana\nTrzeci kwartyl (Q3)\nWartość maksymalna (z wyłączeniem wartości odstających)\n\nDodatkowo wykresy pudełkowe pokazują:\n\nWąsy: Linie rozciągające się od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających)\nWartości odstające: Indywidualne punkty poza wąsami\n\n\n14.4.1.1 Obliczanie Kwartyli i Wartości Odstających\nAby stworzyć wykres pudełkowy, postępuj zgodnie z tymi krokami:\n\nUporządkuj dane od najmniejszej do największej wartości.\nZnajdź medianę (środkowa wartość dla nieparzystej liczby punktów danych, średnia z dwóch środkowych wartości dla parzystej).\nZnajdź Q1 (mediana dolnej połowy danych) i Q3 (mediana górnej połowy danych).\nOblicz Rozstęp Międzykwartylowy (IQR) = Q3 - Q1\nOkreśl wartości odstające używając reguły Tukeya:\n\nDolne wartości odstające: &lt; Q1 - 1.5 * IQR\nGórne wartości odstające: &gt; Q3 + 1.5 * IQR\n\n\n\n\n\n\n\n\nNote\n\n\n\nWspółczynnik 1.5 w regule Tukeya dla wartości odstających opiera się na właściwościach rozkładu normalnego. Dla danych o rozkładzie normalnym, ta reguła identyfikuje około 0.7% danych jako potencjalne wartości odstające.\n\n\n\n\n14.4.1.2 Przykładowe Dane\nUżyjmy małego zbioru danych do ilustracji:\n\ndane &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 50)\n\n\n\n\n14.4.2 Ręcznie Rysowany Wykres Pudełkowy Tukeya\nAby stworzyć wykres pudełkowy Tukeya ręcznie:\n\nNarysuj linię pionową reprezentującą zakres od minimum do maksimum (2 do 15 w naszym przykładzie, z wyłączeniem wartości odstającej).\nNarysuj pudełko od Q1 do Q3.\nNarysuj poziomą linię przez pudełko na poziomie mediany.\nNarysuj wąsy od pudełka do wartości minimalnej i maksymalnej (z wyłączeniem wartości odstających).\nPrzedstaw wartość odstającą (50) jako indywidualny punkt poza wąsem.\nDodaj skalę do osi pionowej i oznacz ją.\n\n\n\n14.4.3 Wykres Pudełkowy w Podstawowym R\n\n# Tworzenie wykresu pudełkowego\nboxplot(dane, main = \"Wykres Pudełkowy Przykładowych Danych\",\n        ylab = \"Wartości\", outcol = \"red\", outpch = 20)\n\n\n\n\n\n\n\n\n\n\n14.4.4 Wykres Pudełkowy Tukeya z ggplot2\n\n# Tworzenie ramki danych\ndf &lt;- data.frame(wartosc = dane)\n\n# Tworzenie wykresu pudełkowego Tukeya z ggplot2\nggplot(df, aes(x = \"\", y = wartosc)) +\n  stat_boxplot(geom = \"errorbar\", width = 0.2) +\n  geom_boxplot(fill = \"white\", outlier.shape = 20, outlier.color = \"red\") +\n  labs(title = \"Wykres Pudełkowy Tukeya Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal() +\n  theme(axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n14.4.5 Interpretacja Wykresów Pudełkowych\nPodczas interpretacji wykresu pudełkowego zwróć uwagę na następujące elementy:\n\nTendencja Centralna: Mediana pokazuje środek rozkładu.\nRozrzut: Pudełko (IQR) reprezentuje środkowe 50% danych.\nSkośność: Jeśli linia mediany jest bliżej jednego końca pudełka, rozkład jest skośny.\nWartości Odstające: Punkty poza wąsami są potencjalnymi wartościami odstającymi.\nPorównania: Przy porównywaniu wielu wykresów pudełkowych, zwróć uwagę na względne położenie median, rozmiary pudełek i obecność wartości odstających.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "href": "rozdzial6.html#zaawansowane-techniki-wizualizacji",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.5 Zaawansowane Techniki Wizualizacji",
    "text": "14.5 Zaawansowane Techniki Wizualizacji\nOprócz podstawowych typów wykresów, warto poznać kilka bardziej zaawansowanych technik wizualizacji, które mogą być przydatne w analizie danych.\n\n14.5.1 Wykresy Skrzypcowe\nWykresy skrzypcowe łączą cechy wykresów pudełkowych i wykresów gęstości, dając bardziej kompletny obraz rozkładu danych.\n\n# Tworzenie wykresu skrzypcowego\nggplot(df, aes(x = \"\", y = wartosc)) +\n  geom_violin(fill = \"lightblue\") +\n  geom_boxplot(width = 0.1, fill = \"white\") +\n  labs(title = \"Wykres Skrzypcowy Przykładowych Danych\",\n       x = \"\", y = \"Wartości\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n14.5.2 Wykresy Rozrzutu z Marginesami\nŁączenie wykresów rozrzutu z histogramami na marginesach może dostarczyć więcej informacji o rozkładzie danych w dwóch wymiarach.\n\n# Generowanie danych do wykresu rozrzutu\nset.seed(123)\ndf_scatter &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100)\n)\n\n# Tworzenie wykresu rozrzutu z marginesami\nlibrary(ggExtra)\np &lt;- ggplot(df_scatter, aes(x = x, y = y)) +\n  geom_point() +\n  theme_minimal()\nggMarginal(p, type = \"histogram\", fill = \"lightblue\")",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#wnioski",
    "href": "rozdzial6.html#wnioski",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.6 Wnioski",
    "text": "14.6 Wnioski\nW tym rozdziale poznaliśmy trzy podstawowe typy wizualizacji danych: wykresy słupkowe, histogramy i wykresy pudełkowe. Pokazaliśmy, jak tworzyć te wykresy ręcznie, używając podstawowego systemu wykresów R oraz biblioteki ggplot2.\nKażdy typ wykresu służy innemu celowi: - Wykresy słupkowe doskonale nadają się do porównywania kategorii. - Histogramy pokazują rozkład zmiennej ciągłej. - Wykresy pudełkowe dostarczają zwięzłego podsumowania rozkładu, podkreślając tendencję centralną, rozrzut i wartości odstające.\nPamiętaj, że wybór wizualizacji zależy od typu danych i wniosków, które chcesz przekazać. Zawsze bierz pod uwagę swoją docelową grupę odbiorców i historię, którą chcesz opowiedzieć za pomocą swoich danych, wybierając i projektując wizualizacje.\nĆwicz tworzenie tych wykresów ręcznie, aby pogłębić zrozumienie ich konstrukcji i interpretacji. Następnie wykorzystaj moc R i ggplot2, aby szybko tworzyć i dostosowywać te wizualizacje dla większych zbiorów danych i bardziej złożonych analiz.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "rozdzial6.html#ćwiczenia-praktyczne",
    "href": "rozdzial6.html#ćwiczenia-praktyczne",
    "title": "14  Wizualizacja Danych: z przykładami w R",
    "section": "14.7 Ćwiczenia Praktyczne",
    "text": "14.7 Ćwiczenia Praktyczne\n\nZbierz dane o popularności różnych gatunków muzycznych wśród Twoich znajomych. Stwórz wykres słupkowy przedstawiający te dane.\nZmierz czas reakcji 30 osób na bodziec dźwiękowy (w milisekundach). Utwórz histogram tych danych.\nZbierz dane o wzroście 50 osób w Twojej społeczności. Stwórz wykres pudełkowy dla tych danych, osobno dla mężczyzn i kobiet.\nZnajdź zestaw danych online (np. na Kaggle) i stwórz trzy różne wizualizacje dla tych danych. Opisz, jakie wnioski można wyciągnąć z każdej wizualizacji.\nStwórz wykres skrzypcowy dla danych o cenach domów w różnych dzielnicach miasta. Porównaj go z wykresem pudełkowym tych samych danych. Jakie dodatkowe informacje dostarcza wykres skrzypcowy?\n\nPamiętaj, że praktyka jest kluczem do opanowania sztuki wizualizacji danych. Eksperymentuj z różnymi typami wykresów i parametrami, aby znaleźć najlepszy sposób przedstawienia swoich danych.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Wizualizacja Danych: z przykładami w R</span>"
    ]
  },
  {
    "objectID": "correg_en.html",
    "href": "correg_en.html",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "",
    "text": "15.1 Bivariate Statistics - introduction\nBivariate statistics describe the relationship between two variables. We’ll explore several measures, starting with covariance.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#bivariate-statistics---introduction",
    "href": "correg_en.html#bivariate-statistics---introduction",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "",
    "text": "Understanding Variable Relationships in Social Research\n\n\n\nThis section examines how different social variables interact and correlate with each other. We will analyze four distinct types of relationships commonly observed in social science research, using empirical examples to illustrate key patterns and their implications for data analysis.\n\n\n\n\n\n\n\n\n\nAnalysis of Variable Relationships:\n\nPositive Linear Correlation (Healthcare Access and Life Expectancy)\n\nThe data demonstrates a positive linear relationship between healthcare access and life expectancy. Statistical analysis indicates that a 10-percentage-point increase in healthcare access corresponds to approximately 2.5 years of additional life expectancy. This relationship maintains statistical significance across the observed range.\n\nNegative Linear Correlation (Digital Device Usage and Sleep Quality)\n\nThe analysis reveals an inverse relationship between daily device usage and sleep quality metrics. The data indicates that each additional hour of device usage correlates with a measurable decrease in sleep quality scores, demonstrating a consistent negative linear relationship.\n\nAbsence of Correlation (Infrastructure and Economic Indicators (e.g. GDP per capita in PLN))\n\nThe relationship between infrastructure density and economic indicators like GDP per capita in PLN shows no statistically significant correlation. This absence of correlation suggests that these variables operate independently within the observed parameters, indicating the presence of other determining factors not captured in this analysis.\n\nNon-linear Relationship (Team Size and Productivity)\n\nThe relationship between team size and productivity follows a curvilinear pattern. The data shows an optimal range for team size, with productivity declining both below and above this range. This demonstrates the importance of considering non-linear patterns in organizational research.\nMethodological Considerations:\n\nStatistical Significance: Observed relationships must be evaluated for statistical significance before drawing conclusions.\nVariable Independence: The assumption of variable independence requires verification through appropriate statistical tests.\nConfounding Variables: Analyses must account for potential confounding variables that may influence observed relationships.\nCausality: Correlation patterns do not necessarily imply causal relationships; additional research methods are required to establish causation.\n\nResearch Applications:\nThe understanding of these relationship patterns has significant implications for:\n\nResearch design and methodology\nStatistical analysis procedures\nPolicy implementation and evaluation\nTheory development and testing\n\nCritical evaluation of these relationships enables more robust research design and more reliable conclusions in social science research.\n\n\n\n\n\n\n\n\nThe Critical Distinction Between Correlation and Causation [See e.g. https://www.tylervigen.com/spurious-correlations]\n\n\n\n\n\n\nhttps://x.com/EUFIC/status/1324667630238814209?prefetchTimestamp=1732463940216\n\n\n\n\n\nhttps://sitn.hms.harvard.edu/flash/2021/when-correlation-does-not-imply-causation-why-your-gut-microbes-may-not-yet-be-a-silver-bullet-to-all-your-problems/\n\n\nStatistical relationships between variables represent one of the most frequently misinterpreted aspects of data analysis. While correlations can reveal patterns in data, they require careful interpretation to avoid drawing incorrect causal conclusions. Let us examine this concept through real-world examples.\n\n15.1.1 Seasonal Patterns and Spurious Correlations: A Case Study\n\n\n\n\n\n\n\n\n\nThis visualization demonstrates a classic example of confounding in statistical analysis. The apparent correlation between ice cream sales and crime rates (r = 0.85, p &lt; 0.001) exemplifies how seasonal variation can create misleading statistical relationships. The correlation emerges from a common causal factor: seasonal temperature variations that independently influence both variables through distinct mechanisms.\n\n\n15.1.2 Temporal Trends and Spurious Associations\n\n\n\n\n\n\n\n\n\nThis second analysis illustrates temporal correlation bias, where two independently declining trends create an artificial statistical association. Despite the strong correlation coefficient (r = 0.91, p &lt; 0.001), there is no plausible causal mechanism linking these variables.\n\n\n15.1.3 Understanding Mechanisms of Spurious Correlation\nStatistical analysis can be compromised by several systematic biases that create apparent but meaningless correlations. Here are the primary mechanisms:\n1. Confounding Variables\nA confounding variable creates an apparent relationship between independent variables by independently affecting each one. This statistical phenomenon requires careful experimental design and multivariate analysis to detect and control for potential confounders.\n2. Temporal Autocorrelation\nWhen variables exhibit strong temporal trends, they may show correlation simply because they change over time, rather than due to any meaningful relationship. This effect can be controlled for through methods such as detrending or differencing the time series.\n3. Simultaneous Causality Bias\nThis occurs when the direction of causality is ambiguous or bidirectional. For example, economic growth and investment rates may exhibit simultaneous causality, as each variable potentially influences the other through complex feedback mechanisms.\n\n\n15.1.4 Statistical Methods for Causal Inference\nModern statistical approaches offer several techniques for moving beyond simple correlation toward causal inference, e.g.:\n1. Experimental Design\nRandomized controlled trials represent the gold standard for causal inference, allowing researchers to isolate the effect of individual variables while controlling for confounders.\n2. Instrumental Variables\nThis statistical technique uses a variable that affects the outcome only through its effect on the variable of interest, helping to establish causal relationships in observational data.\n3. Regression Discontinuity\nThis quasi-experimental design exploits naturally occurring thresholds to approximate randomized experiments, providing evidence for causal relationships.\n\n\n15.1.5 Critical Framework for Correlation Analysis\nWhen evaluating correlational findings, consider the following analytical framework:\n\nTheoretical Plausibility: Examine whether there exists a logical mechanism through which one variable could influence the other.\nTemporal Precedence: Verify that the proposed cause precedes the effect in time.\nDose-Response Relationship: Assess whether changes in the magnitude of the proposed cause correspond to proportional changes in the effect.\nConsistency: Evaluate whether the relationship holds across different contexts and populations.\nAlternative Explanations: Systematically consider and test alternative explanations for the observed correlation.\n\nRemember: The path from correlation to causation requires careful experimental design, robust statistical methodology, and systematic consideration of alternative explanations. Statistical correlation represents a necessary but insufficient condition for establishing causality.\n\n\n\n\n15.1.6 Covariance\nCovariance measures how two variables vary together.\nFormula: cov(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n\n\n\n\n\n\nFrom Covariance to Different Correlation Measures\n\n\n\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Generate different types of relationships\nset.seed(123)\nn &lt;- 100\n\n# Linear relationship\nx1 &lt;- rnorm(n)\ny1 &lt;- 0.8*x1 + rnorm(n, sd=0.5)\ndata1 &lt;- data.frame(x=x1, y=y1, type=\"Linear Relationship\")\n\n# Monotonic but nonlinear\nx2 &lt;- rnorm(n)\ny2 &lt;- sign(x2)*(x2^2) + rnorm(n, sd=0.5)\ndata2 &lt;- data.frame(x=x2, y=y2, type=\"Monotonic Nonlinear\")\n\n# Non-monotonic relationship\nx3 &lt;- seq(-3, 3, length.out=n)\ny3 &lt;- x3^2 + rnorm(n, sd=0.5)\ndata3 &lt;- data.frame(x=x3, y=y3, type=\"Non-monotonic\")\n\n# Combine data\nall_data &lt;- rbind(data1, data2, data3)\n\n# Create plot\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6) +\n  facet_wrap(~type, scales = \"free\") +\n  labs(title = \"Different Types of Relationships Between Variables\",\n       x = \"Variable X\",\n       y = \"Variable Y\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    strip.text = element_text(size = 12),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n15.1.7 The Concept of Correlation\nCorrelation is a broad concept that describes how variables are related to each other. This relationship can take many forms, as shown in our plots.\n\n\n15.1.8 Starting with Covariance\nCovariance is the fundamental measure of how variables move together:\nCov(X,Y) = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\nIt tells us:\n\nIf variables tend to move in the same direction (positive covariance)\nIf they move in opposite directions (negative covariance)\nIf they don’t have a clear linear pattern (covariance near zero)\n\nHowever, covariance has a limitation: its value depends on the units of measurement. For example:\n\nHeight in meters vs. weight in kg gives one covariance value\nHeight in centimeters vs. weight in kg gives a different value\nSame relationship, different scales!\n\n\n\n15.1.9 Standardizing to Get Correlation Measures\n\nPearson’s correlation coefficient standardizes covariance: r = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\n\n\nRemoves unit dependency\nAlways between -1 and 1\nMeasures linear relationships\n\n\nSpearman’s rank correlation:\n\n\nBased on ranks rather than raw values\nCaptures monotonic relationships (even nonlinear ones)\nAlso ranges from -1 to 1\n\n\n\n15.1.10 Key Points\n\nStart with covariance to understand joint movement\nUse correlation coefficients for standardized measures\nChoose your correlation measure based on:\n\nType of relationship you expect\nNature of your data\nResearch question\n\nAlways visualize your data\n\n\n\n\n\n\n\n\n\n\nRanks: Positions in an Ordered Sequence\n\n\n\nRanks are simply position numbers in an ordered dataset:\n\n15.1.11 How to Determine Ranks?\n\nOrder data from smallest to largest value\nAssign consecutive natural numbers:\n\nSmallest value → rank 1\nSubsequent values → subsequent ranks\nLargest value → rank n (number of observations)\nFor ties → average of ranks\n\n\n\n\n15.1.12 Example\nWe have 5 students with heights:\nHeight:    165, 182, 170, 168, 175\nRanks:      1,   5,   3,   2,   4\nFor data with ties (e.g., grades):\nGrades:     2,   3,   3,   4,   5\nRanks:      1,  2.5, 2.5,  4,   5\n\n\n\nManual Calculation Example:\nLet’s calculate the covariance for two variables:\n\nx: 1, 2, 3, 4, 5\ny: 2, 4, 5, 4, 5\n\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate means\n\\bar{x} = 3, \\bar{y} = 4\n\n\n2\nCalculate (x_i - \\bar{x})(y_i - \\bar{y}) for each pair\n(-2)(-2) = 4\n\n\n\n\n(-1)(0) = 0\n\n\n\n\n(0)(1) = 0\n\n\n\n\n(1)(0) = 0\n\n\n\n\n(2)(1) = 2\n\n\n3\nSum the results\n4 + 0 + 0 + 0 + 2 = 6\n\n\n4\nDivide by (n-1)\n6 / 4 = 1.5\n\n\n\nR calculation:\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 5, 4, 5)\ncov(x, y)\n\n[1] 1.5\n\n\nInterpretation: - The positive covariance (1.5) indicates that x and y tend to increase together.\nPros:\n\nProvides direction of relationship (positive or negative)\nUseful in calculating other measures like correlation\n\nCons:\n\nScale-dependent, making it difficult to compare across different variable pairs\nDoesn’t provide information about the strength of the relationship\n\n\n\n15.1.13 Pearson Correlation\nPearson correlation measures the strength and direction of the linear relationship between two continuous variables.\nFormula: r = \\frac{cov(X,Y)}{s_X s_Y} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\nManual Calculation Example:\nUsing the same data as above:\n\n\n\n\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nCalculate covariance\n(From previous calculation) 1.5\n\n\n2\nCalculate standard deviations\ns_X = \\sqrt{\\frac{10}{4}} = 1.58, s_Y = \\sqrt{\\frac{6}{4}} = 1.22\n\n\n3\nDivide covariance by product of standard deviations\n1.5 / (1.58 * 1.22) = 0.7746\n\n\n\nR calculation:\n\ncor(x, y, method = \"pearson\")\n\n[1] 0.7745967\n\n\nInterpretation: - The correlation coefficient of 0.7746 indicates a strong positive linear relationship between x and y.\nPros:\n\nScale-independent, always between -1 and 1\nWidely understood and used\nTests for linear relationships\n\nCons:\n\nSensitive to outliers\nOnly measures linear relationships\nAssumes normally distributed variables\n\n\n\n15.1.14 Spearman Correlation\nSpearman correlation measures the strength and direction of the monotonic relationship between two variables, which can be continuous or ordinal.\nFormula: \\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}, where d_i is the difference between ranks.\nManual Calculation Example:\nLet’s use slightly different data:\n\nx: 1, 2, 3, 4, 5\ny: 1, 3, 2, 5, 4\n\n\n\n\nStep\nDescription\nCalculation\n\n\n\n\n1\nRank both variables\nx_rank: 1, 2, 3, 4, 5\n\n\n\n\ny_rank: 1, 3, 2, 5, 4\n\n\n2\nCalculate differences in ranks (d)\n0, -1, 1, -1, 1\n\n\n3\nSquare the differences\n0, 1, 1, 1, 1\n\n\n4\nSum the squared differences\n\\sum d_i^2 = 4\n\n\n5\nApply the formula\n\\rho = 1 - \\frac{6(4)}{5(5^2 - 1)} = 0.8\n\n\n\nR calculation:\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(1, 3, 2, 5, 4)\ncor(x, y, method = \"spearman\")\n\n[1] 0.8\n\n\nInterpretation: - The Spearman correlation of 0.8 indicates a strong positive monotonic relationship between x and y.\nPros:\n\nRobust to outliers\nCan detect non-linear monotonic relationships\nSuitable for ordinal data\n\nCons:\n\nLess powerful than Pearson for detecting linear relationships in normally distributed data\nDoesn’t provide information about the shape of the relationship beyond monotonicity\n\n\n\n15.1.15 Cross-tabulation\nCross-tabulation (contingency table) shows the relationship between two categorical variables.\nExample:\nLet’s create a cross-tabulation of two variables: - Education level: High School, College, Graduate - Employment status: Employed, Unemployed\n\neducation &lt;- factor(c(\"High School\", \"College\", \"Graduate\", \"High School\", \"College\", \"Graduate\", \"High School\", \"College\", \"Graduate\"))\nemployment &lt;- factor(c(\"Employed\", \"Employed\", \"Employed\", \"Unemployed\", \"Employed\", \"Employed\", \"Unemployed\", \"Unemployed\", \"Employed\"))\n\ntable(education, employment)\n\n             employment\neducation     Employed Unemployed\n  College            2          1\n  Graduate           3          0\n  High School        1          2\n\n\nInterpretation:\n\nThis table shows the count of individuals in each combination of education level and employment status.\nFor example, we can see how many high school graduates are employed versus unemployed.\n\nPros:\n\nProvides a clear visual representation of the relationship between categorical variables\nEasy to understand and interpret\nBasis for many statistical tests (e.g., chi-square test of independence)\n\nCons:\n\nLimited to categorical data\nCan become unwieldy with many categories\nDoesn’t provide a single summary statistic of association strength\n\n\n\n15.1.16 Choosing the Appropriate Measure\nWhen deciding which bivariate statistic to use, consider:\n\nData type:\n\nContinuous data: Covariance, Pearson correlation\nOrdinal data: Spearman correlation\nCategorical data: Cross-tabulation\n\nRelationship type:\n\nLinear: Pearson correlation\nMonotonic but potentially non-linear: Spearman correlation\n\nPresence of outliers:\n\nIf outliers are a concern, Spearman correlation is more robust\n\nDistribution:\n\nFor normally distributed data, Pearson correlation is most powerful\nFor non-normal distributions, consider Spearman correlation\n\nSample size:\n\nFor very small samples, non-parametric methods like Spearman correlation might be preferred\n\n\nRemember, it’s often valuable to use multiple measures and visualizations (like scatter plots) to get a comprehensive understanding of the relationship between variables.\n\n\n15.1.17 Comparing Pearson, Spearman, and Kendall Correlations\nCorrelation measures quantify the strength and direction of relationships between variables. We’ll explore three key correlation coefficients:\n\nPearson’s product-moment correlation (r)\nSpearman’s rank correlation (ρ)\nKendall’s rank correlation (τ)\n\nMathematical Foundations\n\n15.1.17.1 Pearson Correlation Coefficient\nThe Pearson correlation coefficient measures linear relationships between two continuous variables.\n r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\sum_{i=1}^{n} (y_i - \\bar{y})^2}} \nwhere:\n\nx_i, y_i are the individual observations\n\\bar{x}, \\bar{y} are the means\nn is the sample size\n\n\n\n15.1.17.2 Spearman’s Rank Correlation\nSpearman’s ρ assesses monotonic relationships using ranks.\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} \nwhere:\n\nd_i is the difference between ranks for observation i\nn is the sample size\n\n\n\n15.1.17.3 Kendall’s Tau\nKendall’s τ measures ordinal association based on concordant and discordant pairs.\n \\tau = \\frac{2(P - Q)}{n(n-1)} \nwhere:\n\nP is the number of concordant pairs\nQ is the number of discordant pairs\nn is the sample size\n\n\n\n\n15.1.18 Implementation in R\n\n# Sample data\nset.seed(123)\nx &lt;- c(2, 4, 5, 3, 8)\ny &lt;- c(3, 5, 4, 4, 7)\ndata &lt;- data.frame(x = x, y = y)\n\n# Calculate correlations\npearson_cor &lt;- cor(x, y, method = \"pearson\")\nspearman_cor &lt;- cor(x, y, method = \"spearman\")\nkendall_cor &lt;- cor(x, y, method = \"kendall\")\n\n# Display results\ncat(\"Pearson correlation:\", round(pearson_cor, 3), \"\\n\")\n\nPearson correlation: 0.917 \n\ncat(\"Spearman correlation:\", round(spearman_cor, 3), \"\\n\")\n\nSpearman correlation: 0.821 \n\ncat(\"Kendall correlation:\", round(kendall_cor, 3), \"\\n\")\n\nKendall correlation: 0.738 \n\n\n\n\n15.1.19 Visualization\n\nlibrary(ggplot2)\n\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(title = \"Correlation Example\",\n       subtitle = paste(\"Pearson r =\", round(pearson_cor, 3)),\n       x = \"X variable\",\n       y = \"Y variable\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n15.1.20 Key Properties\n\n15.1.20.1 Pearson Correlation (r)\n\nRange: [-1, 1]\nProperties:\n\nr = 1: Perfect positive linear relationship\nr = -1: Perfect negative linear relationship\nr = 0: No linear relationship\n\n\n\n\n15.1.20.2 Spearman Correlation (ρ)\n\nRange: [-1, 1]\nProperties:\n\nBased on ranks, not raw values\nResistant to outliers\nDetects monotonic relationships\n\n\n\n\n15.1.20.3 Kendall Correlation (τ)\n\nRange: [-1, 1]\nProperties:\n\nBased on concordant/discordant pairs\nMore robust than Spearman\nOften preferred for small samples\n\n\n\n\n\n15.1.21 When to Use Each Measure\n\nUse Pearson when:\n\nRelationship appears linear\nData is continuous\nNormal distribution assumed\nNo significant outliers\n\nUse Spearman when:\n\nRelationship is monotonic but not linear\nData is ordinal\nOutliers present\nNon-normal distribution\n\nUse Kendall when:\n\nSmall sample size\nMany tied ranks\nMost robust measure needed\nOrdinal data with few categories\n\n\n\n\n15.1.22 Example Analysis\n\n# Generate non-linear relationship\nset.seed(456)\nx2 &lt;- seq(1, 10, length.out = 20)\ny2 &lt;- x2^2 + rnorm(20, 0, 5)\ndata2 &lt;- data.frame(x = x2, y = y2)\n\n# Calculate correlations\ncor_results &lt;- data.frame(\n  Method = c(\"Pearson\", \"Spearman\", \"Kendall\"),\n  Correlation = c(\n    cor(x2, y2, method = \"pearson\"),\n    cor(x2, y2, method = \"spearman\"),\n    cor(x2, y2, method = \"kendall\")\n  )\n)\n\nprint(cor_results)\n\n    Method Correlation\n1  Pearson   0.9699718\n2 Spearman   0.9849624\n3  Kendall   0.9368421\n\n# Visualize\nggplot(data2, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  labs(title = \"Non-linear Relationship Example\",\n       subtitle = \"Compare different correlation measures\",\n       x = \"X variable\",\n       y = \"Y variable\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n15.1.23 Practical Considerations\n\nSample Size\n\nSmall samples (n &lt; 10): Consider Kendall’s τ\nLarger samples: Any method appropriate\n\nOutliers\n\nPresent: Prefer Spearman or Kendall\nAbsent: Pearson may be more powerful\n\nType of Relationship\n\nLinear: Pearson optimal\nMonotonic: Spearman or Kendall\nComplex: Consider other methods\n\n\n\n\n15.1.24 Common Pitfalls\n\nMisinterpretation\n\nCorrelation ≠ causation\nHigh correlation doesn’t imply good fit\nLow correlation doesn’t mean no relationship\n\nTechnical Issues\n\nMissing values handling\nTied ranks in Spearman/Kendall\nScale sensitivity",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#introduction-to-elementary-multivariate-statistics",
    "href": "correg_en.html#introduction-to-elementary-multivariate-statistics",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.2 Introduction to Elementary Multivariate Statistics (*)",
    "text": "15.2 Introduction to Elementary Multivariate Statistics (*)\nMultivariate statistics involve the analysis of relationships among three or more variables simultaneously. This section will introduce some basic concepts and techniques in multivariate analysis, with a focus on correlation-based methods.\n\n15.2.1 Correlation Matrix\nA correlation matrix is a table showing the pairwise correlations of several variables. It’s a fundamental tool in multivariate analysis.\nExample: Let’s create a correlation matrix for four variables: height, weight, age, and income.\n\nset.seed(123)  # For reproducibility\nheight &lt;- rnorm(100, 170, 10)\nweight &lt;- height * 0.5 + rnorm(100, 0, 5)\nage &lt;- rnorm(100, 40, 10)\nincome &lt;- age * 1000 + rnorm(100, 0, 10000)\n\ndata &lt;- data.frame(height, weight, age, income)\n\ncor_matrix &lt;- cor(data)\nprint(cor_matrix)\n\n           height      weight         age      income\nheight  1.0000000  0.66712996 -0.12917601 -0.12246786\nweight  0.6671300  1.00000000 -0.06814187 -0.04579492\nage    -0.1291760 -0.06814187  1.00000000  0.65654902\nincome -0.1224679 -0.04579492  0.65654902  1.00000000\n\n\nInterpretation:\n\nEach cell shows the correlation between two variables.\nThe diagonal is always 1 (correlation of a variable with itself).\nLook for strong correlations (close to 1 or -1) to identify potential relationships.\n\n\n\n15.2.2 Visualizing Multivariate Relationships\n\n15.2.2.1 Scatterplot Matrix\nA scatterplot matrix shows pairwise relationships between multiple variables.\n\npairs(data)\n\n\n\n\n\n\n\n\nInterpretation:\n\nEach plot shows the relationship between two variables.\nDiagonal elements show the distribution of each variable.\nLook for patterns, clusters, or trends in the plots.\n\n\n\n15.2.2.2 Correlation Plot\nA correlation plot provides a visual representation of the correlation matrix.\n\nlibrary(corrplot)\n\ncorrplot 0.94 loaded\n\ncorrplot(cor_matrix, method = \"color\")\n\n\n\n\n\n\n\n\nInterpretation:\n\nColor intensity and size of the circles indicate the strength of correlation.\nBlue colors typically indicate positive correlations, red colors indicate negative correlations.\n\n\n\n\n15.2.3 Partial Correlation\nPartial correlation measures the relationship between two variables while controlling for one or more other variables.\nExample: Let’s calculate the partial correlation between height and weight, controlling for age.\n\nlibrary(ppcor)\npcor.test(data$height, data$weight, data$age)\n\n   estimate      p.value statistic   n gp  Method\n1 0.6654367 5.758157e-14  8.779896 100  1 pearson\n\n\nInterpretation:\n\nCompare this to the simple correlation between height and weight.\nA significant change might indicate that age plays a role in the height-weight relationship.\n\n\n\n15.2.4 Multiple Correlation\nMultiple correlation measures the strength of the relationship between a dependent variable and multiple independent variables.\nExample: Let’s predict weight using height and age.\n\nmodel &lt;- lm(weight ~ height + age, data = data)\nR &lt;- sqrt(summary(model)$r.squared)\nprint(paste(\"Multiple correlation coefficient:\", R))\n\n[1] \"Multiple correlation coefficient: 0.667377840470434\"\n\n\nInterpretation:\n\nR ranges from 0 to 1, with higher values indicating stronger relationships.\nR² (R-squared) represents the proportion of variance in the dependent variable explained by the independent variables.\n\n\n\n15.2.5 Factor Analysis\nFactor analysis is a technique used to reduce many variables to a smaller number of underlying factors.\nExample: Let’s perform a simple factor analysis on our dataset.\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nfa_result &lt;- fa(data, nfactors = 2, rotate = \"varimax\")\nprint(fa_result$loadings, cutoff = 0.3)\n\n\nLoadings:\n       MR2    MR1   \nheight  0.798       \nweight  0.836       \nage            0.729\nincome         0.895\n\n                 MR2   MR1\nSS loadings    1.344 1.341\nProportion Var 0.336 0.335\nCumulative Var 0.336 0.671\n\n\nInterpretation:\n\nLook at which variables load highly on each factor.\nTry to interpret what each factor might represent based on the variables that load on it.\n\n\n\n15.2.6 Considerations in Multivariate Analysis\n\nSample Size: Multivariate techniques often require larger sample sizes for stable results.\nMulticollinearity: High correlations among independent variables can cause issues in some analyses.\nOutliers: Multivariate outliers can have a strong influence on results.\nAssumptions: Many techniques assume multivariate normality and linear relationships.\nInterpretation Complexity: As the number of variables increases, interpretation can become more challenging.\n\n\n\n15.2.7 Conclusion\nThis introduction to multivariate statistics builds upon the concept of correlation to explore relationships among multiple variables. These techniques provide powerful tools for understanding complex datasets, but they also require careful consideration of assumptions and limitations. As you progress in your statistical journey, you’ll encounter more advanced multivariate techniques such as MANOVA, discriminant analysis, and structural equation modeling.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n✔ purrr     1.0.2     ✔ tibble    3.2.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ psych::%+%()         masks ggplot2::%+%()\n✖ psych::alpha()       masks ggplot2::alpha()\n✖ gridExtra::combine() masks dplyr::combine()\n✖ dplyr::filter()      masks stats::filter()\n✖ dplyr::lag()         masks stats::lag()\n✖ MASS::select()       masks dplyr::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(gridExtra)",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#introduction-to-regression-analysis",
    "href": "correg_en.html#introduction-to-regression-analysis",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.3 Introduction to Regression Analysis",
    "text": "15.3 Introduction to Regression Analysis\n\n15.3.1 Why Study Regression?\nRegression analysis is a fundamental statistical tool that helps us understand relationships between variables. Before diving into formulas and technical details, let’s understand what questions regression can help us answer:\n\nHow much does each additional year of education affect someone’s salary?\nWhat is the relationship between advertising spending and sales?\nHow does temperature affect energy consumption?\nDo study hours predict exam scores?\n\nThese questions share a common structure: they all explore how changes in one variable relate to changes in another.\n\n\n\n\n\n\nUnderstanding Linear Regression (OLS): Quickstart guide\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Generate sample data\nset.seed(123)\nn &lt;- 20\nx &lt;- seq(1, 10, length.out = n)\ny &lt;- 2 + 1.5 * x + rnorm(n, sd = 1.5)\ndata &lt;- data.frame(x = x, y = y)\n\n# Calculate OLS parameters\nbeta1 &lt;- cov(x, y) / var(x)\nbeta0 &lt;- mean(y) - beta1 * mean(x)\n\n# Create alternative lines\nlines_data &lt;- data.frame(\n  intercept = c(beta0, beta0 + 1, beta0 - 1),\n  slope = c(beta1, beta1 + 0.3, beta1 - 0.3),\n  line_type = c(\"Best fit (OLS)\", \"Suboptimal 1\", \"Suboptimal 2\")\n)\n\n# Create the plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point(size = 3, alpha = 0.6) +\n  geom_abline(data = lines_data,\n              aes(intercept = intercept, \n                  slope = slope,\n                  color = line_type,\n                  linetype = line_type),\n              size = 1) +\n  scale_color_manual(values = c(\"Best fit (OLS)\" = \"#FF4500\",\n                               \"Suboptimal 1\" = \"#4169E1\",\n                               \"Suboptimal 2\" = \"#228B22\")) +\n  labs(title = \"Finding the Best-Fitting Line\",\n       subtitle = \"Orange line minimizes the sum of squared errors\",\n       x = \"X Variable\",\n       y = \"Y Variable\",\n       color = \"Line Type\",\n       linetype = \"Line Type\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12),\n    legend.position = \"bottom\",\n    panel.grid.minor = element_blank()\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n15.3.2 The Model Concept\nOLS regression is a statistical model that describes the relationship between variables. Two key assumptions define this model:\n\nThe relationship can be described by a straight line (linearity)\nThe errors in our predictions are not systematically related to our x-variable (strict exogeneity)\n\n\n\n15.3.3 Example: Education and Wages\nConsider studying the effect of education (x) on wages (y). Let’s say we estimate:\nwages = \\beta_0 + \\beta_1 \\cdot education + \\epsilon\nThe error term \\epsilon contains all other factors affecting wages. Strict exogeneity is violated if we omit an important variable like “ability” that affects both education and wages. Why? Because more able people tend to get more education AND higher wages, making our estimate of education’s effect biased upward.\n\n\n15.3.4 The Optimization Problem: Understanding OLS\nWhen we analyze relationships between variables like education and wages, we need a systematic method to find the line that best represents this relationship in our data. Ordinary Least Squares (OLS) provides this method through a clear mathematical approach.\nConsider our plot of education levels and wages. Each point represents actual data - one person’s years of education and their corresponding wage. Our goal is to find a single line that most accurately captures the underlying relationship between these variables.\nFor any given observation i, we can express this relationship as: y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\nWhere:\n\ny_i is the actual observed wage\n\\hat{y_i} = \\beta_0 + \\beta_1x_i is our predicted wage\n\\epsilon_i = y_i - \\hat{y_i} is the error term (or residual)\n\nOLS finds the optimal values for \\beta_0 and \\beta_1 by minimizing the sum of squared errors:\n\\min_{\\beta_0, \\beta_1} \\sum \\epsilon_i^2 = \\min_{\\beta_0, \\beta_1} \\sum(y_i - \\hat{y_i})^2 = \\min_{\\beta_0, \\beta_1} \\sum(y_i - (\\beta_0 + \\beta_1x_i))^2\nLooking at our visualization:\n\nThe scattered points show our actual observations (x_i, y_i)\nThe orange line represents our fitted values \\hat{y_i} that minimize \\sum \\epsilon_i^2\nThe blue and green lines represent alternative fits with larger total squared errors\nThe vertical distances from each point to these lines represent the errors \\epsilon_i\n\nThe OLS solution provides us with estimates \\hat{\\beta_0} and \\hat{\\beta_1} that minimize the total squared error, giving us the most accurate linear representation of the relationship between education and wages based on our available data.\n\n\n15.3.5 Finding the Best Line\nThe solution to this minimization gives us:\n\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = \\frac{cov(X, Y)}{var(X)}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\n\n\n15.3.6 Key Points\n\nOLS finds the line that minimizes squared prediction errors\nThis line is “best” in terms of fit, but might not capture true relationships if important variables are omitted\nIn the education-wages example, omitting ability means we attribute all the wage increase to education alone\n\n\n\n\n\n\n15.3.7 Basic Concepts and Terminology\nBefore we dive into the mathematics, let’s establish our key terms:\n\nDependent Variable (Y):\n\nThe outcome we want to understand or predict\nAlso called: response variable, target variable\nExamples: salary, sales, exam scores\n\nIndependent Variable (X):\n\nThe variable we think influences Y\nAlso called: predictor, explanatory variable, regressor\nExamples: years of education, advertising budget, study hours\n\nPopulation Parameters (\\beta):\n\nThe true underlying relationships we want to understand\nUsually unknown in practice\nExamples: \\beta_0 (true intercept), \\beta_1 (true slope)\n\nParameter Estimates (\\hat{\\beta}):\n\nOur best guesses of the true parameters based on data\nCalculated from sample data\nExamples: \\hat{\\beta}_0 (estimated intercept), \\hat{\\beta}_1 (estimated slope)\n\n\n\n\n15.3.8 The Core Idea\nLet’s visualize what regression does with a simple example:\n\n# Generate some example data\nset.seed(123)\nx &lt;- seq(1, 10, by = 0.5)\ny &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit the model\nmodel &lt;- lm(y ~ x, data = data)\n\n# Create the plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Simple Linear Regression Example\",\n       subtitle = \"Points represent data, red line shows regression fit\",\n       x = \"Independent Variable (X)\",\n       y = \"Dependent Variable (Y)\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 15.1: Basic Idea of Regression: Fitting a Line to Data\n\n\n\n\n\nThis plot shows the essence of regression: - Each point represents an observation (X, Y) - The line represents our best guess at the relationship - The spread of points around the line shows the uncertainty",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#the-linear-regression-model",
    "href": "correg_en.html#the-linear-regression-model",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.4 The Linear Regression Model",
    "text": "15.4 The Linear Regression Model\n\n15.4.1 Population Model vs. Sample Estimates\nIn theory, there exists a true population relationship:\nY = \\beta_0 + \\beta_1X + \\varepsilon\nwhere:\n\n\\beta_0 is the true population intercept\n\\beta_1 is the true population slope\n\\varepsilon is the random error term\n\nIn practice, we work with sample data to estimate this relationship:\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X\nLet’s visualize the difference between population and sample relationships:\n\n# Generate population data\nset.seed(456)\nx_pop &lt;- seq(1, 10, by = 0.1)\ntrue_relationship &lt;- 2 + 3*x_pop  # True β₀=2, β₁=3\ny_pop &lt;- true_relationship + rnorm(length(x_pop), 0, 2)\n\n# Create several samples\nsample_size &lt;- 30\nsamples &lt;- data.frame(\n  x = rep(sample(x_pop, sample_size), 3),\n  sample = rep(1:3, each = sample_size)\n)\n\nsamples$y &lt;- 2 + 3*samples$x + rnorm(nrow(samples), 0, 2)\n\n# Fit models to each sample\nmodels &lt;- samples %&gt;%\n  group_by(sample) %&gt;%\n  summarise(\n    intercept = coef(lm(y ~ x))[1],\n    slope = coef(lm(y ~ x))[2]\n  )\n\n# Plot\nggplot() +\n  geom_point(data = samples, aes(x = x, y = y, color = factor(sample)), \n             alpha = 0.5) +\n  geom_abline(data = models, \n              aes(intercept = intercept, slope = slope, \n                  color = factor(sample)),\n              linetype = \"dashed\") +\n  geom_line(aes(x = x_pop, y = true_relationship), \n            color = \"black\", size = 1) +\n  theme_minimal() +\n  labs(title = \"Population vs. Sample Regression Lines\",\n       subtitle = \"Black line: true population relationship\\nDashed lines: sample estimates\",\n       x = \"X\", y = \"Y\",\n       color = \"Sample\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 15.2: Population vs. Sample Regression Lines\n\n\n\n\n\nThis visualization shows: - The true population line (black) we’re trying to discover - Different sample estimates (dashed lines) based on different samples - How sample estimates vary around the true relationship",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#key-assumptions-of-linear-regression",
    "href": "correg_en.html#key-assumptions-of-linear-regression",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.5 Key Assumptions of Linear Regression",
    "text": "15.5 Key Assumptions of Linear Regression\n\n15.5.1 Strict Exogeneity: The Fundamental Assumption\nThe most crucial assumption in regression is strict exogeneity:\nE[\\varepsilon|X] = 0\nThis means:\n\nThe error term has zero mean conditional on X\nX contains no information about the average error\nThere are no systematic patterns in how our predictions are wrong\n\nLet’s visualize when this assumption holds and when it doesn’t:\n\n# Generate data\nset.seed(789)\nx &lt;- seq(1, 10, by = 0.2)\n\n# Case 1: Exogenous errors\ny_exog &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\n\n# Case 2: Non-exogenous errors (error variance increases with x)\ny_nonexog &lt;- 2 + 3*x + 0.5*x*rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_exog &lt;- data.frame(\n  x = x,\n  y = y_exog,\n  type = \"Exogenous Errors\\n(Assumption Satisfied)\"\n)\n\ndata_nonexog &lt;- data.frame(\n  x = x,\n  y = y_nonexog,\n  type = \"Non-Exogenous Errors\\n(Assumption Violated)\"\n)\n\ndata_combined &lt;- rbind(data_exog, data_nonexog)\n\n# Create plots with residuals\nplot_residuals &lt;- function(data, title) {\n  model &lt;- lm(y ~ x, data = data)\n  data$predicted &lt;- predict(model)\n  data$residuals &lt;- residuals(model)\n  \n  p1 &lt;- ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    theme_minimal() +\n    labs(title = title)\n  \n  p2 &lt;- ggplot(data, aes(x = x, y = residuals)) +\n    geom_point() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    labs(y = \"Residuals\")\n  \n  list(p1, p2)\n}\n\n# Generate plots\nplots_exog &lt;- plot_residuals(data_exog, \"Exogenous Errors\")\nplots_nonexog &lt;- plot_residuals(data_nonexog, \"Non-Exogenous Errors\")\n\n# Arrange plots\ngridExtra::grid.arrange(\n  plots_exog[[1]], plots_exog[[2]],\n  plots_nonexog[[1]], plots_nonexog[[2]],\n  ncol = 2\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 15.3: Exogeneity vs. Non-Exogeneity Examples\n\n\n\n\n\n\n\n15.5.2 Linearity: The Form Assumption\nThe relationship between X and Y should be linear in parameters:\nE[Y|X] = \\beta_0 + \\beta_1X\nNote that this doesn’t mean X and Y must have a straight-line relationship - we can transform variables. Let’s see different types of relationships:\n\n# Generate data\nset.seed(101)\nx &lt;- seq(1, 10, by = 0.1)\n\n# Different relationships\ndata_relationships &lt;- data.frame(\n  x = rep(x, 3),\n  y = c(\n    # Linear\n    2 + 3*x + rnorm(length(x), 0, 2),\n    # Quadratic\n    2 + 0.5*x^2 + rnorm(length(x), 0, 2),\n    # Exponential\n    exp(0.3*x) + rnorm(length(x), 0, 2)\n  ),\n  type = rep(c(\"Linear\", \"Quadratic\", \"Exponential\"), each = length(x))\n)\n\n# Plot\nggplot(data_relationships, aes(x = x, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  facet_wrap(~type, scales = \"free_y\") +\n  theme_minimal() +\n  labs(subtitle = \"Red: linear fit, Blue: true relationship\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 15.4: Linear and Nonlinear Relationships\n\n\n\n\n\n\n\n15.5.3 Understanding Violations and Solutions\nWhen linearity is violated:\n\nTransform variables:\n\nLog transformation: for exponential relationships\nSquare root: for moderate nonlinearity\nPower transformations: for more complex relationships\n\n\n\n# Generate exponential data\nset.seed(102)\nx &lt;- seq(1, 10, by = 0.2)\ny &lt;- exp(0.3*x) + rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_trans &lt;- data.frame(\n  x = x,\n  y = y,\n  log_y = log(y)\n)\n\nWarning in log(y): NaNs produced\n\n# Original scale plot\np1 &lt;- ggplot(data_trans, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Original Scale\")\n\n# Log scale plot\np2 &lt;- ggplot(data_trans, aes(x = x, y = log_y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Log-Transformed Y\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 15.5: Effect of Variable Transformations",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#estimation-methods",
    "href": "correg_en.html#estimation-methods",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.6 Estimation Methods",
    "text": "15.6 Estimation Methods\n\n15.6.1 Ordinary Least Squares (OLS)\nOLS finds \\hat{\\beta}_0 and \\hat{\\beta}_1 by minimizing the sum of squared residuals:\n\\min_{\\hat{\\beta}_0, \\hat{\\beta}_1} \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1X_i)^2\nThe solutions are:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\nLet’s visualize how OLS works:\n\n# Generate sample data\nset.seed(103)\nx_sample &lt;- seq(1, 10, by = 1)\ny_sample &lt;- 2 + 3*x_sample + rnorm(length(x_sample), 0, 2)\ndata_sample &lt;- data.frame(x = x_sample, y = y_sample)\n\n# Fit model\nmodel_sample &lt;- lm(y ~ x, data = data_sample)\ndata_sample$predicted &lt;- predict(model_sample)\n\n# Plot\nggplot(data_sample, aes(x = x, y = y)) +\n  geom_point(color = \"blue\") +\n  geom_line(aes(y = predicted), color = \"red\") +\n  geom_segment(aes(xend = x, y = y, yend = predicted), \n              color = \"green\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"OLS Regression with Residuals\",\n       subtitle = \"Blue points: data\\nRed line: OLS fit\\nGreen dashed lines: residuals\")\n\n\n\n\n\n\n\nFigure 15.6: OLS Minimizing Square Residuals",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#assessing-model-fit",
    "href": "correg_en.html#assessing-model-fit",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.7 Assessing Model Fit",
    "text": "15.7 Assessing Model Fit\n\n15.7.1 Decomposition of Variance\nThe total variability in Y can be broken down into explained and unexplained components:\n\\underbrace{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}_{SST} = \\underbrace{\\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2}_{SSR} + \\underbrace{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}_{SSE}\nwhere:\n\nSST (Total Sum of Squares): Total variation in Y\nSSR (Regression Sum of Squares): Variation explained by regression\nSSE (Error Sum of Squares): Unexplained variation\n\nLet’s visualize this decomposition:\n\n# Generate sample data\nset.seed(104)\nx &lt;- seq(1, 10, by = 0.5)\ny &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\ndf &lt;- data.frame(x = x, y = y)\n\n# Fit model\nmodel &lt;- lm(y ~ x, data = df)\ndf$predicted &lt;- predict(model)\ndf$residuals &lt;- residuals(model)\ndf$mean_y &lt;- mean(df$y)\n\n# Calculate components for one point\npoint_index &lt;- 10\nexample_point &lt;- df[point_index, ]\n\n# Create main plot\np_main &lt;- ggplot(df, aes(x = x)) +\n  geom_point(aes(y = y), color = \"blue\") +\n  geom_line(aes(y = predicted), color = \"red\") +\n  geom_hline(yintercept = mean(df$y), linetype = \"dashed\") +\n  # Total deviation\n  geom_segment(data = example_point,\n               aes(x = x, y = mean_y, xend = x, yend = y),\n               color = \"purple\", size = 1, alpha = 0.5) +\n  # Explained deviation\n  geom_segment(data = example_point,\n               aes(x = x, y = mean_y, xend = x, yend = predicted),\n               color = \"green\", size = 1, alpha = 0.5) +\n  # Unexplained deviation\n  geom_segment(data = example_point,\n               aes(x = x, y = predicted, xend = x, yend = y),\n               color = \"orange\", size = 1, alpha = 0.5) +\n  theme_minimal() +\n  labs(title = \"Decomposition of Variance\",\n       subtitle = \"Purple: Total deviation (Yi - Ȳ)\\nGreen: Explained (Ŷi - Ȳ)\\nOrange: Unexplained (Yi - Ŷi)\")\n\n# Calculate R-squared\nsummary_stats &lt;- glance(model)\nr2 &lt;- round(summary_stats$r.squared, 3)\n\n# Add R-squared to plot\nprint(p_main)\ncat(sprintf(\"\\nR² = %.3f\\nThis means %.1f%% of the variance in Y is explained by X\\n\", r2, r2*100))\n\n\nR² = 0.965\nThis means 96.5% of the variance in Y is explained by X\n\n\n\n\n\n\n\n\nFigure 15.7: Variance Decomposition in Linear Regression\n\n\n\n\n\n\n\n15.7.2 Understanding the Three Types of Variation\n\nTotal Variation (SST)\n\nQuestion: “How much do observations vary from the mean?”\nFormula: SST = \\sum(y_i - \\bar{y})^2\nVisual: Purple dashed lines in the plot\nIntuition: The “spread” of our data around its average\n\nExplained Variation (SSR)\n\nQuestion: “How much of the variation did our model explain?”\nFormula: SSR = \\sum(\\hat{y}_i - \\bar{y})^2\nVisual: Green dashed lines in the plot\nIntuition: The improvement we gained by using X\n\nUnexplained Variation (SSE)\n\nQuestion: “What variation remains unexplained?”\nFormula: SSE = \\sum(y_i - \\hat{y}_i)^2\nVisual: Red dashed lines in the plot\nIntuition: The errors that remain after using X\n\n\n\n\n15.7.3 R² Demystified\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nThink of R² as answering the question: “What percentage of the original variation in Y can we explain using X?”\n\n15.7.3.1 Intuitive Examples:\n\nR² = 0.80: Using X eliminated 80% of our prediction errors\nR² = 0.25: Using X eliminated 25% of our prediction errors\nR² = 0.00: Using X didn’t help at all\n\n\n\n\n15.7.4 When to Be Cautious\n\nHigh R² Isn’t Everything\n\nA high R² might indicate overfitting\nAlways check if your model makes practical sense\nConsider the context of your field\n\nLow R² Isn’t Always Bad\n\nIn some fields, R² = 0.30 might be impressive\nSocial sciences often have lower R² values\nFocus on practical significance\n\nSample Size Matters\n\nUse adjusted R² for multiple regression: R^2_{adj} = 1 - \\frac{SSE/(n-p)}{SST/(n-1)}\nPenalizes for adding unnecessary predictors\n\n\n\n\n15.7.5 Practical Tips for Analysis\n\nVisual Inspection\n\nAlways plot your data\nLook for patterns in residuals\nCheck for influential points\n\nContext Consideration\n\nWhat’s a “good” R² in your field?\nWhat’s the practical impact of your errors?\nAre your predictors meaningful?\n\nModel Diagnostics\n\nCheck residual normality\nLook for heteroscedasticity\nExamine influential points\n\n\n\n\n15.7.6 Key Takeaways\n\nVariance decomposition helps us understand prediction improvement\nR² quantifies the proportion of variance explained\nVisual understanding is crucial for interpretation\nContext matters more than absolute R² values\nAlways combine R² with other diagnostic tools\n\n\n\n15.7.7 Further Reading\n\nFor deeper understanding of vector spaces and projections\nFor multiple regression extensions\nFor robust regression alternatives\nFor non-linear relationships :::\n\n\n\n15.7.8 Measures of Fit\n\nR-squared (R^2): R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nRoot Mean Square Error (RMSE): RMSE = \\sqrt{\\frac{SSE}{n}}\nMean Absolute Error (MAE): MAE = \\frac{1}{n}\\sum_{i=1}^n |Y_i - \\hat{Y}_i|\n\nLet’s calculate and visualize these measures:\n\n# Calculate measures\nrmse &lt;- sqrt(mean(residuals(model)^2))\nmae &lt;- mean(abs(residuals(model)))\n\n# Create residual plot with different measures\nggplot(df, aes(x = predicted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_hline(yintercept = c(rmse, -rmse), linetype = \"dotted\", color = \"blue\") +\n  geom_hline(yintercept = c(mae, -mae), linetype = \"dotted\", color = \"green\") +\n  theme_minimal() +\n  labs(title = \"Residual Plot with Error Measures\",\n       subtitle = sprintf(\"RMSE = %.2f (blue lines)\\nMAE = %.2f (green lines)\", \n                         rmse, mae))\n\n\n\n\n\n\n\nFigure 15.8: Different Measures of Model Fit",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#diagnostic-plots",
    "href": "correg_en.html#diagnostic-plots",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.8 Diagnostic Plots",
    "text": "15.8 Diagnostic Plots\n\n15.8.1 Residual Analysis\nFour key diagnostic plots:\n\n# Create diagnostic plots\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\nFigure 15.9: Key Diagnostic Plots for Linear Regression\n\n\n\n\n\nLet’s interpret each plot:\n\nResiduals vs. Fitted:\n\nChecks linearity assumption\nLook for patterns/trends\nWant random scatter around zero\n\nNormal Q-Q Plot:\n\nChecks normality of residuals\nPoints should follow diagonal line\nDeviations at tails often acceptable\n\nScale-Location:\n\nChecks homoscedasticity\nWant horizontal line with random scatter\nHelps detect changing variance\n\nResiduals vs. Leverage:\n\nIdentifies influential points\nLook for points outside Cook’s distance lines\nHelps identify problematic observations\n\n\n\n\n\n\n\n\nUnderstanding Ordinary Least Squares (OLS) Intuitively\n\n\n\n\n15.8.2 The Basic Problem\nLet’s start with a real-world scenario: understanding how study time affects exam performance. We collect data from your class where:\n\nEach student records their study hours (x)\nAnd their final exam score (y)\nSo student 1 might have studied x_1 = 3 hours and scored y_1 = 75 points\nStudent 2 might have studied x_2 = 5 hours and scored y_2 = 82 points\nAnd so on for all n students in the class\n\nOur goal is to find a straight line that best describes this relationship. We’re trying to estimate the true relationship (which we never know exactly) using our sample of data. Let’s explore this step by step.\n\nlibrary(tidyverse)\n\n# Create sample data\nset.seed(123)\nstudy_hours &lt;- runif(20, 1, 8)\nexam_scores &lt;- 60 + 5 * study_hours + rnorm(20, 0, 5)\ndata &lt;- data.frame(study_hours, exam_scores)\n\n# Basic scatter plot with multiple lines\nggplot(data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  labs(x = \"Study Hours\", y = \"Exam Scores\",\n       title = \"Your Class Data: Study Hours vs. Exam Scores\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n\n\n\n\n\n\n\n\n\n15.8.3 What Makes a Line “Good”?\nAny straight line can be written in the form:\ny = \\hat{\\beta}_0 + \\hat{\\beta}_1x\nWhere:\n\n\\hat{\\beta}_0 is our estimate of the y-intercept (the predicted score for zero study hours)\n\\hat{\\beta}_1 is our estimate of the slope (how many points you gain per extra hour of study)\nThe hats (^) indicate these are our estimates of the true (unknown) parameters \\beta_0 and \\beta_1\n\nLet’s look at three possible lines through our data:\n\nggplot(data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  geom_abline(intercept = 50, slope = 8, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_abline(intercept = 70, slope = 2, color = \"green\", linetype = \"dashed\", size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"purple\") +\n  labs(x = \"Study Hours\", y = \"Exam Scores\",\n       title = \"Three Different Lines: Which is Best?\") +\n  annotate(\"text\", x = 7.5, y = 120, color = \"red\", label = \"Line A: Too Steep\") +\n  annotate(\"text\", x = 7.5, y = 85, color = \"green\", label = \"Line B: Too Flat\") +\n  annotate(\"text\", x = 7.5, y = 100, color = \"purple\", label = \"Line C: Just Right\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n15.8.4 Understanding Prediction Errors (Residuals)\nHere’s where the magic of OLS begins. For each student in our data:\n\nWe look at their actual exam score (y_i)\nWe calculate their predicted score using our line (\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nThe difference between these is called a residual:\n\n\\text{residual}_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nLet’s visualize these residuals for one line:\n\n# Fit the model and show residuals\nmodel &lt;- lm(exam_scores ~ study_hours, data = data)\n\nggplot(data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"purple\") +\n  geom_segment(aes(xend = study_hours, \n                  yend = predict(model, data)),\n              color = \"orange\", alpha = 0.5) +\n  labs(x = \"Study Hours\", y = \"Exam Scores\",\n       title = \"Understanding Residuals: The Gaps Between Predictions and Reality\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe orange vertical lines show how far off our predictions are for each student. Some predictions are too high (positive residuals), others too low (negative residuals).\n\n\n15.8.5 Why Do We Square the Residuals?\nThis is a crucial concept! Let’s walk through it with a simple example:\nImagine we have just two students:\n\nAlice: Predicted 80, actual score 85 (residual = +5)\nBob: Predicted 90, actual score 85 (residual = -5)\n\nIf we just add these residuals: (+5) + (-5) = 0\nThis would suggest our line is perfect (total error = 0), but we know it’s not! Both predictions were off by 5 points.\nSolution: Square the residuals before adding them:\n\nAlice’s squared residual: (+5)^2 = 25\nBob’s squared residual: (-5)^2 = 25\nTotal squared error: 25 + 25 = 50\n\nThis gives us a much better measure of how wrong our predictions are!\n\n\n15.8.6 The Complete Picture: Sum of Squared Residuals (SSR)\nFor all students combined, we calculate:\nSSR = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nThis formula might look scary, but it just means:\n\nTake each student’s residual\nSquare it\nAdd up all these squared residuals\n\nThe smaller this sum, the better our line fits the data!\n\n# Compare good vs bad fit\nbad_predictions &lt;- 70 + 2 * data$study_hours\ngood_predictions &lt;- predict(model, data)\n\nbad_sse &lt;- sum((data$exam_scores - bad_predictions)^2)\ngood_sse &lt;- sum((data$exam_scores - good_predictions)^2)\n\nggplot(data, aes(x = study_hours, y = exam_scores)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  geom_abline(intercept = 70, slope = 2, color = \"red\", \n              linetype = \"dashed\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"purple\") +\n  annotate(\"text\", x = 2, y = 95, \n           label = paste(\"Red Line: Total Error =\", round(bad_sse)), \n           color = \"red\") +\n  annotate(\"text\", x = 2, y = 90, \n           label = paste(\"Purple Line: Total Error =\", round(good_sse)), \n           color = \"purple\") +\n  labs(x = \"Study Hours\", y = \"Exam Scores\",\n       title = \"Comparing Total Prediction Errors\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n15.8.7 Why Is This Method “Ordinary Least Squares”?\nLet’s break down the name:\n\n“Squares”: We square the residuals\n“Least”: We want the smallest possible total\n“Ordinary”: This is the basic version (there are fancier versions!)\n\nThe OLS line has some nice properties:\n\nThe mean of all residuals equals zero\nThe line always passes through the point (\\bar{x}, \\bar{y}) - the average study hours and average score\nSmall changes in the data lead to small changes in the line (it’s “stable”)\nOur estimates \\hat{\\beta}_0 and \\hat{\\beta}_1 are the best possible estimates of the true parameters \\beta_0 and \\beta_1 under certain conditions\n\nExperiment with different lines and see how the total squared error changes:\n\nlibrary(manipulate)\n\nmanipulate(\n  {\n    predictions &lt;- b0 + b1 * data$study_hours\n    ssr &lt;- sum((data$exam_scores - predictions)^2)\n    \n    ggplot(data, aes(x = study_hours, y = exam_scores)) +\n      geom_point(color = \"blue\", alpha = 0.6) +\n      geom_abline(slope = b1, intercept = b0, color = \"red\") +\n      labs(title = paste(\"Total Squared Error =\", round(ssr, 1))) +\n      theme_minimal()\n  },\n  b0 = slider(40, 80, initial = 60, label = \"Intercept (β̂₀)\"),\n  b1 = slider(0, 10, initial = 5, label = \"Slope (β̂₁)\")\n)\n\nCan you find the line that gives the smallest total squared error? That’s the OLS line!\n\n\n15.8.8 Important Notes\n\nThe hat notation (\\hat{\\beta}_0, \\hat{\\beta}_1) reminds us that we’re estimating the true relationship from our sample. We never know the true \\beta_0 and \\beta_1 - we can only estimate them from our data.\nOLS gives us the best possible estimates when certain conditions are met (like having randomly sampled data and a truly linear relationship).\nThe interactive tool above helps you understand what OLS does automatically: it finds the values of \\hat{\\beta}_0 and \\hat{\\beta}_1 that give us the smallest possible sum of squared residuals.\n\n\n\n\n\n\n\n\n\n\nFormal Derivation of OLS Estimators: Step-by-Step Approach\n\n\n\n\n15.8.9 Initial Setup\nWe seek to minimize the sum of squared residuals:\nSSR = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nLet’s break this into manageable pieces:\n\nEach residual is: e_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nWe square each residual: e_i^2 = (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nSum them all: SSR = \\sum_{i=1}^n e_i^2\n\n\n\n15.8.10 Chain Rule Review\nBefore proceeding, let’s recall the chain rule. For a composite function f(g(x)):\n\\frac{d}{dx}[f(g(x))] = f'(g(x)) \\cdot g'(x)\nIn our case, we’re dealing with the square function f(u) = u^2, where:\n\nf'(u) = 2u\nu = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\n\n\n\n15.8.11 Step 1: Finding \\hat{\\beta}_0 Using First Derivative\nLet’s take the partial derivative with respect to \\hat{\\beta}_0 step by step:\n\nStart with one term of the sum:\n\\frac{\\partial}{\\partial \\hat{\\beta}_0}(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nApply chain rule:\n\nOuter function: f(u) = u^2, so f'(u) = 2u\nInner function: g(\\hat{\\beta}_0) = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nInner derivative: g'(\\hat{\\beta}_0) = -1\n\nTherefore, for each term: \\frac{\\partial}{\\partial \\hat{\\beta}_0}(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2 = 2(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))(-1)\nNow sum all terms and set to zero: \\sum_{i=1}^n 2(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))(-1) = 0\nSimplify: -2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\nRemove the -2: \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\nExpand the sum: \\sum_{i=1}^n y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum_{i=1}^n x_i = 0\nSolve for \\hat{\\beta}_0: n\\hat{\\beta}_0 = \\sum_{i=1}^n y_i - \\hat{\\beta}_1\\sum_{i=1}^n x_i\n\\hat{\\beta}_0 = \\frac{\\sum_{i=1}^n y_i}{n} - \\hat{\\beta}_1\\frac{\\sum_{i=1}^n x_i}{n}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\n\n\n\n15.8.12 Step 2: Finding \\hat{\\beta}_1 Using First Derivative\nNow let’s find \\hat{\\beta}_1 with the same careful approach:\n\nFor one term: \\frac{\\partial}{\\partial \\hat{\\beta}_1}(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nApply chain rule:\n\nOuter function: f(u) = u^2, so f'(u) = 2u\nInner function: g(\\hat{\\beta}_1) = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nInner derivative: g'(\\hat{\\beta}_1) = -x_i\n\nTherefore: \\frac{\\partial}{\\partial \\hat{\\beta}_1}(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2 = 2(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))(-x_i)\nSum all terms and set to zero: \\sum_{i=1}^n 2(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))(-x_i) = 0\nSimplify: -2\\sum_{i=1}^n x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\nSubstitute \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}: -2\\sum_{i=1}^n x_i(y_i - (\\bar{y} - \\hat{\\beta}_1\\bar{x}) - \\hat{\\beta}_1x_i) = 0\nExpand: -2\\sum_{i=1}^n x_i(y_i - \\bar{y} + \\hat{\\beta}_1\\bar{x} - \\hat{\\beta}_1x_i) = 0\nDistribute x_i: -2\\sum_{i=1}^n (x_iy_i - x_i\\bar{y} + x_i\\hat{\\beta}_1\\bar{x} - x_i^2\\hat{\\beta}_1) = 0\nCollect terms with \\hat{\\beta}_1: \\sum_{i=1}^n (x_i^2\\hat{\\beta}_1 - x_i\\hat{\\beta}_1\\bar{x}) = \\sum_{i=1}^n (x_iy_i - x_i\\bar{y})\nFactor out \\hat{\\beta}_1: \\hat{\\beta}_1\\sum_{i=1}^n (x_i^2 - x_i\\bar{x}) = \\sum_{i=1}^n (x_iy_i - x_i\\bar{y})\nFinal form: \\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n\n\n\n15.8.13 Step 3: Verifying We Have a Minimum\nTo confirm these critical points are minima, we check the second derivatives:\n\nSecond derivative with respect to \\hat{\\beta}_0: \\frac{\\partial^2 SSR}{\\partial \\hat{\\beta}_0^2} = \\frac{\\partial}{\\partial \\hat{\\beta}_0}(-2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)) = 2n &gt; 0\nSecond derivative with respect to \\hat{\\beta}_1: \\frac{\\partial^2 SSR}{\\partial \\hat{\\beta}_1^2} = \\frac{\\partial}{\\partial \\hat{\\beta}_1}(-2\\sum_{i=1}^n x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)) = 2\\sum_{i=1}^n x_i^2 &gt; 0\nCross partial derivatives: \\frac{\\partial^2 SSR}{\\partial \\hat{\\beta}_0\\partial \\hat{\\beta}_1} = \\frac{\\partial^2 SSR}{\\partial \\hat{\\beta}_1\\partial \\hat{\\beta}_0} = 2\\sum_{i=1}^n x_i\nThe Hessian matrix is positive definite: \\mathbf{H} = \\begin{bmatrix} 2n & 2\\sum x_i \\\\ 2\\sum x_i & 2\\sum x_i^2 \\end{bmatrix}\n\nThis confirms we have found a minimum.\n\n\n15.8.14 Visualizing the Process\n\nlibrary(tidyverse)\n\n# Create sample data\nset.seed(123)\nx &lt;- runif(20, 1, 8)\ny &lt;- 2 + 3 * x + rnorm(20, 0, 1)\ndata &lt;- data.frame(x = x, y = y)\n\n# Calculate means\nx_mean &lt;- mean(x)\ny_mean &lt;- mean(y)\n\n# Create visualization\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_hline(yintercept = y_mean, linetype = \"dashed\", color = \"gray\") +\n  geom_vline(xintercept = x_mean, linetype = \"dashed\", color = \"gray\") +\n  geom_segment(aes(xend = x, yend = y_mean), color = \"green\", alpha = 0.3) +\n  geom_segment(aes(yend = y, xend = x_mean), color = \"purple\", alpha = 0.3) +\n  labs(title = \"Understanding the OLS Derivation\",\n       subtitle = \"Green: y deviations, Purple: x deviations\\nTheir product forms the numerator of β̂₁\",\n       x = \"x\", y = \"y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n15.8.15 Key Insights\n\nThe chain rule is crucial in deriving both estimators\n\\hat{\\beta}_0 ensures the line goes through (\\bar{x}, \\bar{y})\n\\hat{\\beta}_1 is a ratio of covariance to variance\nThe second derivatives confirm we’ve found a minimum\nThe entire process relies on calculus to find the optimal values that minimize the sum of squared residuals\n\n\n\n\n\n\n\n\n\n\nUnderstanding Variance Decomposition in Linear Regression\n\n\n\n\n15.8.16 Why It Matters: Improving Predictions with Extra Information\nImagine you’re trying to predict house prices. A simple way to make predictions is by using the average price of all houses. But what if you know additional details, like the size of the house? Would that help improve your predictions? Variance decomposition helps us measure exactly how much better our predictions are when we include more information, like house size.\n\n\n15.8.17 The Process: From a Simple Guess to a Smarter Prediction\n\n15.8.17.1 Step 1: Starting with the Average\n\nWhat’s our first guess? The mean of all house prices (\\bar{y}).\nThink of this as your “uninformed guess.”\nFor every house, we predict the same price (the average), which leads to our baseline errors.\n\n\n\n15.8.17.2 Step 2: Using Extra Information\n\nWe now use additional information (like house size, X) to refine our predictions.\nThis allows us to make different predictions for each house.\nThe errors we make after including this information are usually smaller.\n\n\n\n\n15.8.18 Visualizing Variance Decomposition\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(patchwork)\n\n\nAttaching package: 'patchwork'\n\n\nThe following object is masked from 'package:MASS':\n\n    area\n\n# Generate data with clearer pattern\nset.seed(123)\nx &lt;- seq(1, 10, length.out = 50)\ny &lt;- 2 + 0.5 * x + rnorm(50, sd = 0.8)\ndata &lt;- data.frame(x = x, y = y)\n\n# Model and calculations\nmodel &lt;- lm(y ~ x, data)\nmean_y &lt;- mean(y)\ndata$predicted &lt;- predict(model)\n\n# Select specific points for demonstration that are well-spaced\ndemonstration_points &lt;- c(8, 25, 42)  # Changed points for better spacing\n\n# Create main plot with improved aesthetics\np1 &lt;- ggplot(data, aes(x = x, y = y)) +\n  # Add background grid for better readability\n  geom_hline(yintercept = seq(0, 8, by = 0.5), color = \"gray90\", linewidth = 0.2) +\n  geom_vline(xintercept = seq(0, 10, by = 0.5), color = \"gray90\", linewidth = 0.2) +\n  \n  # Add regression line and mean line\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E41A1C\", linewidth = 1.2) +\n  geom_hline(yintercept = mean_y, linetype = \"longdash\", color = \"#377EB8\", linewidth = 1) +\n  \n  # Add data points\n  geom_point(size = 3, alpha = 0.6, color = \"#4A4A4A\") +\n  \n  # Add decomposition segments with improved colors and positioning\n  # Total deviation (purple)\n  geom_segment(data = data[demonstration_points,],\n              aes(x = x, xend = x, y = y, yend = mean_y),\n              color = \"#984EA3\", linetype = \"dashed\", linewidth = 1.8) +\n  # Explained component (green)\n  geom_segment(data = data[demonstration_points,],\n              aes(x = x, xend = x, y = mean_y, yend = predicted),\n              color = \"#4DAF4A\", linetype = \"dashed\", linewidth = 1) +\n  # Unexplained component (orange)\n  geom_segment(data = data[demonstration_points,],\n              aes(x = x, xend = x, y = predicted, yend = y),\n              color = \"#FF7F00\", linetype = \"dashed\", linewidth = 1) +\n  \n  # Add annotations for better understanding\n  annotate(\"text\", x = data$x[demonstration_points[2]], y = mean_y - 0.2,\n           label = \"Mean\", color = \"#377EB8\", hjust = -0.2) +\n  annotate(\"text\", x = data$x[demonstration_points[2]], \n           y = data$predicted[demonstration_points[2]] + 0.2,\n           label = \"Regression Line\", color = \"#E41A1C\", hjust = -0.2) +\n  \n  # Improve theme and labels\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 12),\n    panel.grid = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  labs(\n    title = \"Variance Decomposition in Linear Regression\",\n    subtitle = \"Decomposing total variance into explained and unexplained components\",\n    x = \"Predictor (X)\",\n    y = \"Response (Y)\"\n  )\n\n# Create error distribution plot with improved aesthetics\ndata$mean_error &lt;- y - mean_y\ndata$regression_error &lt;- y - data$predicted\n\np2 &lt;- ggplot(data) +\n  geom_density(aes(x = mean_error, fill = \"Deviation from Mean\"), \n               alpha = 0.5) +\n  geom_density(aes(x = regression_error, fill = \"Regression Residuals\"), \n               alpha = 0.5) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  ) +\n  labs(\n    title = \"Error Distribution Comparison\",\n    x = \"Error Magnitude\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(\n    values = c(\"#377EB8\", \"#E41A1C\")\n  )\n\n# Add legend explaining the decomposition components\nlegend_plot &lt;- ggplot() +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    legend.box = \"horizontal\"\n  ) +\n  annotate(\"text\", x = 0, y = 0, label = \"\") +\n  scale_color_manual(\n    name = \"Variance Components\",\n    values = c(\"#984EA3\", \"#4DAF4A\", \"#FF7F00\"),\n    labels = c(\"Total Deviation\", \"Explained Variance\", \"Unexplained Variance\")\n  )\n\n# Combine plots with adjusted heights\ncombined_plot &lt;- (p1 / p2) +\n  plot_layout(heights = c(2, 1))\n\n# Print the combined plot\ncombined_plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n15.8.19 Breaking Down the Types of Variation\n\nTotal Variation (SST)\n\nQuestion: How much do the data points vary from the mean?\nFormula: SST = \\sum(y_i - \\bar{y})^2\nVisual: Purple points in the plot\nIntuition: This is the overall “spread” of the data around its average.\n\nExplained Variation (SSR)\n\nQuestion: How much of the variation can our model explain?\nFormula: SSR = \\sum(\\hat{y}_i - \\bar{y})^2\nVisual: Green dashed lines in the plot\nIntuition: This is the improvement we get from including additional information (like house size).\n\nUnexplained Variation (SSE)\n\nQuestion: How much variation is left unexplained after using the model?\nFormula: SSE = \\sum(y_i - \\hat{y}_i)^2\nVisual: Orange dashed lines in the plot\nIntuition: These are the errors remaining after our model has made predictions.\n\n\n\n\n15.8.20 What is R²?\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nThink of R^2 as answering: “What percentage of the original variation in the data can we explain using our model?”\n\n15.8.20.1 Examples of R^2 Values:\n\nR^2 = 0.80: Our model explains 80% of the variation in house prices.\nR^2 = 0.25: Our model explains 25% of the variation (a weaker model).\nR^2 = 0.00: Our model explains none of the variation (not helpful).\n\n\n\n\n15.8.21 Important Things to Remember About R²\n\nHigh R^2 Isn’t Always Good\n\nA very high R^2 could suggest overfitting. Your model might be too complex, capturing noise rather than real patterns.\nAlways interpret R^2 in the context of your data.\n\nLow R^2 Isn’t Always Bad\n\nIn some fields (like social sciences), a low R^2 can still be useful.\nFocus on practical significance, not just the R^2 value.\n\nConsider Sample Size\n\nFor multiple regression models, use adjusted R^2 to account for the number of predictors.\nFormula: R^2_{adj} = 1 - \\frac{SSE/(n-p)}{SST/(n-1)}\n\n\n\n\n15.8.22 Tips for Effective Analysis\n\nVisualize the Data\n\nPlot your data and residuals to spot patterns.\nCheck for influential points that could skew your results.\n\nUnderstand Your Field’s Context\n\nWhat’s considered a good R^2 value in your field?\nWhat’s the practical impact of the errors you’re seeing?\n\nRun Diagnostics\n\nCheck residuals for normality.\nLook for heteroscedasticity (changing variability of errors).\nWatch for influential data points that affect the model’s accuracy.\n\n\n\n\n15.8.23 Key Takeaways\n\nVariance decomposition explains how prediction improves with more information.\nR^2 shows how much of the total variation is explained by the model.\nVisualizing data is key to understanding regression performance.\nContext is more important than just looking at R^2 values.\nAlways use diagnostic tools along with R^2 to evaluate your model’s quality.\n\n\n\n15.8.24 Further Reading\n\nExplore advanced topics like vector spaces and projections.\nLearn about multiple regression techniques and robust methods.\nUnderstand non-linear relationships in regression models.\n\n\n\n\n\n\n\n\n\n\nUnderstanding Endogeneity\n\n\n\nThink of endogeneity as a “hidden relationship problem” in your analysis. It’s like trying to solve a puzzle while some pieces are affecting each other in ways you can’t see directly. In technical terms, endogeneity occurs when an explanatory variable in a regression model is correlated with the error term, but let’s understand this intuitively!\n\nOmitted Variable Bias (OVB)\n\nThink of it like trying to understand why some plants grow taller than others, and you only measure the amount of water they receive. But you forgot about sunlight, which affects both how much water the plant needs AND how tall it grows!\nReal-Life Example: Education and Income * What we see: More education → Higher income * What we might miss: Natural talent/ability - Affects how long people stay in school - Affects how much they can earn * Result: We might overestimate how much education helps\nThe Math Behind It (don’t worry, this helps visualize!): * True picture: y_i = \\beta_0 + \\beta_1x_i + \\beta_2z_i + \\epsilon_i\n* What we actually estimate: y_i = \\beta_0 + \\beta_1x_i + u_i\n* Think of it like a recipe: If you forget an important ingredient (z), your final dish (y) won’t turn out as expected!\n\nSimultaneity (Reverse Causation)\n\nRemember the chicken and egg problem? Sometimes two things affect each other at the same time. Here are some examples you might encounter:\na) The Study Hours Puzzle * Do better grades lead to studying more? * Or does studying more lead to better grades? * Actually… both! They affect each other\nb) The Social Media Effect * More followers → More posts * More posts → More followers * It’s a continuous cycle!\nc) The Exercise-Energy Cycle * More exercise → More energy * More energy → More likely to exercise * Think of it like two friends pushing each other on swings!\n\nMeasurement Error\n\nImagine trying to measure your height while standing on an uneven floor - your measurements won’t be quite right! Here’s how this shows up in real life:\nExamples You’ll Recognize: * Self-Reported Study Time - “I study 5 hours a day” might really mean 3-4 hours - Makes it hard to know true impact on grades * Fitness App Tracking - App says you burned 500 calories - Actually might be 400 or 600 - Affects analysis of exercise impact\n\n15.8.24.1 How to Spot These Problems in Your Own Research?\n1. For Omitted Variables, Ask: * What else could affect both variables? * Am I missing any obvious factors? * What would my parents/friends say influences this?\n2. For Simultaneity, Consider: * Could A cause B, or could B cause A? * Might they affect each other? * What happened first? (if you can tell)\n3. For Measurement Error, Think: * How accurate are my measurements? * Are people likely to report truthfully? * What might cause measurement problems?\nSimple Solutions\n1. For Omitted Variables:\n\n# Instead of:\n# simple_model &lt;- lm(grades ~ study_hours)\n\n# Try:\n# better_model &lt;- lm(grades ~ study_hours + sleep_hours + stress_level + prior_knowledge)\n\n2. For Simultaneity: * Look for “external” factors that affect only one variable * Consider time lags * Use natural experiments when possible\n3. For Measurement Error: * Use multiple measurements * Find more reliable data sources * Acknowledge uncertainty in your conclusions\n\n\n15.8.24.2 Key Takeaways for Students\n\nReality is Complex\n\nMost relationships aren’t simple A → B\nLook for hidden factors\nConsider two-way relationships\n\nAlways Ask\n\n“What am I missing?”\n“Could these affect each other?”\n“How well am I measuring things?”\n\nWhen Writing Papers\n\nDiscuss potential endogeneity\nExplain how you addressed it\nBe honest about limitations\n\n\n\n\n15.9 Recommended Reading\n\n“Mastering Metrics” by Angrist & Pischke\n“Naked Statistics” by Charles Wheelan\n\nRemember: In the real world, relationships are usually more complex than they first appear. When you find a connection between A and B, always ask what else might be going on behind the scenes!",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#recommended-reading",
    "href": "correg_en.html#recommended-reading",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.9 Recommended Reading",
    "text": "15.9 Recommended Reading\n\n“Mastering Metrics” by Angrist & Pischke\n“Naked Statistics” by Charles Wheelan\n\nRemember: In the real world, relationships are usually more complex than they first appear. When you find a connection between A and B, always ask what else might be going on behind the scenes!",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#multiple-regression",
    "href": "correg_en.html#multiple-regression",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.10 Multiple Regression (*)",
    "text": "15.10 Multiple Regression (*)\n\n15.10.1 Extending to Multiple Predictors\nThe multiple regression model extends our simple model to include several predictors:\nPopulation Model: Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k + \\varepsilon\nSample Estimation: \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 + ... + \\hat{\\beta}_kX_k\nLet’s create an example with multiple predictors:\n\n# Generate sample data with two predictors\nset.seed(105)\nn &lt;- 100\nX1 &lt;- rnorm(n, mean = 50, sd = 10)\nX2 &lt;- rnorm(n, mean = 20, sd = 5)\nY &lt;- 10 + 0.5*X1 + 0.8*X2 + rnorm(n, 0, 5)\n\ndata_multiple &lt;- data.frame(Y = Y, X1 = X1, X2 = X2)\n\n# Fit multiple regression model\nmodel_multiple &lt;- lm(Y ~ X1 + X2, data = data_multiple)\n\n# Create 3D visualization using scatter plots\np1 &lt;- ggplot(data_multiple, aes(x = X1, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Y vs X1\")\n\np2 &lt;- ggplot(data_multiple, aes(x = X2, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Y vs X2\")\n\ngrid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nMultiple Regression Example\n\n\n\n# Print model summary\nsummary(model_multiple)\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = data_multiple)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8598  -3.6005   0.1166   3.0892  14.6102 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.77567    4.01351   2.934  0.00418 ** \nX1           0.45849    0.05992   7.651 1.47e-11 ***\nX2           0.81639    0.11370   7.180 1.42e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.122 on 97 degrees of freedom\nMultiple R-squared:  0.5062,    Adjusted R-squared:  0.4961 \nF-statistic: 49.72 on 2 and 97 DF,  p-value: 1.367e-15\n\n\n\n\n15.10.2 Interpretation of Coefficients\nIn multiple regression, each \\hat{\\beta}_k represents the expected change in Y for a one-unit increase in X_k, holding all other variables constant.\n\n# Create prediction grid for X1 (holding X2 at its mean)\nX1_grid &lt;- seq(min(X1), max(X1), length.out = 100)\npred_data_X1 &lt;- data.frame(\n  X1 = X1_grid,\n  X2 = mean(X2)\n)\npred_data_X1$Y_pred &lt;- predict(model_multiple, newdata = pred_data_X1)\n\n# Create prediction grid for X2 (holding X1 at its mean)\nX2_grid &lt;- seq(min(X2), max(X2), length.out = 100)\npred_data_X2 &lt;- data.frame(\n  X1 = mean(X1),\n  X2 = X2_grid\n)\npred_data_X2$Y_pred &lt;- predict(model_multiple, newdata = pred_data_X2)\n\n# Plot partial effects\np3 &lt;- ggplot() +\n  geom_point(data = data_multiple, aes(x = X1, y = Y)) +\n  geom_line(data = pred_data_X1, aes(x = X1, y = Y_pred), \n            color = \"red\", size = 1) +\n  theme_minimal() +\n  labs(title = \"Partial Effect of X1\",\n       subtitle = paste(\"(X2 held at mean =\", round(mean(X2), 2), \")\"))\n\np4 &lt;- ggplot() +\n  geom_point(data = data_multiple, aes(x = X2, y = Y)) +\n  geom_line(data = pred_data_X2, aes(x = X2, y = Y_pred), \n            color = \"red\", size = 1) +\n  theme_minimal() +\n  labs(title = \"Partial Effect of X2\",\n       subtitle = paste(\"(X1 held at mean =\", round(mean(X1), 2), \")\"))\n\ngrid.arrange(p3, p4, ncol = 2)\n\n\n\n\nPartial Effects in Multiple Regression\n\n\n\n\n\n\n15.10.3 Multicollinearity\nMulticollinearity occurs when predictors are highly correlated. Let’s demonstrate its effects:\n\n# Generate data with multicollinearity\nset.seed(106)\nX1_new &lt;- rnorm(n, mean = 50, sd = 10)\nX2_new &lt;- 2*X1_new + rnorm(n, 0, 5)  # X2 highly correlated with X1\nY_new &lt;- 10 + 0.5*X1_new + 0.8*X2_new + rnorm(n, 0, 5)\n\ndata_collinear &lt;- data.frame(Y = Y_new, X1 = X1_new, X2 = X2_new)\n\n# Fit model with multicollinearity\nmodel_collinear &lt;- lm(Y ~ X1 + X2, data = data_collinear)\n\n# Calculate VIF\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nThe following object is masked from 'package:psych':\n\n    logit\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nvif_results &lt;- vif(model_collinear)\n\n# Plot correlation\nggplot(data_collinear, aes(x = X1, y = X2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Correlation between Predictors\",\n       subtitle = paste(\"Correlation =\", \n                       round(cor(X1_new, X2_new), 3)))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nEffects of Multicollinearity",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#advanced-topics",
    "href": "correg_en.html#advanced-topics",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.11 Advanced Topics",
    "text": "15.11 Advanced Topics\n\n15.11.1 Interaction Terms\nInteraction terms allow the effect of one predictor to depend on another:\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3(X_1 \\times X_2) + \\varepsilon\n\n# Generate data with interaction\nset.seed(107)\nX1_int &lt;- rnorm(n, mean = 0, sd = 1)\nX2_int &lt;- rnorm(n, mean = 0, sd = 1)\nY_int &lt;- 1 + 2*X1_int + 3*X2_int + 4*X1_int*X2_int + rnorm(n, 0, 1)\n\ndata_int &lt;- data.frame(X1 = X1_int, X2 = X2_int, Y = Y_int)\nmodel_int &lt;- lm(Y ~ X1 * X2, data = data_int)\n\n# Create interaction plot\nX1_levels &lt;- quantile(X1_int, probs = c(0.25, 0.75))\nX2_seq &lt;- seq(min(X2_int), max(X2_int), length.out = 100)\n\npred_data &lt;- expand.grid(\n  X1 = X1_levels,\n  X2 = X2_seq\n)\npred_data$Y_pred &lt;- predict(model_int, newdata = pred_data)\npred_data$X1_level &lt;- factor(pred_data$X1, \n                            labels = c(\"Low X1\", \"High X1\"))\n\nggplot(pred_data, aes(x = X2, y = Y_pred, color = X1_level)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Interaction Effect\",\n       subtitle = \"Effect of X2 depends on level of X1\",\n       color = \"X1 Level\")\n\n\n\n\nVisualization of Interaction Effects\n\n\n\n\n\n\n15.11.2 Polynomial Terms\nWhen relationships are non-linear, we can add polynomial terms:\nY = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\varepsilon\n\n# Generate data with quadratic relationship\nset.seed(108)\nX_poly &lt;- seq(-3, 3, length.out = 100)\nY_poly &lt;- 1 - 2*X_poly + 3*X_poly^2 + rnorm(length(X_poly), 0, 2)\ndata_poly &lt;- data.frame(X = X_poly, Y = Y_poly)\n\n# Fit linear and quadratic models\nmodel_linear &lt;- lm(Y ~ X, data = data_poly)\nmodel_quad &lt;- lm(Y ~ X + I(X^2), data = data_poly)\n\n# Add predictions\ndata_poly$pred_linear &lt;- predict(model_linear)\ndata_poly$pred_quad &lt;- predict(model_quad)\n\n# Plot\nggplot(data_poly, aes(x = X, y = Y)) +\n  geom_point(alpha = 0.5) +\n  geom_line(aes(y = pred_linear, color = \"Linear\"), size = 1) +\n  geom_line(aes(y = pred_quad, color = \"Quadratic\"), size = 1) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme_minimal() +\n  labs(title = \"Linear vs. Quadratic Fit\",\n       color = \"Model Type\")\n\n\n\n\nPolynomial Regression Example",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#practical-guidelines-for-regression-analysis",
    "href": "correg_en.html#practical-guidelines-for-regression-analysis",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.12 Practical Guidelines for Regression Analysis",
    "text": "15.12 Practical Guidelines for Regression Analysis\n\n15.12.1 Model Building Process\n\nData Exploration\n\n\n# Generate example dataset\nset.seed(109)\nn &lt;- 100\ndata_example &lt;- data.frame(\n  x1 = rnorm(n, mean = 50, sd = 10),\n  x2 = rnorm(n, mean = 20, sd = 5),\n  x3 = runif(n, 0, 100)\n)\ndata_example$y &lt;- 10 + 0.5*data_example$x1 + 0.8*data_example$x2 - \n                 0.3*data_example$x3 + rnorm(n, 0, 5)\n\n# Correlation matrix plot\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nggpairs(data_example) +\n  theme_minimal() +\n  labs(title = \"Exploratory Data Analysis\",\n       subtitle = \"Correlation matrix and distributions\")\n\n\n\n\nData Exploration Example\n\n\n\n\n\nVariable Selection\n\n\n# Fit models with different variables\nmodel1 &lt;- lm(y ~ x1, data = data_example)\nmodel2 &lt;- lm(y ~ x1 + x2, data = data_example)\nmodel3 &lt;- lm(y ~ x1 + x2 + x3, data = data_example)\n\n# Compare models\nmodels_comparison &lt;- data.frame(\n  Model = c(\"y ~ x1\", \"y ~ x1 + x2\", \"y ~ x1 + x2 + x3\"),\n  R_squared = c(summary(model1)$r.squared,\n                summary(model2)$r.squared,\n                summary(model3)$r.squared),\n  Adj_R_squared = c(summary(model1)$adj.r.squared,\n                    summary(model2)$adj.r.squared,\n                    summary(model3)$adj.r.squared)\n)\n\nknitr::kable(models_comparison, digits = 3,\n             caption = \"Model Comparison Summary\")\n\n\nModel Comparison Summary\n\n\nModel\nR_squared\nAdj_R_squared\n\n\n\n\ny ~ x1\n0.323\n0.316\n\n\ny ~ x1 + x2\n0.433\n0.421\n\n\ny ~ x1 + x2 + x3\n0.893\n0.890\n\n\n\nVariable Selection Process\n\n\n\n\n15.12.2 Common Pitfalls and Solutions\n\nOutliers and Influential Points\n\n\n# Create data with outlier\nset.seed(110)\nx_clean &lt;- rnorm(50, mean = 0, sd = 1)\ny_clean &lt;- 2 + 3*x_clean + rnorm(50, 0, 0.5)\ndata_clean &lt;- data.frame(x = x_clean, y = y_clean)\n\n# Add outlier\ndata_outlier &lt;- rbind(data_clean,\n                      data.frame(x = 4, y = -10))\n\n# Fit models\nmodel_clean &lt;- lm(y ~ x, data = data_clean)\nmodel_outlier &lt;- lm(y ~ x, data = data_outlier)\n\n# Plot\nggplot() +\n  geom_point(data = data_clean, aes(x = x, y = y), color = \"blue\") +\n  geom_point(data = data_outlier[51,], aes(x = x, y = y), \n             color = \"red\", size = 3) +\n  geom_line(data = data_clean, \n            aes(x = x, y = predict(model_clean), \n                color = \"Without Outlier\")) +\n  geom_line(data = data_outlier, \n            aes(x = x, y = predict(model_outlier), \n                color = \"With Outlier\")) +\n  theme_minimal() +\n  labs(title = \"Effect of Outliers on Regression\",\n       color = \"Model\") +\n  scale_color_manual(values = c(\"blue\", \"red\"))\n\n\n\n\nIdentifying and Handling Outliers\n\n\n\n\n\nMissing Data Patterns\n\n\n# Create data with missing values\nset.seed(111)\ndata_missing &lt;- data_example\ndata_missing$x1[sample(1:n, 10)] &lt;- NA\ndata_missing$x2[sample(1:n, 15)] &lt;- NA\ndata_missing$x3[sample(1:n, 20)] &lt;- NA\n\n# Visualize missing patterns\nlibrary(naniar)\nvis_miss(data_missing) +\n  theme_minimal() +\n  labs(title = \"Missing Data Patterns\")\n\n\n\n\nMissing Data Patterns\n\n\n\n\n\nHeteroscedasticity\n\n\n# Generate heteroscedastic data\nset.seed(112)\nx_hetero &lt;- seq(-3, 3, length.out = 100)\ny_hetero &lt;- 2 + 1.5*x_hetero + rnorm(100, 0, abs(x_hetero)/2)\ndata_hetero &lt;- data.frame(x = x_hetero, y = y_hetero)\n\n# Fit model\nmodel_hetero &lt;- lm(y ~ x, data = data_hetero)\n\n# Plot\np1 &lt;- ggplot(data_hetero, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Heteroscedastic Data\")\n\np2 &lt;- ggplot(data_hetero, aes(x = fitted(model_hetero), \n                             y = residuals(model_hetero))) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Residual Plot\",\n       x = \"Fitted values\",\n       y = \"Residuals\")\n\ngrid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nDetecting and Visualizing Heteroscedasticity\n\n\n\n\n\n\n15.12.3 Best Practices\n\nModel Validation\n\n\n# Simple cross-validation example\nset.seed(113)\n\n# Create training and test sets\ntrain_index &lt;- sample(1:nrow(data_example), 0.7*nrow(data_example))\ntrain_data &lt;- data_example[train_index, ]\ntest_data &lt;- data_example[-train_index, ]\n\n# Fit model on training data\nmodel_train &lt;- lm(y ~ x1 + x2 + x3, data = train_data)\n\n# Predict on test data\npredictions &lt;- predict(model_train, newdata = test_data)\nactual &lt;- test_data$y\n\n# Calculate performance metrics\nrmse &lt;- sqrt(mean((predictions - actual)^2))\nmae &lt;- mean(abs(predictions - actual))\nr2 &lt;- cor(predictions, actual)^2\n\n# Plot predictions vs actual\ndata_validation &lt;- data.frame(\n  Predicted = predictions,\n  Actual = actual\n)\n\nggplot(data_validation, aes(x = Actual, y = Predicted)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Model Validation: Predicted vs Actual\",\n       subtitle = sprintf(\"RMSE = %.2f, MAE = %.2f, R² = %.2f\", \n                         rmse, mae, r2))\n\n\n\n\nCross-Validation Example\n\n\n\n\n\nReporting Results\n\nExample of a professional regression results table:\n\n# Create regression results table\nlibrary(broom)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nmodel_final &lt;- lm(y ~ x1 + x2 + x3, data = data_example)\nresults &lt;- tidy(model_final, conf.int = TRUE)\n\nkable(results, digits = 3,\n      caption = \"Regression Results Summary\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nRegression Results Summary\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n9.116\n2.835\n3.216\n0.002\n3.489\n14.743\n\n\nx1\n0.497\n0.039\n12.756\n0.000\n0.419\n0.574\n\n\nx2\n0.905\n0.086\n10.468\n0.000\n0.734\n1.077\n\n\nx3\n-0.324\n0.016\n-20.322\n0.000\n-0.356\n-0.292",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#conclusion-1",
    "href": "correg_en.html#conclusion-1",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.13 Conclusion",
    "text": "15.13 Conclusion\n\n15.13.1 Key Takeaways\n\nAlways start with exploratory data analysis\nCheck assumptions before interpreting results\nBe aware of common pitfalls:\n\nOutliers\nMissing data\nMulticollinearity\nHeteroscedasticity\n\nValidate your model using:\n\nDiagnostic plots\nCross-validation\nResidual analysis\n\nReport results clearly and completely\n\n\n\nFurther Reading\nFor deeper understanding:\n\nWooldridge, J.M. “Introductory Econometrics: A Modern Approach”\nFox, J. “Applied Regression Analysis and Generalized Linear Models”\nAngrist, J.D. and Pischke, J.S. “Mostly Harmless Econometrics”\nStock & Watson “Introduction to Econometrics”",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "href": "correg_en.html#appendix-a.1-understanding-correlation-measures-a-self-study-tutorial-using-stress-level-and-cognitive-performance-data",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.14 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)",
    "text": "15.14 Appendix A.1: Understanding Correlation Measures: A Self-Study Tutorial (Using Stress Level and Cognitive Performance Data)\n\n15.14.1 Dataset Overview\n\ndata &lt;- data.frame(\n  anxiety_level = c(8, 5, 11, 14, 7, 10),\n  cognitive_performance = c(85, 90, 62, 55, 80, 65)\n)\n\n\n\n15.14.2 1. Covariance Calculation\n\n15.14.2.1 Step 1: Calculate Means\n\n\n\n\n\n\n\n\nVariable\nCalculation\nResult\n\n\n\n\nMean Anxiety (\\bar{x})\n(8 + 5 + 11 + 14 + 7 + 10) ÷ 6\n9.17\n\n\nMean Cognitive (\\bar{y})\n(85 + 90 + 62 + 55 + 80 + 65) ÷ 6\n72.83\n\n\n\n\n\n15.14.2.2 Step 2: Calculate Deviations and Products\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})(y_i - \\bar{y})\n\n\n\n\n1\n8\n85\n-1.17\n12.17\n-14.24\n\n\n2\n5\n90\n-4.17\n17.17\n-71.60\n\n\n3\n11\n62\n1.83\n-10.83\n-19.82\n\n\n4\n14\n55\n4.83\n-17.83\n-86.12\n\n\n5\n7\n80\n-2.17\n7.17\n-15.56\n\n\n6\n10\n65\n0.83\n-7.83\n-6.50\n\n\nSum\n55\n437\n0.00\n0.00\n-213.84\n\n\n\n\n\n15.14.2.3 Step 3: Calculate Covariance\n \\text{Cov}(X,Y) = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{n-1} = \\frac{-213.84}{5} = -42.77 \n\n\n\n15.14.3 2. Pearson Correlation Coefficient\n\n15.14.3.1 Step 1: Calculate Squared Deviations\n\n\n\n\n\n\n\n\n\n\ni\n(x_i - \\bar{x})\n(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n-1.17\n12.17\n1.37\n148.11\n\n\n2\n-4.17\n17.17\n17.39\n294.81\n\n\n3\n1.83\n-10.83\n3.35\n117.29\n\n\n4\n4.83\n-17.83\n23.33\n317.91\n\n\n5\n-2.17\n7.17\n4.71\n51.41\n\n\n6\n0.83\n-7.83\n0.69\n61.31\n\n\nSum\n0.00\n0.00\n50.84\n990.84\n\n\n\n\n\n15.14.3.2 Step 2: Calculate Standard Deviations\n\n\n\n\n\n\n\n\n\nMeasure\nFormula\nCalculation\nResult\n\n\n\n\ns_x\n\\sqrt{\\frac{\\sum (x_i - \\bar{x})^2}{n-1}}\n\\sqrt{\\frac{50.84}{5}}\n3.19\n\n\ns_y\n\\sqrt{\\frac{\\sum (y_i - \\bar{y})^2}{n-1}}\n\\sqrt{\\frac{990.84}{5}}\n14.08\n\n\n\n\n\n15.14.3.3 Step 3: Calculate Pearson Correlation\n r = \\frac{\\text{Cov}(X,Y)}{s_x s_y} = \\frac{-42.77}{3.19 \\times 14.08} = -0.95 \n\n\n\n15.14.4 3. Spearman Rank Correlation\n\n15.14.4.1 Step 1: Assign Ranks\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n8\n85\n3\n2\n1\n1\n\n\n2\n5\n90\n1\n1\n0\n0\n\n\n3\n11\n62\n5\n5\n0\n0\n\n\n4\n14\n55\n6\n6\n0\n0\n\n\n5\n7\n80\n2\n3\n-1\n1\n\n\n6\n10\n65\n4\n4\n0\n0\n\n\nSum\n\n\n\n\n\n2\n\n\n\n\n\n15.14.4.2 Step 2: Calculate Spearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} = 1 - \\frac{6(2)}{6(36-1)} = 1 - \\frac{12}{210} = 0.94 \n\n\n\n15.14.5 Verification using R\n\n# Calculate correlations using R\ncor(data$anxiety_level, data$cognitive_performance, method = \"pearson\")\n\n[1] -0.9527979\n\ncor(data$anxiety_level, data$cognitive_performance, method = \"spearman\")\n\n[1] -0.9428571\n\n\n\n\n15.14.6 Interpretation\n\nThe strong negative Pearson correlation (r = -0.95) indicates a very strong negative linear relationship between anxiety level and cognitive performance.\nThe strong positive Spearman correlation (ρ = 0.94) shows that the relationship is also strongly monotonic.\nThe difference between Pearson and Spearman correlations suggests that while there is a strong relationship, it might not be perfectly linear.\n\n\n\n15.14.7 Exercise\n\nVerify each calculation step in the tables above.\nTry calculating these measures with a modified dataset:\n\nAdd one outlier and observe how it affects both correlation coefficients\nChange one pair of values and recalculate\n\n\n\n\n15.14.8 References\n\nUnderstanding Correlation:\n\nPearson, K. (1895). “Notes on regression and inheritance in the case of two parents.”\nSpearman, C. (1904). “The proof and measurement of association between two things.”",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.2-calculating-covariance-pearson-and-spearman-correlation---worked-example",
    "href": "correg_en.html#appendix-a.2-calculating-covariance-pearson-and-spearman-correlation---worked-example",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.15 Appendix A.2: Calculating Covariance, Pearson, and Spearman Correlation - worked example",
    "text": "15.15 Appendix A.2: Calculating Covariance, Pearson, and Spearman Correlation - worked example\nGiven data on Electoral District Magnitude (\\text{DM}) and Gallagher index:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18.2\n\n\n3\n16.7\n\n\n4\n15.8\n\n\n5\n15.3\n\n\n6\n15.0\n\n\n7\n14.8\n\n\n8\n14.7\n\n\n9\n14.6\n\n\n10\n14.55\n\n\n11\n14.52\n\n\n\n\n15.15.1 Step 1: Calculate Basic Statistics\nCalculation of means:\nFor \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nDetailed calculation:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6.5\nFor Gallagher index (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nDetailed calculation:\n18.2 + 16.7 + 15.8 + 15.3 + 15.0 + 14.8 + 14.7 + 14.6 + 14.55 + 14.52 = 154.17 \\bar{y} = \\frac{154.17}{10} = 15.417\n\n\n15.15.2 Step 2: Detailed Covariance Calculations\nComplete working table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n-4.5\n2.783\n-12.5235\n20.25\n7.7451\n\n\n2\n3\n16.7\n-3.5\n1.283\n-4.4905\n12.25\n1.6461\n\n\n3\n4\n15.8\n-2.5\n0.383\n-0.9575\n6.25\n0.1467\n\n\n4\n5\n15.3\n-1.5\n-0.117\n0.1755\n2.25\n0.0137\n\n\n5\n6\n15.0\n-0.5\n-0.417\n0.2085\n0.25\n0.1739\n\n\n6\n7\n14.8\n0.5\n-0.617\n-0.3085\n0.25\n0.3807\n\n\n7\n8\n14.7\n1.5\n-0.717\n-1.0755\n2.25\n0.5141\n\n\n8\n9\n14.6\n2.5\n-0.817\n-2.0425\n6.25\n0.6675\n\n\n9\n10\n14.55\n3.5\n-0.867\n-3.0345\n12.25\n0.7517\n\n\n10\n11\n14.52\n4.5\n-0.897\n-4.0365\n20.25\n0.8047\n\n\nSum\n65\n154.17\n0\n0\n-28.085\n82.5\n12.8442\n\n\n\nCovariance calculation: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28.085}{9} = -3.120556\n\n\n15.15.3 Step 3: Standard Deviation Calculations\nFor \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82.5}{9}} = \\sqrt{9.1667} = 3.026582\nFor Gallagher (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12.8442}{9}} = \\sqrt{1.4271} = 1.194612\n\n\n15.15.4 Step 4: Pearson Correlation Calculation\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3.120556}{3.026582 \\times 1.194612} = \\frac{-3.120556}{3.615752} = -0.863044\n\n\n15.15.5 Step 5: Spearman Rank Correlation Calculation\nComplete ranking table with all calculations:\n\n\n\ni\nX_i\nY_i\nRank X_i\nRank Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18.2\n1\n10\n-9\n81\n\n\n2\n3\n16.7\n2\n9\n-7\n49\n\n\n3\n4\n15.8\n3\n8\n-5\n25\n\n\n4\n5\n15.3\n4\n7\n-3\n9\n\n\n5\n6\n15.0\n5\n6\n-1\n1\n\n\n6\n7\n14.8\n6\n5\n1\n1\n\n\n7\n8\n14.7\n7\n4\n3\n9\n\n\n8\n9\n14.6\n8\n3\n5\n25\n\n\n9\n10\n14.55\n9\n2\n7\n49\n\n\n10\n11\n14.52\n10\n1\n9\n81\n\n\nSum\n\n\n\n\n\n330\n\n\n\nSpearman correlation calculation: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\n15.15.6 Step 6: R Verification\n\n# Create vectors\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Calculate covariance\ncov(DM, GH)\n\n[1] -3.120556\n\n# Calculate correlations\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\n15.15.7 Step 7: Basic Visualization\n\nlibrary(ggplot2)\n\n# Create data frame\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Create scatter plot\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"District Magnitude vs Gallagher Index\",\n    x = \"District Magnitude (DM)\",\n    y = \"Gallagher Index (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n15.15.8 OLS Estimation and Goodness-of-Fit Measures\n\n\n15.15.9 Step 1: Calculate OLS Estimates\nUsing previously calculated values: - \\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28.085 - \\sum(X_i - \\bar{X})^2 = 82.5 - \\bar{X} = 6.5 - \\bar{Y} = 15.417\nCalculate slope (\\hat{\\beta_1}): \\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 ÷ 82,5 = -0,3404\nCalculate intercept (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 × 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nTherefore, the OLS regression equation is: \\hat{Y} = 17.6296 - 0.3404X\n\n\n15.15.10 Step 2: Calculate Fitted Values and Residuals\nComplete table showing all calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18.2\n16.9488\n1.2512\n1.5655\n7.7451\n2.3404\n\n\n2\n3\n16.7\n16.6084\n0.0916\n0.0084\n1.6461\n1.4241\n\n\n3\n4\n15.8\n16.2680\n-0.4680\n0.2190\n0.1467\n0.7225\n\n\n4\n5\n15.3\n15.9276\n-0.6276\n0.3939\n0.0137\n0.2601\n\n\n5\n6\n15.0\n15.5872\n-0.5872\n0.3448\n0.1739\n0.0289\n\n\n6\n7\n14.8\n15.2468\n-0.4468\n0.1996\n0.3807\n0.0290\n\n\n7\n8\n14.7\n14.9064\n-0.2064\n0.0426\n0.5141\n0.2610\n\n\n8\n9\n14.6\n14.5660\n0.0340\n0.0012\n0.6675\n0.7241\n\n\n9\n10\n14.55\n14.2256\n0.3244\n0.1052\n0.7517\n1.4184\n\n\n10\n11\n14.52\n13.8852\n0.6348\n0.4030\n0.8047\n2.3439\n\n\nSum\n65\n154.17\n154.17\n0\n3.2832\n12.8442\n9.5524\n\n\n\nCalculations for fitted values:\nFor X = 2:\nŶ = 17.6296 + (-0.3404 × 2) = 16.9488\n\nFor X = 3:\nŶ = 17.6296 + (-0.3404 × 3) = 16.6084\n\n[... continue for all values]\n\n\n15.15.11 Step 3: Calculate Goodness-of-Fit Measures\nSum of Squared Errors (SSE): SSE = \\sum e_i^2\nSSE = 3.2832\nSum of Squared Total (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12.8442\nSum of Squared Regression (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9.5524\nVerify decomposition: SST = SSR + SSE\n12.8442 = 9.5524 + 3.2832 (within rounding error)\nR-squared calculation: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR² = 9.5524 ÷ 12.8442\n   = 0.7438\n\n\n15.15.12 Step 4: R Verification\n\n# Fit linear model\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# View summary statistics\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 4.67e-10 ***\nDM          -0.34042    0.07053  -4.827  0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Calculate R-squared manually\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\n15.15.13 Step 5: Residual Analysis\n\n# Create residual plots\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\n15.15.14 Step 6: Predicted vs Actual Values Plot\n\n# Create predicted vs actual plot\nggplot(data.frame(\n  Actual = GH,\n  Predicted = fitted(model)\n), aes(x = Predicted, y = Actual)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Predicted vs Actual Values\",\n    x = \"Predicted Gallagher Index\",\n    y = \"Actual Gallagher Index\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n15.15.15 Log-Transformed Models\n\n\n15.15.16 Step 1: Data Transformation\nFirst, calculate natural logarithms of variables:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18.2\n0.6931\n2.9014\n\n\n2\n3\n16.7\n1.0986\n2.8154\n\n\n3\n4\n15.8\n1.3863\n2.7600\n\n\n4\n5\n15.3\n1.6094\n2.7278\n\n\n5\n6\n15.0\n1.7918\n2.7081\n\n\n6\n7\n14.8\n1.9459\n2.6946\n\n\n7\n8\n14.7\n2.0794\n2.6878\n\n\n8\n9\n14.6\n2.1972\n2.6810\n\n\n9\n10\n14.55\n2.3026\n2.6777\n\n\n10\n11\n14.52\n2.3979\n2.6757\n\n\n\n\n\n15.15.17 Step 2: Compare Different Model Specifications\nWe estimate three alternative specifications:\n\nLog-linear model: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nLinear-log model: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nLog-log model: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Create transformed variables\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Fit models\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Compare R-squared values\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Linear\", \"Log-linear\", \"Linear-log\", \"Log-log\"),\n  R_squared = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Display comparison\nmodels_comparison\n\n       Model R_squared\n1     Linear 0.7443793\n2 Log-linear 0.7670346\n3 Linear-log 0.9141560\n4    Log-log 0.9288088\n\n\n\n\n15.15.18 Step 3: Visual Comparison\n\n# Create plots for each model\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear Model\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-linear Model\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Linear-log Model\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Log-log Model\") +\n  theme_minimal()\n\n# Arrange plots in a grid\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n15.15.19 Step 4: Residual Analysis for Best Model\nBased on R-squared values, analyze residuals for the best-fitting model:\n\n# Residual plots for best model\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\n15.15.20 Step 5: Interpretation of Best Model\nThe linear-log model coefficients:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 4.94e-11 ***\nlog_DM       -2.0599     0.2232   -9.23 1.54e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 1.539e-05\n\n\nInterpretation: - \\hat{\\beta_0} represents the expected Gallagher Index when ln(DM) = 0 (i.e., when DM = 1) - \\hat{\\beta_1} represents the change in Gallagher Index associated with a one-unit increase in ln(DM)\n\n\n15.15.21 Step 6: Model Predictions\n\n# Create prediction plot for best model\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Linear-log Model: Gallagher Index vs ln(District Magnitude)\",\n    x = \"ln(District Magnitude)\",\n    y = \"Gallagher Index\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n15.15.22 Step 7: Elasticity Analysis\nFor the log-log model, coefficients represent elasticities directly. Calculate average elasticity for the linear-log model:\n\n# Calculate elasticity at means\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelasticity &lt;- beta1 * (1/mean_GH)\nelasticity\n\n    log_DM \n-0.1336136 \n\n\nThis represents the percentage change in the Gallagher Index for a 1% change in District Magnitude.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "href": "correg_en.html#appendix-a.3-understanding-pearson-spearman-and-kendall",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.16 Appendix A.3: Understanding Pearson, Spearman, and Kendall",
    "text": "15.16 Appendix A.3: Understanding Pearson, Spearman, and Kendall\n\n15.16.1 Dataset\n\ndata &lt;- data.frame(\n  x = c(2, 4, 5, 3, 8),\n  y = c(3, 5, 4, 4, 7)\n)\n\n\n\n15.16.2 Pearson Correlation\n r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}} \n\n15.16.2.1 Step-by-Step Calculations:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n2\n3\n-2.4\n-1.6\n3.84\n5.76\n2.56\n\n\n2\n4\n5\n-0.4\n0.4\n-0.16\n0.16\n0.16\n\n\n3\n5\n4\n0.6\n-0.6\n-0.36\n0.36\n0.36\n\n\n4\n3\n4\n-1.4\n-0.6\n0.84\n1.96\n0.36\n\n\n5\n8\n7\n3.6\n2.4\n8.64\n12.96\n5.76\n\n\nSum\n22\n23\n0\n0\n12.8\n21.2\n9.2\n\n\n\n\\bar{x} = 4.4 \\bar{y} = 4.6\n r = \\frac{12.8}{\\sqrt{21.2 \\times 9.2}} = \\frac{12.8}{\\sqrt{195.04}} = \\frac{12.8}{13.97} = 0.92 \n\n\n\n15.16.3 Spearman Correlation\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} \n\n15.16.3.1 Step-by-Step Calculations:\n\n\n\ni\nx_i\ny_i\nRank x_i\nRank y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n3\n1\n1\n0\n0\n\n\n2\n4\n5\n3\n5\n-2\n4\n\n\n3\n5\n4\n4\n2.5\n1.5\n2.25\n\n\n4\n3\n4\n2\n2.5\n-0.5\n0.25\n\n\n5\n8\n7\n5\n4\n1\n1\n\n\nSum\n\n\n\n\n\n7.5\n\n\n\n \\rho = 1 - \\frac{6(7.5)}{5(25-1)} = 1 - \\frac{45}{120} = 0.82 \n\n\n\n15.16.4 Kendall’s Tau\n \\tau = \\frac{\\text{number of concordant pairs} - \\text{number of discordant pairs}}{\\frac{1}{2}n(n-1)} \n\n15.16.4.1 Step-by-Step Calculations:\n\n\n\nPair (i,j)\nx_i,x_j\ny_i,y_j\nx_j-x_i\ny_j-y_i\nResult\n\n\n\n\n(1,2)\n2,4\n3,5\n+2\n+2\nC\n\n\n(1,3)\n2,5\n3,4\n+3\n+1\nC\n\n\n(1,4)\n2,3\n3,4\n+1\n+1\nC\n\n\n(1,5)\n2,8\n3,7\n+6\n+4\nC\n\n\n(2,3)\n4,5\n5,4\n+1\n-1\nD\n\n\n(2,4)\n4,3\n5,4\n-1\n-1\nC\n\n\n(2,5)\n4,8\n5,7\n+4\n+2\nC\n\n\n(3,4)\n5,3\n4,4\n-2\n0\nD\n\n\n(3,5)\n5,8\n4,7\n+3\n+3\nC\n\n\n(4,5)\n3,8\n4,7\n+5\n+3\nC\n\n\n\nNumber of concordant pairs = 8 Number of discordant pairs = 2  \\tau = \\frac{8-2}{10} = 0.74 \n\n\n\n15.16.5 Verification in R\n\ncat(\"Pearson:\", round(cor(data$x, data$y, method=\"pearson\"), 2), \"\\n\")\n\nPearson: 0.92 \n\ncat(\"Spearman:\", round(cor(data$x, data$y, method=\"spearman\"), 2), \"\\n\")\n\nSpearman: 0.82 \n\ncat(\"Kendall:\", round(cor(data$x, data$y, method=\"kendall\"), 2), \"\\n\")\n\nKendall: 0.74 \n\n\n\n\n15.16.6 Interpretation of Results\n\nPearson Correlation (r = 0.92)\n\nStrong positive linear correlation\nIndicates a very strong linear relationship between variables\n\nSpearman Correlation (ρ = 0.82)\n\nAlso strong positive correlation\nSlightly lower than Pearson’s, suggesting some deviations from monotonicity\n\nKendall’s Tau (τ = 0.74)\n\nLowest of the three values, but still indicates strong association\nMore robust to outliers\n\n\n\n\n15.16.7 Comparison of Measures\n\nDifferences in Values:\n\nPearson (0.92) - highest value, strong linearity\nSpearman (0.82) - considers only ranking\nKendall (0.74) - most conservative measure\n\nPractical Application:\n\nAll measures confirm strong positive association\nDifferences between measures indicate slight deviations from perfect linearity\nKendall provides the most conservative estimate of relationship strength\n\n\n\n\n15.16.8 Exercises\n\nChange y[3] from 4 to 6 and recalculate all three correlations\nAdd an outlier (x=10, y=2) and recalculate correlations\nCompare which measure is most sensitive to changes in the data\n\n\n\n15.16.9 Key Points to Remember\n\nPearson Correlation:\n\nMeasures linear relationship\nMost sensitive to outliers\nRequires interval or ratio data\n\nSpearman Correlation:\n\nMeasures monotonic relationship\nLess sensitive to outliers\nWorks with ordinal data\n\nKendall’s Tau:\n\nMeasures ordinal association\nMost robust to outliers\nBest for small samples and tied ranks",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "href": "correg_en.html#appendix-b-bias-in-ols-estimation-with-endogenous-regressors",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.17 Appendix B: Bias in OLS Estimation with Endogenous Regressors",
    "text": "15.17 Appendix B: Bias in OLS Estimation with Endogenous Regressors\nIn this tutorial, we will explore the bias in Ordinary Least Squares (OLS) estimation when the error term is correlated with the explanatory variable, a situation known as endogeneity. We will first derive the bias mathematically and then illustrate it using a simulated dataset in R.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#theoretical-derivation",
    "href": "correg_en.html#theoretical-derivation",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.18 Theoretical Derivation",
    "text": "15.18 Theoretical Derivation\nConsider a data generating process (DGP) where the true relationship between x and y is:\n y = 2x + e \nHowever, there is an endogeneity problem because the error term e is correlated with x in the following way:\n e = 1x + u \nwhere u is an independent error term.\nIf we estimate the simple linear model y = \\hat{\\beta_0} + \\hat{\\beta_1}x + \\varepsilon using OLS, the OLS estimator of \\hat{\\beta_1} will be biased due to the endogeneity issue.\nTo understand the bias, let’s derive the expected value of the OLS estimator \\hat{\\beta}_1:\n\\begin{align*}\nE[\\hat{\\beta}_1] &= E[(X'X)^{-1}X'y] \\\\\n                 &= E[(X'X)^{-1}X'(2x + 1x + u)] \\\\\n                 &= E[(X'X)^{-1}X'(3x + u)] \\\\\n                 &= 3 + E[(X'X)^{-1}X'u]\n\\end{align*}\nIf the error term u is uncorrelated with x, then E[(X'X)^{-1}X'u] = 0, and the OLS estimator would be unbiased: E[\\hat{\\beta}_1] = 3. However, in this case, the original error term e is correlated with x, so u is also likely to be correlated with x.\nAssuming E[(X'X)^{-1}X'u] \\neq 0, the OLS estimator will be biased:\n\\begin{align*}\n\\text{Bias}(\\hat{\\beta}_1) &= E[\\hat{\\beta}_1] - \\beta_{1,\\text{true}} \\\\\n                           &= 3 + E[(X'X)^{-1}X'u] - 2 \\\\\n                           &= 1 + E[(X'X)^{-1}X'u]\n\\end{align*}\nThe direction and magnitude of the bias will depend on the correlation between x and u. If x and u are positively correlated, the bias will be positive, and the OLS estimator will overestimate the true coefficient. Conversely, if x and u are negatively correlated, the bias will be negative, and the OLS estimator will underestimate the true coefficient.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#simulation-in-r",
    "href": "correg_en.html#simulation-in-r",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.19 Simulation in R",
    "text": "15.19 Simulation in R\nLet’s create a simple dataset with 10 observations where x is in the interval 1:10, and generate y values based on the given DGP: y = 2x + e, where e = 1x + u, and u is a random error term.\n\nset.seed(123)  # for reproducibility\nx &lt;- 1:10\nu &lt;- rnorm(10, mean = 0, sd = 1)\ne &lt;- 1*x + u\n# e &lt;- 1*x\ny &lt;- 2*x + e\n\n# Generate the data frame\ndata &lt;- data.frame(x = x, y = y)\n\n# Estimate the OLS model\nmodel &lt;- lm(y ~ x, data = data)\n\n# Print the model summary\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1348 -0.5624 -0.1393  0.3854  1.6814 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.5255     0.6673   0.787    0.454    \nx             2.9180     0.1075  27.134 3.67e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9768 on 8 degrees of freedom\nMultiple R-squared:  0.9893,    Adjusted R-squared:  0.9879 \nF-statistic: 736.3 on 1 and 8 DF,  p-value: 3.666e-09\n\n\nIn this example, the true relationship is y = 2x + e, where e = 1x + u. However, when we estimate the OLS model, we get:\n \\hat{y} = 0.18376 + 3.05874x \nThe estimated coefficient for x is 3.05874, which is biased upward from the true value of 2. This bias is due to the correlation between the error term e and the explanatory variable x.\nTo visualize the bias using ggplot2, we can plot the true relationship (y = 2x) and the estimated OLS relationship:\n\nlibrary(ggplot2)\n\nggplot(data, aes(x = x, y = y)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 2, color = \"blue\", linewidth = 1, linetype = \"dashed\") +\n  geom_abline(intercept = coef(model)[1], slope = coef(model)[2], color = \"red\", linewidth = 1) +\n  labs(title = \"True vs. Estimated Relationship\", x = \"x\", y = \"y\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  scale_color_manual(name = \"Lines\", values = c(\"blue\", \"red\"), \n                     labels = c(\"True\", \"OLS\"))\n\n\n\n\n\n\n\n\nThe plot will show that the estimated OLS line (red) is steeper than the true relationship line (blue), illustrating the upward bias in the estimated coefficient.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#conclusion-2",
    "href": "correg_en.html#conclusion-2",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.20 Conclusion",
    "text": "15.20 Conclusion\nIn summary, when the error term is correlated with the explanatory variable (endogeneity), the OLS estimator will be biased. The direction and magnitude of the bias depend on the nature of the correlation between the error term and the explanatory variable. This tutorial demonstrated the bias both mathematically and through a simulated example in R, using ggplot2 for visualization.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#appendix-c.-worked-examples",
    "href": "correg_en.html#appendix-c.-worked-examples",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.21 Appendix C. Worked Examples",
    "text": "15.21 Appendix C. Worked Examples",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "href": "correg_en.html#descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.22 Descriptive Statistics and OLS Example - Income and Voter Turnout",
    "text": "15.22 Descriptive Statistics and OLS Example - Income and Voter Turnout\nBackground\nIn preparation for the 2024 municipal elections, the Amsterdam Electoral Commission conducted research on voter participation patterns across different city neighborhoods. A key question emerged:\nDoes economic prosperity of a neighborhood correlate with civic engagement, specifically voter turnout?\nData Collection\nSample: 5 representative neighborhoods in Amsterdam\nTime Period: Data from the 2022 municipal elections\nVariables:\n\nIncome: Average annual household income per capita (thousands €)\nTurnout: Percentage of registered voters who voted in the election\n\n\n15.22.1 Initial R Output for Reference\n\n# Data\nincome &lt;- c(50, 45, 56, 40, 60)  # thousands €\nturnout &lt;- c(60, 56, 70, 50, 75) # %\n\n# Full model check\nmodel &lt;- lm(turnout ~ income)\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n\n\n\n15.22.2 Dispersion Measures\nMeans:\n\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} = \\frac{50 + 45 + 56 + 40 + 60}{5} = \\frac{251}{5} = 50.2\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} = \\frac{60 + 56 + 70 + 50 + 75}{5} = \\frac{311}{5} = 62.2\n\n# Verification\nmean(income)  # 50.2\n\n[1] 50.2\n\nmean(turnout) # 62.2\n\n[1] 62.2\n\n\nVariances:\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X: (-0.2, -5.2, 5.8, -10.2, 9.8)\ns^2_X = \\frac{0.04 + 27.04 + 33.64 + 104.04 + 96.04}{4} = \\frac{260.8}{4} = 65.2\nDeviations for Y: (-2.2, -6.2, 7.8, -12.2, 12.8)\ns^2_Y = \\frac{4.84 + 38.44 + 60.84 + 148.84 + 163.84}{4} = \\frac{416.8}{4} = 104.2\n\n# Verification\nvar(income)  # 65.2\n\n[1] 65.2\n\nvar(turnout) # 104.2\n\n[1] 104.2\n\n\n\n\n15.22.3 Covariance and Correlation\nCovariance:\ns_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-0.2 \\times -2.2) = 0.44 (-5.2 \\times -6.2) = 32.24 (5.8 \\times 7.8) = 45.24 (-10.2 \\times -12.2) = 124.44 (9.8 \\times 12.8) = 125.44\ns_{XY} = \\frac{327.8}{4} = 81.95\n\n# Verification\ncov(income, turnout) # 81.95\n\n[1] 81.95\n\n\nCorrelation:\nr_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{81.95}{\\sqrt{65.2}\\sqrt{104.2}} = 0.994\n\n# Verification\ncor(income, turnout) # 0.994\n\n[1] 0.9942402\n\n\n\n\n15.22.4 OLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{81.95}{65.2} = 1.2571429\nIntercept:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\nStep by step:\n\n1.2571429 \\times 50.2 = 63.1085714\n\\hat{\\beta_0} = 62.2 - 63.1085714 = -0.9085714\n\n\n# Verification\ncoef(model)  # Exact coefficients from R\n\n(Intercept)      income \n -0.8964724   1.2569018 \n\n\n\n\n15.22.5 Detailed Decomposition of Variance and R-squared\nStep 1: Calculate predicted values (\\hat{Y}):\n \\hat{Y} = -0.9085714 + 1.2571429X\nThe predicted values \\hat{Y} for each X value:\nFor X = 50:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (50)\n \\hat{Y} = -0.9085714 + 62.857145 \\hat{Y} = 61.9485736\nFor X = 45:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (45)\n \\hat{Y} = -0.9085714 + 56.5714305 \\hat{Y} = 55.6535591\nFor X = 56:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (56)\n \\hat{Y} = -0.9085714 + 70.4200024 \\hat{Y} = 69.5114310\nFor X = 40:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (40)\n \\hat{Y} = -0.9085714 + 50.2657160 \\hat{Y} = 49.3571446\nFor X = 60:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (60)\n \\hat{Y} = -0.9085714 + 75.4285740 \\hat{Y} = 74.5200026\n\n# Verification of predicted values\ny_hat &lt;- -0.9085714 + 1.2571429 * income\ndata.frame(\n  X = income,\n  Y = turnout,\n  Y_hat = y_hat,\n  row.names = 1:5\n)\n\n   X  Y    Y_hat\n1 50 60 61.94857\n2 45 56 55.66286\n3 56 70 69.49143\n4 40 50 49.37714\n5 60 75 74.52000\n\n\nStep 2: Calculate SST (Total Sum of Squares)\nSST = \\sum(Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = 62.2\n(60 - 62.2)^2 = (-2.2)^2 = 4.84 (56 - 62.2)^2 = (-6.2)^2 = 38.44 (70 - 62.2)^2 = (7.8)^2 = 60.84 (50 - 62.2)^2 = (-12.2)^2 = 148.84 (75 - 62.2)^2 = (12.8)^2 = 163.84\nSST = 4.84 + 38.44 + 60.84 + 148.84 + 163.84 = 416.8\nStep 3: Calculate SSR (Regression Sum of Squares)\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\n(61.9485736 - 62.2)^2 = (-0.2514264)^2 = 0.0632151 (55.6535591 - 62.2)^2 = (-6.5464409)^2 = 42.8558689 (69.5114310 - 62.2)^2 = (7.3114310)^2 = 53.4570178 (49.3571446 - 62.2)^2 = (-12.8428554)^2 = 164.9389370 (74.5200026 - 62.2)^2 = (12.3200026)^2 = 151.7824640\nSSR = 413.0975028\nStep 4: Calculate SSE (Error Sum of Squares)\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n(60 - 61.9485736)^2 = (-1.9485736)^2 = 3.7969384 (56 - 55.6535591)^2 = (0.3464409)^2 = 0.1200212 (70 - 69.5114310)^2 = (0.4885690)^2 = 0.2387198 (50 - 49.3571446)^2 = (0.6428554)^2 = 0.4132631 (75 - 74.5200026)^2 = (0.4799974)^2 = 0.2303975\nSSE = 4.7024972\nStep 5: Verify decomposition\nSST = SSR + SSE 416.8 = 413.0975028 + 4.7024972\nStep 6: Calculate R-squared\nR^2 = \\frac{SSR}{SST} = \\frac{413.0975028}{416.8} = 0.9916\n\n# Verification\nsummary(model)$r.squared  # Should match our calculation\n\n[1] 0.9885135\n\n\n\n\n15.22.6 Visualization\n\nlibrary(ggplot2)\ndf &lt;- data.frame(income = income, turnout = turnout)\n\nggplot(df, aes(x = income, y = turnout)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Voter Turnout vs Income per Capita\",\n    x = \"Income per Capita (thousands €)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\n15.22.7 Interpretation\nThe analysis shows:\n\nA very strong positive correlation (r = 0.994) between income and voter turnout\nThe regression equation \\hat{Y} = -0.9085714 + 1.2571429X indicates that:\n\nFor each €1,000 increase in income, turnout increases by about 1.26 percentage points\nThe intercept (-0.9086) has little practical meaning as income is never zero\n\nThe R-squared of 0.9916 indicates that 99.16% of the variance in turnout is explained by income",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "href": "correg_en.html#anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.23 Anxiety Levels and Cognitive Performance: A Laboratory Study",
    "text": "15.23 Anxiety Levels and Cognitive Performance: A Laboratory Study\n\n15.23.1 Data and Context\nIn a psychology experiment, researchers measured the relationship between anxiety levels (measured by galvanic skin response, GSR) and cognitive performance (score on a working memory task).\n\n# Data\nanxiety &lt;- c(2.1, 3.4, 4.2, 5.1, 5.8, 6.4, 7.2, 8.0)  # GSR readings\nperformance &lt;- c(92, 88, 84, 78, 74, 70, 65, 62)      # Working memory scores\n\n# Initial model check\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8993 -0.6660  0.2162  0.6106  1.5262 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 105.3248     1.3189   79.86 2.60e-10 ***\nanxiety      -5.4407     0.2359  -23.06 4.35e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 6 degrees of freedom\nMultiple R-squared:  0.9888,    Adjusted R-squared:  0.987 \nF-statistic: 531.9 on 1 and 6 DF,  p-value: 4.355e-07\n\n\n\n\n15.23.2 Descriptive Statistics\nMeans: \\bar{X} = \\frac{2.1 + 3.4 + 4.2 + 5.1 + 5.8 + 6.4 + 7.2 + 8.0}{8} = \\frac{42.2}{8} = 5.275\n\\bar{Y} = \\frac{92 + 88 + 84 + 78 + 74 + 70 + 65 + 62}{8} = \\frac{613}{8} = 76.625\n\n# Verification\nmean(anxiety)\n\n[1] 5.275\n\nmean(performance)\n\n[1] 76.625\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(2.1 - 5.275) = -3.175\n(3.4 - 5.275) = -1.875\n(4.2 - 5.275) = -1.075\n(5.1 - 5.275) = -0.175\n(5.8 - 5.275) = 0.525\n(6.4 - 5.275) = 1.125\n(7.2 - 5.275) = 1.925\n(8.0 - 5.275) = 2.725\n\nSquared deviations:\n10.08063 + 3.51563 + 1.15563 + 0.03063 + 0.27563 + 1.26563 + 3.70563 +\n7.42563 = 27.45500\ns^2_X = \\frac{27.45500}{7} = 3.922143\nSimilarly for Y: Deviations:\n15.375, 11.375, 7.375, 1.375, -2.625, -6.625, -11.625, -14.625\ns^2_Y = \\frac{236.875 + 129.391 + 54.391 + 1.891 + 6.891 + 43.891 + 135.141 + 213.891}{7} = \\frac{822.362}{7} = 117.4803\n\n# Verification\nvar(anxiety)\n\n[1] 3.922143\n\nvar(performance)\n\n[1] 117.4107\n\n\n\n\n15.23.3 Covariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-3.175 × 15.375) = -48.815625\n(-1.875 × 11.375) = -21.328125\n(-1.075 × 7.375) = -7.928125\n(-0.175 × 1.375) = -0.240625\n(0.525 × -2.625) = -1.378125\n(1.125 × -6.625) = -7.453125\n(1.925 × -11.625) = -22.378125\n(2.725 × -14.625) = -39.853125\nSum = -149.375\ns_{XY} = \\frac{-149.375}{7} = -21.33929\n\n# Verification\ncov(anxiety, performance)\n\n[1] -21.33929\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-21.33929}{\\sqrt{3.922143}\\sqrt{117.4803}} = -0.9932\n\n# Verification\ncor(anxiety, performance)\n\n[1] -0.9944073\n\n\n\n\n15.23.4 OLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-21.33929}{3.922143} = -5.4407\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-5.4407 × 5.275 = -28.6997\n\\hat{\\beta_0} = 76.625 - (-28.6997) = 105.3247\n\n\n# Verification\ncoef(model)\n\n(Intercept)     anxiety \n 105.324804   -5.440721 \n\n\n\n\n15.23.5 4. R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}): \\hat{Y} = 105.3247 - 5.4407X\n\n# Predicted values\ny_hat &lt;- 105.3247 - 5.4407 * anxiety\ndata.frame(\n  Anxiety = anxiety,\n  Performance = performance,\n  Predicted = y_hat,\n  row.names = 1:8\n)\n\n  Anxiety Performance Predicted\n1     2.1          92  93.89923\n2     3.4          88  86.82632\n3     4.2          84  82.47376\n4     5.1          78  77.57713\n5     5.8          74  73.76864\n6     6.4          70  70.50422\n7     7.2          65  66.15166\n8     8.0          62  61.79910\n\n\nStep 2: Sum of Squares\nSST = \\sum(Y_i - \\bar{Y})^2 = 822.362\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 816.3094\nSSE = \\sum(Y_i - \\hat{Y}_i)^2 = 6.0526\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{816.3094}{822.362} = 0.9926\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.9888459\n\n\n\n\n15.23.6 Visualization\n\nlibrary(ggplot2)\n\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Cognitive Performance vs. Anxiety Levels\",\n    x = \"Anxiety (GSR)\",\n    y = \"Performance Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n15.23.7 Interpretation\n\nStrong negative correlation (r = -0.993) between anxiety and cognitive performance\nFor each unit increase in GSR (anxiety), performance decreases by approximately 5.44 points\nThe model explains 99.26% of the variance in performance scores\nThe relationship appears to be strongly linear, suggesting a reliable anxiety-performance relationship\nThe high intercept (105.32) represents the theoretical maximum performance at zero anxiety\n\n\n\n15.23.8 Study Limitations\n\nSmall sample size (n=8)\nPossible other confounding variables\nLimited range of anxiety levels\nCross-sectional rather than longitudinal data",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_en.html#district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "href": "correg_en.html#district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "title": "15  Introduction to Correlation and Regression Analysis",
    "section": "15.24 District Magnitude and Electoral Disproportionality: A Comparative Analysis",
    "text": "15.24 District Magnitude and Electoral Disproportionality: A Comparative Analysis\n\n15.24.1 Data Generating Process\nLet’s set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\n15.24.2 Descriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\n15.24.3 Covariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 × 3.2833) = -18.6057\n(-3.6667 × 2.0833) = -7.6387\n(-1.6667 × 3.4833) = -5.8056\n(1.3333 × -1.6167) = -2.1556\n(3.3333 × -3.2167) = -10.7223\n(6.3333 × -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\n15.24.4 OLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 × 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\n15.24.5 R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\n15.24.6 Visualization - True vs. Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + ε\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + ε\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\n15.24.7 Observations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (ε)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\n15.24.8 Interpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\n15.24.9 Study Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\n15.24.10 Limitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introduction to Correlation and Regression Analysis</span>"
    ]
  },
  {
    "objectID": "correg_pl.html",
    "href": "correg_pl.html",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "",
    "text": "16.1 Statystyki Dwuwymiarowe\nStatystyki dwuwymiarowe opisują związek między dwiema zmiennymi. Omówimy kilka miar, zaczynając od kowariancji.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#statystyki-dwuwymiarowe",
    "href": "correg_pl.html#statystyki-dwuwymiarowe",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "",
    "text": "Analiza Zależności Zmiennych w Badaniach Społecznych\n\n\n\nW tej sekcji przeanalizujemy, w jaki sposób różne zmienne społeczne wchodzą ze sobą w interakcje i korelują. Zbadamy cztery charakterystyczne typy zależności często obserwowane w naukach społecznych, wykorzystując empiryczne przykłady do zilustrowania kluczowych wzorców i ich znaczenia dla analizy danych.\n\n\n\n\n\n\n\n\n\nAnaliza Zależności między Zmiennymi:\n\nDodatnia Korelacja Liniowa (Dostęp do Opieki Zdrowotnej a Długość Życia)\n\nDane wykazują dodatnią zależność liniową między dostępem do opieki zdrowotnej a oczekiwaną długością życia. Analiza statystyczna wskazuje, że wzrost dostępności opieki zdrowotnej o 10 punktów procentowych koreluje z wydłużeniem oczekiwanej długości życia o około 2,5 roku. Zależność ta utrzymuje istotność statystyczną w całym obserwowanym zakresie.\n\nUjemna Korelacja Liniowa (Użytkowanie Urządzeń Cyfrowych a Jakość Snu)\n\nAnaliza ujawnia odwrotną zależność między czasem spędzonym przed ekranami urządzeń cyfrowych a wskaźnikami jakości snu. Dane wskazują, że każda dodatkowa godzina użytkowania urządzeń koreluje z mierzalnym spadkiem wskaźników jakości snu, wykazując spójną negatywną zależność liniową.\n\nBrak Korelacji (Infrastruktura a Wskaźniki Ekonomiczne (np. GDP per capita in PLN))\n\nZależność między gęstością infrastruktury a wskaźnikami ekonomicznymi (np. GDP per capita in PLN) nie wykazuje statystycznie istotnej korelacji. Ten brak korelacji sugeruje, że zmienne te funkcjonują niezależnie w ramach obserwowanych parametrów, wskazując na obecność innych czynników determinujących, nieuwzględnionych w tej analizie.\n\nZależność Nieliniowa (Wielkość Zespołu a Produktywność)\n\nZależność między wielkością zespołu a produktywnością wykazuje charakterystykę krzywoliniową. Dane wskazują na istnienie optymalnego przedziału liczebności zespołu, przy czym produktywność maleje zarówno poniżej, jak i powyżej tego przedziału. Pokazuje to, jak ważne jest uwzględnianie wzorców nieliniowych w badaniach organizacyjnych.\nAspekty Metodologiczne:\n\nIstotność Statystyczna: Obserwowane zależności wymagają weryfikacji pod kątem istotności statystycznej przed wyciągnięciem wniosków.\nNiezależność Zmiennych: Założenie o niezależności zmiennych wymaga weryfikacji poprzez odpowiednie testy statystyczne.\nZmienne Zakłócające: Analizy muszą uwzględniać potencjalne zmienne zakłócające, które mogą wpływać na obserwowane zależności.\nPrzyczynowość: Wzorce korelacji nie implikują koniecznie związków przyczynowych; ustalenie przyczynowości wymaga zastosowania dodatkowych metod badawczych.\n\nZastosowania Badawcze:\nZrozumienie tych wzorców zależności ma istotne znaczenie dla:\n\nProjektowania i metodologii badań\nProcedur analizy statystycznej\nRozwoju i testowania teorii\n\nKrytyczna ocena tych zależności umożliwia bardziej rzetelne projektowanie badań i formułowanie wiarygodnych wniosków w naukach społecznych.\n\n\n\n\n\n\n\n\nRozróżnienie między Korelacją a Przyczynowością [Zob. np. https://www.tylervigen.com/spurious-correlations]\n\n\n\n\n\n\nhttps://x.com/EUFIC/status/1324667630238814209?prefetchTimestamp=1732463940216\n\n\n\n\n\nhttps://sitn.hms.harvard.edu/flash/2021/when-correlation-does-not-imply-causation-why-your-gut-microbes-may-not-yet-be-a-silver-bullet-to-all-your-problems/\n\n\nZależności statystyczne między zmiennymi stanowią jeden z najczęściej błędnie interpretowanych aspektów analizy danych. Mimo że korelacje mogą ujawniać wzorce w danych, wymagają starannej interpretacji, aby uniknąć wyciągania nieprawidłowych wniosków przyczynowych. Przeanalizujmy to zagadnienie na przykładach.\n\n16.1.1 Wzorce Sezonowe i Pozorne Korelacje: Studium Przypadku\n\n\n\n\n\n\n\n\n\nTa wizualizacja przedstawia klasyczny przykład zmiennej zakłócającej w analizie statystycznej. Pozorna korelacja między sprzedażą lodów a wskaźnikiem przestępczości (r = 0,85, p &lt; 0,001) pokazuje, jak wahania sezonowe mogą tworzyć mylące zależności statystyczne. Korelacja wynika ze wspólnego czynnika przyczynowego: sezonowych zmian temperatury, które niezależnie wpływają na obie zmienne poprzez odrębne mechanizmy.\n\n\n16.1.2 Trendy Czasowe i Pozorne Związki\n\n\n\n\n\n\n\n\n\nTa druga analiza ilustruje błąd korelacji czasowej, gdzie dwa niezależnie malejące trendy tworzą sztuczny związek statystyczny. Pomimo silnego współczynnika korelacji (r = 0,91, p &lt; 0,001), nie istnieje żaden wiarygodny mechanizm przyczynowy łączący te zmienne.\n\n\n16.1.3 Mechanizmy Powstawania Pozornych Korelacji\nAnaliza statystyczna może być zakłócona przez kilka systematycznych błędów, które tworzą pozorne, ale pozbawione znaczenia korelacje. Oto główne mechanizmy:\n1. Zmienne Zakłócające\nZmienna zakłócająca tworzy pozorny związek między niezależnymi zmiennymi poprzez niezależny wpływ na każdą z nich. To zjawisko statystyczne wymaga starannego planowania eksperymentów i analizy wielowymiarowej w celu wykrycia i kontroli potencjalnych czynników zakłócających.\n2. Autokorelacja Czasowa\nGdy zmienne wykazują silne trendy czasowe, mogą wykazywać korelację po prostu dlatego, że zmieniają się w czasie, a nie z powodu jakiejkolwiek znaczącej relacji. Efekt ten można kontrolować poprzez metody takie jak usuwanie trendu lub różnicowanie szeregów czasowych.\n3. Błąd Jednoczesnej Przyczynowości\nWystępuje, gdy kierunek przyczynowości jest niejednoznaczny lub dwukierunkowy. Na przykład, wzrost gospodarczy i stopy inwestycji mogą wykazywać jednoczesną przyczynowość, ponieważ każda zmienna potencjalnie wpływa na drugą poprzez złożone mechanizmy sprzężenia zwrotnego.\n\n\n16.1.4 Metody Statystyczne Wnioskowania Przyczynowego\nWspółczesne podejścia statystyczne oferują kilka technik pozwalających wyjść poza prostą korelację w kierunku wnioskowania przyczynowego, np.:\n1. Planowanie Eksperymentów\nRandomizowane badania kontrolowane stanowią złoty standard wnioskowania przyczynowego, pozwalając badaczom na izolację wpływu pojedynczych zmiennych przy jednoczesnej kontroli czynników zakłócających.\n2. Zmienne Instrumentalne\nTa technika statystyczna wykorzystuje zmienną, która wpływa na wynik wyłącznie poprzez jej wpływ na zmienną będącą przedmiotem zainteresowania, pomagając ustalić związki przyczynowe w danych obserwacyjnych.\n3. Regresja Nieciągła\nTen quasi-eksperymentalny projekt wykorzystuje naturalnie występujące progi do przybliżenia eksperymentów randomizowanych, dostarczając dowodów na istnienie związków przyczynowych.\n\n\n16.1.5 Ramy Krytycznej Analizy Korelacji\nPrzy ocenie wyników korelacyjnych należy wziąć pod uwagę następujące ramy analityczne:\n\nWiarygodność Teoretyczna: Zbadanie, czy istnieje logiczny mechanizm, poprzez który jedna zmienna mogłaby wpływać na drugą.\nPierwszeństwo Czasowe: Weryfikacja, czy proponowana przyczyna poprzedza skutek w czasie.\nZależność Dawka-Odpowiedź: Ocena, czy zmiany w wielkości proponowanej przyczyny odpowiadają proporcjonalnym zmianom w skutku.\nSpójność: Ocena, czy zależność utrzymuje się w różnych kontekstach i populacjach.\nAlternatywne Wyjaśnienia: Systematyczne rozważanie i testowanie alternatywnych wyjaśnień zaobserwowanej korelacji.\n\nPamiętaj: Korelacja statystyczna stanowi warunek konieczny, ale niewystarczający do ustalenia przyczynowości.\n\n\n\n\n16.1.6 Kowariancja\nKowariancja mierzy, jak dwie zmienne są powiązane (zmieniają się razem).\nWzór: cov(X,Y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n\n\n\n\n\n\nOd Kowariancji do Różnych Miar Korelacji\n\n\n\n\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n# Generowanie różnych typów zależności\nset.seed(123)\nn &lt;- 100\n\n# Zależność liniowa\nx1 &lt;- rnorm(n)\ny1 &lt;- 0.8*x1 + rnorm(n, sd=0.5)\ndata1 &lt;- data.frame(x=x1, y=y1, type=\"Zależność Liniowa\")\n\n# Zależność monotoniczna nieliniowa\nx2 &lt;- rnorm(n)\ny2 &lt;- sign(x2)*(x2^2) + rnorm(n, sd=0.5)\ndata2 &lt;- data.frame(x=x2, y=y2, type=\"Monotoniczna Nieliniowa\")\n\n# Zależność niemonotoniczna\nx3 &lt;- seq(-3, 3, length.out=n)\ny3 &lt;- x3^2 + rnorm(n, sd=0.5)\ndata3 &lt;- data.frame(x=x3, y=y3, type=\"Niemonotoniczna\")\n\n# Łączenie danych\nall_data &lt;- rbind(data1, data2, data3)\n\n# Tworzenie wykresu\nggplot(all_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6) +\n  facet_wrap(~type, scales = \"free\") +\n  labs(title = \"Różne Typy Zależności Między Zmiennymi\",\n       x = \"Zmienna X\",\n       y = \"Zmienna Y\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    strip.text = element_text(size = 12),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n16.1.7 Pojęcie Korelacji\nKorelacja to szerokie pojęcie opisujące, jak zmienne są ze sobą powiązane. Jak widać na wykresach, zależności te mogą przybierać różne formy.\n\n\n16.1.8 Rozpoczynając od Kowariancji\nKowariancja jest podstawową miarą wspólnej zmienności zmiennych:\nCov(X,Y) = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{n-1}\nInformuje nas o tym:\n\nCzy zmienne zmieniają się w tym samym kierunku (kowariancja dodatnia)\nCzy zmieniają się w przeciwnych kierunkach (kowariancja ujemna)\nCzy brak wyraźnego wzorca liniowego (kowariancja bliska zeru)\n\nJednak kowariancja ma ograniczenie: jej wartość zależy od jednostek pomiaru. Na przykład:\n\nWzrost w metrach vs waga w kg daje jedną wartość kowariancji\nWzrost w centymetrach vs waga w kg daje inną wartość\nTa sama zależność, różne skale!\n\n\n\n16.1.9 Standaryzacja i Miary Korelacji\n\nWspółczynnik korelacji Pearsona standaryzuje kowariancję: r = \\frac{Cov(X,Y)}{\\sqrt{Var(X)Var(Y)}}\n\n\nEliminuje zależność od jednostek\nZawsze między -1 a 1\nMierzy zależności liniowe\n\n\nWspółczynnik korelacji rangowej Spearmana:\n\n\nBazuje na rangach zamiast surowych wartości\nWychwytuje zależności monotoniczne (także nieliniowe)\nRównież przyjmuje wartości od -1 do 1\n\n\n\n16.1.10 Kluczowe Wnioski\n\nKowariancja pokazuje wspólną zmienność\nMiary korelacji dają standaryzowane wartości\nWybór miary korelacji zależy od:\n\nSpodziewanego typu zależności\nCharakteru danych\nPytania badawczego\n\nZawsze wizualizuj dane\n\n\n\n\n\n\n\n\n\n\nRangi: Pozycje w Uporządkowanym Ciągu\n\n\n\nRangi to po prostu numery pozycji w uporządkowanym zbiorze danych:\n\n16.1.11 Jak Wyznaczyć Rangi?\n\nPorządkujemy dane od najmniejszej do największej wartości\nPrzypisujemy kolejne liczby naturalne:\n\nNajmniejsza wartość → ranga 1\nKolejne wartości → kolejne rangi\nNajwiększa wartość → ranga n (liczba obserwacji)\nDla remisów → średnia rang\n\n\n\n\n16.1.12 Przykład\nMamy 5 studentów o wzroście:\nWzrost:   165, 182, 170, 168, 175\nRangi:     1,   5,   3,   2,   4\nDla danych z remisami (np. oceny):\nOceny:     2,   3,   3,   4,   5\nRangi:     1,  2.5, 2.5,  4,   5\n\n\n\nPrzykład Ręcznego Obliczenia:\nObliczmy kowariancję dla dwóch zmiennych:\n\nx: 1, 2, 3, 4, 5\ny: 2, 4, 5, 4, 5\n\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz średnie\n\\bar{x} = 3, \\bar{y} = 4\n\n\n2\nOblicz (x_i - \\bar{x})(y_i - \\bar{y}) dla każdej pary\n(-2)(-2) = 4\n\n\n\n\n(-1)(0) = 0\n\n\n\n\n(0)(1) = 0\n\n\n\n\n(1)(0) = 0\n\n\n\n\n(2)(1) = 2\n\n\n3\nZsumuj wyniki\n4 + 0 + 0 + 0 + 2 = 6\n\n\n4\nPodziel przez (n-1)\n6 / 4 = 1,5\n\n\n\nObliczenie w R:\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(2, 4, 5, 4, 5)\ncov(x, y)\n\n[1] 1.5\n\n\nInterpretacja: - Dodatnia kowariancja (1,5) wskazuje, że x i y mają tendencję do wzrostu razem.\nZalety:\n\nDostarcza informacji o kierunku związku (dodatni lub ujemny)\nPrzydatna w obliczaniu innych miar, takich jak korelacja\n\nWady:\n\nZależna od skali, co utrudnia porównywanie między różnymi parami zmiennych\nNie dostarcza informacji o sile związku\n\n\n\n16.1.13 Korelacja Pearsona\nKorelacja Pearsona mierzy siłę i kierunek liniowego związku między dwiema zmiennymi ciągłymi.\nWzór: r = \\frac{cov(X,Y)}{s_X s_Y} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}}\nPrzykład Ręcznego Obliczenia:\nUżywając tych samych danych co wyżej:\n\n\n\n\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nOblicz kowariancję\n(Z poprzedniego obliczenia) 1,5\n\n\n2\nOblicz odchylenia standardowe\ns_X = \\sqrt{\\frac{10}{4}} = 1,58, s_Y = \\sqrt{\\frac{6}{4}} = 1,22\n\n\n3\nPodziel kowariancję przez iloczyn odchyleń standardowych\n1,5 / (1,58 * 1,22) = 0,7746\n\n\n\nObliczenie w R:\n\ncor(x, y, method = \"pearson\")\n\n[1] 0.7745967\n\n\nInterpretacja: - Współczynnik korelacji 0,7746 wskazuje na silny dodatni związek liniowy między x i y.\nZalety:\n\nNiezależna od skali, zawsze między -1 a 1\nSzeroko rozumiana i stosowana\nTestuje związki liniowe\n\nWady:\n\nWrażliwa na wartości odstające\nMierzy tylko związki liniowe\nZakłada normalnie rozłożone zmienne\n\n\n\n16.1.14 Korelacja Spearmana\nKorelacja Spearmana mierzy siłę i kierunek monotonicznego związku między dwiema zmiennymi, które mogą być ciągłe lub porządkowe.\nWzór: \\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}, gdzie d_i to różnica między rangami.\nPrzykład Ręcznego Obliczenia:\nUżyjmy nieco innych danych:\n\nx: 1, 2, 3, 4, 5\ny: 1, 3, 2, 5, 4\n\n\n\n\nKrok\nOpis\nObliczenie\n\n\n\n\n1\nPrzypisz rangi obu zmiennym\nx_ranga: 1, 2, 3, 4, 5\n\n\n\n\ny_ranga: 1, 3, 2, 5, 4\n\n\n2\nOblicz różnice w rangach (d)\n0, -1, 1, -1, 1\n\n\n3\nPodnieś różnice do kwadratu\n0, 1, 1, 1, 1\n\n\n4\nZsumuj kwadraty różnic\n\\sum d_i^2 = 4\n\n\n5\nZastosuj wzór\n\\rho = 1 - \\frac{6(4)}{5(5^2 - 1)} = 0,8\n\n\n\nObliczenie w R:\n\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(1, 3, 2, 5, 4)\ncor(x, y, method = \"spearman\")\n\n[1] 0.8\n\n\nInterpretacja: - Korelacja Spearmana 0,8 wskazuje na silny dodatni związek monotoniczny między x i y.\nZalety:\n\nOdporna na wartości odstające\nMoże wykrywać nieliniowe związki monotoniczne\nOdpowiednia dla danych porządkowych\n\nWady:\n\nMniej odporna niż korelacja Pearsona do wykrywania związków liniowych w normalnie rozłożonych danych\nNie dostarcza informacji o kształcie związku poza monotonicznością\n\n\n\n16.1.15 Tabela Krzyżowa\nTabela krzyżowa (tabela kontyngencji) pokazuje związek między dwiema zmiennymi kategorycznymi.\nPrzykład:\nStwórzmy tabelę krzyżową dla dwóch zmiennych: - Poziom wykształcenia: Średnie, Wyższe, Podyplomowe - Status zatrudnienia: Zatrudniony, Bezrobotny\n\nwyksztalcenie &lt;- factor(c(\"Średnie\", \"Wyższe\", \"Podyplomowe\", \"Średnie\", \"Wyższe\", \"Podyplomowe\", \"Średnie\", \"Wyższe\", \"Podyplomowe\"))\nzatrudnienie &lt;- factor(c(\"Zatrudniony\", \"Zatrudniony\", \"Zatrudniony\", \"Bezrobotny\", \"Zatrudniony\", \"Zatrudniony\", \"Bezrobotny\", \"Bezrobotny\", \"Zatrudniony\"))\n\ntable(wyksztalcenie, zatrudnienie)\n\n             zatrudnienie\nwyksztalcenie Bezrobotny Zatrudniony\n  Podyplomowe          0           3\n  Średnie              2           1\n  Wyższe               1           2\n\n\nInterpretacja:\n\nTa tabela pokazuje liczbę osób w każdej kombinacji poziomu wykształcenia i statusu zatrudnienia.\nNa przykład, możemy zobaczyć, ilu absolwentów szkół średnich jest zatrudnionych, a ilu bezrobotnych.\n\nZalety:\n\nZapewnia jasną wizualną reprezentację związku między zmiennymi kategorycznymi\nŁatwa do zrozumienia i interpretacji\nPodstawa dla wielu testów statystycznych (np. test chi-kwadrat niezależności)\n\nWady:\n\nOgraniczona do danych kategorycznych\nMoże stać się nieporęczna przy wielu kategoriach\nNie dostarcza pojedynczej statystyki podsumowującej siłę związku\n\n\n\n16.1.16 Wybór Odpowiedniej Miary\nPrzy wyborze statystyki dwuwymiarowej należy wziąć pod uwagę:\n\nTyp danych:\n\nDane ciągłe: Kowariancja, korelacja Pearsona\nDane porządkowe: Korelacja Spearmana\nDane kategoryczne: Tabela krzyżowa\n\nTyp związku:\n\nLiniowy: Korelacja Pearsona\nMonotoniczny, ale potencjalnie nieliniowy: Korelacja Spearmana\n\nObecność wartości odstających:\n\nJeśli wartości odstające są problemem, korelacja Spearmana jest bardziej odporna\n\nRozkład:\n\nDla normalnie rozłożonych danych korelacja Pearsona jest najbardziej odporna (robust)\nDla rozkładów “nienormalnych” rozważ korelację Spearmana\n\nWielkość próby:\n\nDla bardzo małych prób metody nieparametryczne, takie jak korelacja Spearmana, mogą być preferowane\n\n\nPamiętaj, że często wartościowe jest użycie wielu miar i wizualizacji (takich jak wykresy rozrzutu), aby uzyskać kompleksowe zrozumienie związku między zmiennymi.\n\n\n16.1.17 Korelacja Pearsona, Spearmana i Kendalla – porównanie\nMiary korelacji określają siłę i kierunek zależności między zmiennymi. Omówimy trzy kluczowe współczynniki korelacji:\n\nWspółczynnik korelacji liniowej Pearsona (r)\nWspółczynnik korelacji rangowej Spearmana (ρ)\nWspółczynnik korelacji τ Kendalla (τ)\n\nPodstawy Matematyczne\n\n16.1.17.1 Współczynnik Korelacji Pearsona\nWspółczynnik korelacji Pearsona mierzy zależność liniową między dwiema zmiennymi ciągłymi.\n r = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\sum_{i=1}^{n} (y_i - \\bar{y})^2}} \ngdzie:\n\nx_i, y_i to poszczególne obserwacje\n\\bar{x}, \\bar{y} to średnie arytmetyczne\nn to liczebność próby\n\n\n\n16.1.17.2 Korelacja Rangowa Spearmana\nWspółczynnik ρ Spearmana ocenia zależności monotoniczne wykorzystując rangi.\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} \ngdzie:\n\nd_i to różnica między rangami dla i-tej obserwacji\nn to liczebność próby\n\n\n\n16.1.17.3 Współczynnik τ Kendalla\nTau Kendalla mierzy związek porządkowy na podstawie par zgodnych i niezgodnych.\n \\tau = \\frac{2(P - Q)}{n(n-1)} \ngdzie:\n\nP to liczba par zgodnych\nQ to liczba par niezgodnych\nn to liczebność próby\n\n\n\n\n16.1.18 Implementacja w R\n\n# Przykładowe dane\nset.seed(123)\nx &lt;- c(2, 4, 5, 3, 8)\ny &lt;- c(3, 5, 4, 4, 7)\ndane &lt;- data.frame(x = x, y = y)\n\n# Obliczanie korelacji\nkorelacja_pearson &lt;- cor(x, y, method = \"pearson\")\nkorelacja_spearman &lt;- cor(x, y, method = \"spearman\")\nkorelacja_kendall &lt;- cor(x, y, method = \"kendall\")\n\n# Wyświetlenie wyników\ncat(\"Korelacja Pearsona:\", round(korelacja_pearson, 3), \"\\n\")\n\nKorelacja Pearsona: 0.917 \n\ncat(\"Korelacja Spearmana:\", round(korelacja_spearman, 3), \"\\n\")\n\nKorelacja Spearmana: 0.821 \n\ncat(\"Korelacja Kendalla:\", round(korelacja_kendall, 3), \"\\n\")\n\nKorelacja Kendalla: 0.738 \n\n\n\n\n16.1.19 Wizualizacja\n\nlibrary(ggplot2)\n\nggplot(dane, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(title = \"Przykład Korelacji\",\n       subtitle = paste(\"Korelacja Pearsona r =\", round(korelacja_pearson, 3)),\n       x = \"Zmienna X\",\n       y = \"Zmienna Y\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n16.1.20 Główne Właściwości\n\n16.1.20.1 Korelacja Pearsona (r)\n\nZakres: [-1, 1]\nWłaściwości:\n\nr = 1: Idealna dodatnia zależność liniowa\nr = -1: Idealna ujemna zależność liniowa\nr = 0: Brak zależności liniowej\n\n\n\n\n16.1.20.2 Korelacja Spearmana (ρ)\n\nZakres: [-1, 1]\nWłaściwości:\n\nOparta na rangach, nie na wartościach surowych\nOdporna na wartości odstające\nWykrywa zależności monotoniczne\n\n\n\n\n16.1.20.3 Korelacja Kendalla (τ)\n\nZakres: [-1, 1]\nWłaściwości:\n\nOparta na parach zgodnych/niezgodnych\nBardziej odporna niż Spearman\nPreferowana dla małych prób\n\n\n\n\n\n16.1.21 Kiedy Stosować Poszczególne Miary\n\nStosuj Pearsona gdy:\n\nZależność wydaje się liniowa\nDane są ciągłe\nZakładamy rozkład normalny\nBrak znaczących wartości odstających\n\nStosuj Spearmana gdy:\n\nZależność jest monotonniczna, ale nieliniowa\nDane są porządkowe\nWystępują wartości odstające\nRozkład nie jest normalny\n\nStosuj Kendalla gdy:\n\nMała liczebność próby\nWiele rang wiązanych\nPotrzebna najbardziej odporna miara\nDane porządkowe z małą liczbą kategorii\n\n\n\n\n16.1.22 Przykład Analizy\n\n# Generowanie nieliniowej zależności\nset.seed(456)\nx2 &lt;- seq(1, 10, length.out = 20)\ny2 &lt;- x2^2 + rnorm(20, 0, 5)\ndane2 &lt;- data.frame(x = x2, y = y2)\n\n# Obliczanie korelacji\nwyniki_korelacji &lt;- data.frame(\n  Metoda = c(\"Pearson\", \"Spearman\", \"Kendall\"),\n  Korelacja = c(\n    cor(x2, y2, method = \"pearson\"),\n    cor(x2, y2, method = \"spearman\"),\n    cor(x2, y2, method = \"kendall\")\n  )\n)\n\nprint(wyniki_korelacji)\n\n    Metoda Korelacja\n1  Pearson 0.9699718\n2 Spearman 0.9849624\n3  Kendall 0.9368421\n\n# Wizualizacja\nggplot(dane2, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  labs(title = \"Przykład Zależności Nieliniowej\",\n       subtitle = \"Porównanie różnych miar korelacji\",\n       x = \"Zmienna X\",\n       y = \"Zmienna Y\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n16.1.23 Praktyczne Uwagi\n\nLiczebność Próby\n\nMałe próby (n &lt; 10): Zalecany τ Kendalla\nWiększe próby: Wszystkie metody odpowiednie\n\nWartości Odstające\n\nObecne: Preferuj Spearmana lub Kendalla\nBrak: Pearson może być bardziej efektywny\n\nTyp Zależności\n\nLiniowa: Optymalny Pearson\nMonotonniczna: Spearman lub Kendall\nZłożona: Rozważ inne metody\n\n\n\n\n16.1.24 Częste Błędy\n\nBłędna Interpretacja\n\nKorelacja ≠ przyczynowość\nWysoka korelacja nie oznacza dobrego dopasowania\nNiska korelacja nie oznacza braku zależności\n\nProblemy Techniczne\n\nObsługa brakujących wartości\nRangi wiązane w Spearman/Kendall\nWrażliwość na skalę pomiaru",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#wprowadzenie-do-podstawowej-statystyki-wielowymiarowej",
    "href": "correg_pl.html#wprowadzenie-do-podstawowej-statystyki-wielowymiarowej",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.2 Wprowadzenie do Podstawowej Statystyki Wielowymiarowej (*)",
    "text": "16.2 Wprowadzenie do Podstawowej Statystyki Wielowymiarowej (*)\nStatystyki wielowymiarowe obejmują analizę związków między trzema lub więcej zmiennymi jednocześnie. Ta sekcja wprowadzi niektóre podstawowe koncepcje i techniki analizy wielowymiarowej, koncentrując się na metodach opartych na korelacji.\n\n16.2.1 Macierz Korelacji\nMacierz korelacji to tabela pokazująca korelacje parami dla kilku zmiennych. Jest to podstawowe narzędzie w analizie wielowymiarowej.\nPrzykład: Stwórzmy macierz korelacji dla czterech zmiennych: wzrost, waga, wiek i dochód.\n\nset.seed(123)  # Dla powtarzalności\nwzrost &lt;- rnorm(100, 170, 10)\nwaga &lt;- wzrost * 0.5 + rnorm(100, 0, 5)\nwiek &lt;- rnorm(100, 40, 10)\ndochod &lt;- wiek * 1000 + rnorm(100, 0, 10000)\n\ndane &lt;- data.frame(wzrost, waga, wiek, dochod)\n\nmacierz_kor &lt;- cor(dane)\nprint(macierz_kor)\n\n           wzrost        waga        wiek      dochod\nwzrost  1.0000000  0.66712996 -0.12917601 -0.12246786\nwaga    0.6671300  1.00000000 -0.06814187 -0.04579492\nwiek   -0.1291760 -0.06814187  1.00000000  0.65654902\ndochod -0.1224679 -0.04579492  0.65654902  1.00000000\n\n\nInterpretacja: - Każda komórka pokazuje korelację między dwiema zmiennymi. - Przekątna zawsze wynosi 1 (korelacja zmiennej z samą sobą). - Szukaj silnych korelacji (bliskich 1 lub -1), aby zidentyfikować potencjalne związki.\n\n\n16.2.2 Wizualizacja Związków Wielowymiarowych\n\n16.2.2.1 Macierz Wykresów Rozrzutu\nMacierz wykresów rozrzutu pokazuje parami związki między wieloma zmiennymi.\n\npairs(dane)\n\n\n\n\n\n\n\n\nInterpretacja:\n\nKażdy wykres pokazuje związek między dwiema zmiennymi.\nElementy na przekątnej pokazują rozkład każdej zmiennej.\nSzukaj wzorców, skupisk lub trendów na wykresach.\n\n\n\n16.2.2.2 Wykres Korelacji\nWykres korelacji zapewnia wizualną reprezentację macierzy korelacji.\n\nlibrary(corrplot)\n\ncorrplot 0.94 loaded\n\ncorrplot(macierz_kor, method = \"color\")\n\n\n\n\n\n\n\n\nInterpretacja:\n\nIntensywność koloru i rozmiar kół wskazują na siłę korelacji.\nNiebieskie kolory zazwyczaj wskazują na dodatnie korelacje, czerwone na ujemne.\n\n\n\n\n16.2.3 Korelacja Cząstkowa\nKorelacja cząstkowa mierzy związek między dwiema zmiennymi przy kontrolowaniu jednej lub więcej innych zmiennych.\nPrzykład: Obliczmy korelację cząstkową między wzrostem a wagą, kontrolując wiek.\n\nlibrary(ppcor)\npcor.test(dane$wzrost, dane$waga, dane$wiek)\n\n   estimate      p.value statistic   n gp  Method\n1 0.6654367 5.758157e-14  8.779896 100  1 pearson\n\n\nInterpretacja:\n\nPorównaj to z prostą korelacją między wzrostem a wagą.\nZnacząca zmiana może wskazywać, że wiek odgrywa rolę w związku między wzrostem a wagą.\n\n\n\n16.2.4 Korelacja Wielokrotna\nKorelacja wielokrotna mierzy siłę związku między zmienną zależną a wieloma zmiennymi niezależnymi.\nPrzykład: Przewidźmy wagę na podstawie wzrostu i wieku.\n\nmodel &lt;- lm(waga ~ wzrost + wiek, data = dane)\nR &lt;- sqrt(summary(model)$r.squared)\nprint(paste(\"Współczynnik korelacji wielokrotnej:\", R))\n\n[1] \"Współczynnik korelacji wielokrotnej: 0.667377840470434\"\n\n\nInterpretacja:\n\nR waha się od 0 do 1, przy czym wyższe wartości wskazują na silniejsze związki.\nR² (R-kwadrat) reprezentuje proporcję wariancji w zmiennej zależnej wyjaśnioną przez zmienne niezależne.\n\n\n\n16.2.5 Analiza Czynnikowa\nAnaliza czynnikowa to technika używana do zredukowania wielu zmiennych do mniejszej liczby czynników leżących u podstaw.\nPrzykład: Wykonajmy prostą analizę czynnikową na naszym zbiorze danych.\n\nlibrary(psych)\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\nwynik_fa &lt;- fa(dane, nfactors = 2, rotate = \"varimax\")\nprint(wynik_fa$loadings, cutoff = 0.3)\n\n\nLoadings:\n       MR2    MR1   \nwzrost  0.798       \nwaga    0.836       \nwiek           0.729\ndochod         0.895\n\n                 MR2   MR1\nSS loadings    1.344 1.341\nProportion Var 0.336 0.335\nCumulative Var 0.336 0.671\n\n\nInterpretacja:\n\nSpójrz, które zmienne ładują się wysoko na każdy czynnik.\nSpróbuj zinterpretować, co każdy czynnik może reprezentować na podstawie zmiennych, które się na niego ładują.\n\n\n\n16.2.6 Uwagi dotyczące Analizy Wielowymiarowej\n\nWielkość próby: Techniki wielowymiarowe często wymagają większych prób dla stabilnych wyników.\nWspółliniowość: Wysokie korelacje między zmiennymi niezależnymi mogą powodować problemy w niektórych analizach.\nWartości odstające: Wielowymiarowe wartości odstające mogą mieć silny wpływ na wyniki.\nZałożenia: Wiele technik zakłada wielowymiarową normalność i liniowe związki.\nZłożoność interpretacji: Wraz ze wzrostem liczby zmiennych interpretacja może stać się bardziej wyzwająca.\n\n\n\n16.2.7 Podsumowanie\nTo wprowadzenie do statystyki wielowymiarowej opiera się na koncepcji korelacji, aby badać związki między wieloma zmiennymi. Techniki te zapewniają potężne narzędzia do zrozumienia złożonych zbiorów danych, ale wymagają również starannego rozważenia założeń i ograniczeń. W miarę postępu w Twojej podróży statystycznej napotkasz bardziej zaawansowane techniki wielowymiarowe, takie jak MANOVA, analiza dyskryminacyjna i modelowanie równań strukturalnych.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.3     ✔ stringr   1.5.1\n✔ purrr     1.0.2     ✔ tibble    3.2.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ psych::%+%()         masks ggplot2::%+%()\n✖ psych::alpha()       masks ggplot2::alpha()\n✖ gridExtra::combine() masks dplyr::combine()\n✖ dplyr::filter()      masks stats::filter()\n✖ dplyr::lag()         masks stats::lag()\n✖ MASS::select()       masks dplyr::select()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(broom)\nlibrary(gridExtra)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#wprowadzenie-do-analizy-regresji",
    "href": "correg_pl.html#wprowadzenie-do-analizy-regresji",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.3 Wprowadzenie do Analizy Regresji",
    "text": "16.3 Wprowadzenie do Analizy Regresji\nAnaliza regresji jest fundamentalnym narzędziem statystycznym, które pomaga nam zrozumieć relacje między zmiennymi. Zanim zagłębimy się w formuły i szczegóły techniczne, zrozummy, na jakie pytania może odpowiedzieć regresja:\n\nJak każdy dodatkowy rok edukacji wpływa na czyjeś wynagrodzenie?\nJaka jest zależność między wydatkami na reklamę a sprzedażą?\nJak temperatura wpływa na zużycie energii?\nCzy liczba godzin nauki przewiduje wyniki egzaminów?\n\nTe pytania mają wspólną strukturę: wszystkie badają, jak zmiany jednej zmiennej wiążą się ze zmianami innej.\n\n\n\n\n\n\nModel Regresji Liniowej (MNK): Szybki Start\n\n\n\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Generate sample data\nset.seed(123)\nn &lt;- 20\nx &lt;- seq(1, 10, length.out = n)\ny &lt;- 2 + 1.5 * x + rnorm(n, sd = 1.5)\ndata &lt;- data.frame(x = x, y = y)\n\n# Calculate OLS parameters\nbeta1 &lt;- cov(x, y) / var(x)\nbeta0 &lt;- mean(y) - beta1 * mean(x)\n\n# Create alternative lines\nlines_data &lt;- data.frame(\n  intercept = c(beta0, beta0 + 1, beta0 - 1),\n  slope = c(beta1, beta1 + 0.3, beta1 - 0.3),\n  line_type = c(\"Best fit (OLS)\", \"Suboptimal 1\", \"Suboptimal 2\")\n)\n\n# Create the plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point(size = 3, alpha = 0.6) +\n  geom_abline(data = lines_data,\n              aes(intercept = intercept, \n                  slope = slope,\n                  color = line_type,\n                  linetype = line_type),\n              size = 1) +\n  scale_color_manual(values = c(\"Best fit (OLS)\" = \"#FF4500\",\n                               \"Suboptimal 1\" = \"#4169E1\",\n                               \"Suboptimal 2\" = \"#228B22\")) +\n  labs(title = \"Finding the Best-Fitting Line\",\n       subtitle = \"Orange line minimizes the sum of squared errors\",\n       x = \"X Variable\",\n       y = \"Y Variable\",\n       color = \"Line Type\",\n       linetype = \"Line Type\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    plot.subtitle = element_text(size = 12),\n    legend.position = \"bottom\",\n    panel.grid.minor = element_blank()\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n16.3.1 Koncepcja Modelu\nRegresja MNK (Metoda Najmniejszych Kwadratów) to model statystyczny opisujący związek między zmiennymi. Dwa kluczowe założenia definiują ten model:\n\nZwiązek można opisać linią prostą (liniowość)\nBłędy w naszych przewidywaniach nie są systematycznie powiązane ze zmienną x (ścisła egzogeniczność)\n\n\n\n16.3.2 Przykład: Edukacja i Wynagrodzenia\nRozważmy badanie wpływu edukacji (x) na wynagrodzenia (y). Powiedzmy, że szacujemy:\nwynagrodzenia = \\beta_0 + \\beta_1 \\cdot edukacja + \\epsilon\nSkładnik błędu \\epsilon zawiera wszystkie inne czynniki wpływające na wynagrodzenia. Ścisła egzogeniczność jest naruszona, jeśli pominiemy ważną zmienną, jak “zdolności”, która wpływa zarówno na edukację, jak i wynagrodzenia. Dlaczego? Ponieważ bardziej zdolni ludzie mają tendencję do zdobywania lepszego wykształcenia I wyższych wynagrodzeń, co powoduje zawyżenie szacowanego efektu edukacji.\n\n\n16.3.3 Problem Optymalizacji: Metoda Najmniejszych Kwadratów (OLS)\nKiedy analizujemy zależności między zmiennymi, takimi jak poziom wykształcenia a wynagrodzenie, potrzebujemy systematycznej metody znalezienia linii, która najlepiej odzwierciedla tę relację w naszych danych. Metoda Najmniejszych Kwadratów (OLS - Ordinary Least Squares) dostarcza nam takiego rozwiązania poprzez precyzyjne podejście matematyczne.\nSpójrzmy na nasz wykres poziomów wykształcenia i wynagrodzeń. Każdy punkt reprezentuje rzeczywiste dane - poziom wykształcenia danej osoby i odpowiadające mu wynagrodzenie. Naszym celem jest znalezienie pojedynczej linii, która najdokładniej uchwyci podstawową zależność między tymi zmiennymi.\nDla każdej obserwacji i możemy wyrazić tę relację jako:\ny_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\nGdzie:\n\ny_i to rzeczywiste zaobserwowane wynagrodzenie\n\\hat{y_i} = \\beta_0 + \\beta_1x_i to przewidywane wynagrodzenie\n\\epsilon_i = y_i - \\hat{y_i} to składnik błędu (reszta)\n\nMetoda OLS znajduje optymalne wartości dla \\beta_0 i \\beta_1 poprzez minimalizację sumy kwadratów błędów:\n\\min_{\\beta_0, \\beta_1} \\sum \\epsilon_i^2 = \\min_{\\beta_0, \\beta_1} \\sum(y_i - \\hat{y_i})^2 = \\min_{\\beta_0, \\beta_1} \\sum(y_i - (\\beta_0 + \\beta_1x_i))^2\nAnalizując naszą wizualizację:\n\nRozproszone punkty pokazują rzeczywiste obserwacje (x_i, y_i)\nPomarańczowa linia reprezentuje dopasowane wartości \\hat{y_i}, które minimalizują \\sum \\epsilon_i^2\nNiebieska i zielona linia przedstawiają alternatywne dopasowania o większych sumach kwadratów błędów\nPionowe odległości od każdego punktu do tych linii reprezentują błędy \\epsilon_i\n\nRozwiązanie OLS dostarcza nam estymatorów \\hat{\\beta_0} i \\hat{\\beta_1}, które minimalizują całkowity błąd kwadratowy, dając nam najdokładniejszą liniową reprezentację zależności między poziomem wykształcenia a wynagrodzeniem na podstawie dostępnych danych.\n\n\n16.3.4 Znalezienie Najlepszej Linii\nRozwiązanie tego problemu minimalizacji daje nam:\n\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = \\frac{cov(X, Y)}{var(X)}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\n\n\n16.3.5 Kluczowe Punkty\n\nMNK znajduje linię, która minimalizuje kwadraty błędów predykcji\nTa linia jest “najlepsza” pod względem dopasowania, ale może nie ujmować prawdziwych relacji, jeśli pominięto ważne zmienne\nW przykładzie edukacja-wynagrodzenia, pominięcie zdolności oznacza, że przypisujemy cały wzrost wynagrodzeń samej edukacji\n\n\n\n\n\n16.3.6 Podstawowe Pojęcia i Terminologia\nZanim zagłębimy się w matematykę, ustalmy kluczowe terminy:\n\nZmienna Zależna (Y):\n\nWynik, który chcemy zrozumieć lub przewidzieć\nNazywana również: zmienna odpowiedzi, zmienna docelowa\nPrzykłady: wynagrodzenie, sprzedaż, wyniki egzaminów\n\nZmienna Niezależna (X):\n\nZmienna, która naszym zdaniem wpływa na Y\nNazywana również: predyktor, zmienna objaśniająca, regresor\nPrzykłady: lata edukacji, budżet reklamowy, godziny nauki\n\nParametry Populacji (\\beta):\n\nPrawdziwe podstawowe zależności, które chcemy zrozumieć\nZazwyczaj nieznane w praktyce\nPrzykłady: \\beta_0 (prawdziwy wyraz wolny), \\beta_1 (prawdziwe nachylenie)\n\nOszacowania Parametrów (\\hat{\\beta}):\n\nNasze najlepsze przypuszczenia dotyczące prawdziwych parametrów na podstawie danych\nObliczane na podstawie danych próby\nPrzykłady: \\hat{\\beta}_0 (oszacowany wyraz wolny), \\hat{\\beta}_1 (oszacowane nachylenie)\n\n\n\n\n16.3.7 Główna Idea\nZobaczmy na przykładzie, co robi regresja:\n\n# Generate some example data\nset.seed(123)\nx &lt;- seq(1, 10, by = 0.5)\ny &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\ndata &lt;- data.frame(x = x, y = y)\n\n# Fit the model\nmodel &lt;- lm(y ~ x, data = data)\n\n# Create the plot\nggplot(data, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Przykład Prostej Regresji Liniowej\",\n       subtitle = \"Punkty reprezentują dane, czerwona linia pokazuje dopasowanie regresji\",\n       x = \"Zmienna Niezależna (X)\",\n       y = \"Zmienna Zależna (Y)\") +\n  theme(plot.title = element_text(face = \"bold\"))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 16.1: Podstawowa Idea Regresji: Dopasowanie Linii do Danych\n\n\n\n\n\nTen wykres pokazuje istotę regresji: - Każdy punkt reprezentuje obserwację (X, Y) - Linia reprezentuje nasze najlepsze przypuszczenie dotyczące zależności - Rozproszenie punktów wokół linii pokazuje niepewność",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#model-regresji-liniowej",
    "href": "correg_pl.html#model-regresji-liniowej",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.4 Model Regresji Liniowej",
    "text": "16.4 Model Regresji Liniowej\n\n16.4.1 Model Populacyjny vs. Oszacowania Próby\nW teorii istnieje prawdziwa zależność populacyjna:\nY = \\beta_0 + \\beta_1X + \\varepsilon\ngdzie:\n\n\\beta_0 to prawdziwy wyraz wolny populacji\n\\beta_1 to prawdziwe nachylenie populacji\n\\varepsilon to składnik losowy błędu\n\nW praktyce pracujemy z danymi próby, aby oszacować tę zależność:\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X\nZobaczmy wizualizację różnicy między zależnościami populacyjnymi a próbkowymi:\n\n# Generate population data\nset.seed(456)\nx_pop &lt;- seq(1, 10, by = 0.1)\ntrue_relationship &lt;- 2 + 3*x_pop  # True β₀=2, β₁=3\ny_pop &lt;- true_relationship + rnorm(length(x_pop), 0, 2)\n\n# Create several samples\nsample_size &lt;- 30\nsamples &lt;- data.frame(\n  x = rep(sample(x_pop, sample_size), 3),\n  sample = rep(1:3, each = sample_size)\n)\n\nsamples$y &lt;- 2 + 3*samples$x + rnorm(nrow(samples), 0, 2)\n\n# Fit models to each sample\nmodels &lt;- samples %&gt;%\n  group_by(sample) %&gt;%\n  summarise(\n    intercept = coef(lm(y ~ x))[1],\n    slope = coef(lm(y ~ x))[2]\n  )\n\n# Plot\nggplot() +\n  geom_point(data = samples, aes(x = x, y = y, color = factor(sample)), \n             alpha = 0.5) +\n  geom_abline(data = models, \n              aes(intercept = intercept, slope = slope, \n                  color = factor(sample)),\n              linetype = \"dashed\") +\n  geom_line(aes(x = x_pop, y = true_relationship), \n            color = \"black\", size = 1) +\n  theme_minimal() +\n  labs(title = \"Linie Regresji: Populacyjna vs. Próbkowe\",\n       subtitle = \"Czarna linia: prawdziwa zależność populacyjna\\nLinie przerywane: oszacowania próbkowe\",\n       x = \"X\", y = \"Y\",\n       color = \"Próba\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 16.2: Linie Regresji: Populacyjna vs. Próbkowe\n\n\n\n\n\nTa wizualizacja pokazuje: - Prawdziwą linię populacyjną (czarną), którą próbujemy odkryć - Różne oszacowania próbkowe (linie przerywane) oparte na różnych próbach - Jak oszacowania próbkowe różnią się wokół prawdziwej zależności",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#kluczowe-założenia-regresji-liniowej",
    "href": "correg_pl.html#kluczowe-założenia-regresji-liniowej",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.5 Kluczowe Założenia Regresji Liniowej",
    "text": "16.5 Kluczowe Założenia Regresji Liniowej\n\n16.5.1 Ścisła Egzogeniczność: Podstawowe Założenie\nNajważniejszym założeniem w regresji jest ścisła egzogeniczność:\nE[\\varepsilon|X] = 0\nOznacza to:\n\nWartość oczekiwana składnika błędu warunkowego względem X wynosi zero\nX nie zawiera informacji o przeciętnym błędzie\nNie ma systematycznych wzorców w tym, jak nasze przewidywania są błędne\n\nZobaczmy wizualizację sytuacji, gdy to założenie jest spełnione i gdy nie jest:\n\n# Generate data\nset.seed(789)\nx &lt;- seq(1, 10, by = 0.2)\n\n# Case 1: Exogenous errors\ny_exog &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\n\n# Case 2: Non-exogenous errors (error variance increases with x)\ny_nonexog &lt;- 2 + 3*x + 0.5*x*rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_exog &lt;- data.frame(\n  x = x,\n  y = y_exog,\n  type = \"Błędy Egzogeniczne\\n(Założenie Spełnione)\"\n)\n\ndata_nonexog &lt;- data.frame(\n  x = x,\n  y = y_nonexog,\n  type = \"Błędy Nieegzogeniczne\\n(Założenie Niespełnione)\"\n)\n\ndata_combined &lt;- rbind(data_exog, data_nonexog)\n\n# Create plots with residuals\nplot_residuals &lt;- function(data, title) {\n  model &lt;- lm(y ~ x, data = data)\n  data$predicted &lt;- predict(model)\n  data$residuals &lt;- residuals(model)\n  \n  p1 &lt;- ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    theme_minimal() +\n    labs(title = title)\n  \n  p2 &lt;- ggplot(data, aes(x = x, y = residuals)) +\n    geom_point() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n    theme_minimal() +\n    labs(y = \"Reszty\")\n  \n  list(p1, p2)\n}\n\n# Generate plots\nplots_exog &lt;- plot_residuals(data_exog, \"Błędy Egzogeniczne\")\nplots_nonexog &lt;- plot_residuals(data_nonexog, \"Błędy Nieegzogeniczne\")\n\n# Arrange plots\ngridExtra::grid.arrange(\n  plots_exog[[1]], plots_exog[[2]],\n  plots_nonexog[[1]], plots_nonexog[[2]],\n  ncol = 2\n)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 16.3: Przykłady Egzogeniczności vs. Nieegzogeniczności\n\n\n\n\n\n\n\n16.5.2 Liniowość: Założenie o Formie\nZależność między X a Y powinna być liniowa w parametrach:\nE[Y|X] = \\beta_0 + \\beta_1X\nZauważ, że nie oznacza to, że X i Y muszą mieć zależność w postaci linii prostej - możemy transformować zmienne. Zobaczmy różne rodzaje zależności:\n\n# Generate data\nset.seed(101)\nx &lt;- seq(1, 10, by = 0.1)\n\n# Different relationships\ndata_relationships &lt;- data.frame(\n  x = rep(x, 3),\n  y = c(\n    # Linear\n    2 + 3*x + rnorm(length(x), 0, 2),\n    # Quadratic\n    2 + 0.5*x^2 + rnorm(length(x), 0, 2),\n    # Exponential\n    exp(0.3*x) + rnorm(length(x), 0, 2)\n  ),\n  type = rep(c(\"Liniowa\", \"Kwadratowa\", \"Wykładnicza\"), each = length(x))\n)\n\n# Plot\nggplot(data_relationships, aes(x = x, y = y)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  facet_wrap(~type, scales = \"free_y\") +\n  theme_minimal() +\n  labs(subtitle = \"Czerwona: dopasowanie liniowe, Niebieska: prawdziwa zależność\")\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nFigure 16.4: Zależności Liniowe i Nieliniowe\n\n\n\n\n\n\n\n16.5.3 Zrozumienie Naruszeń i Rozwiązania\nGdy założenie liniowości jest naruszone:\n\nTransformacja zmiennych:\n\nTransformacja logarytmiczna: dla zależności wykładniczych\nPierwiastek kwadratowy: dla umiarkowanej nieliniowości\nTransformacje potęgowe: dla bardziej złożonych zależności\n\n\n\n# Generate exponential data\nset.seed(102)\nx &lt;- seq(1, 10, by = 0.2)\ny &lt;- exp(0.3*x) + rnorm(length(x), 0, 2)\n\n# Create datasets\ndata_trans &lt;- data.frame(\n  x = x,\n  y = y,\n  log_y = log(y)\n)\n\nWarning in log(y): NaNs produced\n\n# Original scale plot\np1 &lt;- ggplot(data_trans, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Skala Oryginalna\")\n\n# Log scale plot\np2 &lt;- ggplot(data_trans, aes(x = x, y = log_y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Y po Transformacji Logarytmicznej\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFigure 16.5: Efekt Transformacji Zmiennych",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#metody-estymacji",
    "href": "correg_pl.html#metody-estymacji",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.6 Metody Estymacji",
    "text": "16.6 Metody Estymacji\n\n16.6.1 Metoda Najmniejszych Kwadratów (OLS)\nOLS znajduje \\hat{\\beta}_0 i \\hat{\\beta}_1 minimalizując sumę kwadratów reszt:\n\\min_{\\hat{\\beta}_0, \\hat{\\beta}_1} \\sum_{i=1}^n (Y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1X_i)^2\nRozwiązania to:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (X_i - \\bar{X})^2}\n\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\nZobaczmy wizualizację działania OLS:\n\n# Generate sample data\nset.seed(103)\nx_sample &lt;- seq(1, 10, by = 1)\ny_sample &lt;- 2 + 3*x_sample + rnorm(length(x_sample), 0, 2)\ndata_sample &lt;- data.frame(x = x_sample, y = y_sample)\n\n# Fit model\nmodel_sample &lt;- lm(y ~ x, data = data_sample)\ndata_sample$predicted &lt;- predict(model_sample)\n\n# Plot\nggplot(data_sample, aes(x = x, y = y)) +\n  geom_point(color = \"blue\") +\n  geom_line(aes(y = predicted), color = \"red\") +\n  geom_segment(aes(xend = x, y = y, yend = predicted), \n              color = \"green\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Regresja OLS z Resztami\",\n       subtitle = \"Niebieskie punkty: dane\\nCzerwona linia: dopasowanie OLS\\nZielone linie przerywane: reszty\")\n\n\n\n\n\n\n\nFigure 16.6: OLS Minimalizująca Kwadraty Reszt",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#ocena-dopasowania-modelu",
    "href": "correg_pl.html#ocena-dopasowania-modelu",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.7 Ocena Dopasowania Modelu",
    "text": "16.7 Ocena Dopasowania Modelu\n\n16.7.1 Dekompozycja Wariancji\nCałkowita zmienność Y może być rozłożona na komponenty wyjaśnione i niewyjaśnione:\n\\underbrace{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}_{SST} = \\underbrace{\\sum_{i=1}^n (\\hat{Y}_i - \\bar{Y})^2}_{SSR} + \\underbrace{\\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}_{SSE}\ngdzie:\n\nSST (Całkowita Suma Kwadratów): Całkowita zmienność Y\nSSR (Regresyjna Suma Kwadratów): Zmienność wyjaśniona przez regresję\nSSE (Suma Kwadratów Błędów): Zmienność niewyjaśniona\n\nZobaczmy wizualizację tej dekompozycji:\n\n# Generate sample data\nset.seed(104)\nx &lt;- seq(1, 10, by = 0.5)\ny &lt;- 2 + 3*x + rnorm(length(x), 0, 2)\ndf &lt;- data.frame(x = x, y = y)\n\n# Fit model\nmodel &lt;- lm(y ~ x, data = df)\ndf$predicted &lt;- predict(model)\ndf$residuals &lt;- residuals(model)\ndf$mean_y &lt;- mean(df$y)\n\n# Calculate components for one point\npoint_index &lt;- 10\nexample_point &lt;- df[point_index, ]\n\n# Create main plot\np_main &lt;- ggplot(df, aes(x = x)) +\n  geom_point(aes(y = y), color = \"blue\") +\n  geom_line(aes(y = predicted), color = \"red\") +\n  geom_hline(yintercept = mean(df$y), linetype = \"dashed\") +\n  # Total deviation\n  geom_segment(data = example_point,\n               aes(x = x, y = mean_y, xend = x, yend = y),\n               color = \"purple\", size = 1, alpha = 0.5) +\n  # Explained deviation\n  geom_segment(data = example_point,\n               aes(x = x, y = mean_y, xend = x, yend = predicted),\n               color = \"green\", size = 1, alpha = 0.5) +\n  # Unexplained deviation\n  geom_segment(data = example_point,\n               aes(x = x, y = predicted, xend = x, yend = y),\n               color = \"orange\", size = 1, alpha = 0.5) +\n  theme_minimal() +\n  labs(title = \"Dekompozycja Wariancji\",\n       subtitle = \"Fioletowy: Całkowite odchylenie (Yi - Ȳ)\\nZielony: Wyjaśnione (Ŷi - Ȳ)\\nPomarańczowy: Niewyjaśnione (Yi - Ŷi)\")\n\n# Calculate R-squared\nsummary_stats &lt;- glance(model)\nr2 &lt;- round(summary_stats$r.squared, 3)\n\n# Add R-squared to plot\nprint(p_main)\ncat(sprintf(\"\\nR² = %.3f\\nOznacza to, że %.1f%% wariancji w Y jest wyjaśnione przez X\\n\", r2, r2*100))\n\n\nR² = 0.965\nOznacza to, że 96.5% wariancji w Y jest wyjaśnione przez X\n\n\n\n\n\n\n\n\nFigure 16.7: Dekompozycja Wariancji w Regresji Liniowej\n\n\n\n\n\n\n\n16.7.2 Miary Dopasowania\n\nR-kwadrat (R^2): R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nPierwiastek Błędu Średniokwadratowego (RMSE): RMSE = \\sqrt{\\frac{SSE}{n}}\nŚredni Błąd Bezwzględny (MAE): MAE = \\frac{1}{n}\\sum_{i=1}^n |Y_i - \\hat{Y}_i|\n\nZobaczmy obliczenie i wizualizację tych miar:\n\n# Calculate measures\nrmse &lt;- sqrt(mean(residuals(model)^2))\nmae &lt;- mean(abs(residuals(model)))\n\n# Create residual plot with different measures\nggplot(df, aes(x = predicted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_hline(yintercept = c(rmse, -rmse), linetype = \"dotted\", color = \"blue\") +\n  geom_hline(yintercept = c(mae, -mae), linetype = \"dotted\", color = \"green\") +\n  theme_minimal() +\n  labs(title = \"Wykres Reszt z Miarami Błędu\",\n       subtitle = sprintf(\"RMSE = %.2f (niebieskie linie)\\nMAE = %.2f (zielone linie)\", \n                         rmse, mae))\n\n\n\n\n\n\n\nFigure 16.8: Różne Miary Dopasowania Modelu",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#wykresy-diagnostyczne",
    "href": "correg_pl.html#wykresy-diagnostyczne",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.8 Wykresy Diagnostyczne",
    "text": "16.8 Wykresy Diagnostyczne\n\n16.8.1 Analiza Reszt\nCztery kluczowe wykresy diagnostyczne:\n\n# Create diagnostic plots\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\nFigure 16.9: Kluczowe Wykresy Diagnostyczne dla Regresji Liniowej\n\n\n\n\n\nInterpretacja każdego wykresu:\n\nReszty vs. Wartości Dopasowane:\n\nSprawdza założenie liniowości\nSzukamy wzorców/trendów\nOczekujemy losowego rozrzutu wokół zera\n\nWykres Kwantyl-Kwantyl:\n\nSprawdza normalność reszt\nPunkty powinny układać się wzdłuż linii przekątnej\nOdchylenia na końcach są często akceptowalne\n\nSkala-Położenie:\n\nSprawdza homoskedastyczność\nOczekujemy linii poziomej z losowym rozrzutem\nPomaga wykryć zmienną wariancję\n\nReszty vs. Wpływ:\n\nIdentyfikuje punkty wpływowe\nSzukamy punktów poza liniami odległości Cooka\nPomaga zidentyfikować problematyczne obserwacje\n\n\n\n\n\n\n\n\nIntuicyjne zrozumienie Metody Najmniejszych Kwadratów (MNK)\n\n\n\n\n16.8.2 Podstawowy Problem\nZacznijmy od rzeczywistego scenariusza: chcemy zrozumieć, jak czas nauki wpływa na wyniki egzaminu. Zbieramy dane z Twojej klasy, gdzie:\n\nKażdy student zapisuje swoje godziny nauki (x)\nOraz swój końcowy wynik egzaminu (y)\nWięc student 1 mógł się uczyć x_1 = 3 godziny i uzyskać y_1 = 75 punktów\nStudent 2 mógł się uczyć x_2 = 5 godzin i uzyskać y_2 = 82 punkty\nI tak dalej dla wszystkich n studentów w klasie\n\nNaszym celem jest znalezienie prostej, która najlepiej opisuje tę zależność. Próbujemy oszacować prawdziwą zależność (której nigdy nie znamy dokładnie) używając naszej próby danych. Przeanalizujmy to krok po kroku.\n\nlibrary(tidyverse)\n\n# Tworzenie przykładowych danych\nset.seed(123)\ngodziny_nauki &lt;- runif(20, 1, 8)\nwyniki_egzaminu &lt;- 60 + 5 * godziny_nauki + rnorm(20, 0, 5)\ndane &lt;- data.frame(godziny_nauki, wyniki_egzaminu)\n\n# Podstawowy wykres punktowy\nggplot(dane, aes(x = godziny_nauki, y = wyniki_egzaminu)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  labs(x = \"Godziny nauki\", y = \"Wyniki egzaminu\",\n       title = \"Dane z Twojej klasy: Godziny nauki vs. Wyniki egzaminu\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n\n\n\n\n\n\n\n\n\n16.8.3 Co sprawia, że prosta jest “dobra”?\nKażdą prostą można zapisać w postaci:\ny = \\hat{\\beta}_0 + \\hat{\\beta}_1x\nGdzie:\n\n\\hat{\\beta}_0 to nasza estymata wyrazu wolnego (przewidywany wynik dla zero godzin nauki)\n\\hat{\\beta}_1 to nasza estymata nachylenia (ile punktów zyskujemy za każdą dodatkową godzinę nauki)\nDaszki (^) wskazują, że są to nasze estymaty prawdziwych (nieznanych) parametrów \\beta_0 i \\beta_1\n\nSpójrzmy na trzy możliwe proste przechodzące przez nasze dane:\n\nggplot(dane, aes(x = godziny_nauki, y = wyniki_egzaminu)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  geom_abline(intercept = 50, slope = 8, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_abline(intercept = 70, slope = 2, color = \"green\", linetype = \"dashed\", size = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"purple\") +\n  labs(x = \"Godziny nauki\", y = \"Wyniki egzaminu\",\n       title = \"Trzy różne proste: Która jest najlepsza?\") +\n  annotate(\"text\", x = 7.5, y = 120, color = \"red\", label = \"Prosta A: Za stroma\") +\n  annotate(\"text\", x = 7.5, y = 85, color = \"green\", label = \"Prosta B: Za płaska\") +\n  annotate(\"text\", x = 7.5, y = 100, color = \"purple\", label = \"Prosta C: W sam raz\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n16.8.4 Zrozumienie błędów przewidywania (reszt)\nTu zaczyna się magia MNK. Dla każdego studenta w naszych danych:\n\nPatrzymy na jego rzeczywisty wynik egzaminu (y_i)\nObliczamy przewidywany wynik używając naszej prostej (\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nRóżnica między nimi nazywana jest resztą:\n\n\\text{reszta}_i = y_i - \\hat{y}_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i)\nZobaczmy wizualizację tych reszt dla jednej prostej:\n\n# Dopasowanie modelu i pokazanie reszt\nmodel &lt;- lm(wyniki_egzaminu ~ godziny_nauki, data = dane)\n\nggplot(dane, aes(x = godziny_nauki, y = wyniki_egzaminu)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"purple\") +\n  geom_segment(aes(xend = godziny_nauki, \n                  yend = predict(model, dane)),\n              color = \"orange\", alpha = 0.5) +\n  labs(x = \"Godziny nauki\", y = \"Wyniki egzaminu\",\n       title = \"Zrozumienie reszt: Różnice między przewidywaniami a rzeczywistością\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nPomarańczowe pionowe linie pokazują, jak bardzo nasze przewidywania odbiegają od rzeczywistości dla każdego studenta. Niektóre przewidywania są za wysokie (dodatnie reszty), inne za niskie (ujemne reszty).\n\n\n16.8.5 Dlaczego podnosimy reszty do kwadratu?\nTo kluczowa koncepcja! Przeanalizujmy to na prostym przykładzie:\nWyobraź sobie, że mamy tylko dwóch studentów: 1. Ala: Przewidywane 80, rzeczywisty wynik 85 (reszta = +5) 2. Bob: Przewidywane 90, rzeczywisty wynik 85 (reszta = -5)\nJeśli po prostu dodamy te reszty: (+5) + (-5) = 0\nTo sugerowałoby, że nasza prosta jest idealna (całkowity błąd = 0), ale wiemy, że tak nie jest! Oba przewidywania były nietrafne o 5 punktów.\nRozwiązanie: Podnosimy reszty do kwadratu przed dodaniem: - Kwadrat reszty Ali: (+5)^2 = 25 - Kwadrat reszty Boba: (-5)^2 = 25 - Całkowity błąd kwadratowy: 25 + 25 = 50\nTo daje nam znacznie lepszą miarę tego, jak bardzo nasze przewidywania są błędne!\n\n\n16.8.6 Suma kwadratów reszt (SKR)\nDla wszystkich studentów razem obliczamy:\nSKR = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nTen wzór może wyglądać groźnie, ale oznacza po prostu:\n\nWeź resztę każdego studenta\nPodnieś ją do kwadratu\nDodaj wszystkie te kwadraty reszt\n\nIm mniejsza ta suma, tym lepiej nasza prosta pasuje do danych!\n\n# Porównanie dobrego i złego dopasowania\nzle_przewidywania &lt;- 70 + 2 * dane$godziny_nauki\ndobre_przewidywania &lt;- predict(model, dane)\n\nzle_sse &lt;- sum((dane$wyniki_egzaminu - zle_przewidywania)^2)\ndobre_sse &lt;- sum((dane$wyniki_egzaminu - dobre_przewidywania)^2)\n\nggplot(dane, aes(x = godziny_nauki, y = wyniki_egzaminu)) +\n  geom_point(color = \"blue\", size = 3, alpha = 0.6) +\n  geom_abline(intercept = 70, slope = 2, color = \"red\", \n              linetype = \"dashed\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"purple\") +\n  annotate(\"text\", x = 2, y = 95, \n           label = paste(\"Czerwona prosta: Błąd =\", round(zle_sse)), \n           color = \"red\") +\n  annotate(\"text\", x = 2, y = 90, \n           label = paste(\"Fioletowa prosta: Błąd =\", round(dobre_sse)), \n           color = \"purple\") +\n  labs(x = \"Godziny nauki\", y = \"Wyniki egzaminu\",\n       title = \"Porównanie całkowitych błędów przewidywania\") +\n  theme_minimal() +\n  theme(text = element_text(size = 12))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n16.8.7 Dlaczego nazywamy to “Metodą Najmniejszych Kwadratów”?\nPrzeanalizujmy nazwę:\n\n“Kwadratów”: Podnosimy reszty do kwadratu\n“Najmniejszych”: Chcemy najmniejszej możliwej sumy\n“Zwykłych” (w angielskim “Ordinary”): To podstawowa wersja (istnieją bardziej zaawansowane warianty!)\n\nProsta MNK ma kilka przyjemnych właściwości:\n\nŚrednia wszystkich reszt równa się zero\nProsta zawsze przechodzi przez punkt (\\bar{x}, \\bar{y}) - średnie godziny nauki i średni wynik\nMałe zmiany w danych prowadzą do małych zmian w prostej (jest “stabilna”)\nNasze estymaty \\hat{\\beta}_0 i \\hat{\\beta}_1 są najlepszymi możliwymi estymatorami prawdziwych parametrów \\beta_0 i \\beta_1 przy pewnych założeniach\n\nEksperymentuj z różnymi prostymi i zobacz, jak zmienia się całkowity błąd kwadratowy:\n\nlibrary(manipulate)\n\nmanipulate(\n  {\n    przewidywania &lt;- b0 + b1 * dane$godziny_nauki\n    skr &lt;- sum((dane$wyniki_egzaminu - przewidywania)^2)\n    \n    ggplot(dane, aes(x = godziny_nauki, y = wyniki_egzaminu)) +\n      geom_point(color = \"blue\", alpha = 0.6) +\n      geom_abline(slope = b1, intercept = b0, color = \"red\") +\n      labs(title = paste(\"Całkowity błąd kwadratowy =\", round(skr, 1))) +\n      theme_minimal()\n  },\n  b0 = slider(40, 80, initial = 60, label = \"Wyraz wolny (β̂₀)\"),\n  b1 = slider(0, 10, initial = 5, label = \"Nachylenie (β̂₁)\")\n)\n\nCzy potrafisz znaleźć prostą, która daje najmniejszą sumę kwadratów reszt? To właśnie prosta MNK!\n\n\n16.8.8 Ważne uwagi\n\nOznaczenie z daszkiem (\\hat{\\beta}_0, \\hat{\\beta}_1) przypomina nam, że estymujemy prawdziwą zależność z naszej próby. Nigdy nie znamy prawdziwych \\beta_0 i \\beta_1 - możemy je tylko oszacować z naszych danych.\nMNK daje nam najlepsze możliwe estymaty, gdy spełnione są pewne warunki (jak losowo pobrana próba i rzeczywiście liniowa zależność).\nPowyższe narzędzie interaktywne pomaga zrozumieć to, co MNK robi automatycznie: znajduje wartości \\hat{\\beta}_0 i \\hat{\\beta}_1, które dają nam najmniejszą możliwą sumę kwadratów reszt.\n\n\n\n\n\n\n\n\n\n\nFormalne wyprowadzenie estymatorów MNK\n\n\n\n\n16.8.9 Założenia wstępne\nChcemy znaleźć prostą y = \\hat{\\beta}_0 + \\hat{\\beta}_1x, która minimalizuje sumę kwadratów reszt. Wyprowadźmy to krok po kroku:\n\nNajpierw zapisujemy funkcję, którą chcemy zminimalizować:\nSKR = \\sum_{i=1}^n (y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_i))^2\nAby znaleźć minimum, musimy obliczyć pochodne cząstkowe względem \\hat{\\beta}_0 i \\hat{\\beta}_1 oraz przyrównać je do zera:\n\\frac{\\partial SKR}{\\partial \\hat{\\beta}_0} = 0 oraz \\frac{\\partial SKR}{\\partial \\hat{\\beta}_1} = 0\n\n\n\n16.8.10 Krok 1: Znalezienie \\hat{\\beta}_0\nObliczmy pochodną cząstkową względem \\hat{\\beta}_0:\n\\frac{\\partial SKR}{\\partial \\hat{\\beta}_0} = \\frac{\\partial}{\\partial \\hat{\\beta}_0} \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2\nKorzystając z reguły łańcuchowej:\n\\frac{\\partial SKR}{\\partial \\hat{\\beta}_0} = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)(-1) = 0\nUpraszczając:\n-2\\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\n\\sum_{i=1}^n y_i - n\\hat{\\beta}_0 - \\hat{\\beta}_1\\sum_{i=1}^n x_i = 0\nRozwiązując względem \\hat{\\beta}_0:\n\\hat{\\beta}_0 = \\frac{\\sum_{i=1}^n y_i}{n} - \\hat{\\beta}_1\\frac{\\sum_{i=1}^n x_i}{n} = \\bar{y} - \\hat{\\beta}_1\\bar{x}\nGdzie \\bar{y} i \\bar{x} to średnie z próby.\n\n\n16.8.11 Krok 2: Znalezienie \\hat{\\beta}_1\nTeraz obliczamy pochodną cząstkową względem \\hat{\\beta}_1:\n\\frac{\\partial SKR}{\\partial \\hat{\\beta}_1} = \\frac{\\partial}{\\partial \\hat{\\beta}_1} \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2\nKorzystając z reguły łańcuchowej:\n\\frac{\\partial SKR}{\\partial \\hat{\\beta}_1} = \\sum_{i=1}^n 2(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i)(-x_i) = 0\nUpraszczając:\n-2\\sum_{i=1}^n x_i(y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1x_i) = 0\nPodstawiając nasze wyrażenie na \\hat{\\beta}_0:\n-2\\sum_{i=1}^n x_i(y_i - (\\bar{y} - \\hat{\\beta}_1\\bar{x}) - \\hat{\\beta}_1x_i) = 0\n\\sum_{i=1}^n x_i(y_i - \\bar{y} + \\hat{\\beta}_1\\bar{x} - \\hat{\\beta}_1x_i) = 0\nPo przekształceniach algebraicznych (rozwinięciu i zgrupowaniu wyrazów):\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n\n\n16.8.12 Wyniki końcowe\nWyprowadziliśmy estymatory MNK:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}\nZrozummy, co oznaczają te wzory:\n\nEstymator nachylenia \\hat{\\beta}_1:\n\nLicznik: Mierzy, jak x i y zmieniają się razem (kowariancja)\nMianownik: Mierzy, jak bardzo zmienia się x (wariancja)\nWięc \\hat{\\beta}_1 jest zasadniczo stosunkiem kowariancji do wariancji\n\nEstymator wyrazu wolnego \\hat{\\beta}_0:\n\nZapewnia, że prosta przechodzi przez punkt (\\bar{x}, \\bar{y})\nDostosowuje wysokość prostej na podstawie nachylenia\n\n\n\n\n16.8.13 Weryfikacja: Drugie pochodne\nAby potwierdzić, że znaleźliśmy minimum (a nie maksimum), sprawdzamy drugie pochodne:\n\\frac{\\partial^2 SKR}{\\partial \\hat{\\beta}_0^2} = 2n &gt; 0\n\\frac{\\partial^2 SKR}{\\partial \\hat{\\beta}_1^2} = 2\\sum_{i=1}^n x_i^2 &gt; 0\nPonieważ obie drugie pochodne są dodatnie, rzeczywiście znaleźliśmy minimum.\n\n\n16.8.14 Postać macierzowa (Temat zaawansowany, opcjonalny)\nDla osób znających algebrę liniową, możemy zapisać to zwięźlej:\n\\mathbf{X} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}\n\\mathbf{y} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}\nWtedy estymator MNK w postaci macierzowej to:\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'\\mathbf{y}\nTo daje nam zarówno \\hat{\\beta}_0 jak i \\hat{\\beta}_1 w jednym eleganckim wyrażeniu.\n\n\n16.8.15 Wizualizacja wyprowadzenia\n\nlibrary(tidyverse)\n\n# Tworzenie przykładowych danych\nset.seed(123)\nx &lt;- runif(20, 1, 8)\ny &lt;- 2 + 3 * x + rnorm(20, 0, 1)\ndane &lt;- data.frame(x = x, y = y)\n\n# Obliczanie średnich\nx_srednia &lt;- mean(x)\ny_srednia &lt;- mean(y)\n\n# Tworzenie wizualizacji odchyleń\nggplot(dane, aes(x = x, y = y)) +\n  geom_point(color = \"blue\", alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  geom_hline(yintercept = y_srednia, linetype = \"dashed\", color = \"gray\") +\n  geom_vline(xintercept = x_srednia, linetype = \"dashed\", color = \"gray\") +\n  geom_segment(aes(xend = x, yend = y_srednia), color = \"green\", alpha = 0.3) +\n  geom_segment(aes(yend = y, xend = x_srednia), color = \"purple\", alpha = 0.3) +\n  labs(title = \"Wizualizacja odchyleń od średnich\",\n       subtitle = \"Zielone: Odchylenia w y, Fioletowe: Odchylenia w x\",\n       x = \"x\", y = \"y\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nPowyższy wykres pokazuje, jak działają estymatory MNK z odchyleniami od średnich. Iloczyn tych odchyleń (zielone × fioletowe) dla każdego punktu, zsumowany i znormalizowany, daje nam nasz estymator nachylenia \\hat{\\beta}_1.\n\n\n16.8.16 Ważne uwagi\n\nWyprowadzone estymatory są BLUE (Best Linear Unbiased Estimators - Najlepsze Liniowe Nieobciążone Estymatory) przy spełnieniu założeń Gaussa-Markowa.\nZałożenia te obejmują:\n\nLiniowość zależności\nLosowość próby\nBrak współliniowości idealnej\nHomoskedastyczność (stała wariancja reszt)\nNiezależność obserwacji\n\nMetoda ta minimalizuje sumę kwadratów reszt w kierunku pionowym (odchylenia w y), a nie prostopadłym do prostej.\n\n\n\n\n\n\n\n\n\n\nDekompozycja Wariancji w Regresji Liniowej: Intuicyjny Przewodnik\n\n\n\n\n16.8.17 Szerszy Kontekst: Dlaczego To Jest Ważne?\nWyobraź sobie, że próbujesz przewidzieć ceny domów. Najprostszym przypuszczeniem byłoby użycie średniej ceny wszystkich domów. Ale co jeśli znamy również wielkość każdego domu? Czy ta informacja pomoże nam w lepszym prognozowaniu? Dekompozycja wariancji pomaga nam dokładnie określić, o ile lepsze stają się nasze prognozy, gdy wykorzystujemy dodatkowe informacje.\n\n\n16.8.18 Główna Koncepcja: Od Prostych do Inteligentnych Prognoz\n\n16.8.18.1 Krok 1: Zaczynając od Średniej\n\nNaszą podstawową prognozą jest średnia (\\bar{y})\nTraktuj to jako nasze “nieświadome przypuszczenie”\nDla każdego domu przewidywalibyśmy tę samą cenę (średnią)\nTo tworzy nasze podstawowe błędy\n\n\n\n16.8.18.2 Krok 2: Wykorzystanie Dodatkowych Informacji\n\nWłączamy informacje z naszego predyktora (X)\nTeraz możemy tworzyć różne prognozy dla różnych domów\nKażda prognoza (\\hat{y}_i) jest dostosowana na podstawie X\nTo tworzy nasze nowe, miejmy nadzieję mniejsze, błędy\n\n\n\n\n16.8.19 Wizualizacja Dekompozycji Wariancji\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(patchwork)\n\n\nAttaching package: 'patchwork'\n\n\nThe following object is masked from 'package:MASS':\n\n    area\n\n# Generate data with clearer pattern\nset.seed(123)\nx &lt;- seq(1, 10, length.out = 50)\ny &lt;- 2 + 0.5 * x + rnorm(50, sd = 0.8)\ndata &lt;- data.frame(x = x, y = y)\n\n# Model and calculations\nmodel &lt;- lm(y ~ x, data)\nmean_y &lt;- mean(y)\ndata$predicted &lt;- predict(model)\n\n# Select specific points for demonstration that are well-spaced\ndemonstration_points &lt;- c(8, 25, 42)  # Changed points for better spacing\n\n# Create main plot with improved aesthetics\np1 &lt;- ggplot(data, aes(x = x, y = y)) +\n  # Add background grid for better readability\n  geom_hline(yintercept = seq(0, 8, by = 0.5), color = \"gray90\", linewidth = 0.2) +\n  geom_vline(xintercept = seq(0, 10, by = 0.5), color = \"gray90\", linewidth = 0.2) +\n  \n  # Add regression line and mean line\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E41A1C\", linewidth = 1.2) +\n  geom_hline(yintercept = mean_y, linetype = \"longdash\", color = \"#377EB8\", linewidth = 1) +\n  \n  # Add data points\n  geom_point(size = 3, alpha = 0.6, color = \"#4A4A4A\") +\n  \n  # Add decomposition segments with improved colors and positioning\n  # Total deviation (purple)\n  geom_segment(data = data[demonstration_points,],\n              aes(x = x, xend = x, y = y, yend = mean_y),\n              color = \"#984EA3\", linetype = \"dashed\", linewidth = 1.8) +\n  # Explained component (green)\n  geom_segment(data = data[demonstration_points,],\n              aes(x = x, xend = x, y = mean_y, yend = predicted),\n              color = \"#4DAF4A\", linetype = \"dashed\", linewidth = 1) +\n  # Unexplained component (orange)\n  geom_segment(data = data[demonstration_points,],\n              aes(x = x, xend = x, y = predicted, yend = y),\n              color = \"#FF7F00\", linetype = \"dashed\", linewidth = 1) +\n  \n  # Add annotations for better understanding\n  annotate(\"text\", x = data$x[demonstration_points[2]], y = mean_y - 0.2,\n           label = \"Mean\", color = \"#377EB8\", hjust = -0.2) +\n  annotate(\"text\", x = data$x[demonstration_points[2]], \n           y = data$predicted[demonstration_points[2]] + 0.2,\n           label = \"Regression Line\", color = \"#E41A1C\", hjust = -0.2) +\n  \n  # Improve theme and labels\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    plot.subtitle = element_text(size = 12),\n    panel.grid = element_blank(),\n    legend.position = \"bottom\"\n  ) +\n  labs(\n    title = \"Variance Decomposition in Linear Regression\",\n    subtitle = \"Decomposing total variance into explained and unexplained components\",\n    x = \"Predictor (X)\",\n    y = \"Response (Y)\"\n  )\n\n# Create error distribution plot with improved aesthetics\ndata$mean_error &lt;- y - mean_y\ndata$regression_error &lt;- y - data$predicted\n\np2 &lt;- ggplot(data) +\n  geom_density(aes(x = mean_error, fill = \"Deviation from Mean\"), \n               alpha = 0.5) +\n  geom_density(aes(x = regression_error, fill = \"Regression Residuals\"), \n               alpha = 0.5) +\n  theme_minimal(base_size = 14) +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  ) +\n  labs(\n    title = \"Error Distribution Comparison\",\n    x = \"Error Magnitude\",\n    y = \"Density\"\n  ) +\n  scale_fill_manual(\n    values = c(\"#377EB8\", \"#E41A1C\")\n  )\n\n# Add legend explaining the decomposition components\nlegend_plot &lt;- ggplot() +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    legend.box = \"horizontal\"\n  ) +\n  annotate(\"text\", x = 0, y = 0, label = \"\") +\n  scale_color_manual(\n    name = \"Variance Components\",\n    values = c(\"#984EA3\", \"#4DAF4A\", \"#FF7F00\"),\n    labels = c(\"Total Deviation\", \"Explained Variance\", \"Unexplained Variance\")\n  )\n\n# Combine plots with adjusted heights\ncombined_plot &lt;- (p1 / p2) +\n  plot_layout(heights = c(2, 1))\n\n# Print the combined plot\ncombined_plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n16.8.20 Zrozumienie Trzech Rodzajów Wariancji\n\nWariancja Całkowita (SST)\n\nPytanie: “Jak bardzo obserwacje różnią się od średniej?”\nWzór: SST = \\sum(y_i - \\bar{y})^2\nWizualizacja: Fioletowe punkty na wykresie\nIntuicja: “Rozrzut” naszych danych wokół średniej\n\nWariancja Wyjaśniona (SSR)\n\nPytanie: “Ile wariancji wyjaśnił nasz model?”\nWzór: SSR = \\sum(\\hat{y}_i - \\bar{y})^2\nWizualizacja: Zielone przerywane linie na wykresie\nIntuicja: Poprawa, którą uzyskaliśmy dzięki użyciu X\n\nWariancja Niewyjaśniona (SSE)\n\nPytanie: “Jaka wariancja pozostaje niewyjaśniona?”\nWzór: SSE = \\sum(y_i - \\hat{y}_i)^2\nWizualizacja: Pomarańczowe przerywane linie na wykresie\nIntuicja: Błędy pozostające po użyciu X\n\n\n\n\n16.8.21 R² Wyjaśnione\nR^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nMyśl o R² jako o odpowiedzi na pytanie: “Jaki procent pierwotnej wariancji Y możemy wyjaśnić używając X?”\n\n16.8.21.1 Intuicyjne Przykłady:\n\nR² = 0,80: Użycie X wyeliminowało 80% naszych błędów predykcji\nR² = 0,25: Użycie X wyeliminowało 25% naszych błędów predykcji\nR² = 0,00: Użycie X wcale nie pomogło\n\n\n\n\n16.8.22 Kiedy Zachować Ostrożność\n\nWysoki R² To Nie Wszystko\n\nWysoki R² może wskazywać na przeuczenie\nZawsze sprawdzaj, czy twój model ma praktyczny sens\nWeź pod uwagę kontekst swojej dziedziny\n\nNiski R² Nie Zawsze Jest Zły\n\nW niektórych dziedzinach R² = 0,30 może być imponujący\nNauki społeczne często mają niższe wartości R²\nSkup się na znaczeniu praktycznym\n\nWielkość Próby Ma Znaczenie\n\nUżywaj skorygowanego R² dla regresji wielorakiej: R^2_{adj} = 1 - \\frac{SSE/(n-p)}{SST/(n-1)}\nPenalizuje dodawanie niepotrzebnych predyktorów\n\n\n\n\n16.8.23 Praktyczne Wskazówki do Analizy\n\nInspekcja Wizualna\n\nZawsze wizualizuj swoje dane\nSzukaj wzorców w resztach\nSprawdzaj punkty wpływowe\n\nUwzględnienie Kontekstu\n\nCo jest “dobrym” R² w twojej dziedzinie?\nJaki jest praktyczny wpływ twoich błędów?\nCzy twoje predyktory są znaczące?\n\nDiagnostyka Modelu\n\nSprawdź normalność reszt\nSzukaj heteroskedastyczności\nBadaj punkty wpływowe\n\n\n\n\n16.8.24 Kluczowe Wnioski\n\nDekompozycja wariancji pomaga zrozumieć poprawę predykcji\nR² określa ilościowo proporcję wyjaśnionej wariancji\nZrozumienie wizualne jest kluczowe dla interpretacji\nKontekst jest ważniejszy niż bezwzględne wartości R²\nZawsze łącz R² z innymi narzędziami diagnostycznymi\n\n\n\n\n\n\n\n\n\n\nZrozumienie Endogeniczności\n\n\n\nPomyśl o endogeniczności jako o “problemie ukrytych zależności” w twojej analizie. To jak próba rozwiązania układanki, gdzie niektóre elementy wpływają na siebie nawzajem w sposób, którego nie możesz bezpośrednio zobaczyć. W języku technicznym endogeniczność występuje, gdy zmienna objaśniająca w modelu regresji jest skorelowana ze składnikiem losowym.\n\nBłąd Pominiętej Zmiennej (OVB)\n\nWyobraź sobie, że próbujesz zrozumieć, dlaczego niektóre rośliny rosną wyżej niż inne, i mierzysz tylko ilość wody, którą otrzymują. Ale zapomniałeś o świetle słonecznym, które wpływa zarówno na to, ile wody roślina potrzebuje, JAK I na to, jak wysoko urośnie!\nPrzykład z Życia: Edukacja i Dochód * Co widzimy: Więcej edukacji → Wyższy dochód * Co możemy przeoczyć: Naturalny talent/zdolności - Wpływa na to, jak długo ludzie się uczą - Wpływa na to, ile mogą zarabiać * Rezultat: Możemy przeszacować wpływ edukacji\nMatematyka za tym stojąca (nie martw się, to pomoże zwizualizować!): * Prawdziwy obraz: y_i = \\beta_0 + \\beta_1x_i + \\beta_2z_i + \\epsilon_i * Co faktycznie szacujemy: y_i = \\beta_0 + \\beta_1x_i + u_i * Pomyśl o tym jak o przepisie: Jeśli zapomnisz ważnego składnika (z), twoje końcowe danie (y) nie wyjdzie zgodnie z oczekiwaniami!\n\nSymultaniczność (Odwrotna Przyczynowość)\n\nPamiętasz problem jajka i kury? Czasami dwie rzeczy wpływają na siebie jednocześnie. Oto kilka przykładów, które możesz napotkać:\na) Zagadka Godzin Nauki * Czy lepsze oceny prowadzą do dłuższej nauki? * Czy dłuższa nauka prowadzi do lepszych ocen? * W rzeczywistości… obie odpowiedzi! Wzajemnie na siebie wpływają\nb) Efekt Mediów Społecznościowych * Więcej obserwujących → Więcej postów * Więcej postów → Więcej obserwujących * To ciągły cykl!\nc) Cykl Ćwiczenia-Energia * Więcej ćwiczeń → Więcej energii * Więcej energii → Większa chęć do ćwiczeń * Pomyśl o tym jak o dwóch przyjaciołach pchających się nawzajem na huśtawkach!\n\nBłąd Pomiaru\n\nWyobraź sobie próbę zmierzenia swojego wzrostu stojąc na nierównej podłodze - twoje pomiary nie będą całkiem dokładne! Oto jak to się przejawia w rzeczywistości:\nPrzykłady, Które Rozpoznasz: * Samodzielnie Raportowany Czas Nauki - “Uczę się 5 godzin dziennie” może w rzeczywistości oznaczać 3-4 godziny - Utrudnia poznanie prawdziwego wpływu na oceny * Śledzenie w Aplikacji Fitness - Aplikacja pokazuje, że spaliłeś 500 kalorii - W rzeczywistości może to być 400 lub 600 - Wpływa na analizę wpływu ćwiczeń\n\n16.8.24.1 Jak Wykryć Te Problemy we Własnych Badaniach?\n1. W przypadku Pominiętych Zmiennych, Zapytaj: * Co jeszcze mogłoby wpływać na obie zmienne? * Czy nie pomijam oczywistych czynników? * Co powiedzieliby rodzice/przyjaciele, że ma na to wpływ?\n2. W przypadku Symultaniczności, Rozważ: * Czy A może powodować B, czy B może powodować A? * Czy mogą na siebie wpływać? * Co wydarzyło się najpierw? (jeśli można to określić)\n3. W przypadku Błędu Pomiaru, Pomyśl: * Jak dokładne są moje pomiary? * Czy ludzie prawdopodobnie podają prawdziwe informacje? * Co może powodować problemy z pomiarem?\nProste Rozwiązania\n1. Dla Pominiętych Zmiennych:\n\n# Zamiast:\n# simple_model &lt;- lm(grades ~ study_hours)\n\n# Spróbuj:\n# better_model &lt;- lm(grades ~ study_hours + sleep_hours + stress_level + prior_knowledge)\n\n2. Dla Symultaniczności: * Szukaj “zewnętrznych” czynników, które wpływają tylko na jedną zmienną * Rozważ opóźnienia czasowe * Wykorzystuj eksperymenty naturalne, gdy to możliwe\n3. Dla Błędu Pomiaru: * Wykorzystuj wielokrotne pomiary * Znajdź bardziej wiarygodne źródła danych * Uznaj niepewność w swoich wnioskach\n\n\n16.8.24.2 Kluczowe Wnioski\n\nRzeczywistość Jest Złożona\n\nWiększość relacji nie jest prostym A → B\nSzukaj ukrytych czynników\nRozważ dwukierunkowe zależności\n\nZawsze Pytaj\n\n“Co pomijam?”\n“Czy te pominięte zmienne mogą na siebie wpływać?”\n“Jak dobrze mierzę te zmienne?”\n\nPodczas Pisania Prac\n\nOmów potencjalną endogeniczność\nWyjaśnij, jak się do niej odniosłeś\nBądź szczery odnośnie ograniczeń\n\n\n\n\n16.9 Zalecana Literatura\n\n“Mastering Metrics” autorstwa Angrist & Pischke\n“Naked Statistics” autorstwa Charles Wheelan\n\nPamiętaj: W rzeczywistym świecie relacje są zwykle bardziej złożone, niż się początkowo wydaje. Gdy znajdziesz połączenie między A i B, zawsze zadaj sobie pytanie, co jeszcze może może wpływać na zmienne A i B!",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#zalecana-literatura",
    "href": "correg_pl.html#zalecana-literatura",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.9 Zalecana Literatura",
    "text": "16.9 Zalecana Literatura\n\n“Mastering Metrics” autorstwa Angrist & Pischke\n“Naked Statistics” autorstwa Charles Wheelan\n\nPamiętaj: W rzeczywistym świecie relacje są zwykle bardziej złożone, niż się początkowo wydaje. Gdy znajdziesz połączenie między A i B, zawsze zadaj sobie pytanie, co jeszcze może może wpływać na zmienne A i B!",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#regresja-wieloraka",
    "href": "correg_pl.html#regresja-wieloraka",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.10 Regresja Wieloraka (*)",
    "text": "16.10 Regresja Wieloraka (*)\n\n16.10.1 Rozszerzenie do Wielu Predyktorów\nModel regresji wielorakiej rozszerza nasz prosty model o kilka predyktorów:\nModel Populacyjny: Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_kX_k + \\varepsilon\nOszacowanie Próbkowe: \\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\hat{\\beta}_2X_2 + ... + \\hat{\\beta}_kX_k\nStwórzmy przykład z wieloma predyktorami:\n\n# Generate sample data with two predictors\nset.seed(105)\nn &lt;- 100\nX1 &lt;- rnorm(n, mean = 50, sd = 10)\nX2 &lt;- rnorm(n, mean = 20, sd = 5)\nY &lt;- 10 + 0.5*X1 + 0.8*X2 + rnorm(n, 0, 5)\n\ndata_multiple &lt;- data.frame(Y = Y, X1 = X1, X2 = X2)\n\n# Fit multiple regression model\nmodel_multiple &lt;- lm(Y ~ X1 + X2, data = data_multiple)\n\n# Create 3D visualization using scatter plots\np1 &lt;- ggplot(data_multiple, aes(x = X1, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Y vs X1\")\n\np2 &lt;- ggplot(data_multiple, aes(x = X2, y = Y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Y vs X2\")\n\ngrid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nPrzykład Regresji Wielorakiej\n\n\n\n# Print model summary\nsummary(model_multiple)\n\n\nCall:\nlm(formula = Y ~ X1 + X2, data = data_multiple)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8598  -3.6005   0.1166   3.0892  14.6102 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.77567    4.01351   2.934  0.00418 ** \nX1           0.45849    0.05992   7.651 1.47e-11 ***\nX2           0.81639    0.11370   7.180 1.42e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.122 on 97 degrees of freedom\nMultiple R-squared:  0.5062,    Adjusted R-squared:  0.4961 \nF-statistic: 49.72 on 2 and 97 DF,  p-value: 1.367e-15\n\n\n\n\n16.10.2 Interpretacja Współczynników\nW regresji wielorakiej, każdy \\hat{\\beta}_k reprezentuje oczekiwaną zmianę w Y przy jednostkowym wzroście X_k, przy utrzymaniu wszystkich innych zmiennych na stałym poziomie.\n\n# Create prediction grid for X1 (holding X2 at its mean)\nX1_grid &lt;- seq(min(X1), max(X1), length.out = 100)\npred_data_X1 &lt;- data.frame(\n  X1 = X1_grid,\n  X2 = mean(X2)\n)\npred_data_X1$Y_pred &lt;- predict(model_multiple, newdata = pred_data_X1)\n\n# Create prediction grid for X2 (holding X1 at its mean)\nX2_grid &lt;- seq(min(X2), max(X2), length.out = 100)\npred_data_X2 &lt;- data.frame(\n  X1 = mean(X1),\n  X2 = X2_grid\n)\npred_data_X2$Y_pred &lt;- predict(model_multiple, newdata = pred_data_X2)\n\n# Plot partial effects\np3 &lt;- ggplot() +\n  geom_point(data = data_multiple, aes(x = X1, y = Y)) +\n  geom_line(data = pred_data_X1, aes(x = X1, y = Y_pred), \n            color = \"red\", size = 1) +\n  theme_minimal() +\n  labs(title = \"Efekt Cząstkowy X1\",\n       subtitle = paste(\"(X2 utrzymane na średniej =\", round(mean(X2), 2), \")\"))\n\np4 &lt;- ggplot() +\n  geom_point(data = data_multiple, aes(x = X2, y = Y)) +\n  geom_line(data = pred_data_X2, aes(x = X2, y = Y_pred), \n            color = \"red\", size = 1) +\n  theme_minimal() +\n  labs(title = \"Efekt Cząstkowy X2\",\n       subtitle = paste(\"(X1 utrzymane na średniej =\", round(mean(X1), 2), \")\"))\n\ngrid.arrange(p3, p4, ncol = 2)\n\n\n\n\nEfekty Cząstkowe w Regresji Wielorakiej\n\n\n\n\n\n\n16.10.3 Współliniowość\nWspółliniowość występuje, gdy predyktory są silnie skorelowane. Zobaczmy jej efekty:\n\n# Generate data with multicollinearity\nset.seed(106)\nX1_new &lt;- rnorm(n, mean = 50, sd = 10)\nX2_new &lt;- 2*X1_new + rnorm(n, 0, 5)  # X2 silnie skorelowane z X1\nY_new &lt;- 10 + 0.5*X1_new + 0.8*X2_new + rnorm(n, 0, 5)\n\ndata_collinear &lt;- data.frame(Y = Y_new, X1 = X1_new, X2 = X2_new)\n\n# Fit model with multicollinearity\nmodel_collinear &lt;- lm(Y ~ X1 + X2, data = data_collinear)\n\n# Calculate VIF\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nThe following object is masked from 'package:psych':\n\n    logit\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nvif_results &lt;- vif(model_collinear)\n\n# Plot correlation\nggplot(data_collinear, aes(x = X1, y = X2)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Korelacja między Predyktorami\",\n       subtitle = paste(\"Korelacja =\", \n                       round(cor(X1_new, X2_new), 3)))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nEfekty Współliniowości",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#tematy-zaawansowane",
    "href": "correg_pl.html#tematy-zaawansowane",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.11 Tematy Zaawansowane",
    "text": "16.11 Tematy Zaawansowane\n\n16.11.1 Efekty Interakcji\nEfekty interakcji pozwalają na to, by wpływ jednego predyktora zależał od innego:\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3(X_1 \\times X_2) + \\varepsilon\n\n# Generate data with interaction\nset.seed(107)\nX1_int &lt;- rnorm(n, mean = 0, sd = 1)\nX2_int &lt;- rnorm(n, mean = 0, sd = 1)\nY_int &lt;- 1 + 2*X1_int + 3*X2_int + 4*X1_int*X2_int + rnorm(n, 0, 1)\n\ndata_int &lt;- data.frame(X1 = X1_int, X2 = X2_int, Y = Y_int)\nmodel_int &lt;- lm(Y ~ X1 * X2, data = data_int)\n\n# Create interaction plot\nX1_levels &lt;- quantile(X1_int, probs = c(0.25, 0.75))\nX2_seq &lt;- seq(min(X2_int), max(X2_int), length.out = 100)\n\npred_data &lt;- expand.grid(\n  X1 = X1_levels,\n  X2 = X2_seq\n)\npred_data$Y_pred &lt;- predict(model_int, newdata = pred_data)\npred_data$X1_level &lt;- factor(pred_data$X1, \n                            labels = c(\"Niskie X1\", \"Wysokie X1\"))\n\nggplot(pred_data, aes(x = X2, y = Y_pred, color = X1_level)) +\n  geom_line() +\n  theme_minimal() +\n  labs(title = \"Efekt Interakcji\",\n       subtitle = \"Wpływ X2 zależy od poziomu X1\",\n       color = \"Poziom X1\")\n\n\n\n\nWizualizacja Efektów Interakcji\n\n\n\n\n\n\n16.11.2 Wyrazy Wielomianowe\nGdy zależności są nieliniowe, możemy dodać wyrazy wielomianowe:\nY = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\varepsilon\n\n# Generate data with quadratic relationship\nset.seed(108)\nX_poly &lt;- seq(-3, 3, length.out = 100)\nY_poly &lt;- 1 - 2*X_poly + 3*X_poly^2 + rnorm(length(X_poly), 0, 2)\ndata_poly &lt;- data.frame(X = X_poly, Y = Y_poly)\n\n# Fit linear and quadratic models\nmodel_linear &lt;- lm(Y ~ X, data = data_poly)\nmodel_quad &lt;- lm(Y ~ X + I(X^2), data = data_poly)\n\n# Add predictions\ndata_poly$pred_linear &lt;- predict(model_linear)\ndata_poly$pred_quad &lt;- predict(model_quad)\n\n# Plot\nggplot(data_poly, aes(x = X, y = Y)) +\n  geom_point(alpha = 0.5) +\n  geom_line(aes(y = pred_linear, color = \"Liniowy\"), size = 1) +\n  geom_line(aes(y = pred_quad, color = \"Kwadratowy\"), size = 1) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  theme_minimal() +\n  labs(title = \"Dopasowanie Liniowe vs Kwadratowe\",\n       color = \"Typ Modelu\")\n\n\n\n\nPrzykład Regresji Wielomianowej",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#praktyczne-wskazówki-do-analizy-regresji",
    "href": "correg_pl.html#praktyczne-wskazówki-do-analizy-regresji",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.12 Praktyczne Wskazówki do Analizy Regresji",
    "text": "16.12 Praktyczne Wskazówki do Analizy Regresji\n\n16.12.1 Proces Budowy Modelu\n\nEksploracja Danych\n\n\n# Generate example dataset\nset.seed(109)\nn &lt;- 100\ndata_example &lt;- data.frame(\n  x1 = rnorm(n, mean = 50, sd = 10),\n  x2 = rnorm(n, mean = 20, sd = 5),\n  x3 = runif(n, 0, 100)\n)\ndata_example$y &lt;- 10 + 0.5*data_example$x1 + 0.8*data_example$x2 - \n                 0.3*data_example$x3 + rnorm(n, 0, 5)\n\n# Correlation matrix plot\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nggpairs(data_example) +\n  theme_minimal() +\n  labs(title = \"Analiza Eksploracyjna Danych\",\n       subtitle = \"Macierz korelacji i rozkłady\")\n\n\n\n\nPrzykład Eksploracji Danych\n\n\n\n\n\nWybór Zmiennych\n\n\n# Fit models with different variables\nmodel1 &lt;- lm(y ~ x1, data = data_example)\nmodel2 &lt;- lm(y ~ x1 + x2, data = data_example)\nmodel3 &lt;- lm(y ~ x1 + x2 + x3, data = data_example)\n\n# Compare models\nmodels_comparison &lt;- data.frame(\n  Model = c(\"y ~ x1\", \"y ~ x1 + x2\", \"y ~ x1 + x2 + x3\"),\n  R_kwadrat = c(summary(model1)$r.squared,\n                summary(model2)$r.squared,\n                summary(model3)$r.squared),\n  Skorygowany_R_kwadrat = c(summary(model1)$adj.r.squared,\n                    summary(model2)$adj.r.squared,\n                    summary(model3)$adj.r.squared)\n)\n\nknitr::kable(models_comparison, digits = 3,\n             caption = \"Podsumowanie Porównania Modeli\")\n\n\nPodsumowanie Porównania Modeli\n\n\nModel\nR_kwadrat\nSkorygowany_R_kwadrat\n\n\n\n\ny ~ x1\n0.323\n0.316\n\n\ny ~ x1 + x2\n0.433\n0.421\n\n\ny ~ x1 + x2 + x3\n0.893\n0.890\n\n\n\nProces Wyboru Zmiennych\n\n\n\n\n16.12.2 Typowe Pułapki i Rozwiązania\n\nWartości Odstające i Punkty Wpływowe\n\n\n# Create data with outlier\nset.seed(110)\nx_clean &lt;- rnorm(50, mean = 0, sd = 1)\ny_clean &lt;- 2 + 3*x_clean + rnorm(50, 0, 0.5)\ndata_clean &lt;- data.frame(x = x_clean, y = y_clean)\n\n# Add outlier\ndata_outlier &lt;- rbind(data_clean,\n                      data.frame(x = 4, y = -10))\n\n# Fit models\nmodel_clean &lt;- lm(y ~ x, data = data_clean)\nmodel_outlier &lt;- lm(y ~ x, data = data_outlier)\n\n# Plot\nggplot() +\n  geom_point(data = data_clean, aes(x = x, y = y), color = \"blue\") +\n  geom_point(data = data_outlier[51,], aes(x = x, y = y), \n             color = \"red\", size = 3) +\n  geom_line(data = data_clean, \n            aes(x = x, y = predict(model_clean), \n                color = \"Bez Wartości Odstającej\")) +\n  geom_line(data = data_outlier, \n            aes(x = x, y = predict(model_outlier), \n                color = \"Z Wartością Odstającą\")) +\n  theme_minimal() +\n  labs(title = \"Wpływ Wartości Odstających na Regresję\",\n       color = \"Model\") +\n  scale_color_manual(values = c(\"blue\", \"red\"))\n\n\n\n\nIdentyfikacja i Obsługa Wartości Odstających\n\n\n\n\n\nWzorce Brakujących Danych\n\n\n# Create data with missing values\nset.seed(111)\ndata_missing &lt;- data_example\ndata_missing$x1[sample(1:n, 10)] &lt;- NA\ndata_missing$x2[sample(1:n, 15)] &lt;- NA\ndata_missing$x3[sample(1:n, 20)] &lt;- NA\n\n# Visualize missing patterns\nlibrary(naniar)\nvis_miss(data_missing) +\n  theme_minimal() +\n  labs(title = \"Wzorce Brakujących Danych\")\n\n\n\n\nWzorce Brakujących Danych\n\n\n\n\n\nHeteroskedastyczność\n\n\n# Generate heteroscedastic data\nset.seed(112)\nx_hetero &lt;- seq(-3, 3, length.out = 100)\ny_hetero &lt;- 2 + 1.5*x_hetero + rnorm(100, 0, abs(x_hetero)/2)\ndata_hetero &lt;- data.frame(x = x_hetero, y = y_hetero)\n\n# Fit model\nmodel_hetero &lt;- lm(y ~ x, data = data_hetero)\n\n# Plot\np1 &lt;- ggplot(data_hetero, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  labs(title = \"Dane Heteroskedastyczne\")\n\np2 &lt;- ggplot(data_hetero, aes(x = fitted(model_hetero), \n                             y = residuals(model_hetero))) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Wykres Reszt\",\n       x = \"Wartości dopasowane\",\n       y = \"Reszty\")\n\ngrid.arrange(p1, p2, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWykrywanie i Wizualizacja Heteroskedastyczności",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#najlepsze-praktyki",
    "href": "correg_pl.html#najlepsze-praktyki",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.13 Najlepsze Praktyki",
    "text": "16.13 Najlepsze Praktyki\n\n16.13.1 Walidacja Modelu\n\n# Simple cross-validation example\nset.seed(113)\n\n# Create training and test sets\ntrain_index &lt;- sample(1:nrow(data_example), 0.7*nrow(data_example))\ntrain_data &lt;- data_example[train_index, ]\ntest_data &lt;- data_example[-train_index, ]\n\n# Fit model on training data\nmodel_train &lt;- lm(y ~ x1 + x2 + x3, data = train_data)\n\n# Predict on test data\npredictions &lt;- predict(model_train, newdata = test_data)\nactual &lt;- test_data$y\n\n# Calculate performance metrics\nrmse &lt;- sqrt(mean((predictions - actual)^2))\nmae &lt;- mean(abs(predictions - actual))\nr2 &lt;- cor(predictions, actual)^2\n\n# Plot predictions vs actual\ndata_validation &lt;- data.frame(\n  Przewidywane = predictions,\n  Rzeczywiste = actual\n)\n\nggplot(data_validation, aes(x = Rzeczywiste, y = Przewidywane)) +\n  geom_point() +\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Walidacja Modelu: Przewidywane vs Rzeczywiste\",\n       subtitle = sprintf(\"RMSE = %.2f, MAE = %.2f, R² = %.2f\", \n                         rmse, mae, r2))\n\n\n\n\nPrzykład Walidacji Krzyżowej\n\n\n\n\n\n\n16.13.2 Prezentacja Wyników\nPrzykład profesjonalnej tabeli wyników regresji:\n\n# Create regression results table\nlibrary(broom)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nmodel_final &lt;- lm(y ~ x1 + x2 + x3, data = data_example)\nresults &lt;- tidy(model_final, conf.int = TRUE)\n\nkable(results, digits = 3,\n      caption = \"Podsumowanie Wyników Regresji\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nPodsumowanie Wyników Regresji\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n9.116\n2.835\n3.216\n0.002\n3.489\n14.743\n\n\nx1\n0.497\n0.039\n12.756\n0.000\n0.419\n0.574\n\n\nx2\n0.905\n0.086\n10.468\n0.000\n0.734\n1.077\n\n\nx3\n-0.324\n0.016\n-20.322\n0.000\n-0.356\n-0.292",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#podsumowanie-1",
    "href": "correg_pl.html#podsumowanie-1",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.14 Podsumowanie",
    "text": "16.14 Podsumowanie\n\n16.14.1 Kluczowe Wnioski\n\nZawsze zaczynaj od eksploracyjnej analizy danych\nSprawdzaj założenia przed interpretacją wyników\nBądź świadomy typowych pułapek:\n\nWartości odstające\nBrakujące dane\nWspółliniowość\nHeteroskedastyczność\n\nWaliduj swój model używając:\n\nWykresów diagnostycznych\nWalidacji krzyżowej\nAnalizy reszt\n\nPrezentuj wyniki jasno i kompletnie\n\n\n\nLiteratura Uzupełniająca\nDla głębszego zrozumienia:\n\nWooldridge, J.M. “Wprowadzenie do Ekonometrii: Współczesne Ujęcie”\nFox, J. “Analiza Regresji Stosowana i Uogólnione Modele Liniowe”\nAngrist, J.D. i Pischke, J.S. “W Większości Nieszkodliwa Ekonometria”\nStock & Watson “Wprowadzenie do Ekonometrii”",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#dodatek-a.1-obliczanie-kowariancji-oraz-korelacji-pearsona-i-spearmana---przykład-z-obliczeniami",
    "href": "correg_pl.html#dodatek-a.1-obliczanie-kowariancji-oraz-korelacji-pearsona-i-spearmana---przykład-z-obliczeniami",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.15 Dodatek A.1: Obliczanie Kowariancji oraz Korelacji Pearsona i Spearmana - przykład z obliczeniami",
    "text": "16.15 Dodatek A.1: Obliczanie Kowariancji oraz Korelacji Pearsona i Spearmana - przykład z obliczeniami\nDane dotyczące wielkości okręgu wyborczego (\\text{DM}) i indeksu Gallaghera:\n\n\n\n\\text{DM} (X)\nGallagher (Y)\n\n\n\n\n2\n18,2\n\n\n3\n16,7\n\n\n4\n15,8\n\n\n5\n15,3\n\n\n6\n15,0\n\n\n7\n14,8\n\n\n8\n14,7\n\n\n9\n14,6\n\n\n10\n14,55\n\n\n11\n14,52\n\n\n\n\n16.15.1 Krok 1: Obliczanie Podstawowych Statystyk\nObliczanie średnich:\nDla \\text{DM} (X): \\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}\nSzczegółowe obliczenia:\n2 + 3 + 4 + 5 + 6 + 7 + 8 + 9 + 10 + 11 = 65 \\bar{x} = \\frac{65}{10} = 6,5\nDla indeksu Gallaghera (Y): \\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n}\nSzczegółowe obliczenia:\n18,2 + 16,7 + 15,8 + 15,3 + 15,0 + 14,8 + 14,7 + 14,6 + 14,55 + 14,52 = 154,17 \\bar{y} = \\frac{154,17}{10} = 15,417\n\n\n16.15.2 Krok 2: Szczegółowe Obliczenia Kowariancji\nPełna tabela robocza ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n(X_i - \\bar{X})\n(Y_i - \\bar{Y})\n(X_i - \\bar{X})(Y_i - \\bar{Y})\n(X_i - \\bar{X})^2\n(Y_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n-4,5\n2,783\n-12,5235\n20,25\n7,7451\n\n\n2\n3\n16,7\n-3,5\n1,283\n-4,4905\n12,25\n1,6461\n\n\n3\n4\n15,8\n-2,5\n0,383\n-0,9575\n6,25\n0,1467\n\n\n4\n5\n15,3\n-1,5\n-0,117\n0,1755\n2,25\n0,0137\n\n\n5\n6\n15,0\n-0,5\n-0,417\n0,2085\n0,25\n0,1739\n\n\n6\n7\n14,8\n0,5\n-0,617\n-0,3085\n0,25\n0,3807\n\n\n7\n8\n14,7\n1,5\n-0,717\n-1,0755\n2,25\n0,5141\n\n\n8\n9\n14,6\n2,5\n-0,817\n-2,0425\n6,25\n0,6675\n\n\n9\n10\n14,55\n3,5\n-0,867\n-3,0345\n12,25\n0,7517\n\n\n10\n11\n14,52\n4,5\n-0,897\n-4,0365\n20,25\n0,8047\n\n\nSuma\n65\n154,17\n0\n0\n-28,085\n82,5\n12,8442\n\n\n\nObliczanie kowariancji: \\text{Cov}(X,Y) = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\n\\text{Cov}(X,Y) = \\frac{-28,085}{9} = -3,120556\n\n\n16.15.3 Krok 3: Obliczanie Odchylenia Standardowego\nDla \\text{DM} (X): \\sigma_X = \\sqrt{\\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}}\n\\sigma_x = \\sqrt{\\frac{82,5}{9}} = \\sqrt{9,1667} = 3,026582\nDla Gallaghera (Y): \\sigma_Y = \\sqrt{\\frac{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}{n-1}}\n\\sigma_y = \\sqrt{\\frac{12,8442}{9}} = \\sqrt{1,4271} = 1,194612\n\n\n16.15.4 Krok 4: Obliczanie Korelacji Pearsona\nr = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\nr = \\frac{-3,120556}{3,026582 \\times 1,194612} = \\frac{-3,120556}{3,615752} = -0,863044\n\n\n16.15.5 Krok 5: Obliczanie Korelacji Rangowej Spearmana\nPełna tabela rangowa ze wszystkimi obliczeniami:\n\n\n\ni\nX_i\nY_i\nRanga X_i\nRanga Y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n18,2\n1\n10\n-9\n81\n\n\n2\n3\n16,7\n2\n9\n-7\n49\n\n\n3\n4\n15,8\n3\n8\n-5\n25\n\n\n4\n5\n15,3\n4\n7\n-3\n9\n\n\n5\n6\n15,0\n5\n6\n-1\n1\n\n\n6\n7\n14,8\n6\n5\n1\n1\n\n\n7\n8\n14,7\n7\n4\n3\n9\n\n\n8\n9\n14,6\n8\n3\n5\n25\n\n\n9\n10\n14,55\n9\n2\n7\n49\n\n\n10\n11\n14,52\n10\n1\n9\n81\n\n\nSuma\n\n\n\n\n\n330\n\n\n\nObliczanie korelacji Spearmana: \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)}\n\\rho = 1 - \\frac{6 \\times 330}{10(100 - 1)} = 1 - \\frac{1980}{990} = 1 - 2 = -1\n\n\n16.15.6 Krok 6: Weryfikacja w R\n\n# Tworzenie wektorów\nDM &lt;- c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11)\nGH &lt;- c(18.2, 16.7, 15.8, 15.3, 15.0, 14.8, 14.7, 14.6, 14.55, 14.52)\n\n# Obliczanie kowariancji\ncov(DM, GH)\n\n[1] -3.120556\n\n# Obliczanie korelacji\ncor(DM, GH, method = \"pearson\")\n\n[1] -0.8627742\n\ncor(DM, GH, method = \"spearman\")\n\n[1] -1\n\n\n\n\n16.15.7 Krok 7: Podstawowa Wizualizacja\n\nlibrary(ggplot2)\n\n# Tworzenie ramki danych\ndata &lt;- data.frame(DM = DM, GH = GH)\n\n# Tworzenie wykresu rozrzutu\nggplot(data, aes(x = DM, y = GH)) +\n  geom_point(size = 3, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Wielkość Okręgu vs Indeks Gallaghera\",\n    x = \"Wielkość Okręgu (DM)\",\n    y = \"Indeks Gallaghera (GH)\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n16.15.8 Estymacja OLS i Miary Dopasowania Modelu\n\n\n16.15.9 Krok 1: Obliczanie Estymatorów OLS\nKorzystając z wcześniej obliczonych wartości:\n\n\\sum(X_i - \\bar{X})(Y_i - \\bar{Y}) = -28,085\n\\sum(X_i - \\bar{X})^2 = 82,5\n\\bar{X} = 6,5\n\\bar{Y} = 15,417\n\nObliczanie nachylenia (\\hat{\\beta_1}):\n\\hat{\\beta_1} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - \\bar{X})^2}\n\\hat{\\beta_1} = -28,085 ÷ 82,5 = -0,3404\nObliczanie wyrazu wolnego (\\hat{\\beta_0}): \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\n\\hat{\\beta_0} = 15,417 - (-0,3404 × 6,5)\n   = 15,417 + 2,2126\n   = 17,6296\nZatem równanie regresji OLS ma postać: \\hat{Y} = 17,6296 - 0,3404X\n\n\n16.15.10 Krok 2: Obliczanie Wartości Dopasowanych i Reszt\nPełna tabela ze wszystkimi obliczeniami:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nY_i\n\\hat{Y}_i\ne_i = Y_i - \\hat{Y}_i\ne_i^2\n(Y_i - \\bar{Y})^2\n(\\hat{Y}_i - \\bar{Y})^2\n\n\n\n\n1\n2\n18,2\n16,9488\n1,2512\n1,5655\n7,7451\n2,3404\n\n\n2\n3\n16,7\n16,6084\n0,0916\n0,0084\n1,6461\n1,4241\n\n\n3\n4\n15,8\n16,2680\n-0,4680\n0,2190\n0,1467\n0,7225\n\n\n4\n5\n15,3\n15,9276\n-0,6276\n0,3939\n0,0137\n0,2601\n\n\n5\n6\n15,0\n15,5872\n-0,5872\n0,3448\n0,1739\n0,0289\n\n\n6\n7\n14,8\n15,2468\n-0,4468\n0,1996\n0,3807\n0,0290\n\n\n7\n8\n14,7\n14,9064\n-0,2064\n0,0426\n0,5141\n0,2610\n\n\n8\n9\n14,6\n14,5660\n0,0340\n0,0012\n0,6675\n0,7241\n\n\n9\n10\n14,55\n14,2256\n0,3244\n0,1052\n0,7517\n1,4184\n\n\n10\n11\n14,52\n13,8852\n0,6348\n0,4030\n0,8047\n2,3439\n\n\nSuma\n65\n154,17\n154,17\n0\n3,2832\n12,8442\n9,5524\n\n\n\nObliczenia dla wartości dopasowanych:\nDla X = 2:\nŶ = 17,6296 + (-0,3404 × 2) = 16,9488\n\nDla X = 3:\nŶ = 17,6296 + (-0,3404 × 3) = 16,6084\n\n[... kontynuacja dla wszystkich wartości]\n\n\n16.15.11 Krok 3: Obliczanie Miar Dopasowania\nSuma kwadratów reszt (SSE): SSE = \\sum e_i^2\nSSE = 3,2832\nCałkowita suma kwadratów (SST): SST = \\sum(Y_i - \\bar{Y})^2\nSST = 12,8442\nSuma kwadratów regresji (SSR): SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\nSSR = 9,5524\nWeryfikacja dekompozycji: SST = SSR + SSE\n12,8442 = 9,5524 + 3,2832 (w granicach błędu zaokrąglenia)\nObliczanie współczynnika determinacji R-kwadrat: R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}\nR² = 9,5524 ÷ 12,8442\n   = 0,7438\n\n\n16.15.12 Krok 4: Weryfikacja w R\n\n# Dopasowanie modelu liniowego\nmodel &lt;- lm(GH ~ DM, data = data)\n\n# Podsumowanie statystyk\nsummary(model)\n\n\nCall:\nlm(formula = GH ~ DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.62764 -0.46274 -0.08615  0.26624  1.25109 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.62976    0.50121  35.174 4.67e-10 ***\nDM          -0.34042    0.07053  -4.827  0.00131 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6406 on 8 degrees of freedom\nMultiple R-squared:  0.7444,    Adjusted R-squared:  0.7124 \nF-statistic:  23.3 on 1 and 8 DF,  p-value: 0.00131\n\n# Ręczne obliczenie R-kwadrat\nSST &lt;- sum((GH - mean(GH))^2)\nSSE &lt;- sum(residuals(model)^2)\nSSR &lt;- SST - SSE\nR2_manual &lt;- SSR/SST\nR2_manual\n\n[1] 0.7443793\n\n\n\n\n16.15.13 Krok 5: Analiza Reszt\n\n# Tworzenie wykresów reszt\npar(mfrow = c(2, 2))\nplot(model)\n\n\n\n\n\n\n\n\n\n\n16.15.14 Krok 6: Wykres Wartości Przewidywanych vs Rzeczywistych\n\n# Tworzenie wykresu wartości przewidywanych vs rzeczywistych\nggplot(data.frame(\n  Rzeczywiste = GH,\n  Przewidywane = fitted(model)\n), aes(x = Przewidywane, y = Rzeczywiste)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Wartości Przewidywane vs Rzeczywiste\",\n    x = \"Przewidywany Indeks Gallaghera\",\n    y = \"Rzeczywisty Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n16.15.15 Modele z Transformacją Logarytmiczną\n\n\n16.15.16 Krok 1: Transformacja Danych\nNajpierw obliczamy logarytmy naturalne zmiennych:\n\n\n\ni\nX_i\nY_i\n\\ln(X_i)\n\\ln(Y_i)\n\n\n\n\n1\n2\n18,2\n0,6931\n2,9014\n\n\n2\n3\n16,7\n1,0986\n2,8154\n\n\n3\n4\n15,8\n1,3863\n2,7600\n\n\n4\n5\n15,3\n1,6094\n2,7278\n\n\n5\n6\n15,0\n1,7918\n2,7081\n\n\n6\n7\n14,8\n1,9459\n2,6946\n\n\n7\n8\n14,7\n2,0794\n2,6878\n\n\n8\n9\n14,6\n2,1972\n2,6810\n\n\n9\n10\n14,55\n2,3026\n2,6777\n\n\n10\n11\n14,52\n2,3979\n2,6757\n\n\n\n\n\n16.15.17 Krok 2: Porównanie Różnych Specyfikacji Modelu\nSzacujemy trzy alternatywne specyfikacje:\n\nModel log-liniowy: \\ln(Y_i) = \\beta_0 + \\beta_1 X_i + \\epsilon_i\nModel liniowo-logarytmiczny: Y_i = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\nModel log-log: \\ln(Y_i) = \\beta_0 + \\beta_1\\ln(X_i) + \\epsilon_i\n\n\n# Tworzenie zmiennych transformowanych\ndata$log_DM &lt;- log(data$DM)\ndata$log_GH &lt;- log(data$GH)\n\n# Dopasowanie modeli\nmodel_linear &lt;- lm(GH ~ DM, data = data)\nmodel_loglinear &lt;- lm(log_GH ~ DM, data = data)\nmodel_linearlog &lt;- lm(GH ~ log_DM, data = data)\nmodel_loglog &lt;- lm(log_GH ~ log_DM, data = data)\n\n# Porównanie wartości R-kwadrat\nmodels_comparison &lt;- data.frame(\n  Model = c(\"Liniowy\", \"Log-liniowy\", \"Liniowo-logarytmiczny\", \"Log-log\"),\n  R_kwadrat = c(\n    summary(model_linear)$r.squared,\n    summary(model_loglinear)$r.squared,\n    summary(model_linearlog)$r.squared,\n    summary(model_loglog)$r.squared\n  )\n)\n\n# Wyświetlenie porównania\nmodels_comparison\n\n                  Model R_kwadrat\n1               Liniowy 0.7443793\n2           Log-liniowy 0.7670346\n3 Liniowo-logarytmiczny 0.9141560\n4               Log-log 0.9288088\n\n\n\n\n16.15.18 Krok 3: Porównanie Wizualne\n\n# Tworzenie wykresów dla każdego modelu\np1 &lt;- ggplot(data, aes(x = DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowy\") +\n  theme_minimal()\n\np2 &lt;- ggplot(data, aes(x = DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-liniowy\") +\n  theme_minimal()\n\np3 &lt;- ggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Liniowo-logarytmiczny\") +\n  theme_minimal()\n\np4 &lt;- ggplot(data, aes(x = log_DM, y = log_GH)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Model Log-log\") +\n  theme_minimal()\n\n# Układanie wykresów w siatkę\nlibrary(gridExtra)\ngrid.arrange(p1, p2, p3, p4, ncol = 2)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n16.15.19 Krok 4: Analiza Reszt dla Najlepszego Modelu\nNa podstawie wartości R-kwadrat, analiza reszt dla najlepiej dopasowanego modelu:\n\n# Wykresy reszt dla najlepszego modelu\npar(mfrow = c(2, 2))\nplot(model_linearlog)\n\n\n\n\n\n\n\n\n\n\n16.15.20 Krok 5: Interpretacja Najlepszego Modelu\nWspółczynniki modelu liniowo-logarytmicznego:\n\nsummary(model_linearlog)\n\n\nCall:\nlm(formula = GH ~ log_DM, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40702 -0.30207 -0.04907  0.22905  0.60549 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  19.0223     0.4079   46.64 4.94e-11 ***\nlog_DM       -2.0599     0.2232   -9.23 1.54e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3712 on 8 degrees of freedom\nMultiple R-squared:  0.9142,    Adjusted R-squared:  0.9034 \nF-statistic: 85.19 on 1 and 8 DF,  p-value: 1.539e-05\n\n\nInterpretacja: - \\hat{\\beta_0} reprezentuje oczekiwany Indeks Gallaghera, gdy ln(DM) = 0 (czyli gdy DM = 1) - \\hat{\\beta_1} reprezentuje zmianę Indeksu Gallaghera związaną z jednostkowym wzrostem ln(DM)\n\n\n16.15.21 Krok 6: Predykcje Modelu\n\n# Tworzenie wykresu predykcji dla najlepszego modelu\nggplot(data, aes(x = log_DM, y = GH)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Model Liniowo-logarytmiczny: Indeks Gallaghera vs ln(Wielkość Okręgu)\",\n    x = \"ln(Wielkość Okręgu)\",\n    y = \"Indeks Gallaghera\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n16.15.22 Krok 7: Analiza Elastyczności\nDla modelu log-log współczynniki bezpośrednio reprezentują elastyczności. Obliczenie średniej elastyczności dla modelu liniowo-logarytmicznego:\n\n# Obliczenie elastyczności przy wartościach średnich\nmean_DM &lt;- mean(data$DM)\nmean_GH &lt;- mean(data$GH)\nbeta1 &lt;- coef(model_linearlog)[2]\nelastycznosc &lt;- beta1 * (1/mean_GH)\nelastycznosc\n\n    log_DM \n-0.1336136 \n\n\nWartość ta reprezentuje procentową zmianę Indeksu Gallaghera przy jednoprocentowej zmianie Wielkości Okręgu.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#dodatek-a.2-porównanie-popularnych-miar-korelacji-pearson-spearman-i-kendall",
    "href": "correg_pl.html#dodatek-a.2-porównanie-popularnych-miar-korelacji-pearson-spearman-i-kendall",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.16 Dodatek A.2: Porównanie Popularnych Miar Korelacji: Pearson, Spearman i Kendall",
    "text": "16.16 Dodatek A.2: Porównanie Popularnych Miar Korelacji: Pearson, Spearman i Kendall\n\n16.16.1 Zbiór Danych\n\ndane &lt;- data.frame(\n  x = c(2, 4, 5, 3, 8),\n  y = c(3, 5, 4, 4, 7)\n)\n\n\n\n16.16.2 Korelacja Pearsona\n r = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2 \\sum (y_i - \\bar{y})^2}} \n\n16.16.2.1 Obliczenia krok po kroku:\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nx_i\ny_i\nx_i - \\bar{x}\ny_i - \\bar{y}\n(x_i - \\bar{x})(y_i - \\bar{y})\n(x_i - \\bar{x})^2\n(y_i - \\bar{y})^2\n\n\n\n\n1\n2\n3\n-2,4\n-1,6\n3,84\n5,76\n2,56\n\n\n2\n4\n5\n-0,4\n0,4\n-0,16\n0,16\n0,16\n\n\n3\n5\n4\n0,6\n-0,6\n-0,36\n0,36\n0,36\n\n\n4\n3\n4\n-1,4\n-0,6\n0,84\n1,96\n0,36\n\n\n5\n8\n7\n3,6\n2,4\n8,64\n12,96\n5,76\n\n\nSuma\n22\n23\n0\n0\n12,8\n21,2\n9,2\n\n\n\n\\bar{x} = 4,4 \\bar{y} = 4,6\n r = \\frac{12,8}{\\sqrt{21,2 \\times 9,2}} = \\frac{12,8}{\\sqrt{195,04}} = \\frac{12,8}{13,97} = 0,92 \n\n\n\n16.16.3 Korelacja Spearmana\n \\rho = 1 - \\frac{6\\sum d_i^2}{n(n^2-1)} \n\n16.16.3.1 Obliczenia krok po kroku:\n\n\n\ni\nx_i\ny_i\nRanga x_i\nRanga y_i\nd_i\nd_i^2\n\n\n\n\n1\n2\n3\n1\n1\n0\n0\n\n\n2\n4\n5\n3\n5\n-2\n4\n\n\n3\n5\n4\n4\n2,5\n1,5\n2,25\n\n\n4\n3\n4\n2\n2,5\n-0,5\n0,25\n\n\n5\n8\n7\n5\n4\n1\n1\n\n\nSuma\n\n\n\n\n\n7,5\n\n\n\n \\rho = 1 - \\frac{6(7,5)}{5(25-1)} = 1 - \\frac{45}{120} = 0,82 \n\n\n\n16.16.4 Tau Kendalla\n \\tau = \\frac{\\text{liczba par zgodnych} - \\text{liczba par niezgodnych}}{\\frac{1}{2}n(n-1)} \n\n16.16.4.1 Obliczenia krok po kroku:\n\n\n\nPara (i,j)\nx_i,x_j\ny_i,y_j\nx_j-x_i\ny_j-y_i\nWynik\n\n\n\n\n(1,2)\n2,4\n3,5\n+2\n+2\nZ\n\n\n(1,3)\n2,5\n3,4\n+3\n+1\nZ\n\n\n(1,4)\n2,3\n3,4\n+1\n+1\nZ\n\n\n(1,5)\n2,8\n3,7\n+6\n+4\nZ\n\n\n(2,3)\n4,5\n5,4\n+1\n-1\nN\n\n\n(2,4)\n4,3\n5,4\n-1\n-1\nZ\n\n\n(2,5)\n4,8\n5,7\n+4\n+2\nZ\n\n\n(3,4)\n5,3\n4,4\n-2\n0\nN\n\n\n(3,5)\n5,8\n4,7\n+3\n+3\nZ\n\n\n(4,5)\n3,8\n4,7\n+5\n+3\nZ\n\n\n\nLiczba par zgodnych = 8 Liczba par niezgodnych = 2  \\tau = \\frac{8-2}{10} = 0,74 \n\n\n\n16.16.5 Weryfikacja w R\n\ncat(\"Pearson:\", round(cor(dane$x, dane$y, method=\"pearson\"), 2), \"\\n\")\n\nPearson: 0.92 \n\ncat(\"Spearman:\", round(cor(dane$x, dane$y, method=\"spearman\"), 2), \"\\n\")\n\nSpearman: 0.82 \n\ncat(\"Kendall:\", round(cor(dane$x, dane$y, method=\"kendall\"), 2), \"\\n\")\n\nKendall: 0.74 \n\n\n\n\n16.16.6 Interpretacja Wyników\n\nKorelacja Pearsona (r = 0,92)\n\nSilna dodatnia korelacja liniowa\nWskazuje na bardzo silny liniowy związek między zmiennymi\n\nKorelacja Spearmana (ρ = 0,82)\n\nRównież silna dodatnia korelacja\nNieco niższa niż Pearsona, co sugeruje pewne odchylenia od monotoniczności\n\nTau Kendalla (τ = 0,74)\n\nNajniższa z trzech wartości, ale wciąż wskazuje na silną zależność\nBardziej odporna na wartości odstające\n\n\n\n\n16.16.7 Porównanie Miar\n\nRóżnice w wartościach:\n\nPearson (0,92) - najwyższa wartość, silna liniowość\nSpearman (0,82) - uwzględnia tylko uporządkowanie\nKendall (0,74) - najbardziej konserwatywna miara\n\nPraktyczne zastosowanie:\n\nWszystkie miary potwierdzają silną dodatnią zależność\nRóżnice między miarami wskazują na nieznaczne odchylenia od idealnej liniowości\nKendall daje najbardziej ostrożną ocenę siły związku",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-b.",
    "href": "correg_pl.html#appendix-b.",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.17 Appendix B.",
    "text": "16.17 Appendix B.\n(…)",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#appendix-c.-przykłady",
    "href": "correg_pl.html#appendix-c.-przykłady",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.18 Appendix C. Przykłady",
    "text": "16.18 Appendix C. Przykłady",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "href": "correg_pl.html#descriptive-statistics-and-ols-example---income-and-voter-turnout",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.19 Descriptive Statistics and OLS Example - Income and Voter Turnout",
    "text": "16.19 Descriptive Statistics and OLS Example - Income and Voter Turnout\nBackground\nIn preparation for the 2024 municipal elections, the Amsterdam Electoral Commission conducted research on voter participation patterns across different city neighborhoods. A key question emerged:\nDoes economic prosperity of a neighborhood correlate with civic engagement, specifically voter turnout?\nData Collection\nSample: 5 representative neighborhoods in Amsterdam\nTime Period: Data from the 2022 municipal elections\nVariables:\n\nIncome: Average annual household income per capita (thousands €)\nTurnout: Percentage of registered voters who voted in the election\n\n\n16.19.1 Initial R Output for Reference\n\n# Data\nincome &lt;- c(50, 45, 56, 40, 60)  # thousands €\nturnout &lt;- c(60, 56, 70, 50, 75) # %\n\n# Full model check\nmodel &lt;- lm(turnout ~ income)\nsummary(model)\n\n\nCall:\nlm(formula = turnout ~ income)\n\nResiduals:\n      1       2       3       4       5 \n-1.9486  0.3359  0.5100  0.6204  0.4824 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.89647    3.96731  -0.226 0.835748    \nincome       1.25690    0.07822  16.068 0.000524 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.263 on 3 degrees of freedom\nMultiple R-squared:  0.9885,    Adjusted R-squared:  0.9847 \nF-statistic: 258.2 on 1 and 3 DF,  p-value: 0.0005243\n\n\n\n\n16.19.2 Dispersion Measures\nMeans:\n\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n} = \\frac{50 + 45 + 56 + 40 + 60}{5} = \\frac{251}{5} = 50.2\n\\bar{Y} = \\frac{\\sum_{i=1}^n Y_i}{n} = \\frac{60 + 56 + 70 + 50 + 75}{5} = \\frac{311}{5} = 62.2\n\n# Verification\nmean(income)  # 50.2\n\n[1] 50.2\n\nmean(turnout) # 62.2\n\n[1] 62.2\n\n\nVariances:\ns^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X: (-0.2, -5.2, 5.8, -10.2, 9.8)\ns^2_X = \\frac{0.04 + 27.04 + 33.64 + 104.04 + 96.04}{4} = \\frac{260.8}{4} = 65.2\nDeviations for Y: (-2.2, -6.2, 7.8, -12.2, 12.8)\ns^2_Y = \\frac{4.84 + 38.44 + 60.84 + 148.84 + 163.84}{4} = \\frac{416.8}{4} = 104.2\n\n# Verification\nvar(income)  # 65.2\n\n[1] 65.2\n\nvar(turnout) # 104.2\n\n[1] 104.2\n\n\n\n\n16.19.3 Covariance and Correlation\nCovariance:\ns_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-0.2 \\times -2.2) = 0.44 (-5.2 \\times -6.2) = 32.24 (5.8 \\times 7.8) = 45.24 (-10.2 \\times -12.2) = 124.44 (9.8 \\times 12.8) = 125.44\ns_{XY} = \\frac{327.8}{4} = 81.95\n\n# Verification\ncov(income, turnout) # 81.95\n\n[1] 81.95\n\n\nCorrelation:\nr_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{81.95}{\\sqrt{65.2}\\sqrt{104.2}} = 0.994\n\n# Verification\ncor(income, turnout) # 0.994\n\n[1] 0.9942402\n\n\n\n\n16.19.4 OLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient:\n\\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{81.95}{65.2} = 1.2571429\nIntercept:\n\\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X}\nStep by step:\n\n1.2571429 \\times 50.2 = 63.1085714\n\\hat{\\beta_0} = 62.2 - 63.1085714 = -0.9085714\n\n\n# Verification\ncoef(model)  # Exact coefficients from R\n\n(Intercept)      income \n -0.8964724   1.2569018 \n\n\n\n\n16.19.5 Detailed Decomposition of Variance and R-squared\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = -0.9085714 + 1.2571429X\nThe predicted values \\hat{Y} for each X value:\nFor X = 50:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (50)\n \\hat{Y} = -0.9085714 + 62.857145 \\hat{Y} = 61.9485736\nFor X = 45:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (45)\n \\hat{Y} = -0.9085714 + 56.5714305 \\hat{Y} = 55.6535591\nFor X = 56:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (56)\n \\hat{Y} = -0.9085714 + 70.4200024 \\hat{Y} = 69.5114310\nFor X = 40:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (40)\n \\hat{Y} = -0.9085714 + 50.2657160 \\hat{Y} = 49.3571446\nFor X = 60:\n\n\\hat{Y} = -0.9085714 + 1.2571429 \\times (60)\n \\hat{Y} = -0.9085714 + 75.4285740 \\hat{Y} = 74.5200026\n\n# Verification of predicted values\ny_hat &lt;- -0.9085714 + 1.2571429 * income\ndata.frame(\n  X = income,\n  Y = turnout,\n  Y_hat = y_hat,\n  row.names = 1:5\n)\n\n   X  Y    Y_hat\n1 50 60 61.94857\n2 45 56 55.66286\n3 56 70 69.49143\n4 40 50 49.37714\n5 60 75 74.52000\n\n\nStep 2: Calculate SST (Total Sum of Squares)\nSST = \\sum(Y_i - \\bar{Y})^2 \\text{ where } \\bar{Y} = 62.2\n(60 - 62.2)^2 = (-2.2)^2 = 4.84 (56 - 62.2)^2 = (-6.2)^2 = 38.44 (70 - 62.2)^2 = (7.8)^2 = 60.84 (50 - 62.2)^2 = (-12.2)^2 = 148.84 (75 - 62.2)^2 = (12.8)^2 = 163.84\nSST = 4.84 + 38.44 + 60.84 + 148.84 + 163.84 = 416.8\nStep 3: Calculate SSR (Regression Sum of Squares)\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2\n(61.9485736 - 62.2)^2 = (-0.2514264)^2 = 0.0632151 (55.6535591 - 62.2)^2 = (-6.5464409)^2 = 42.8558689 (69.5114310 - 62.2)^2 = (7.3114310)^2 = 53.4570178 (49.3571446 - 62.2)^2 = (-12.8428554)^2 = 164.9389370 (74.5200026 - 62.2)^2 = (12.3200026)^2 = 151.7824640\nSSR = 413.0975028\nStep 4: Calculate SSE (Error Sum of Squares)\nSSE = \\sum(Y_i - \\hat{Y}_i)^2\n(60 - 61.9485736)^2 = (-1.9485736)^2 = 3.7969384 (56 - 55.6535591)^2 = (0.3464409)^2 = 0.1200212 (70 - 69.5114310)^2 = (0.4885690)^2 = 0.2387198 (50 - 49.3571446)^2 = (0.6428554)^2 = 0.4132631 (75 - 74.5200026)^2 = (0.4799974)^2 = 0.2303975\nSSE = 4.7024972\nStep 5: Verify decomposition\nSST = SSR + SSE 416.8 = 413.0975028 + 4.7024972\nStep 6: Calculate R-squared\nR^2 = \\frac{SSR}{SST} = \\frac{413.0975028}{416.8} = 0.9916\n\n# Verification\nsummary(model)$r.squared  # Should match our calculation\n\n[1] 0.9885135\n\n\n\n\n16.19.6 Visualization\n\nlibrary(ggplot2)\ndf &lt;- data.frame(income = income, turnout = turnout)\n\nggplot(df, aes(x = income, y = turnout)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Voter Turnout vs Income per Capita\",\n    x = \"Income per Capita (thousands €)\",\n    y = \"Voter Turnout (%)\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    text = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\n16.19.7 Interpretation\nThe analysis shows:\n\nA very strong positive correlation (r = 0.994) between income and voter turnout\nThe regression equation \\hat{Y} = -0.9085714 + 1.2571429X indicates that:\n\nFor each €1,000 increase in income, turnout increases by about 1.26 percentage points\nThe intercept (-0.9086) has little practical meaning as income is never zero\n\nThe R-squared of 0.9916 indicates that 99.16% of the variance in turnout is explained by income",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "href": "correg_pl.html#anxiety-levels-and-cognitive-performance-a-laboratory-study",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.20 Anxiety Levels and Cognitive Performance: A Laboratory Study",
    "text": "16.20 Anxiety Levels and Cognitive Performance: A Laboratory Study\n\n16.20.1 Data and Context\nIn a psychology experiment, researchers measured the relationship between anxiety levels (measured by galvanic skin response, GSR) and cognitive performance (score on a working memory task).\n\n# Data\nanxiety &lt;- c(2.1, 3.4, 4.2, 5.1, 5.8, 6.4, 7.2, 8.0)  # GSR readings\nperformance &lt;- c(92, 88, 84, 78, 74, 70, 65, 62)      # Working memory scores\n\n# Initial model check\nmodel &lt;- lm(performance ~ anxiety)\nsummary(model)\n\n\nCall:\nlm(formula = performance ~ anxiety)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8993 -0.6660  0.2162  0.6106  1.5262 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 105.3248     1.3189   79.86 2.60e-10 ***\nanxiety      -5.4407     0.2359  -23.06 4.35e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 6 degrees of freedom\nMultiple R-squared:  0.9888,    Adjusted R-squared:  0.987 \nF-statistic: 531.9 on 1 and 6 DF,  p-value: 4.355e-07\n\n\n\n\n16.20.2 Descriptive Statistics\nMeans: \\bar{X} = \\frac{2.1 + 3.4 + 4.2 + 5.1 + 5.8 + 6.4 + 7.2 + 8.0}{8} = \\frac{42.2}{8} = 5.275\n\\bar{Y} = \\frac{92 + 88 + 84 + 78 + 74 + 70 + 65 + 62}{8} = \\frac{613}{8} = 76.625\n\n# Verification\nmean(anxiety)\n\n[1] 5.275\n\nmean(performance)\n\n[1] 76.625\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(2.1 - 5.275) = -3.175\n(3.4 - 5.275) = -1.875\n(4.2 - 5.275) = -1.075\n(5.1 - 5.275) = -0.175\n(5.8 - 5.275) = 0.525\n(6.4 - 5.275) = 1.125\n(7.2 - 5.275) = 1.925\n(8.0 - 5.275) = 2.725\n\nSquared deviations:\n10.08063 + 3.51563 + 1.15563 + 0.03063 + 0.27563 + 1.26563 + 3.70563 +\n7.42563 = 27.45500\ns^2_X = \\frac{27.45500}{7} = 3.922143\nSimilarly for Y: Deviations:\n15.375, 11.375, 7.375, 1.375, -2.625, -6.625, -11.625, -14.625\ns^2_Y = \\frac{236.875 + 129.391 + 54.391 + 1.891 + 6.891 + 43.891 + 135.141 + 213.891}{7} = \\frac{822.362}{7} = 117.4803\n\n# Verification\nvar(anxiety)\n\n[1] 3.922143\n\nvar(performance)\n\n[1] 117.4107\n\n\n\n\n16.20.3 Covariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n(-3.175 × 15.375) = -48.815625\n(-1.875 × 11.375) = -21.328125\n(-1.075 × 7.375) = -7.928125\n(-0.175 × 1.375) = -0.240625\n(0.525 × -2.625) = -1.378125\n(1.125 × -6.625) = -7.453125\n(1.925 × -11.625) = -22.378125\n(2.725 × -14.625) = -39.853125\nSum = -149.375\ns_{XY} = \\frac{-149.375}{7} = -21.33929\n\n# Verification\ncov(anxiety, performance)\n\n[1] -21.33929\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-21.33929}{\\sqrt{3.922143}\\sqrt{117.4803}} = -0.9932\n\n# Verification\ncor(anxiety, performance)\n\n[1] -0.9944073\n\n\n\n\n16.20.4 OLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-21.33929}{3.922143} = -5.4407\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-5.4407 × 5.275 = -28.6997\n\\hat{\\beta_0} = 76.625 - (-28.6997) = 105.3247\n\n\n# Verification\ncoef(model)\n\n(Intercept)     anxiety \n 105.324804   -5.440721 \n\n\n\n\n16.20.5 4. R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}): \\hat{Y} = 105.3247 - 5.4407X\n\n# Predicted values\ny_hat &lt;- 105.3247 - 5.4407 * anxiety\ndata.frame(\n  Anxiety = anxiety,\n  Performance = performance,\n  Predicted = y_hat,\n  row.names = 1:8\n)\n\n  Anxiety Performance Predicted\n1     2.1          92  93.89923\n2     3.4          88  86.82632\n3     4.2          84  82.47376\n4     5.1          78  77.57713\n5     5.8          74  73.76864\n6     6.4          70  70.50422\n7     7.2          65  66.15166\n8     8.0          62  61.79910\n\n\nStep 2: Sum of Squares\nSST = \\sum(Y_i - \\bar{Y})^2 = 822.362\nSSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 816.3094\nSSE = \\sum(Y_i - \\hat{Y}_i)^2 = 6.0526\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{816.3094}{822.362} = 0.9926\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.9888459\n\n\n\n\n16.20.6 Visualization\n\nlibrary(ggplot2)\n\nggplot(data.frame(anxiety, performance), aes(x = anxiety, y = performance)) +\n  geom_point(color = \"blue\", size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Cognitive Performance vs. Anxiety Levels\",\n    x = \"Anxiety (GSR)\",\n    y = \"Performance Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n16.20.7 Interpretation\n\nStrong negative correlation (r = -0.993) between anxiety and cognitive performance\nFor each unit increase in GSR (anxiety), performance decreases by approximately 5.44 points\nThe model explains 99.26% of the variance in performance scores\nThe relationship appears to be strongly linear, suggesting a reliable anxiety-performance relationship\nThe high intercept (105.32) represents the theoretical maximum performance at zero anxiety\n\n\n\n16.20.8 Study Limitations\n\nSmall sample size (n=8)\nPossible other confounding variables\nLimited range of anxiety levels\nCross-sectional rather than longitudinal data",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "correg_pl.html#district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "href": "correg_pl.html#district-magnitude-and-electoral-disproportionality-a-comparative-analysis",
    "title": "16  Wprowadzenie do Analizy Korelacji i Regresji",
    "section": "16.21 District Magnitude and Electoral Disproportionality: A Comparative Analysis",
    "text": "16.21 District Magnitude and Electoral Disproportionality: A Comparative Analysis\n\n16.21.1 Data Generating Process\nLet’s set up a DGP where:\n\\begin{aligned}\n& Y_{\\text{Gallagher}} = 12 - 0.8X_{\\text{DM}} + \\varepsilon \\\\\n& \\varepsilon \\sim \\mathcal{N}(0, 1) \\\\\n& X_{\\text{DM}} \\in \\{3, 5, 7, 10, 12, 15\\}\n\\end{aligned}\n\n# DGP\nmagnitude &lt;- c(3, 5, 7, 10, 12, 15)\nepsilon &lt;- rnorm(6, mean = 0, sd = 1)\ngallagher &lt;- 12 - 0.8 * magnitude + epsilon\n\n# Round (sampled from the DGP) Gallagher indices to one decimal place\ngallagher &lt;- round(c(9.0, 7.8, 9.2, 4.1, 2.5, 1.7), 1)\n\n# Show data\ndata.frame(\n  District_Magnitude = magnitude,\n  Gallagher_Index = gallagher\n)\n\n  District_Magnitude Gallagher_Index\n1                  3             9.0\n2                  5             7.8\n3                  7             9.2\n4                 10             4.1\n5                 12             2.5\n6                 15             1.7\n\n# Initial model check\nmodel &lt;- lm(gallagher ~ magnitude)\nsummary(model)\n\n\nCall:\nlm(formula = gallagher ~ magnitude)\n\nResiduals:\n      1       2       3       4       5       6 \n-0.6516 -0.4628  2.3260 -0.6908 -0.9020  0.3813 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.7349     1.3034   9.003 0.000843 ***\nmagnitude    -0.6944     0.1359  -5.110 0.006934 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.368 on 4 degrees of freedom\nMultiple R-squared:  0.8672,    Adjusted R-squared:  0.834 \nF-statistic: 26.11 on 1 and 4 DF,  p-value: 0.006934\n\n\n\n\n16.21.2 Descriptive Statistics\nMeans: \\bar{X} = \\frac{3 + 5 + 7 + 10 + 12 + 15}{6} = \\frac{52}{6} = 8.6667\n\\bar{Y} = \\frac{9.0 + 7.8 + 9.2 + 4.1 + 2.5 + 1.7}{6} = \\frac{34.3}{6} = 5.7167\n\n# Verification\nmean(magnitude)\n\n[1] 8.666667\n\nmean(gallagher)\n\n[1] 5.716667\n\n\nVariances: s^2_X = \\frac{\\sum(X_i - \\bar{X})^2}{n-1}\nDeviations for X:\n\n(3 - 8.6667) = -5.6667\n(5 - 8.6667) = -3.6667\n(7 - 8.6667) = -1.6667\n(10 - 8.6667) = 1.3333\n(12 - 8.6667) = 3.3333\n(15 - 8.6667) = 6.3333\n\nSquared deviations:\n\n32.1115 + 13.4445 + 2.7779 + 1.7777 + 11.1109 + 40.1107 = 101.3332\n\ns^2_X = \\frac{101.3332}{5} = 20.2666\nFor Y: Deviations: 3.2833, 2.0833, 3.4833, -1.6167, -3.2167, -4.0167\ns^2_Y = \\frac{56.3483}{5} = 11.2697\n\n# Verification\nvar(magnitude)\n\n[1] 20.26667\n\nvar(gallagher)\n\n[1] 11.26967\n\n\n\n\n16.21.3 Covariance and Correlation\nCovariance: s_{XY} = \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\nProducts of deviations:\n\n(-5.6667 × 3.2833) = -18.6057\n(-3.6667 × 2.0833) = -7.6387\n(-1.6667 × 3.4833) = -5.8056\n(1.3333 × -1.6167) = -2.1556\n(3.3333 × -3.2167) = -10.7223\n(6.3333 × -4.0167) = -25.4391\n\nSum = -70.3670\ns_{XY} = \\frac{-70.3670}{5} = -14.0734\n\n# Verification\ncov(magnitude, gallagher)\n\n[1] -14.07333\n\n\nCorrelation: r_{XY} = \\frac{s_{XY}}{\\sqrt{s^2_X}\\sqrt{s^2_Y}} = \\frac{-14.0734}{\\sqrt{20.2666}\\sqrt{11.2697}} = -0.9279\n\n# Verification\ncor(magnitude, gallagher)\n\n[1] -0.9312157\n\n\n\n\n16.21.4 OLS Regression (\\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta_1}X)\nSlope coefficient: \\hat{\\beta_1} = \\frac{s_{XY}}{s^2_X} = \\frac{-14.0734}{20.2666} = -0.6944\nIntercept: \\hat{\\beta_0} = \\bar{Y} - \\hat{\\beta_1}\\bar{X} Steps:\n\n-0.6944 × 8.6667 = -6.0181\n\\hat{\\beta_0} = 5.7167 - (-6.0181) = 11.7348\n\n\n# Verification\ncoef(model)\n\n(Intercept)   magnitude \n 11.7348684  -0.6944079 \n\n\n\n\n16.21.5 R-squared Calculation\nStep 1: Calculate predicted values (\\hat{Y}):\n\\hat{Y} = 11.7348 - 0.6944X\n\n# Predicted values\ny_hat &lt;- 11.7348 - 0.6944 * magnitude\ndata.frame(\n  Magnitude = magnitude,\n  Gallagher = gallagher,\n  Predicted = y_hat,\n  row.names = 1:6\n)\n\n  Magnitude Gallagher Predicted\n1         3       9.0    9.6516\n2         5       7.8    8.2628\n3         7       9.2    6.8740\n4        10       4.1    4.7908\n5        12       2.5    3.4020\n6        15       1.7    1.3188\n\n\nStep 2: Sum of Squares SST = \\sum(Y_i - \\bar{Y})^2 = 56.3483 SSR = \\sum(\\hat{Y}_i - \\bar{Y})^2 = 48.5271 SSE = \\sum(Y_i - \\hat{Y}_i)^2 = 7.8212\nR-squared: R^2 = \\frac{SSR}{SST} = \\frac{48.5271}{56.3483} = 0.8612\n\n# Verification\nsummary(model)$r.squared\n\n[1] 0.8671626\n\n\n\n\n16.21.6 Visualization - True vs. Estimated Parameters\n\nTrue DGP: Y = 12 - 0.8X + ε\nEstimated Model: Y = 11.7348 - 0.6944X\n\n\nlibrary(ggplot2)\n\n# Create data frame with original data\ndf &lt;- data.frame(\n  magnitude = magnitude,\n  gallagher = gallagher\n)\n\n# Create sequence for smooth lines\nx_seq &lt;- seq(min(magnitude), max(magnitude), length.out = 100)\n\n# Calculate predicted values for both lines\ntrue_dgp &lt;- 12 - 0.8 * x_seq\nestimated &lt;- 11.7348 - 0.6944 * x_seq\n\n# Combine into a data frame for plotting\nlines_df &lt;- data.frame(\n  magnitude = rep(x_seq, 2),\n  value = c(true_dgp, estimated),\n  Model = rep(c(\"True DGP\", \"Estimated\"), each = length(x_seq))\n)\n\n# Create plot\nggplot() +\n  geom_line(data = lines_df, \n            aes(x = magnitude, y = value, color = Model, linetype = Model),\n            size = 1) +\n  geom_point(data = df, \n             aes(x = magnitude, y = gallagher),\n             color = \"black\", \n             size = 3) +\n  scale_color_manual(values = c(\"red\", \"blue\")) +\n  scale_linetype_manual(values = c(\"dashed\", \"solid\")) +\n  labs(\n    title = \"True DGP vs. Estimated Regression Line\",\n    subtitle = \"Black points show observed data with random noise\",\n    x = \"District Magnitude\",\n    y = \"Gallagher Index\",\n    caption = \"True DGP: Y = 12 - 0.8X + ε\\nEstimated: Y = 11.73 - 0.69X\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"top\",\n    plot.caption = element_text(hjust = 0)\n  )\n\n\n\n\n\n\n\n\n\n\n16.21.7 Observations about Model Fit\n\nSlope Comparison\n\nTrue slope: -0.8\nEstimated slope: -0.69\nThe estimated slope is reasonably close to the true parameter\n\nIntercept Comparison\n\nTrue intercept: 12\nEstimated intercept: 11.73\nThe estimated intercept very closely approximates the true value\n\nVisual Patterns\n\nThe lines are nearly parallel, showing good slope recovery\nPoints scatter around both lines due to the random error term (ε)\nThe small sample size (n=6) leads to some imprecision in estimation\nThe estimated line (blue) provides a good approximation of the true DGP (red dashed)\n\nImpact of Random Error\n\nThe scatter of points around the true DGP line reflects the N(0,1) error term\nThis noise leads to the slight differences in estimated parameters\nWith a larger sample, we would expect even closer convergence to true parameters\n\n\n\n\n16.21.8 Interpretation\n\nStrong negative correlation (r = -0.93) between district magnitude and electoral disproportionality\nFor each unit increase in district magnitude, the Gallagher index decreases by approximately 0.69 points\nThe model explains 86.12% of the variance in disproportionality\nThe relationship appears strongly linear with moderate scatter\nThe intercept (11.73) represents the expected disproportionality in a hypothetical single-member district system\n\n\n\n16.21.9 Study Context\n\nData represents simulated observations from a DGP with moderate noise\nSample shows how increasing district magnitude tends to reduce disproportionality\nRandom component reflects other institutional and political factors affecting disproportionality\n\n\n\n16.21.10 Limitations\n\nSmall sample size (n=6)\nSimulated rather than real-world data\nAssumes linear relationship\nDoes not account for other institutional features",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Wprowadzenie do Analizy Korelacji i Regresji</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\n\n\n\nBlair, G., Coppock, A., & Humphreys, M. (2023). Research design in the social sciences: declaration, diagnosis, and redesign. Princeton University Press. https://book.declaredesign.org/\nBryman, A., 2016. Social research methods. Oxford University Press.\nBueno de Mesquita, Ethan and Anthony Fowler. 2021. Thinking Clearly with Data: A Guide to Quantitative Reasoning and Analysis. Princeton University Press.\nCausality for Machine Learning. https://ff13.fastforwardlabs.com/\nCetinkaya-Rundel, M., Diez, D.M. and Barr, C.D., 2019 (4th ed.). OpenIntro Statistics: an Open-source Textbook: https://www.openintro.org/book/os/\nClaude [Large language model], 2024. https://www.anthropic.com\nConcepts and Computation: An Introduction to Political Methodology. https://pos3713.github.io/notes/\nHannay, K. (2019). Introduction to statistics and data science. http://khannay.com/StatsBook/\nIsmay, C. and Kim, A.Y., 2019. Statistical Inference via Data Science: A ModernDive into R and the Tidyverse. https://moderndive.com/index.html\nNavarro, D.J. and Foxcroft, D.R. (2019). Learning statistics with Jamovi: a tutorial for psychology students and other beginners. (Version 0.70). DOI: 10.24384/hgc3-7p15\nRemler, D.K. and Van Ryzin, G.G., 2014. Research methods in practice: Strategies for description and causation. Sage Publications.\nSanchez, G., Marzban, E. (2020) All Models Are Wrong: Concepts of Statistical Learning. https://allmodelsarewrong.github.io\nSchneider, W. J. (2023). Psycheval: A psychological evaluation toolkit. https://github.com/wjschne/psycheval\nTimbers, T., Campbell, T., & Lee, M. (2022). Data science: A first introduction. Chapman and Hall/CRC. https://datasciencebook.ca/",
    "crumbs": [
      "References"
    ]
  }
]